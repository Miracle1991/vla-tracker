

<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <title>VLA 每周追踪 · 每周自动更新</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
      body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif; margin: 0; padding: 0; background: #f5f5f7; color: #111827;}
      header { background: #111827; color: white; padding: 1rem 1.5rem; position: relative; }
      header h1 { margin: 0; font-size: 1.5rem; display: flex; align-items: center; gap: 0.75rem; flex-wrap: wrap; }
      header .auto-update-badge { display: inline-flex; align-items: center; gap: 0.35rem; padding: 0.25rem 0.65rem; background: linear-gradient(135deg, #10b981 0%, #059669 100%); border-radius: 999px; font-size: 0.75rem; font-weight: 500; color: white; box-shadow: 0 2px 4px rgba(16, 185, 129, 0.3); }
      header .auto-update-badge::before { content: '🔄'; font-size: 0.7rem; }
      header p { margin: 0.25rem 0 0; font-size: 0.9rem; color: #9ca3af; }
      .star-button { position: absolute; top: 1rem; right: 1.5rem; display: flex; align-items: center; gap: 0.5rem; padding: 0.5rem 1rem; background: #1f2937; border: 1px solid #374151; border-radius: 0.5rem; color: white; text-decoration: none; font-size: 0.9rem; transition: all 0.2s; cursor: pointer; }
      .star-button:hover { background: #374151; border-color: #4b5563; transform: translateY(-1px); }
      .star-button:active { transform: translateY(0); }
      .star-icon { font-size: 1.1rem; }
      .star-count { font-weight: 500; }
      @media (max-width: 768px) {
        .star-button { position: static; margin-top: 0.75rem; display: inline-flex; }
      }
      .container { display: flex; }
      .sidebar { background: white; padding: 1.5rem 1rem; position: sticky; top: 0; height: fit-content; max-height: calc(100vh - 80px); overflow-y: auto; }
      .sidebar-left { width: 180px; border-right: 1px solid #e5e7eb; }
      .sidebar-right { width: 280px; border-left: 1px solid #e5e7eb; }
      .sidebar h3 { margin: 0 0 1rem 0; font-size: 0.9rem; color: #6b7280; text-transform: uppercase; letter-spacing: 0.05em; }
      .sidebar ul { list-style: none; padding: 0; margin: 0; }
      .sidebar li { margin-bottom: 0.5rem; }
      .sidebar a { display: block; padding: 0.5rem 0.75rem; color: #374151; text-decoration: none; border-radius: 0.375rem; font-size: 0.9rem; transition: background-color 0.2s; white-space: nowrap; }
      .sidebar a:hover { background: #f3f4f6; color: #111827; }
      .sidebar a.active { background: #111827; color: white; }
      .week-link { font-weight: 500; }
      main { flex: 1; padding: 1.5rem; max-width: 1200px; }
      .date-list { margin-bottom: 1.5rem; }
      .date-pill { display: inline-block; margin: 0.25rem 0.4rem 0.25rem 0; padding: 0.35rem 0.75rem; border-radius: 999px; background: #e5e7eb; font-size: 0.85rem; text-decoration: none; color: #111827; }
      .date-pill.active { background: #111827; color: #f9fafb; }
      .card { background: white; border-radius: 0.75rem; padding: 1.25rem 1.5rem; margin-bottom: 1rem; box-shadow: 0 10px 15px -3px rgba(15,23,42,0.08), 0 4px 6px -2px rgba(15,23,42,0.05); scroll-margin-top: 1rem; }
      .card h2 { margin-top: 0; font-size: 1.1rem; margin-bottom: 0.4rem; }
      .card small { color: #6b7280; }
      .item-list { margin-top: 0.75rem; padding-left: 1.1rem; }
      .item-list li { margin-bottom: 0.45rem; }
      .item-list a { color: #2563eb; text-decoration: none; }
      .item-list a:hover { text-decoration: underline; }
      .empty { color: #6b7280; font-size: 0.95rem; }
      footer { text-align: center; padding: 1rem; font-size: 0.75rem; color: #6b7280; }
      @media (max-width: 1024px) {
        .sidebar-left { width: 140px; padding: 1rem 0.75rem; }
        .sidebar-right { width: 220px; padding: 1rem 0.75rem; }
      }
      @media (max-width: 768px) {
        .container { flex-direction: column; }
        .sidebar { width: 100%; position: relative; border-right: none; border-left: none; border-bottom: 1px solid #e5e7eb; max-height: none; }
        .sidebar-left { border-right: none; }
        .sidebar-right { border-left: none; }
        .sidebar ul { display: flex; flex-wrap: wrap; gap: 0.5rem; }
        .sidebar li { margin-bottom: 0; }
        main { padding: 1rem; order: -1; }
      }
    </style>
    <script>
      // 平滑滚动到锚点（来源目录）
      document.querySelectorAll('.sidebar a[href^="#"]').forEach(anchor => {
        anchor.addEventListener('click', function (e) {
          e.preventDefault();
          const targetId = this.getAttribute('href').substring(1);
          const targetElement = document.getElementById(targetId);
          if (targetElement) {
            targetElement.scrollIntoView({ behavior: 'smooth', block: 'start' });
            // 更新活动状态
            document.querySelectorAll('.sidebar a[href^="#"]').forEach(a => a.classList.remove('active'));
            this.classList.add('active');
          }
        });
      });
      
      // 根据滚动位置高亮当前站点
      function updateActiveSite() {
        const cards = document.querySelectorAll('.card[id]');
        const siteLinks = document.querySelectorAll('.sidebar a[href^="#"]');
        let currentSite = '';
        
        cards.forEach(card => {
          const rect = card.getBoundingClientRect();
          if (rect.top <= 150 && rect.bottom >= 150) {
            currentSite = card.id;
          }
        });
        
        siteLinks.forEach(link => {
          link.classList.remove('active');
          if (link.getAttribute('href') === '#' + currentSite) {
            link.classList.add('active');
          }
        });
      }
      
      // 监听滚动事件
      window.addEventListener('scroll', updateActiveSite);
      // 页面加载时也更新一次
      window.addEventListener('load', updateActiveSite);
      
      // 获取 GitHub star 数量
      async function fetchStarCount() {
        try {
          const response = await fetch('/api/github-stars');
          if (response.ok) {
            const data = await response.json();
            const starCountEl = document.getElementById('starCount');
            if (starCountEl) {
              starCountEl.textContent = data.stargazers_count || 0;
            }
          } else {
            const starCountEl = document.getElementById('starCount');
            if (starCountEl) {
              starCountEl.textContent = '?';
            }
          }
        } catch (error) {
          console.error('获取 star 数量失败:', error);
          const starCountEl = document.getElementById('starCount');
          if (starCountEl) {
            starCountEl.textContent = '?';
          }
        }
      }
      
      // 处理点赞按钮点击
      function handleStarClick(event) {
        // 不阻止默认行为，让链接正常跳转
        // 按钮已经设置了 href，会直接跳转到 GitHub 仓库页面
        // 用户可以在 GitHub 页面点击 star 按钮
      }
      
      // 页面加载时获取 star 数量
      window.addEventListener('load', fetchStarCount);
    </script>
  </head>
  <body>
    <header>
      <h1>
        VLA 每周追踪
        <span class="auto-update-badge">每周自动更新</span>
      </h1>
      <p>自动聚合来自 知乎 / GitHub / HuggingFace / arXiv 的 VLA 相关更新（专注于机器人、自动驾驶领域）</p>
      <a href="https://github.com/Miracle1991/vla-tracker" target="_blank" rel="noopener noreferrer" class="star-button" id="starButton" onclick="handleStarClick(event)">
        <span class="star-icon">⭐</span>
        <span class="star-text">点赞</span>
        <span class="star-count" id="starCount">加载中...</span>
      </a>
    </header>
    <div class="container">
      <aside class="sidebar sidebar-left">
        <h3>来源目录</h3>
        <ul>
          <li><a href="#arxiv">arXiv</a></li>
          <li><a href="#github">GitHub</a></li>
          <li><a href="#huggingface">HuggingFace</a></li>
          <li><a href="#zhihu">知乎</a></li>
        </ul>
        <h3 style="margin-top: 1.5rem;">头部玩家</h3>
        <ul>
          
            
            
              
            
              
                
              
            
              
            
              
            
              
            
            
              <li style="color: #9ca3af; font-size: 0.85rem; padding: 0.5rem 0.75rem;">本周无更新</li>
            
          
        </ul>
      </aside>
      <main>
        
  
    <h2 style="color:#111827;margin-bottom:1.5rem;padding-bottom:0.5rem;border-bottom:2px solid #e5e7eb;">
      2025-12-22 ~ 2025-12-28
    </h2>
    
      
      
      
       
        
        
      
      <article class="card" id="arxiv" >
        <h2>arxiv.org</h2>
        <small>arxiv.org 上共发现 15 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 15）。</small>
        
          <ul class="item-list">
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.20188v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-23</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Teqiang Zou, Hongliang Zeng, Yuxuan Nong, Yifan Li, Kehui Liu, Haotian Yang, Xinyang Ling, Xin Li, Lianyang Ma</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>大多数视觉语言动作（VLA）系统集成了用于语义推理的视觉语言模型（VLM）和生成连续动作信号的动作专家，但两者通常以单一统一频率运行。因此，策略性能受到大型 VLM 的低推理速度的限制。这种强制同步执行严重限制了全身机器人操作的控制稳定性和实时性，因为全身机器人操作涉及更多的关节、更大的运动空间和动态变化的视图。我们引入了真正的异步快-慢VLA框架（DuoCore-FS），将系统组织成用于高频动作生成的快速路径和用于丰富VLM推理的慢速路径。该系统有两个主要特点。首先，潜在表示缓冲区桥接了慢系统和快系统。它存储与场景指令上下文一致的指令语义和动作推理表示，为快速路径提供高级指导。其次，全身动作标记器提供了全身动作的紧凑、统一的表示。重要的是，VLM 和行动专家仍然接受端到端联合训练，在实现异步执行的同时保留统一的策略学习。DuoCore-FS 支持 3B 参数 VLM，同时实现 30 Hz 全身动作块生成，速度大约是具有类似模型尺寸的先前 VLA 模型的三倍。真实世界的全身操作实验表明，与同步快-慢 VLA 基线相比，任务成功率有所提高，响应能力也显着增强。DuoCore-FS 的实现（包括训练、推理和部署）由 Astribot 作为 Astribot 机器人平台的一部分向商业用户提供。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.20166v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">LoLA: Long Horizon Latent Action Learning for General Robot Manipulation</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-23</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Xiaofan Wang, Xingyu Gao, Jianlong Fu, Zuolei Li, Dean Fortier, Galen Mullins, Andrey Kolobov, Baining Guo</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>执行长期、语言引导的机器人操作任务的能力关键依赖于利用历史信息和生成连贯的动作序列。然而，现有的视觉-语言-动作（VLA）模型经常忽视这些功能。为了解决这一挑战，我们提出了 LoLA（长视野潜在动作学习），这是一种专为机器人操作而设计的框架，它集成了长期多视图观察和机器人本体感觉，以实现多步骤推理和动作生成。我们首先使用视觉语言模型来编码来自历史序列和多视图观察的丰富上下文特征。我们进一步介绍了一个关键模块，即状态感知潜在重新表示，它将视觉输入和语言命令转换为可操作的机器人运动空间。与仅将机器人本体感觉（例如关节角度）与 VL 嵌入连接起来的现有 VLA 方法不同，该模块利用此类机器人状态，通过可学习的“实施例锚定”潜在空间在物理尺度上显式地构建 VL 表示。我们在不同的机器人预训练数据集上训练 LoLA，并对模拟基准（SIMPLER 和 LIBERO）以及 Franka 和 Bi-Manual Aloha 机器人的两项现实任务进行了广泛的评估。结果表明，LoLA 显着优于先前最先进的方法（例如 pi0），特别是在长视野操作任务中。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.22414v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Emergence of Human to Robot Transfer in Vision-Language-Action Models</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-27</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Simar Kareer, Karl Pertsch, James Darpinian, Judy Hoffman, Danfei Xu, Sergey Levine, Chelsea Finn, Suraj Nair</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作（VLA）模型可以实现广泛的开放世界泛化，但需要大量且多样化的数据集。值得考虑的是，其中一些数据是否可以来自人类视频，这些视频涵盖了不同的现实世界情况并且很容易获得。然而，仅用人类视频来训练 VLA 是很困难的，并且建立人类和机器人之间的映射需要手动工程，并且提出了重大的研究挑战。从大型语言模型的进步中汲取灵感，从不同的监督中学习的能力随着规模的增加而出现，我们询问类似的现象是否也适用于包含人类视频数据的 VLA。我们引入了一个简单的协同训练方法，并发现一旦 VLA 在足够的场景、任务和实施例上进行了预训练，人机交互就会出现。我们的分析表明，这种新兴能力的出现是因为不同的预训练为人类和机器人数据产生了与实施例无关的表示。我们通过一系列探索人机技能转移的实验验证了这些发现，并发现通过足够多样化的机器人预训练，我们的方法可以将仅在人类数据中看到的泛化设置的性能提高近一倍。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.19562v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-22</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Martin Sedlacek, Pavlo Yefanov, Georgy Ponimatkin, Jai Bardhan, Simon Pilc, Mederic Fourmy, Evangelos Kazakos, Cees G. M. Snoek, Josef Sivic, Vladimir Petrik</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作（VLA）模型使机器人能够理解和执行自然​​语言指令描述的任务。然而，一个关键的挑战在于它们的泛化能力超出了他们所接受训练的特定环境和条件，而目前在现实世界中评估这一点既困难又昂贵。为了解决这一差距，我们推出了 REALM，这是一种新的模拟环境和基准，旨在评估 VLA 模型的泛化能力，特别强调通过高保真视觉效果和对齐的机器人控制在模拟和现实世界性能之间建立强大的相关性。我们的环境提供了一套 15 个扰动因素、7 种操作技能和超过 3,500 个对象。最后，我们建立了两个任务集来构成我们的基准并评估 π_{0}、π_{0}-FAST 和 GR00T N1.5 VLA 模型，这表明泛化性和鲁棒性仍然是一个开放的挑战。更广泛地说，我们还表明，模拟为我们提供了现实世界的宝贵代理，使我们能够系统地探索和量化 VLA 的弱点和故障模式。项目页面：https://martin-sedlacek.com/realm
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.22615v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Dream-VL &amp; Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-27</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Jiacheng Ye, Shansan Gong, Jiahui Gao, Junming Fan, Shuang Wu, Wei Bi, Haoli Bai, Lifeng Shang, Lingpeng Kong</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>虽然自回归大型视觉语言模型（VLM）取得了显着的成功，但它们的顺序生成通常限制了它们在复杂视觉规划和动态机器人控制中的功效。在这项工作中，我们研究了在基于扩散的大语言模型（dLLM）上构建视觉语言模型以克服这些限制的潜力。我们推出 Dream-VL，这是一种基于开放扩散的 VLM (dVLM)，在之前的 dVLM 中实现了最先进的性能。Dream-VL 可与基于各种基准的开放数据训练的顶级基于 AR 的 VLM 相媲美，但在应用于视觉规划任务时表现出卓越的潜力。在 Dream-VL 的基础上，我们引入了 Dream-VLA，这是一种基于 dLLM 的视觉-语言-动作模型（dVLA），通过对开放机器人数据集的持续预训练而开发。我们证明，这种扩散主干的原生双向性质可以作为 VLA 任务的卓越基础，本质上适合动作分块和并行生成，从而在下游微调中显着加快收敛速度​​。Dream-VLA 在 LIBERO 上实现了 97.2% 的平均成功率，在 SimplerEnv-Bridge 上实现了 71.4% 的总体平均成功率，在 SimplerEnv-Fractal 上实现了 60.5% 的总体平均成功率，超越了 $π_0$ 和 GR00T-N1 等领先模型。我们还验证了 dVLM 在不同训练目标的下游任务上超越了 AR 基线。我们发布了 Dream-VL 和 Dream-VLA 以促进社区的进一步研究。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.22539v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-27</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Borong Zhang, Jiahao Li, Jiachen Shen, Yishuai Cai, Yuhao Zhang, Yuanpei Chen, Juntao Dai, Jiaming Ji, Yaodong Yang</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>虽然视觉-语言-动作模型（VLA）正在迅速向通用机器人策略迈进，但仍然很难定量地了解它们的局限性和故障模式。为了解决这个问题，我们引入了一个名为 VLA-Arena 的综合基准测试。我们提出了一种新颖的结构化任务设计框架来量化三个正交轴的难度：（1）任务结构，（2）语言命令和（3）视觉观察。这使我们能够系统地设计具有细粒度难度级别的任务，从而能够精确测量模型能力前沿。对于任务结构，VLA-Arena 的 170 项任务分为四个维度：安全、干扰、外推和长远。每项任务都设计了三个难度级别（L0-L2），并专门在 L0 上进行微调以评估一般能力。与此正交的是，语言 (W0-W4) 和视觉 (V0-V4) 扰动可以应用于任何任务，以实现鲁棒性的解耦分析。我们对最先进的 VLA 的广泛评估揭示了一些关键的局限性，包括强烈的记忆倾向而不是泛化、不对称的鲁棒性、缺乏对安全约束的考虑，以及无法为长期任务组合学习的技能。为了促进应对这些挑战的研究并确保可重复性，我们提供了完整的 VLA-Arena 框架，包括从任务定义到自动评估的端到端工具链以及用于微调的 VLA-Arena-S/M/L 数据集。我们的基准、数据、模型和排行榜可在 https://vla-arena.github.io 上获取。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.23162v3" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-29</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yufan He, Pengfei Guo, Mengya Xu, Zhaoshuo Li, Andriy Myronenko, Dillan Imans, Bingjie Liu, Dongren Yang, Mingxue Gu, Yongnan Ji, Yueming Jin, Ren Zhao, Baiyong Shen, Daguang Xu</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>数据稀缺仍然是实现完全自主手术机器人的根本障碍。虽然大规模视觉语言动作 (VLA) 模型通过利用来自不同领域的配对视频动作数据，在家庭和工业操作中显示出令人印象深刻的通用性，但手术机器人却面临着缺乏包括视觉观察和精确机器人运动学的数据集的问题。相比之下，存在大量的手术视频语料库，但它们缺乏相应的动作标签，阻碍了模仿学习或 VLA 训练的直接应用。在这项工作中，我们的目标是通过学习 SurgWorld 的政策模型来缓解这个问题，SurgWorld 是一个专为外科物理人工智能设计的世界模型。我们专门为手术机器人策划了手术动作文本对齐（SATA）数据集，其中包含详细的动作描述。然后我们基于最先进的物理AI世界模型和SATA构建了SurgeWorld。它能够生成多样化、通用且逼真的手术视频。我们也是第一个使用逆动力学模型从合成手术视频推断伪运动学，生成合成配对视频动作数据的人。我们证明，使用这些增强数据训练的外科 VLA 策略显着优于仅在真实手术机器人平台上进行真实演示训练的模型。我们的方法通过利用大量未标记的手术视频和生成世界建模，为自主手术技能获取提供了一条可扩展的路径，从而为通用和数据高效的手术机器人策略打开了大门。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.21970v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-26</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Shengliang Deng, Mi Yan, Yixin Zheng, Jiayi Su, Wenhao Zhang, Xiaoguang Zhao, Heming Cui, Zhizheng Zhang, He Wang</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>立体相机密切模仿人类双眼视觉，提供丰富的空间线索，这对于精确的机器人操作至关重要。尽管立体视觉具有优势，但其在视觉语言动作模型（VLA）中的采用仍未得到充分探索。在这项工作中，我们提出了 StereoVLA，这是一种利用立体视觉丰富的几何线索的 VLA 模型。我们提出了一种新颖的几何语义特征提取模块，该模块利用视觉基础模型来提取和融合两个关键特征：1）从细微的立体视图差异中提取空间感知的几何特征；2）单目视图中语义丰富的特征用于指令跟踪。此外，我们提出了辅助交互区域深度估计任务，以进一步增强空间感知并加速模型收敛。大量的实验表明，我们的方法在立体设置下的各种任务中大大优于基线，并且对相机姿势变化表现出强大的鲁棒性。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.22519v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-27</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Khoa Vo, Taisei Hanyu, Yuki Ikebe, Trong Thang Pham, Nhat Chung, Minh Nhat Vu, Duy Nguyen Ho Minh, Anh Nguyen, Anthony Gunderman, Chase Rainwater, Ngan Le</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>最近的视觉语言动作（VLA）模型通过对大型视觉语言模型（VLM）进行动作预测的后期训练，在通用机器人操作方面取得了令人瞩目的进展。然而，大多数 VLA 将感知和控制纠缠在一个纯粹针对行动而优化的整体管道中，这可能会削弱以语言为条件的基础。在我们现实世界的桌面测试中，当目标不存在时，策略会过度掌握，会因杂乱而分散注意力，并且会过度适应背景外观。为了解决这些问题，我们提出了 OBEYED-VLA（以对象为中心和基于几何的 VLA），这是一个明确地将感知基础与动作推理分开的框架。OBEYED-VLA 不是直接在原始 RGB 上操作，而是通过感知模块增强了 VLA，该模块将多视图输入转化为任务条件、以对象为中心和几何感知的观察结果。该模块包括一个基于 VLM 的以对象为中心的基础阶段，可在摄像机视图中选择与任务相关的对象区域，以及一个互补的几何基础阶段，该阶段强调这些对象的 3D 结构而不是其外观。然后将所得的接地视图输入到预先训练的 VLA 策略中，我们专门针对在没有环境杂乱或非目标对象的情况下收集的单个对象演示进行微调。在现实世界的 UR10e 桌面设置中，OBEYED-VLA 在四个具有挑战性的机制和多个​​难度级别上显着提高了强 VLA 基线的鲁棒性：干扰对象、缺失目标拒绝、背景外观变化以及对看不见的对象的杂乱操作。消融研究证实，语义基础和几何感知基础对于这些成果都至关重要。总体而言，结果表明，使感知成为一个明确的、以对象为中心的组件是加强和推广基于 VLA 的机器人操作的有效方法。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.20014v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-23</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Sangoh Lee, Sangwoo Mo, Wook-Shin Han</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>虽然视觉-语言-动作（VLA）模型可以很好地概括通用指令，但它们很难处理诸如“拿来我的杯子”之类的个性化命令，其中机器人必须对视觉上相似的物体中的一个特定实例采取行动。我们研究了这种操纵个人物体的设置，其中 VLA 必须仅使用一些参考图像来识别和控制在训练期间看不见的特定于用户的物体。为了应对这一挑战，我们提出了视觉注意提示（VAP），这是一种简单而有效的免训练感知适配器，为冻结的 VLA 提供自上而下的选择性注意。VAP 将参考图像视为非参数视觉记忆，通过开放词汇检测和基于嵌入的匹配来为场景中的个人对象奠定基础，然后通过突出显示对象并重写指令将这种基础作为视觉提示注入。我们构建了两个模拟基准：Personalized-SIMPLER 和 Personalized-VLABench，以及一个真实世界的桌面基准来评估跨多个机器人和任务的个性化操作。实验表明，VAP 在成功率和正确对象操作方面始终优于通用策略和令牌学习基线，有助于弥合语义理解和实例级控制之间的差距。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.22939v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-28</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Qihang Peng, Xuesong Chen, Chenye Yang, Shaoshuai Shi, Hongsheng Li</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>自动驾驶需要从复杂的多模式输入生成安全可靠的轨迹。传统的模块化管道将感知、预测和规划分开，而最近的端到端（E2E）系统联合学习它们。视觉语言模型（VLM）通过引入跨模态先验和常识推理进一步丰富了这种范式，但当前基于 VLM 的规划器面临三个关键挑战：（i）离散文本推理和连续控制之间的不匹配，（ii）自回归思想链解码的高延迟，以及（iii）限制实时部署的低效或非因果规划器。我们提出了 ColaVLA，一个统一的视觉-语言-动作框架，它将推理从文本转移到统一的潜在空间，并将其与分层的并行轨迹解码器耦合。认知潜在推理器通过自我自适应选择和仅两次 VLM 前向传递，将场景理解压缩为紧凑的、面向决策的元动作嵌入。然后，分层并行规划器在一次前向传递中生成多尺度、因果一致的轨迹。这些组件共同保留了 VLM 的泛化性和可解释性，同时实现高效、准确和安全的轨迹生成。nuScenes 基准测试表明，ColaVLA 在开环和闭环设置中均实现了最先进的性能，并且具有良好的效率和鲁棒性。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.23864v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-29</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Guo Ye, Zexi Zhang, Xu Zhao, Shang Wu, Haoran Lu, Shihan Lu, Han Liu</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作（VLA）模型通过将网络规模的知识映射到机器人控制而显示出显着的泛化性，但它们仍然对物理接触视而不见。因此，他们很难完成需要对力、纹理和滑动进行推理的接触丰富的操作任务。虽然一些方法结合了低维触觉信号，但它们无法捕获此类交互所必需的高分辨率动态。为了解决这一限制，我们引入了 DreamTacVLA，这是一个框架，通过学习感知未来，将 VLA 模型建立在接触物理基础上。我们的模型采用分层感知方案，其中高分辨率触觉图像作为微观视觉输入，加上手腕相机局部视觉和第三人称宏观视觉。为了协调这些多尺度的感觉流，我们首先训练一个具有分层空间对齐（HSA）损失的统一策略，该策略将触觉标记与其在手腕和第三人称视图中的空间对应物对齐。为了进一步加深模型对细粒度接触动力学的理解，我们使用预测未来触觉信号的触觉世界模型对系统进行微调。为了缓解触觉数据的稀缺性和触觉传感器的易磨损特性，我们构建了一个来自高保真数字孪生和现实世界实验的混合大规模数据集。通过预测即将到来的触觉状态，DreamTacVLA 获得了丰富的接触物理模型，并根据真实观察和想象结果来调整其行为。在接触丰富的操作任务中，它的表现优于最先进的 VLA 基线，成功率高达 95%，这凸显了理解物理接触对于强大的触摸感知机器人代理的重要性。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.20276v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-23</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作（VLA）模型已成为机器人感知和控制的统一范例，可实现紧急泛化和长期任务执行。然而，它们在动态的现实环境中的部署受到高推理延迟的严重阻碍。虽然流畅的机器人交互需要 20 至 30 Hz 的控制频率，但由于自回归解码的内存限制性质，当前的 VLA 模型在边缘设备上通常仅以 3-5 Hz 的频率运行。现有的优化通常需要大量的重新训练或妥协模型的准确性。为了弥补这一差距，我们引入了 ActionFlow，这是一个专为资源受限的边缘平台量身定制的系统级推理框架。ActionFlow 的核心是跨请求管道策略，这是一种新颖的调度程序，它将 VLA 推理重新定义为微请求的宏管道。该策略在连续的时间步长内智能地将内存限制的解码阶段与计算限制的预填充阶段进行批处理，以最大限度地提高硬件利用率。此外，为了支持这种调度，我们提出了一个交叉请求状态打包前向运算符和一个统一的 KV 环形缓冲区，它将碎片内存操作融合到高效的密集计算中。实验结果表明，ActionFlow 在 OpenVLA-7B 模型上无需重新训练即可将 FPS 提高 2.55 倍，从而实现边缘硬件上的实时动态操作。我们的工作可在 https://anonymous.4open.science/r/ActionFlow-1D47 获取。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.19024v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-22</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Xu Liu, Yu Liu, Hanshuo Qiu, Yang Qirong, Zhouhui Lian</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉语言导航（VLN）使智能体能够通过遵循基于视觉观察的自然语言指令在复杂的环境中进行导航。尽管大多数现有工作都集中在地面机器人或室外无人机 (UAV) 上，但基于室内无人机的 VLN 仍然未被充分开发，尽管它与有限空间中的检查、交付和搜索救援等实际应用相关。为了弥补这一差距，我们引入了 \textbf{IndoorUAV}，这是一种专为室内无人机 VLN 量身定制的新颖基准和方法。我们首先从 Habitat 模拟器中策划 1,000 多个多样化且结构丰富的 3D 室内场景。在这些环境中，我们模拟真实的无人机飞行动力学，以手动收集不同的 3D 导航轨迹，并通过数据增强技术进一步丰富。此外，我们设计了一个自动注释管道来为每个轨迹生成不同粒度的自然语言指令。该过程产生超过 16,000 个高质量轨迹，包括 \textbf{IndoorUAV-VLN} 子集，该子集专注于长视野 VLN。为了支持短视野规划，我们通过选择语义上显着的关键帧并重新生成简洁的指令，将长轨迹分割成子轨迹，形成 \textbf{IndoorUAV-VLA} 子集。最后，我们介绍 \textbf{IndoorUAV-Agent}，这是一种专为我们的基准测试而设计的新颖导航模型，利用任务分解和多模态推理。我们希望 IndoorUAV 成为推进室内空中导航领域视觉语言人工智能研究的宝贵资源。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2512.18933v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Point What You Mean: Visually Grounded Instruction Policy</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-12-22</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Hang Yu, Juntu Zhao, Yufeng Liu, Kaiyu Li, Cheng Ma, Di Zhang, Yingdong Hu, Guang Chen, Junyuan Xie, Junliang Guo, Junqiao Zhao, Yang Gao</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作（VLA）模型将视觉和语言与具体控制结合起来，但是当仅依赖文本提示时，它们的对象引用能力仍然有限，特别是在杂乱或分布外（OOD）场景中。在本研究中，我们引入了 Point-VLA，这是一种即插即用策略，可通过明确的视觉提示（例如边界框）增强语言指令，以解决引用歧义并实现精确的对象级接地。为了有效地扩展基于视觉的数据集，我们进一步开发了一种需要最少人力的自动数据注释管道。我们在不同的现实世界引用任务上评估 Point-VLA，并观察到比纯文本指令 VLA 始终具有更强的性能，特别是在杂乱或看不见的对象场景中，具有强大的泛化能力。这些结果表明，Point-VLA 通过像素级视觉基础有效解决了对象指代模糊性，实现了更通用的体现控制。
                      </div>
                    
                  </div>
                </div>
              </li>
            
          </ul>
        
      </article>
    
      
      
      
      
        
        
      
      <article class="card" id="organizations" style="border-left: 4px solid #10b981;">
        <h2>🏢 头部玩家</h2>
        <small>头部玩家本周无更新。</small>
        
          <p class="empty" style="padding: 1.5rem; text-align: center; color: #9ca3af; font-size: 0.95rem;">本周无更新</p>
        
      </article>
    
      
      
      
       
        
        
      
      <article class="card" id="github" >
        <h2>github.com</h2>
        <small>github.com 上共发现 15 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 15）。</small>
        
          <ul class="item-list">
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/TianxingChen/Embodied-AI-Guide" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">TianxingChen/Embodied-AI-Guide</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        [Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Trustworthy-AI-Group/Adversarial_Examples_Papers</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        A list of recent papers about adversarial learning
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/RLinf/RLinf" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">RLinf/RLinf</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/Dexmal/dexbotic" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Dexmal/dexbotic</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        Dexbotic: Open-Source Vision-Language-Action Toolbox
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                         Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/IliaLarchenko/behavior-1k-solution" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">IliaLarchenko/behavior-1k-solution</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        1st place solution of 2025 BEHAVIOR Challenge
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">HCPLab-SYSU/Embodied_AI_Paper_List</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        [Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/ZutJoe/KoalaHackerNews" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">ZutJoe/KoalaHackerNews</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        Koala hacker news 周报内容 每周二0点左右更新
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/OpenDriveLab/UniVLA" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">OpenDriveLab/UniVLA</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        [RSS 2025] Learning to Act Anywhere with Task-centric Latent Actions
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/PetroIvaniuk/llms-tools" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">PetroIvaniuk/llms-tools</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        A list of LLMs Tools &amp; Projects
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/gabrielchua/daily-ai-papers" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">gabrielchua/daily-ai-papers</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        All credits go to HuggingFace&#39;s Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/SalvatoreRa/ML-news-of-the-week" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">SalvatoreRa/ML-news-of-the-week</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        A collection of the the best ML and AI news every week (research, news, resources)
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/52CV/CVPR-2025-Papers" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">52CV/CVPR-2025-Papers</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        CVPR-2025-Papers
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/Hub-Tian/UAVs_Meet_LLMs" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Hub-Tian/UAVs_Meet_LLMs</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        UAVs_Meet_LLMs
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/52CV/ECCV-2024-Papers" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">52CV/ECCV-2024-Papers</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        ECCV-2024-Papers
                      </div>
                    
                  </div>
                </div>
              </li>
            
          </ul>
        
      </article>
    
      
      
      
       
        
        
      
      <article class="card" id="huggingface" >
        <h2>huggingface.co</h2>
        <small>huggingface.co 本周无更新。</small>
        
          <p class="empty" style="padding: 1.5rem; text-align: center; color: #9ca3af; font-size: 0.95rem;">本周无更新</p>
        
      </article>
    
      
      
      
       
        
        
      
      <article class="card" id="zhihu" >
        <h2>zhihu.com</h2>
        <small>zhihu.com 本周无更新。</small>
        
          <p class="empty" style="padding: 1.5rem; text-align: center; color: #9ca3af; font-size: 0.95rem;">本周无更新</p>
        
      </article>
    
  

      </main>
      <aside class="sidebar sidebar-right">
        <h3>时间线</h3>
        <ul id="week-timeline">
          
            <li>
              <a href="2026-01-05.html" class="week-link " data-week="2026-01-05">
                2026-01-05 ~ 2026-01-11
              </a>
            </li>
          
            <li>
              <a href="2025-12-29.html" class="week-link " data-week="2025-12-29">
                2025-12-29 ~ 2026-01-04
              </a>
            </li>
          
            <li>
              <a href="2025-12-22.html" class="week-link active" data-week="2025-12-22">
                2025-12-22 ~ 2025-12-28
              </a>
            </li>
          
            <li>
              <a href="2025-12-15.html" class="week-link " data-week="2025-12-15">
                2025-12-15 ~ 2025-12-21
              </a>
            </li>
          
            <li>
              <a href="2025-12-08.html" class="week-link " data-week="2025-12-08">
                2025-12-08 ~ 2025-12-14
              </a>
            </li>
          
            <li>
              <a href="2025-12-01.html" class="week-link " data-week="2025-12-01">
                2025-12-01 ~ 2025-12-07
              </a>
            </li>
          
            <li>
              <a href="2025-11-24.html" class="week-link " data-week="2025-11-24">
                2025-11-24 ~ 2025-11-30
              </a>
            </li>
          
            <li>
              <a href="2025-11-17.html" class="week-link " data-week="2025-11-17">
                2025-11-17 ~ 2025-11-23
              </a>
            </li>
          
            <li>
              <a href="2025-11-10.html" class="week-link " data-week="2025-11-10">
                2025-11-10 ~ 2025-11-16
              </a>
            </li>
          
            <li>
              <a href="2025-11-03.html" class="week-link " data-week="2025-11-03">
                2025-11-03 ~ 2025-11-09
              </a>
            </li>
          
            <li>
              <a href="2025-10-27.html" class="week-link " data-week="2025-10-27">
                2025-10-27 ~ 2025-11-02
              </a>
            </li>
          
            <li>
              <a href="2025-10-20.html" class="week-link " data-week="2025-10-20">
                2025-10-20 ~ 2025-10-26
              </a>
            </li>
          
            <li>
              <a href="2025-10-13.html" class="week-link " data-week="2025-10-13">
                2025-10-13 ~ 2025-10-19
              </a>
            </li>
          
            <li>
              <a href="2025-10-06.html" class="week-link " data-week="2025-10-06">
                2025-10-06 ~ 2025-10-12
              </a>
            </li>
          
            <li>
              <a href="2025-09-29.html" class="week-link " data-week="2025-09-29">
                2025-09-29 ~ 2025-10-05
              </a>
            </li>
          
            <li>
              <a href="2025-09-22.html" class="week-link " data-week="2025-09-22">
                2025-09-22 ~ 2025-09-28
              </a>
            </li>
          
            <li>
              <a href="2025-09-15.html" class="week-link " data-week="2025-09-15">
                2025-09-15 ~ 2025-09-21
              </a>
            </li>
          
            <li>
              <a href="2025-09-08.html" class="week-link " data-week="2025-09-08">
                2025-09-08 ~ 2025-09-14
              </a>
            </li>
          
            <li>
              <a href="2025-09-01.html" class="week-link " data-week="2025-09-01">
                2025-09-01 ~ 2025-09-07
              </a>
            </li>
          
            <li>
              <a href="2025-08-25.html" class="week-link " data-week="2025-08-25">
                2025-08-25 ~ 2025-08-31
              </a>
            </li>
          
            <li>
              <a href="2025-08-18.html" class="week-link " data-week="2025-08-18">
                2025-08-18 ~ 2025-08-24
              </a>
            </li>
          
            <li>
              <a href="2025-08-11.html" class="week-link " data-week="2025-08-11">
                2025-08-11 ~ 2025-08-17
              </a>
            </li>
          
            <li>
              <a href="2025-08-04.html" class="week-link " data-week="2025-08-04">
                2025-08-04 ~ 2025-08-10
              </a>
            </li>
          
            <li>
              <a href="2025-07-28.html" class="week-link " data-week="2025-07-28">
                2025-07-28 ~ 2025-08-03
              </a>
            </li>
          
        </ul>
      </aside>
    </div>
    <footer>
      数据来源于 Google 搜索结果，仅供学习与研究使用。
    </footer>
  </body>
</html>