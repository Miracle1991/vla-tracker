{
  "generated_at": "2026-01-07T13:22:26.927802",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 11 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 11）。",
      "items": [
        {
          "title": "GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions",
          "url": "http://arxiv.org/abs/2508.07650v2",
          "snippet": "Vision-language-action models have emerged as a crucial paradigm in robotic manipulation. However, existing VLA models exhibit notable limitations in handling ambiguous language instructions and unknown environmental states. Furthermore, their perception is largely constrained to static two-dimensional observations, lacking the capability to model three-dimensional interactions between the robot and its environment. To address these challenges, this paper proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's ability to interpret ambiguous instructions and improve task planning, we design a structured Chain-of-Thought reasoning module that integrates high-level task understanding and planning, failed task feedback, and low-level imaginative reasoning about future object positions and robot actions. Additionally, we construct a real-time updatable 3D Pose-Object graph, which captures the spatial configuration of robot joints and the topological relationships between objects in 3D space, enabling the model to better understand and manipulate their interactions. We further integrates a dropout hybrid reasoning strategy to achieve efficient control outputs. Experimental results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA significantly outperforms existing methods in terms of task success rate and response speed, exhibiting strong generalization and robustness in open environments and under uncertain instructions.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-08-11T06:01:00Z",
          "authors": [
            "Helong Huang",
            "Min Cen",
            "Kai Tan",
            "Xingyue Quan",
            "Guowei Huang",
            "Hong Zhang"
          ],
          "arxiv_id": "2508.07650",
          "abstract": "Vision-language-action models have emerged as a crucial paradigm in robotic manipulation. However, existing VLA models exhibit notable limitations in handling ambiguous language instructions and unknown environmental states. Furthermore, their perception is largely constrained to static two-dimensional observations, lacking the capability to model three-dimensional interactions between the robot and its environment. To address these challenges, this paper proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's ability to interpret ambiguous instructions and improve task planning, we design a structured Chain-of-Thought reasoning module that integrates high-level task understanding and planning, failed task feedback, and low-level imaginative reasoning about future object positions and robot actions. Additionally, we construct a real-time updatable 3D Pose-Object graph, which captures the spatial configuration of robot joints and the topological relationships between objects in 3D space, enabling the model to better understand and manipulate their interactions. We further integrates a dropout hybrid reasoning strategy to achieve efficient control outputs. Experimental results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA significantly outperforms existing methods in terms of task success rate and response speed, exhibiting strong generalization and robustness in open environments and under uncertain instructions.",
          "abstract_zh": "视觉-语言-动作模型已成为机器人操作的重要范例。然而，现有的 VLA 模型在处理不明确的语言指令和未知的环境状态方面表现出明显的局限性。此外，它们的感知在很大程度上受限于静态二维观察，缺乏对机器人与其环境之间的三维交互进行建模的能力。为了应对这些挑战，本文提出了 GraphCoT-VLA，一种高效的端到端模型。为了增强模型解释模糊指令和改进任务规划的能力，我们设计了一个结构化的思想链推理模块，该模块集成了高级任务理解和规划、失败的任务反馈以及关于未来物体位置和机器人动作的低级想象推理。此外，我们构建了一个实时可更新的 3D 姿态-对象图，它捕获机器人关节的空间配置以及 3D 空间中对象之间的拓扑关系，使模型能够更好地理解和操纵它们的交互。我们进一步集成了 dropout 混合推理策略以实现高效的控制输出。多个现实世界机器人任务的实验结果表明，GraphCoT-VLA 在任务成功率和响应速度方面显着优于现有方法，在开放环境和不确定指令下表现出很强的泛化性和鲁棒性。"
        },
        {
          "title": "FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2508.02190v1",
          "snippet": "Vision-language-action (VLA) models have significantly advanced robotic manipulation by enabling robots to interpret language instructions for task execution. However, training these models often relies on large-scale user-specific data, raising concerns about privacy and security, which in turn limits their broader adoption. To address this, we propose FedVLA, the first federated VLA learning framework, enabling distributed model training that preserves data privacy without compromising performance. Our framework integrates task-aware representation learning, adaptive expert selection, and expert-driven federated aggregation, enabling efficient and privacy-preserving training of VLA models. Specifically, we introduce an Instruction Oriented Scene-Parsing mechanism, which decomposes and enhances object-level features based on task instructions, improving contextual understanding. To effectively learn diverse task patterns, we design a Dual Gating Mixture-of-Experts (DGMoE) mechanism, where not only input tokens but also self-aware experts adaptively decide their activation. Finally, we propose an Expert-Driven Aggregation strategy at the federated server, where model aggregation is guided by activated experts, ensuring effective cross-client knowledge transfer.Extensive simulations and real-world robotic experiments demonstrate the effectiveness of our proposals. Notably, DGMoE significantly improves computational efficiency compared to its vanilla counterpart, while FedVLA achieves task success rates comparable to centralized training, effectively preserving data privacy.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-08-04T08:39:43Z",
          "authors": [
            "Cui Miao",
            "Tao Chang",
            "Meihan Wu",
            "Hongbin Xu",
            "Chun Li",
            "Ming Li",
            "Xiaodong Wang"
          ],
          "arxiv_id": "2508.02190",
          "abstract": "Vision-language-action (VLA) models have significantly advanced robotic manipulation by enabling robots to interpret language instructions for task execution. However, training these models often relies on large-scale user-specific data, raising concerns about privacy and security, which in turn limits their broader adoption. To address this, we propose FedVLA, the first federated VLA learning framework, enabling distributed model training that preserves data privacy without compromising performance. Our framework integrates task-aware representation learning, adaptive expert selection, and expert-driven federated aggregation, enabling efficient and privacy-preserving training of VLA models. Specifically, we introduce an Instruction Oriented Scene-Parsing mechanism, which decomposes and enhances object-level features based on task instructions, improving contextual understanding. To effectively learn diverse task patterns, we design a Dual Gating Mixture-of-Experts (DGMoE) mechanism, where not only input tokens but also self-aware experts adaptively decide their activation. Finally, we propose an Expert-Driven Aggregation strategy at the federated server, where model aggregation is guided by activated experts, ensuring effective cross-client knowledge transfer.Extensive simulations and real-world robotic experiments demonstrate the effectiveness of our proposals. Notably, DGMoE significantly improves computational efficiency compared to its vanilla counterpart, while FedVLA achieves task success rates comparable to centralized training, effectively preserving data privacy.",
          "abstract_zh": "视觉-语言-动作（VLA）模型使机器人能够解释语言指令以执行任务，从而显着提高了机器人操作的性能。然而，训练这些模型通常依赖于大规模的用户特定数据，引发了人们对隐私和安全的担忧，进而限制了它们的更广泛采用。为了解决这个问题，我们提出了 FedVLA，这是第一个联合 VLA 学习框架，支持分布式模型训练，在不影响性能的情况下保护数据隐私。我们的框架集成了任务感知表示学习、自适应专家选择和专家驱动的联合聚合，从而实现了 VLA 模型的高效且保护隐私的训练。具体来说，我们引入了一种面向指令的场景解析机制，该机制根据任务指令分解和增强对象级特征，从而提高上下文理解。为了有效地学习不同的任务模式，我们设计了一种双重门控专家混合（DGMoE）机制，其中不仅输入令牌，而且有自我意识的专家自适应地决定其激活。最后，我们在联合服务器上提出了专家驱动的聚合策略，其中模型聚合由激活的专家指导，确保有效的跨客户端知识传输。广泛的模拟和现实世界的机器人实验证明了我们建议的有效性。值得注意的是，与普通版本相比，DGMoE 显着提高了计算效率，而 FedVLA 的任务成功率与集中式训练相当，有效保护了数据隐私。"
        },
        {
          "title": "Learning to See and Act: Task-Aware Virtual View Exploration for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2508.05186v4",
          "snippet": "Recent vision-language-action (VLA) models for multi-task robotic manipulation commonly rely on static viewpoints and shared visual encoders, which limit 3D perception and cause task interference, hindering robustness and generalization. In this work, we propose Task-aware Virtual View Exploration (TVVE), a framework designed to overcome these challenges by integrating virtual view exploration with task-specific representation learning. TVVE employs an efficient exploration policy, accelerated by a novel pseudo-environment, to acquire informative views. Furthermore, we introduce a Task-aware Mixture-of-Experts (TaskMoE) visual encoder to disentangle features across different tasks, boosting both representation fidelity and task generalization. By learning to see the world in a task-aware way, TVVE generates more complete and discriminative visual representations, demonstrating significantly enhanced action prediction across a wide array of manipulation challenges. To further validate the robustness and generalization capability of TVVE under out-of-distribution (OOD) settings, we construct a challenging benchmark, RLBench-OG, covering various visual perturbations and camera pose variations. Extensive experiments on RLBench and RLBench-OG show that our TVVE achieves superior performance over state-of-the-art approaches. In real-robot experiments, TVVE demonstrates exceptional performance and generalizes robustly in multiple OOD settings, including visual disturbances and unseen instructions. Visual results and code are provided at: https://hcplab-sysu.github.io/TAVP.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-08-07T09:21:20Z",
          "authors": [
            "Yongjie Bai",
            "Zhouxia Wang",
            "Yang Liu",
            "Kaijun Luo",
            "Yifan Wen",
            "Mingtong Dai",
            "Weixing Chen",
            "Ziliang Chen",
            "Lingbo Liu",
            "Guanbin Li",
            "Liang Lin"
          ],
          "arxiv_id": "2508.05186",
          "abstract": "Recent vision-language-action (VLA) models for multi-task robotic manipulation commonly rely on static viewpoints and shared visual encoders, which limit 3D perception and cause task interference, hindering robustness and generalization. In this work, we propose Task-aware Virtual View Exploration (TVVE), a framework designed to overcome these challenges by integrating virtual view exploration with task-specific representation learning. TVVE employs an efficient exploration policy, accelerated by a novel pseudo-environment, to acquire informative views. Furthermore, we introduce a Task-aware Mixture-of-Experts (TaskMoE) visual encoder to disentangle features across different tasks, boosting both representation fidelity and task generalization. By learning to see the world in a task-aware way, TVVE generates more complete and discriminative visual representations, demonstrating significantly enhanced action prediction across a wide array of manipulation challenges. To further validate the robustness and generalization capability of TVVE under out-of-distribution (OOD) settings, we construct a challenging benchmark, RLBench-OG, covering various visual perturbations and camera pose variations. Extensive experiments on RLBench and RLBench-OG show that our TVVE achieves superior performance over state-of-the-art approaches. In real-robot experiments, TVVE demonstrates exceptional performance and generalizes robustly in multiple OOD settings, including visual disturbances and unseen instructions. Visual results and code are provided at: https://hcplab-sysu.github.io/TAVP.",
          "abstract_zh": "最近用于多任务机器人操作的视觉-语言-动作（VLA）模型通常依赖于静态视点和共享视觉编码器，这限制了 3D 感知并导致任务干扰，从而阻碍了鲁棒性和泛化性。在这项工作中，我们提出了任务感知虚拟视图探索（TVVE），这是一个旨在通过将虚拟视图探索与特定任务表示学习相结合来克服这些挑战的框架。TVVE 采用有效的探索策略，并通过新颖的伪环境加速，以获得信息丰富的视图。此外，我们引入了任务感知混合专家（TaskMoE）视觉编码器来理清不同任务之间的特征，从而提高表示保真度和任务泛化。通过学习以任务感知的方式看待世界，TVVE 生成了更完整和更具辨别力的视觉表示，在各种操纵挑战中展示了显着增强的动作预测。为了进一步验证 TVVE 在分布外（OOD）设置下的鲁棒性和泛化能力，我们构建了一个具有挑战性的基准，RLBench-OG，涵盖各种视觉扰动和相机姿势变化。在 RLBench 和 RLBench-OG 上进行的大量实验表明，我们的 TVVE 比最先进的方法具有更优越的性能。在真实的机器人实验中，TVVE 表现出了卓越的性能，并在多种 OOD 设置中具有稳健的泛化能力，包括视觉干扰和看不见的指令。视觉结果和代码位于：https://hcplab-sysu.github.io/TAVP。"
        },
        {
          "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation",
          "url": "http://arxiv.org/abs/2508.07770v2",
          "snippet": "We introduce AgentWorld, an interactive simulation platform for developing household mobile manipulation capabilities. Our platform combines automated scene construction that encompasses layout generation, semantic asset placement, visual material configuration, and physics simulation, with a dual-mode teleoperation system supporting both wheeled bases and humanoid locomotion policies for data collection. The resulting AgentWorld Dataset captures diverse tasks ranging from primitive actions (pick-and-place, push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.) across living rooms, bedrooms, and kitchens. Through extensive benchmarking of imitation learning methods including behavior cloning, action chunking transformers, diffusion policies, and vision-language-action models, we demonstrate the dataset's effectiveness for sim-to-real transfer. The integrated system provides a comprehensive solution for scalable robotic skill acquisition in complex home environments, bridging the gap between simulation-based training and real-world deployment. The code, datasets will be available at https://yizhengzhang1.github.io/agent_world/",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-08-11T08:56:19Z",
          "authors": [
            "Yizheng Zhang",
            "Zhenjun Yu",
            "Jiaxin Lai",
            "Cewu Lu",
            "Lei Han"
          ],
          "arxiv_id": "2508.07770",
          "abstract": "We introduce AgentWorld, an interactive simulation platform for developing household mobile manipulation capabilities. Our platform combines automated scene construction that encompasses layout generation, semantic asset placement, visual material configuration, and physics simulation, with a dual-mode teleoperation system supporting both wheeled bases and humanoid locomotion policies for data collection. The resulting AgentWorld Dataset captures diverse tasks ranging from primitive actions (pick-and-place, push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.) across living rooms, bedrooms, and kitchens. Through extensive benchmarking of imitation learning methods including behavior cloning, action chunking transformers, diffusion policies, and vision-language-action models, we demonstrate the dataset's effectiveness for sim-to-real transfer. The integrated system provides a comprehensive solution for scalable robotic skill acquisition in complex home environments, bridging the gap between simulation-based training and real-world deployment. The code, datasets will be available at https://yizhengzhang1.github.io/agent_world/",
          "abstract_zh": "我们推出 AgentWorld，一个用于开发家庭移动操控功能的交互式模拟平台。我们的平台将自动化场景构建（包括布局生成、语义资产放置、视觉材料配置和物理模拟）与支持轮式底座和用于数据收集的人形运动策略的双模式远程操作系统相结合。由此产生的 AgentWorld 数据集捕获了各种任务，从原始动作（拾放、推拉等）到客厅、卧室和厨房的多阶段活动（提供饮料、加热食物等）。通过对模仿学习方法（包括行为克隆、动作分块转换器、扩散策略和视觉-语言-动作模型）进行广泛的基准测试，我们证明了数据集在模拟到真实迁移方面的有效性。该集成系统为复杂家庭环境中可扩展的机器人技能获取提供了全面的解决方案，弥合了基于模拟的培训和实际部署之间的差距。代码、数据集可在 https://yizhengzhang1.github.io/agent_world/ 获取"
        },
        {
          "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model",
          "url": "http://arxiv.org/abs/2508.06571v3",
          "snippet": "Vision-Language-Action (VLA) models have demonstrated potential in autonomous driving. However, two critical challenges hinder their development: (1) Existing VLA architectures are typically based on imitation learning in open-loop setup which tends to capture the recorded behaviors in the dataset, leading to suboptimal and constrained performance, (2) Close-loop training relies heavily on high-fidelity sensor simulation, where domain gaps and computational inefficiencies pose significant barriers. In this paper, we introduce IRL-VLA, a novel close-loop Reinforcement Learning via \\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model with a self-built VLA approach. Our framework proceeds in a three-stage paradigm: In the first stage, we propose a VLA architecture and pretrain the VLA policy via imitation learning. In the second stage, we construct a lightweight reward world model via inverse reinforcement learning to enable efficient close-loop reward computation. To further enhance planning performance, finally, we design specialized reward world model guidence reinforcement learning via PPO(Proximal Policy Optimization) to effectively balance the safety incidents, comfortable driving, and traffic efficiency. Our approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that our framework will accelerate VLA research in close-loop autonomous driving.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-08-07T06:30:05Z",
          "authors": [
            "Anqing Jiang",
            "Yu Gao",
            "Yiru Wang",
            "Zhigang Sun",
            "Shuo Wang",
            "Yuwen Heng",
            "Hao Sun",
            "Shichen Tang",
            "Lijuan Zhu",
            "Jinhao Chai",
            "Jijun Wang",
            "Zichong Gu",
            "Hao Jiang",
            "Li Sun"
          ],
          "arxiv_id": "2508.06571",
          "abstract": "Vision-Language-Action (VLA) models have demonstrated potential in autonomous driving. However, two critical challenges hinder their development: (1) Existing VLA architectures are typically based on imitation learning in open-loop setup which tends to capture the recorded behaviors in the dataset, leading to suboptimal and constrained performance, (2) Close-loop training relies heavily on high-fidelity sensor simulation, where domain gaps and computational inefficiencies pose significant barriers. In this paper, we introduce IRL-VLA, a novel close-loop Reinforcement Learning via \\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model with a self-built VLA approach. Our framework proceeds in a three-stage paradigm: In the first stage, we propose a VLA architecture and pretrain the VLA policy via imitation learning. In the second stage, we construct a lightweight reward world model via inverse reinforcement learning to enable efficient close-loop reward computation. To further enhance planning performance, finally, we design specialized reward world model guidence reinforcement learning via PPO(Proximal Policy Optimization) to effectively balance the safety incidents, comfortable driving, and traffic efficiency. Our approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that our framework will accelerate VLA research in close-loop autonomous driving.",
          "abstract_zh": "视觉-语言-动作（VLA）模型已展现出在自动驾驶方面的潜力。然而，两个关键挑战阻碍了它们的发展：(1) 现有的 VLA 架构通常基于开环设置中的模仿学习，往往会捕获数据集中记录的行为，从而导致性能欠佳和受限，(2) 闭环训练严重依赖于高保真传感器模拟，其中域差距和计算效率低下构成了重大障碍。在本文中，我们介绍了 IRL-VLA，这是一种新颖的闭环强化学习，通过 \\textbf{I}nverse \\textbf{R}einforcement \\textbf{L} 赚取奖励世界模型，并采用自建的 VLA 方法。我们的框架采用三阶段范式：在第一阶段，我们提出 VLA 架构并通过模仿学习预训练 VLA 策略。在第二阶段，我们通过逆强化学习构建了一个轻量级的奖励世界模型，以实现高效的闭环奖励计算。为了进一步提高规划性能，最后，我们通过PPO（邻近策略优化）设计了专门的奖励世界模型指导强化学习，以有效平衡安全事件、舒适驾驶和交通效率。我们的方法在 NAVSIM v2 端到端驾驶基准测试中实现了最先进的性能，并在 CVPR2025 自主挑战赛中获得了第一名。我们希望我们的框架能够加速闭环自动驾驶的 VLA 研究。"
        },
        {
          "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control",
          "url": "http://arxiv.org/abs/2508.05342v1",
          "snippet": "Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-08-07T12:48:09Z",
          "authors": [
            "Shunlei Li",
            "Longsen Gao",
            "Jin Wang",
            "Chang Che",
            "Xi Xiao",
            "Jiuwen Cao",
            "Yingbai Hu",
            "Hamid Reza Karimi"
          ],
          "arxiv_id": "2508.05342",
          "abstract": "Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations.",
          "abstract_zh": "由于依赖于低级轨迹模仿，从人类视频中教授机器人灵巧技能仍然具有挑战性，而这种模仿无法概括物体类型、空间布局和操纵器配置。我们提出了图融合视觉语言动作（GF-VLA），这是一个框架，使双臂机器人系统能够直接从 RGB 和深度人类演示中执行任务级推理和执行。GF-VLA 首先提取基于香农信息的线索来识别具有最高任务相关性的手和物体，然后将这些线索编码成按时间顺序排列的场景图，以捕获手与物体以及物体与物体的交互。这些图与语言条件转换器融合，生成分层行为树和可解释的笛卡尔运动命令。为了提高双手设置中的执行效率，我们进一步引入了一种交叉手选择策略，该策略无需显式几何推理即可推断出最佳的夹具分配。我们在涉及符号形状构造和空间泛化的四个结构化双臂块组装任务上评估 GF-VLA。实验结果表明，信息论场景表示实现了超过 95% 的图形准确性和 93% 的子任务分割，支持 LLM 规划者生成可靠且人类可读的任务策略。当由双臂机器人执行时，这些策略在堆叠、字母构建和几何重新配置​​场景中实现了 94% 的抓取成功率、89% 的放置准确性和 90% 的总体任务成功率，在不同的空间和语义变化中表现出强大的泛化性和鲁棒性。"
        },
        {
          "title": "RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2508.02062v1",
          "snippet": "Multi-task ``vision-language-action'' (VLA) models have recently demonstrated increasing promise as generalist foundation models for robotics, achieving non-trivial performance out of the box on new tasks in new environments. However, for such models to be truly useful, an end user must have easy means to teach them to improve. For language and vision models, the emergent ability to perform in-context learning (ICL) has proven to be a versatile and highly useful interface to easily teach new tasks with no parameter finetuning. Unfortunately, VLAs pre-trained with imitation learning objectives do not naturally acquire ICL abilities. In this paper, we demonstrate that, with the right finetuning recipe and a small robot demonstration dataset, it is possible to inject in-context adaptability post hoc into such a VLA. After retraining for in-context learning (RICL), our system permits an end user to provide a small number (10-20) of demonstrations for a new task. RICL then fetches the most relevant portions of those demonstrations into the VLA context to exploit ICL, performing the new task and boosting task performance. We apply RICL to inject ICL into the $π_{0}$-FAST VLA, and show that it permits large in-context improvements for a variety of new manipulation tasks with only 20 demonstrations per task, without any parameter updates. When parameter updates on the target task demonstrations is possible, RICL finetuning further boosts performance. We release code and model weights for RICL-$π_{0}$-FAST alongside the paper to enable, for the first time, a simple in-context learning interface for new manipulation tasks. Website: https://ricl-vla.github.io.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-08-04T05:01:11Z",
          "authors": [
            "Kaustubh Sridhar",
            "Souradeep Dutta",
            "Dinesh Jayaraman",
            "Insup Lee"
          ],
          "arxiv_id": "2508.02062",
          "abstract": "Multi-task ``vision-language-action'' (VLA) models have recently demonstrated increasing promise as generalist foundation models for robotics, achieving non-trivial performance out of the box on new tasks in new environments. However, for such models to be truly useful, an end user must have easy means to teach them to improve. For language and vision models, the emergent ability to perform in-context learning (ICL) has proven to be a versatile and highly useful interface to easily teach new tasks with no parameter finetuning. Unfortunately, VLAs pre-trained with imitation learning objectives do not naturally acquire ICL abilities. In this paper, we demonstrate that, with the right finetuning recipe and a small robot demonstration dataset, it is possible to inject in-context adaptability post hoc into such a VLA. After retraining for in-context learning (RICL), our system permits an end user to provide a small number (10-20) of demonstrations for a new task. RICL then fetches the most relevant portions of those demonstrations into the VLA context to exploit ICL, performing the new task and boosting task performance. We apply RICL to inject ICL into the $π_{0}$-FAST VLA, and show that it permits large in-context improvements for a variety of new manipulation tasks with only 20 demonstrations per task, without any parameter updates. When parameter updates on the target task demonstrations is possible, RICL finetuning further boosts performance. We release code and model weights for RICL-$π_{0}$-FAST alongside the paper to enable, for the first time, a simple in-context learning interface for new manipulation tasks. Website: https://ricl-vla.github.io.",
          "abstract_zh": "多任务“视觉-语言-动作”（VLA）模型最近显示出作为机器人技术通用基础模型的前景越来越大，在新环境中的新任务中实现了非凡的开箱即用性能。然而，为了让这些模型真正有用，最终用户必须有简单的方法来教他们改进。对于语言和视觉模型，执行上下文学习 (ICL) 的新兴能力已被证明是一种多功能且非常有用的界面，可以轻松教授新任务，无需参数微调。不幸的是，以模仿学习为目标进行预训练的 VLA 并不能自然获得 ICL 能力。在本文中，我们证明，通过正确的微调方法和小型机器人演示数据集，可以将上下文适应性事后注入到这样的 VLA 中。经过情境学习 (RICL) 的再培训后，我们的系统允许最终用户为新任务提供少量（10-20）次演示。然后，RICL 将这些演示中最相关的部分提取到 VLA 上下文中以利用 ICL，执行新任务并提高任务性能。我们应用 RICL 将 ICL 注入 $π_{0}$-FAST VLA，并表明它允许对各种新操作任务进行大量上下文改进，每个任务仅进行 20 次演示，无需任何参数更新。当目标任务演示的参数可以更新时，RICL 微调可以进一步提高性能。我们与论文一起发布了 RICL-$π_{0}$-FAST 的代码和模型权重，首次为新的操作任务提供了一个简单的上下文学习界面。网站：https://ricl-vla.github.io。"
        },
        {
          "title": "A tutorial note on collecting simulated data for vision-language-action models",
          "url": "http://arxiv.org/abs/2508.06547v1",
          "snippet": "Traditional robotic systems typically decompose intelligence into independent modules for computer vision, natural language processing, and motion control. Vision-Language-Action (VLA) models fundamentally transform this approach by employing a single neural network that can simultaneously process visual observations, understand human instructions, and directly output robot actions -- all within a unified framework. However, these systems are highly dependent on high-quality training datasets that can capture the complex relationships between visual observations, language instructions, and robotic actions. This tutorial reviews three representative systems: the PyBullet simulation framework for flexible customized data generation, the LIBERO benchmark suite for standardized task definition and evaluation, and the RT-X dataset collection for large-scale multi-robot data acquisition. We demonstrated dataset generation approaches in PyBullet simulation and customized data collection within LIBERO, and provide an overview of the characteristics and roles of the RT-X dataset for large-scale multi-robot data acquisition.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-08-06T01:13:05Z",
          "authors": [
            "Heran Wu",
            "Zirun Zhou",
            "Jingfeng Zhang"
          ],
          "arxiv_id": "2508.06547",
          "abstract": "Traditional robotic systems typically decompose intelligence into independent modules for computer vision, natural language processing, and motion control. Vision-Language-Action (VLA) models fundamentally transform this approach by employing a single neural network that can simultaneously process visual observations, understand human instructions, and directly output robot actions -- all within a unified framework. However, these systems are highly dependent on high-quality training datasets that can capture the complex relationships between visual observations, language instructions, and robotic actions. This tutorial reviews three representative systems: the PyBullet simulation framework for flexible customized data generation, the LIBERO benchmark suite for standardized task definition and evaluation, and the RT-X dataset collection for large-scale multi-robot data acquisition. We demonstrated dataset generation approaches in PyBullet simulation and customized data collection within LIBERO, and provide an overview of the characteristics and roles of the RT-X dataset for large-scale multi-robot data acquisition.",
          "abstract_zh": "传统的机器人系统通常将智能分解为计算机视觉、自然语言处理和运动控制的独立模块。视觉-语言-动作（VLA）模型通过采用单个神经网络从根本上改变了这种方法，该神经网络可以同时处理视觉观察、理解人类指令并直接输出机器人动作——所有这些都在一个统一的框架内。然而，这些系统高度依赖于高质量的训练数据集，这些数据集可以捕获视觉观察、语言指令和机器人动作之间的复杂关系。本教程回顾了三个代表性系统：用于灵活定制数据生成的 PyBullet 模拟框架、用于标准化任务定义和评估的 LIBERO 基准套件以及用于大规模多机器人数据采集的 RT-X 数据集集合。我们演示了 PyBullet 模拟中的数据集生成方法和 LIBERO 中的定制数据收集，并概述了 RT-X 数据集在大规模多机器人数据采集中的特征和作用。"
        },
        {
          "title": "Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction",
          "url": "http://arxiv.org/abs/2508.05294v4",
          "snippet": "Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (LBMs) are increasing the dexterity and capabilities of robotic systems. This survey paper reviews works that advance agentic applications and architectures, including initial efforts with GPT-style interfaces and more complex systems where AI agents function as coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-08-07T11:48:03Z",
          "authors": [
            "Sahar Salimpour",
            "Lei Fu",
            "Kajetan Rachwał",
            "Pascal Bertrand",
            "Kevin O'Sullivan",
            "Robert Jakob",
            "Farhad Keramat",
            "Leonardo Militano",
            "Giovanni Toffetti",
            "Harry Edelman",
            "Jorge Peña Queralta"
          ],
          "arxiv_id": "2508.05294",
          "abstract": "Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (LBMs) are increasing the dexterity and capabilities of robotic systems. This survey paper reviews works that advance agentic applications and architectures, including initial efforts with GPT-style interfaces and more complex systems where AI agents function as coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.",
          "abstract_zh": "基础模型，包括大语言模型（LLM）和视觉语言模型（VLM），最近为机器人自主和人机界面提供了新的方法。与此同时，视觉语言动作模型（VLA）或大型行为模型（LBM）正在提高机器人系统的灵活性和能力。这篇调查论文回顾了推进代理应用程序和架构的工作，包括 GPT 风格的接口和更复杂的系统的初步努力，其中人工智能代理充当协调员、规划者、感知参与者或多面手接口。这种代理架构允许机器人根据自然语言指令进行推理、调用 API、规划任务序列或协助操作和诊断。除了同行评审的研究之外，由于该领域快速发展的性质，我们还重点关注并纳入社区驱动的项目、ROS 包和显示新兴趋势的工业框架。我们提出了一种对模型集成方法进行分类的分类法，并对代理在当今文献中不同解决方案中所扮演的角色进行了比较分析。"
        },
        {
          "title": "CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning",
          "url": "http://arxiv.org/abs/2508.02219v1",
          "snippet": "Vision-Language-Action (VLA) models demonstrate significant potential for developing generalized policies in real-world robotic control. This progress inspires researchers to explore fine-tuning these models with Reinforcement Learning (RL). However, fine-tuning VLA models with RL still faces challenges related to sample efficiency, compatibility with action chunking, and training stability. To address these challenges, we explore the fine-tuning of VLA models through offline reinforcement learning incorporating action chunking. In this work, we propose Chunked RL, a novel reinforcement learning framework specifically designed for VLA models. Within this framework, we extend temporal difference (TD) learning to incorporate action chunking, a prominent characteristic of VLA models. Building upon this framework, we propose CO-RFT, an algorithm aimed at fine-tuning VLA models using a limited set of demonstrations (30 to 60 samples). Specifically, we first conduct imitation learning (IL) with full parameter fine-tuning to initialize both the backbone and the policy. Subsequently, we implement offline RL with action chunking to optimize the pretrained policy. Our empirical results in real-world environments demonstrate that CO-RFT outperforms previous supervised methods, achieving a 57% improvement in success rate and a 22.3% reduction in cycle time. Moreover, our method exhibits robust positional generalization capabilities, attaining a success rate of 44.3% in previously unseen positions.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-08-04T09:11:48Z",
          "authors": [
            "Dongchi Huang",
            "Zhirui Fang",
            "Tianle Zhang",
            "Yihang Li",
            "Lin Zhao",
            "Chunhe Xia"
          ],
          "arxiv_id": "2508.02219",
          "abstract": "Vision-Language-Action (VLA) models demonstrate significant potential for developing generalized policies in real-world robotic control. This progress inspires researchers to explore fine-tuning these models with Reinforcement Learning (RL). However, fine-tuning VLA models with RL still faces challenges related to sample efficiency, compatibility with action chunking, and training stability. To address these challenges, we explore the fine-tuning of VLA models through offline reinforcement learning incorporating action chunking. In this work, we propose Chunked RL, a novel reinforcement learning framework specifically designed for VLA models. Within this framework, we extend temporal difference (TD) learning to incorporate action chunking, a prominent characteristic of VLA models. Building upon this framework, we propose CO-RFT, an algorithm aimed at fine-tuning VLA models using a limited set of demonstrations (30 to 60 samples). Specifically, we first conduct imitation learning (IL) with full parameter fine-tuning to initialize both the backbone and the policy. Subsequently, we implement offline RL with action chunking to optimize the pretrained policy. Our empirical results in real-world environments demonstrate that CO-RFT outperforms previous supervised methods, achieving a 57% improvement in success rate and a 22.3% reduction in cycle time. Moreover, our method exhibits robust positional generalization capabilities, attaining a success rate of 44.3% in previously unseen positions.",
          "abstract_zh": "视觉-语言-动作（VLA）模型展示了在现实世界机器人控制中开发通用策略的巨大潜力。这一进展激励研究人员探索利用强化学习（RL）对这些模型进行微调。然而，使用 RL 微调 VLA 模型仍然面临样本效率、动作分块兼容性和训练稳定性等挑战。为了应对这些挑战，我们通过结合动作分块的离线强化学习探索 VLA 模型的微调。在这项工作中，我们提出了 Chunked RL，这是一种专为 VLA 模型设计的新型强化学习框架。在此框架内，我们扩展了时间差异 (TD) 学习，以纳入动作分块，这是 VLA 模型的一个显着特征。在此框架的基础上，我们提出了 CO-RFT，这是一种旨在使用有限的演示集（30 到 60 个样本）微调 VLA 模型的算法。具体来说，我们首先进行模仿学习（IL）和全参数微调，以初始化骨干网和策略。随后，我们通过动作分块实现离线强化学习，以优化预训练策略。我们在现实环境中的实证结果表明，CO-RFT 优于以前的监督方法，成功率提高了 57%，周期时间缩短了 22.3%。此外，我们的方法表现出强大的位置泛化能力，在以前未见过的位置上达到了 44.3% 的成功率。"
        },
        {
          "title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming",
          "url": "http://arxiv.org/abs/2508.02549v4",
          "snippet": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth inputs to provide rich spatial cues for action planning, but these sensors can be costly or less accessible in real-world deployments. Recent approaches based on Vision-Language Action (VLA) models achieve strong results with monocular input, yet they still lag behind methods using panoramic RGB-D information. We present MonoDream, a lightweight VLA framework that enables monocular agents to learn a Unified Navigation Representation (UNR). This shared feature representation jointly aligns navigation-relevant visual semantics (e.g., global layout, depth, and future cues) and language-grounded action intent, enabling more reliable action prediction. MonoDream further introduces Latent Panoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to predict latent features of panoramic RGB and depth observations at both current and future steps based on only monocular input. Experiments on multiple VLN benchmarks show that MonoDream consistently improves monocular navigation performance and significantly narrows the gap with panoramic-based agents.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-08-04T16:01:30Z",
          "authors": [
            "Shuo Wang",
            "Yongcai Wang",
            "Zhaoxin Fan",
            "Yucheng Wang",
            "Maiyue Chen",
            "Kaihui Wang",
            "Zhizhong Su",
            "Wanting Li",
            "Xudong Cai",
            "Yeying Jin",
            "Deying Li"
          ],
          "arxiv_id": "2508.02549",
          "abstract": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth inputs to provide rich spatial cues for action planning, but these sensors can be costly or less accessible in real-world deployments. Recent approaches based on Vision-Language Action (VLA) models achieve strong results with monocular input, yet they still lag behind methods using panoramic RGB-D information. We present MonoDream, a lightweight VLA framework that enables monocular agents to learn a Unified Navigation Representation (UNR). This shared feature representation jointly aligns navigation-relevant visual semantics (e.g., global layout, depth, and future cues) and language-grounded action intent, enabling more reliable action prediction. MonoDream further introduces Latent Panoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to predict latent features of panoramic RGB and depth observations at both current and future steps based on only monocular input. Experiments on multiple VLN benchmarks show that MonoDream consistently improves monocular navigation performance and significantly narrows the gap with panoramic-based agents.",
          "abstract_zh": "视觉语言导航 (VLN) 任务通常利用全景 RGB 和深度输入来为行动规划提供丰富的空间线索，但这些传感器可能成本高昂，或者在实际部署中不易访问。最近基于视觉语言动作（VLA）模型的方法通过单眼输入取得了很好的结果，但它们仍然落后于使用全景 RGB-D 信息的方法。我们推出了 MonoDream，这是一个轻量级 VLA 框架，使单眼智能体能够学习统一导航表示 (UNR)。这种共享的特征表示联合对齐与导航相关的视觉语义（例如，全局布局、深度和未来线索）和基于语言的动作意图，从而实现更可靠的动作预测。MonoDream 进一步引入了潜在全景梦 (LPD) 任务来监督 UNR，训练模型仅基于单眼输入来预测当前和未来步骤中全景 RGB 和深度观察的潜在特征。在多个 VLN 基准测试上的实验表明，MonoDream 持续改进了单目导航性能，并显着缩小了与基于全景的智能体的差距。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 12 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 12）。",
      "items": [
        {
          "title": "patrick-llgc/Learning-Deep-Learning",
          "url": "https://github.com/patrick-llgc/Learning-Deep-Learning",
          "snippet": "Paper reading notes on Deep Learning and Machine Learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "OpenDriveLab/WholebodyVLA",
          "url": "https://github.com/OpenDriveLab/WholebodyVLA",
          "snippet": "Towards Unified Latent VLA for Whole-body Loco-manipulation Control",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "ReinFlow/ReinFlow",
          "url": "https://github.com/ReinFlow/ReinFlow",
          "snippet": "[NeurIPS 2025] Flow x RL. \"ReinFlow: Fine-tuning Flow Policy with Online Reinforcement Learning\". Support VLAs e.g., pi0, pi0.5. Fully open-sourced. ",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "RoboTwin-Platform/RoboTwin",
          "url": "https://github.com/RoboTwin-Platform/RoboTwin",
          "snippet": "RoboTwin 2.0 Offical Repo",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "Songwxuan/Embodied-AI-Paper-TopConf",
          "url": "https://github.com/Songwxuan/Embodied-AI-Paper-TopConf",
          "snippet": "[Actively Maintained🔥] A list of Embodied AI papers accepted by top conferences (ICLR, NeurIPS, ICML, RSS, CoRL, ICRA, IROS, CVPR, ICCV, ECCV).",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "OpenHelix-Team/VLA-Adapter",
          "url": "https://github.com/OpenHelix-Team/VLA-Adapter",
          "snippet": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model",
          "site": "github.com",
          "rank": 9
        },
        {
          "title": "ChaofanTao/Autoregressive-Models-in-Vision-Survey",
          "url": "https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey",
          "snippet": " [TMLR 2025🔥] A survey for the autoregressive models in vision. ",
          "site": "github.com",
          "rank": 10
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 11
        },
        {
          "title": "yang-zj1026/NaVILA-Bench",
          "url": "https://github.com/yang-zj1026/NaVILA-Bench",
          "snippet": "Vision-Language Navigation Benchmark in Isaac Lab",
          "site": "github.com",
          "rank": 12
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-08-04",
  "week_end": "2025-08-10",
  "last_updated": "2026-01-07"
}