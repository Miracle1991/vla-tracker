{
  "generated_at": "2026-01-07T13:24:59.969054",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 10 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 10）。",
      "items": [
        {
          "title": "Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation",
          "url": "http://arxiv.org/abs/2508.19958v2",
          "snippet": "Vision-Language-Action (VLA) models have become a cornerstone in robotic policy learning, leveraging large-scale multimodal data for robust and scalable control. However, existing VLA frameworks primarily address short-horizon tasks, and their effectiveness on long-horizon, multi-step robotic manipulation remains limited due to challenges in skill chaining and subtask dependencies. In this work, we introduce Long-VLA, the first end-to-end VLA model specifically designed for long-horizon robotic tasks. Our approach features a novel phase-aware input masking strategy that adaptively segments each subtask into moving and interaction phases, enabling the model to focus on phase-relevant sensory cues and enhancing subtask compatibility. This unified strategy preserves the scalability and data efficiency of VLA training, and our architecture-agnostic module can be seamlessly integrated into existing VLA models. We further propose the L-CALVIN benchmark to systematically evaluate long-horizon manipulation. Extensive experiments on both simulated and real-world tasks demonstrate that Long-VLA significantly outperforms prior state-of-the-art methods, establishing a new baseline for long-horizon robotic control.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-08-27T15:12:03Z",
          "authors": [
            "Yiguo Fan",
            "Pengxiang Ding",
            "Shuanghao Bai",
            "Xinyang Tong",
            "Yuyang Zhu",
            "Hongchao Lu",
            "Fengqi Dai",
            "Wei Zhao",
            "Yang Liu",
            "Siteng Huang",
            "Zhaoxin Fan",
            "Badong Chen",
            "Donglin Wang"
          ],
          "arxiv_id": "2508.19958",
          "abstract": "Vision-Language-Action (VLA) models have become a cornerstone in robotic policy learning, leveraging large-scale multimodal data for robust and scalable control. However, existing VLA frameworks primarily address short-horizon tasks, and their effectiveness on long-horizon, multi-step robotic manipulation remains limited due to challenges in skill chaining and subtask dependencies. In this work, we introduce Long-VLA, the first end-to-end VLA model specifically designed for long-horizon robotic tasks. Our approach features a novel phase-aware input masking strategy that adaptively segments each subtask into moving and interaction phases, enabling the model to focus on phase-relevant sensory cues and enhancing subtask compatibility. This unified strategy preserves the scalability and data efficiency of VLA training, and our architecture-agnostic module can be seamlessly integrated into existing VLA models. We further propose the L-CALVIN benchmark to systematically evaluate long-horizon manipulation. Extensive experiments on both simulated and real-world tasks demonstrate that Long-VLA significantly outperforms prior state-of-the-art methods, establishing a new baseline for long-horizon robotic control.",
          "abstract_zh": "视觉-语言-动作（VLA）模型已成为机器人策略学习的基石，利用大规模多模态数据进行稳健且可扩展的控制。然而，现有的 VLA 框架主要解决短期任务，由于技能链和子任务依赖性方面的挑战，它们在长期、多步骤机器人操作上的有效性仍然有限。在这项工作中，我们介绍了 Long-VLA，这是第一个专为长视野机器人任务设计的端到端 VLA 模型。我们的方法采用了一种新颖的阶段感知输入掩蔽策略，可自适应地将每个子任务分割为移动和交互阶段，使模型能够专注于阶段相关的感官线索并增强子任务兼容性。这种统一策略保留了 VLA 训练的可扩展性和数据效率，并且我们的架构无关模块可以无缝集成到现有的 VLA 模型中。我们进一步提出 L-CALVIN 基准来系统地评估长期操纵。对模拟和现实世界任务的大量实验表明，Long-VLA 显着优于先前最先进的方法，为长视野机器人控制建立了新的基线。"
        },
        {
          "title": "MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2508.19236v1",
          "snippet": "Temporal context is essential for robotic manipulation because such tasks are inherently non-Markovian, yet mainstream VLA models typically overlook it and struggle with long-horizon, temporally dependent tasks. Cognitive science suggests that humans rely on working memory to buffer short-lived representations for immediate control, while the hippocampal system preserves verbatim episodic details and semantic gist of past experience for long-term memory. Inspired by these mechanisms, we propose MemoryVLA, a Cognition-Memory-Action framework for long-horizon robotic manipulation. A pretrained VLM encodes the observation into perceptual and cognitive tokens that form working memory, while a Perceptual-Cognitive Memory Bank stores low-level details and high-level semantics consolidated from it. Working memory retrieves decision-relevant entries from the bank, adaptively fuses them with current tokens, and updates the bank by merging redundancies. Using these tokens, a memory-conditioned diffusion action expert yields temporally aware action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on Bridge. On 12 real-world tasks spanning general skills and long-horizon temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon tasks showing a +26 improvement over state-of-the-art baseline. Project Page: https://shihao1895.github.io/MemoryVLA",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-08-26T17:57:16Z",
          "authors": [
            "Hao Shi",
            "Bin Xie",
            "Yingfei Liu",
            "Lin Sun",
            "Fengrong Liu",
            "Tiancai Wang",
            "Erjin Zhou",
            "Haoqiang Fan",
            "Xiangyu Zhang",
            "Gao Huang"
          ],
          "arxiv_id": "2508.19236",
          "abstract": "Temporal context is essential for robotic manipulation because such tasks are inherently non-Markovian, yet mainstream VLA models typically overlook it and struggle with long-horizon, temporally dependent tasks. Cognitive science suggests that humans rely on working memory to buffer short-lived representations for immediate control, while the hippocampal system preserves verbatim episodic details and semantic gist of past experience for long-term memory. Inspired by these mechanisms, we propose MemoryVLA, a Cognition-Memory-Action framework for long-horizon robotic manipulation. A pretrained VLM encodes the observation into perceptual and cognitive tokens that form working memory, while a Perceptual-Cognitive Memory Bank stores low-level details and high-level semantics consolidated from it. Working memory retrieves decision-relevant entries from the bank, adaptively fuses them with current tokens, and updates the bank by merging redundancies. Using these tokens, a memory-conditioned diffusion action expert yields temporally aware action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on Bridge. On 12 real-world tasks spanning general skills and long-horizon temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon tasks showing a +26 improvement over state-of-the-art baseline. Project Page: https://shihao1895.github.io/MemoryVLA",
          "abstract_zh": "时间背景对于机器人操作至关重要，因为此类任务本质上是非马尔可夫的，但主流 VLA 模型通常会忽视它，并难以处理长期的、时间相关的任务。认知科学表明，人类依靠工作记忆来缓冲短暂的表征以进行即时控制，而海马系统则保留过去经验的逐字细节和语义要点，以实现长期记忆。受这些机制的启发，我们提出了 MemoryVLA，一种用于长视野机器人操作的认知-记忆-动作框架。预训练的 VLM 将观察结果编码为形成工作记忆的感知和认知标记，而感知认知记忆库则存储低级细节和从中整合的高级语义。工作记忆从库中检索与决策相关的条目，自适应地将它们与当前令牌融合，并通过合并冗余来更新库。使用这些标记，记忆调节的扩散动作专家产生时间感知的动作序列。我们在三个机器人的 150 多项模拟和现实任务中评估了 MemoryVLA。在 SimplerEnv-Bridge、Fractal 和 LIBERO-5 套件上，它分别实现了 71.9%、72.7% 和 96.5% 的成功率，全部优于最先进的基线 CogACT 和 pi-0，在 Bridge 上显着增加了 +14.6。在涵盖一般技能和长期时间依赖性的 12 项现实世界任务中，MemoryVLA 实现了 84.0% 的成功率，其中长期任务比最先进的基线提高了 +26。项目页面：https://shihao1895.github.io/MemoryVLA"
        },
        {
          "title": "EO-1: Interleaved Vision-Text-Action Pretraining for General Robot Control",
          "url": "http://arxiv.org/abs/2508.21112v4",
          "snippet": "The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-08-28T17:26:15Z",
          "authors": [
            "Delin Qu",
            "Haoming Song",
            "Qizhi Chen",
            "Zhaoqing Chen",
            "Xianqiang Gao",
            "Xinyi Ye",
            "Qi Lv",
            "Modi Shi",
            "Guanghui Ren",
            "Cheng Ruan",
            "Maoqing Yao",
            "Haoran Yang",
            "Jiacheng Bao",
            "Bin Zhao",
            "Dong Wang"
          ],
          "arxiv_id": "2508.21112",
          "abstract": "The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.",
          "abstract_zh": "人类在开放世界中无缝执行多模态推理和物理交互的能力是通用具体智能系统的核心目标。最近的视觉语言动作（VLA）模型在大规模机器人和视觉文本数据上进行联合训练，在通用机器人控制方面取得了显着进展。然而，它们在交错推理和交互方面仍然未能实现人类水平的灵活性。在这项工作中，介绍了EO-Robotics，由EO-1模型和EO-Data1.5M数据集组成。EO-1是一个统一的具身基础模型，通过交错的视觉-文本-动作预训练，在多模态具身推理和机器人控制方面实现了卓越的性能。EO-1 的开发基于两个关键支柱：(i) 一个统一的架构，可以不加区别地处理多模态输入（图像、文本、视频和动作），以及 (ii) 一个大规模、高质量的多模态具体推理数据集 EO-Data1.5M，其中包含超过 150 万个样本，重点是交错的视觉-文本-动作理解。EO-1 通过 EO-Data1.5M 上的自回归解码和流匹配去噪之间的协同作用进行训练，从而实现无缝机器人动作生成和多模态体现推理。大量的实验证明了交错的视觉-文本-动作学习对于开放世界理解和泛化的有效性，并通过跨多个实施例的各种长视野、灵巧的操作任务进行了验证。本文详细介绍了 EO-1 的架构、EO-Data1.5M 的数据构建策略以及训练方法，为开发高级具体化基础模型提供了宝贵的见解。"
        },
        {
          "title": "OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving",
          "url": "http://arxiv.org/abs/2509.00789v1",
          "snippet": "Recent advances in vision-language models (VLMs) have demonstrated impressive spatial reasoning capabilities for autonomous driving, yet existing methods predominantly focus on static scene understanding while neglecting the essential temporal dimension of real-world driving scenarios. To address this critical limitation, we propose the OmniReason framework, which establishes robust spatiotemporal reasoning by jointly modeling dynamic 3D environments and their underlying decision-making processes. Our work makes two fundamental advances: (1) We introduce OmniReason-Data, two large-scale vision-language-action (VLA) datasets with dense spatiotemporal annotations and natural language explanations, generated through a novel hallucination-mitigated auto-labeling pipeline that ensures both physical plausibility and temporal coherence; (2) We develop the OmniReason-Agent architecture, which integrates a sparse temporal memory module for persistent scene context modeling and an explanation generator that produces human-interpretable decision rationales, facilitated by our spatiotemporal knowledge distillation approach that effectively captures spatiotemporal causal reasoning patterns. Comprehensive experiments demonstrate state-of-the-art performance, where OmniReason-Agent achieves significant improvements in both open-loop planning tasks and visual question answering (VQA) benchmarks, while establishing new capabilities for interpretable, temporally-aware autonomous vehicles operating in complex, dynamic environments.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-08-31T10:34:44Z",
          "authors": [
            "Pei Liu",
            "Qingtian Ning",
            "Xinyan Lu",
            "Haipeng Liu",
            "Weiliang Ma",
            "Dangen She",
            "Peng Jia",
            "Xianpeng Lang",
            "Jun Ma"
          ],
          "arxiv_id": "2509.00789",
          "abstract": "Recent advances in vision-language models (VLMs) have demonstrated impressive spatial reasoning capabilities for autonomous driving, yet existing methods predominantly focus on static scene understanding while neglecting the essential temporal dimension of real-world driving scenarios. To address this critical limitation, we propose the OmniReason framework, which establishes robust spatiotemporal reasoning by jointly modeling dynamic 3D environments and their underlying decision-making processes. Our work makes two fundamental advances: (1) We introduce OmniReason-Data, two large-scale vision-language-action (VLA) datasets with dense spatiotemporal annotations and natural language explanations, generated through a novel hallucination-mitigated auto-labeling pipeline that ensures both physical plausibility and temporal coherence; (2) We develop the OmniReason-Agent architecture, which integrates a sparse temporal memory module for persistent scene context modeling and an explanation generator that produces human-interpretable decision rationales, facilitated by our spatiotemporal knowledge distillation approach that effectively captures spatiotemporal causal reasoning patterns. Comprehensive experiments demonstrate state-of-the-art performance, where OmniReason-Agent achieves significant improvements in both open-loop planning tasks and visual question answering (VQA) benchmarks, while establishing new capabilities for interpretable, temporally-aware autonomous vehicles operating in complex, dynamic environments.",
          "abstract_zh": "视觉语言模型（VLM）的最新进展已经展示了自动驾驶的令人印象深刻的空间推理能力，但现有方法主要关注静态场景理解，而忽略了现实世界驾驶场景的基本时间维度。为了解决这一关键限制，我们提出了 OmniReason 框架，该框架通过联合建模动态 3D 环境及其底层决策过程来建立强大的时空推理。我们的工作取得了两个基本进展：（1）我们引入了 OmniReason-Data，这是两个具有密集时空注释和自然语言解释的大规模视觉语言动作（VLA）数据集，通过新颖的减轻幻觉的自动标记管道生成，确保物理合理性和时间连贯性；(2) 我们开发了 OmniReason-Agent 架构，它集成了用于持久场景上下文建模的稀疏时间记忆模块和产生人类可解释的决策原理的解释生成器，并通过有效捕获时空因果推理模式的时空知识蒸馏方法来促进。全面的实验展示了最先进的性能，OmniReason-Agent 在开环规划任务和视觉问答 (VQA) 基准方面均实现了显着改进，同时为在复杂动态环境中运行的可解释、时间感知的自动驾驶车辆建立了新的功能。"
        },
        {
          "title": "Mechanistic interpretability for steering vision-language-action models",
          "url": "http://arxiv.org/abs/2509.00328v1",
          "snippet": "Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control - establishing a new paradigm for transparent and steerable foundation models in robotics.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-08-30T03:01:57Z",
          "authors": [
            "Bear Häon",
            "Kaylene Stocking",
            "Ian Chuang",
            "Claire Tomlin"
          ],
          "arxiv_id": "2509.00328",
          "abstract": "Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control - establishing a new paradigm for transparent and steerable foundation models in robotics.",
          "abstract_zh": "视觉-语言-动作（VLA）模型是实现能够快速适应新任务、模式和环境的通用实体代理的一条有前途的途径。然而，解释和操纵 VLA 的方法远远低于经典的机器人管道，后者基于运动学、动力学和控制的显式模型。缺乏机械洞察力是在现实世界的机器人技术中部署学习策略的主要挑战，其中鲁棒性和可解释性至关重要。受大型语言模型机械可解释性进步的推动，我们引入了第一个通过 VLA 的内部表示来解释和引导 VLA 的框架，从而能够在推理时直接干预模型行为。我们将转换器层内的前馈激活投影到令牌嵌入基础上，识别与动作选择因果相关的稀疏语义方向（例如速度和方向）。利用这些发现，我们引入了一种通用激活控制方法，可以实时调节行为，无需微调、奖励信号或环境交互。我们在最近的两个开源 VLA Pi0 和 OpenVLA 上评估了这种方法，并在模拟 (LIBERO) 和物理机器人 (UR5) 上演示了零样本行为控制。这项工作表明，可以系统地利用具体 VLA 的可解释组件进行控制——为机器人技术中透明且可操纵的基础模型建立新的范例。"
        },
        {
          "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies",
          "url": "http://arxiv.org/abs/2508.20072v3",
          "snippet": "Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge. We also provide ablation study on vision-language ability retention on LIBERO-OOD (Out-of-Distribution) benchmark, with our method improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our code is available at https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-08-27T17:39:11Z",
          "authors": [
            "Zhixuan Liang",
            "Yizhuo Li",
            "Tianshuo Yang",
            "Chengyue Wu",
            "Sitong Mao",
            "Tian Nian",
            "Liuao Pei",
            "Shunbo Zhou",
            "Xiaokang Yang",
            "Jiangmiao Pang",
            "Yao Mu",
            "Ping Luo"
          ],
          "arxiv_id": "2508.20072",
          "abstract": "Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge. We also provide ablation study on vision-language ability retention on LIBERO-OOD (Out-of-Distribution) benchmark, with our method improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our code is available at https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.",
          "abstract_zh": "视觉-语言-动作（VLA）模型采用大型视觉语言主干，将图像和指令映射到机器人动作中。然而，流行的 VLA 要么以固定的从左到右的顺序自动回归生成动作，要么在骨干网外部附加单独的 MLP 或扩散头，从而导致碎片化的信息路径和专门的培训要求，从而阻碍统一、可扩展的架构。我们提出了离散扩散 VLA，这是一种统一的转换器策略，它通过离散扩散对离散动作块进行建模。该设计保留了扩散的渐进式细化范式，同时保持与 VLM 的离散令牌接口本机兼容。我们的方法实现了自适应解码顺序，可以先解决简单的动作元素，然后再解决较难的动作元素，并使用二次重新屏蔽来重新审视各轮细化中的不确定预测，从而提高一致性并实现稳健的纠错。这种统一的解码器保留了预先训练的视觉语言先验，支持并行解码，打破了自回归瓶颈，并减少了函数评估的数量。离散扩散 VLA 平均达到 96.3%。LIBERO 上的成功率，SimplerEnv-Fractal 上的视觉匹配率为 71.2%，SimplerEnv-Bridge 上的总体成功率为 54.2%。我们还提供了 LIBERO-OOD（分布外）基准上视觉语言能力保留的消融研究，我们的方法比自回归、MLP 解码器和连续扩散基线有所改进。这些发现表明离散扩散 VLA 支持精确的动作建模和一致的训练，为将 VLA 扩展到更大的模型和数据集奠定了基础。我们的代码位于 https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero。"
        },
        {
          "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification",
          "url": "http://arxiv.org/abs/2508.21046v2",
          "snippet": "Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-08-28T17:50:58Z",
          "authors": [
            "Wei Li",
            "Renshan Zhang",
            "Rui Shao",
            "Jie He",
            "Liqiang Nie"
          ],
          "arxiv_id": "2508.21046",
          "abstract": "Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA.",
          "abstract_zh": "最近基于预先训练的视觉语言模型（VLM）构建的视觉语言动作（VLA）模型需要大量的后期训练，从而导致高计算开销，限制了可扩展性和部署。我们提出了CogVLA，一种认知对齐的视觉语言动作框架，它利用指令驱动的路由和稀疏化来提高效率和性能。CogVLA 从人类多模式协调中汲取灵感，引入了 3 阶段渐进式架构。1）基于编码器-FiLM的聚合路由（EFA-Routing）将指令信息注入视觉编码器，以选择性地聚合和压缩双流视觉令牌，形成指令感知的潜在表示。2）基于这种紧凑的视觉编码，基于LLM-FiLM的修剪路由（LFP-Routing）通过修剪与指令无关的视觉基础标记将动作意图引入到语言模型中，从而实现标记级稀疏性。3）为了确保压缩的感知输入仍然可以支持准确且连贯的动作生成，我们引入了V-L-A耦合注意（CAtten），它将因果视觉语言注意与双向动作并行解码相结合。针对 LIBERO 基准和实际机器人任务的大量实验表明，与 OpenVLA 相比，CogVLA 实现了最先进的性能，成功率分别为 97.4% 和 70.0%，同时将训练成本降低了 2.5 倍，推理延迟降低了 2.8 倍。CogVLA 是开源的，可在 https://github.com/JiuTian-VL/CogVLA 上公开获取。"
        },
        {
          "title": "FlowVLA: Visual Chain of Thought-based Motion Reasoning for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2508.18269v3",
          "snippet": "Many Vision-Language-Action (VLA) models are built upon an internal world model trained via next-frame prediction ``$v_t \\rightarrow v_{t+1}$''. However, this paradigm attempts to predict the future frame's appearance directly, without explicitly reasoning about the underlying dynamics. \\textbf{This lack of an explicit motion reasoning step} often leads to physically implausible visual forecasts and inefficient policy learning. To address this limitation, we introduce the \\textbf{Visual Chain of Thought (Visual CoT)}, a paradigm that compels the model to first reason about \\textbf{motion dynamics} before generating the future frame. We instantiate this paradigm by proposing \\textbf{FlowVLA}, an autoregressive Transformer that explicitly materializes this reasoning process as ``$v_t \\rightarrow f_t \\rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction that inherently encodes motion. By forcing the model to first follow the motion plan encoded by $f_t$, this process inherently \\textbf{aligns the pre-training objective of dynamics prediction with the downstream task of action generation.} We conduct experiments on challenging robotics manipulation benchmarks, as well as real-robot evaluations. Our FlowVLA not only generates \\textbf{more coherent and physically plausible visual predictions}, but also achieves state-of-the-art policy performance with \\textbf{substantially improved sample efficiency}, pointing toward a more principled foundation for world modeling in VLAs. Project page: https://irpn-lab.github.io/FlowVLA/",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-08-25T17:59:21Z",
          "authors": [
            "Zhide Zhong",
            "Haodong Yan",
            "Junfeng Li",
            "Xiangchen Liu",
            "Xin Gong",
            "Tianran Zhang",
            "Wenxuan Song",
            "Jiayi Chen",
            "Xinhu Zheng",
            "Hesheng Wang",
            "Haoang Li"
          ],
          "arxiv_id": "2508.18269",
          "abstract": "Many Vision-Language-Action (VLA) models are built upon an internal world model trained via next-frame prediction ``$v_t \\rightarrow v_{t+1}$''. However, this paradigm attempts to predict the future frame's appearance directly, without explicitly reasoning about the underlying dynamics. \\textbf{This lack of an explicit motion reasoning step} often leads to physically implausible visual forecasts and inefficient policy learning. To address this limitation, we introduce the \\textbf{Visual Chain of Thought (Visual CoT)}, a paradigm that compels the model to first reason about \\textbf{motion dynamics} before generating the future frame. We instantiate this paradigm by proposing \\textbf{FlowVLA}, an autoregressive Transformer that explicitly materializes this reasoning process as ``$v_t \\rightarrow f_t \\rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction that inherently encodes motion. By forcing the model to first follow the motion plan encoded by $f_t$, this process inherently \\textbf{aligns the pre-training objective of dynamics prediction with the downstream task of action generation.} We conduct experiments on challenging robotics manipulation benchmarks, as well as real-robot evaluations. Our FlowVLA not only generates \\textbf{more coherent and physically plausible visual predictions}, but also achieves state-of-the-art policy performance with \\textbf{substantially improved sample efficiency}, pointing toward a more principled foundation for world modeling in VLAs. Project page: https://irpn-lab.github.io/FlowVLA/",
          "abstract_zh": "许多视觉-语言-动作 (VLA) 模型都是建立在通过下一帧预测“$v_t \\rightarrow v_{t+1}$”训练的内部世界模型之上。然而，这种范式试图直接预测未来框架的外观，而不明确推理潜在的动态。\\textbf{缺乏明确的运动推理步骤}通常会导致物理上不可信的视觉预测和低效的策略学习。为了解决这个限制，我们引入了 \\textbf{视觉思想链 (Visual CoT)}，这是一种范式，迫使模型在生成未来帧之前首先推理 \\textbf{运动动力学}。我们通过提出 \\textbf{FlowVLA} 来实例化这个范式，这是一个自回归 Transformer，它明确地将这个推理过程具体化为“$v_t \\rightarrow f_t \\rightarrow v_{t+1}$”，其中 $f_t$ 是本质上对运动进行编码的中间光流预测。通过强制模型首先遵循 $f_t$ 编码的运动计划，这个过程本质上 \\textbf{将动态预测的预训练目标与动作生成的下游任务对齐。}我们对具有挑战性的机器人操作基准以及真实的机器人评估进行了实验。我们的 FlowVLA 不仅生成 \\textbf{更连贯且物理上合理的视觉预测}，而且还通过 \\textbf{显着提高的样本效率}实现了最先进的策略性能，为 VLA 中的世界建模提供了更有原则的基础。项目页面：https://irpn-lab.github.io/FlowVLA/"
        },
        {
          "title": "Galaxea Open-World Dataset and G0 Dual-System VLA Model",
          "url": "http://arxiv.org/abs/2509.00576v1",
          "snippet": "We present Galaxea Open-World Dataset, a large-scale, diverse collection of robot behaviors recorded in authentic human living and working environments. All demonstrations are gathered using a consistent robotic embodiment, paired with precise subtask-level language annotations to facilitate both training and evaluation. Building on this dataset, we introduce G0, a dual-system framework that couples a Vision-Language Model (VLM) for multimodal planning with a Vision-Language-Action (VLA) model for fine-grained execution. G0 is trained using a three-stage curriculum: cross-embodiment pre-training, single-embodiment pre-training, and task-specific post-training. A comprehensive benchmark spanning tabletop manipulation, few-shot learning, and long-horizon mobile manipulation, demonstrates the effectiveness of our approach. In particular, we find that the single-embodiment pre-training stage, together with the Galaxea Open-World Dataset, plays a critical role in achieving strong performance.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-08-30T18:04:19Z",
          "authors": [
            "Tao Jiang",
            "Tianyuan Yuan",
            "Yicheng Liu",
            "Chenhao Lu",
            "Jianning Cui",
            "Xiao Liu",
            "Shuiqi Cheng",
            "Jiyang Gao",
            "Huazhe Xu",
            "Hang Zhao"
          ],
          "arxiv_id": "2509.00576",
          "abstract": "We present Galaxea Open-World Dataset, a large-scale, diverse collection of robot behaviors recorded in authentic human living and working environments. All demonstrations are gathered using a consistent robotic embodiment, paired with precise subtask-level language annotations to facilitate both training and evaluation. Building on this dataset, we introduce G0, a dual-system framework that couples a Vision-Language Model (VLM) for multimodal planning with a Vision-Language-Action (VLA) model for fine-grained execution. G0 is trained using a three-stage curriculum: cross-embodiment pre-training, single-embodiment pre-training, and task-specific post-training. A comprehensive benchmark spanning tabletop manipulation, few-shot learning, and long-horizon mobile manipulation, demonstrates the effectiveness of our approach. In particular, we find that the single-embodiment pre-training stage, together with the Galaxea Open-World Dataset, plays a critical role in achieving strong performance.",
          "abstract_zh": "我们推出 Galaxea 开放世界数据集，这是在真实的人类生活和工作环境中记录的大规模、多样化的机器人行为集合。所有演示都是使用一致的机器人实施例收集的，并与精确的子任务级语言注释配对，以促进培​​训和评估。在此数据集的基础上，我们引入了 G0，这是一个双系统框架，它将用于多模式规划的视觉语言模型 (VLM) 与用于细粒度执行的视觉语言行动 (VLA) 模型结合起来。G0 使用三阶段课程进行训练：跨实施例预训练、单实施例预训练和特定于任务的后训练。涵盖桌面操作、小样本学习和长视野移动操作的综合基准证明了我们方法的有效性。特别是，我们发现单实施例预训练阶段与 Galaxea 开放世界数据集一起在实现强劲性能方面发挥着关键作用。"
        },
        {
          "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories",
          "url": "http://arxiv.org/abs/2508.19852v2",
          "snippet": "In egocentric scenarios, anticipating both the next action and its visual outcome is essential for understanding human-object interactions and for enabling robotic planning. However, existing paradigms fall short of jointly modeling these aspects. Vision-Language-Action (VLA) models focus on action prediction but lack explicit modeling of how actions influence the visual scene, while video prediction models generate future frames without conditioning on specific actions, often resulting in implausible or contextually inconsistent outcomes. To bridge this gap, we propose a unified two-stage predictive framework that jointly models action and visual future in egocentric scenarios, conditioned on hand trajectories. In the first stage, we perform consecutive state modeling to process heterogeneous inputs (visual observations, language, and action history) and explicitly predict future hand trajectories. In the second stage, we introduce causal cross-attention to fuse multi-modal cues, leveraging inferred action signals to guide an image-based Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our approach is the first unified model designed to handle both egocentric human activity understanding and robotic manipulation tasks, providing explicit predictions of both upcoming actions and their visual consequences. Extensive experiments on Ego4D, BridgeData, and RLBench demonstrate that our method outperforms state-of-the-art baselines in both action prediction and future video synthesis.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-08-27T13:09:55Z",
          "authors": [
            "Binjie Zhang",
            "Mike Zheng Shou"
          ],
          "arxiv_id": "2508.19852",
          "abstract": "In egocentric scenarios, anticipating both the next action and its visual outcome is essential for understanding human-object interactions and for enabling robotic planning. However, existing paradigms fall short of jointly modeling these aspects. Vision-Language-Action (VLA) models focus on action prediction but lack explicit modeling of how actions influence the visual scene, while video prediction models generate future frames without conditioning on specific actions, often resulting in implausible or contextually inconsistent outcomes. To bridge this gap, we propose a unified two-stage predictive framework that jointly models action and visual future in egocentric scenarios, conditioned on hand trajectories. In the first stage, we perform consecutive state modeling to process heterogeneous inputs (visual observations, language, and action history) and explicitly predict future hand trajectories. In the second stage, we introduce causal cross-attention to fuse multi-modal cues, leveraging inferred action signals to guide an image-based Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our approach is the first unified model designed to handle both egocentric human activity understanding and robotic manipulation tasks, providing explicit predictions of both upcoming actions and their visual consequences. Extensive experiments on Ego4D, BridgeData, and RLBench demonstrate that our method outperforms state-of-the-art baselines in both action prediction and future video synthesis.",
          "abstract_zh": "在以自我为中心的场景中，预测下一步动作及其视觉结果对于理解人与物体的交互和实现机器人规划至关重要。然而，现有的范式不足以对这些方面进行联合建模。视觉-语言-动作（VLA）模型专注于动作预测，但缺乏动作如何影响视觉场景的明确建模，而视频预测模型在不以特定动作为条件的情况下生成未来帧，通常会导致令人难以置信或上下文不一致的结果。为了弥补这一差距，我们提出了一个统一的两阶段预测框架，该框架以手部轨迹为条件，联合模拟以自我为中心的场景中的动作和视觉未来。在第一阶段，我们执行连续状态建模来处理异构输入（视觉观察、语言和动作历史）并明确预测未来的手部轨迹。在第二阶段，我们引入因果交叉注意力来融合多模态线索，利用推断的动作信号来指导基于图像的潜在扩散模型（LDM），以实现逐帧的未来视频生成。我们的方法是第一个统一模型，旨在处理以自我为中心的人类活动理解和机器人操作任务，提供对即将发生的动作及其视觉后果的明确预测。Ego4D、BridgeData 和 RLBench 上的大量实验表明，我们的方法在动作预测和未来视频合成方面均优于最先进的基线。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 8 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 8）。",
      "items": [
        {
          "title": "patrick-llgc/Learning-Deep-Learning",
          "url": "https://github.com/patrick-llgc/Learning-Deep-Learning",
          "snippet": "Paper reading notes on Deep Learning and Machine Learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "Vector-Wangel/XLeRobot",
          "url": "https://github.com/Vector-Wangel/XLeRobot",
          "snippet": "XLeRobot: Practical Dual-Arm Mobile Home Robot for $660",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "RoboTwin-Platform/RoboTwin",
          "url": "https://github.com/RoboTwin-Platform/RoboTwin",
          "snippet": "RoboTwin 2.0 Offical Repo",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 8
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-08-25",
  "week_end": "2025-08-31",
  "last_updated": "2026-01-07"
}