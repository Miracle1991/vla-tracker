{
  "generated_at": "2026-01-07T13:23:39.089172",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 14 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 14）。",
      "items": [
        {
          "title": "Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey",
          "url": "http://arxiv.org/abs/2508.13073v2",
          "snippet": "Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-08-18T16:45:48Z",
          "authors": [
            "Rui Shao",
            "Wei Li",
            "Lingsen Zhang",
            "Renshan Zhang",
            "Zhiyang Liu",
            "Ran Chen",
            "Liqiang Nie"
          ],
          "arxiv_id": "2508.13073",
          "abstract": "Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation",
          "abstract_zh": "机器人操纵是机器人技术和嵌入式人工智能的关键前沿，需要精确的运动控制和多模态理解，但传统的基于规则的方法无法在非结构化的新颖环境中进行扩展或泛化。近年来，基于在大量图像文本数据集上预训练的大型视觉语言模型 (VLM) 构建的视觉语言动作 (VLA) 模型已成为一种变革范式。这项调查首次对用于机器人操作的基于 VLM 的大型 VLA 模型进行了系统的、面向分类的审查。我们首先明确定义基于 VLM 的大型 VLA 模型，并描述两个主要架构范例：（1）整体模型，包括具有不同集成级别的单系统和双系统设计；(2)分层模型，它通过可解释的中间表示明确地将计划与执行分离。在此基础上，我们对基于 VLM 的大型 VLA 模型进行了深入研究：（1）与高级领域的集成，包括强化学习、免训练优化、人类视频学习和世界模型集成；(2) 综合独特的特征，整合架构特征、操作优势以及支持其发展的数据集和基准；（3）确定有前景的方向，包括记忆机制、4D感知、高效适应、多智能体合作和其他新兴能力。这项调查整合了最新的进展，以解决现有分类中的不一致问题，减少研究碎片化，并通过大型 VLM 和机器人操作交叉点的研究的系统整合来填补关键空白。我们提供定期更新的项目页面来记录持续进展：https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation"
        },
        {
          "title": "GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions",
          "url": "http://arxiv.org/abs/2508.07650v2",
          "snippet": "Vision-language-action models have emerged as a crucial paradigm in robotic manipulation. However, existing VLA models exhibit notable limitations in handling ambiguous language instructions and unknown environmental states. Furthermore, their perception is largely constrained to static two-dimensional observations, lacking the capability to model three-dimensional interactions between the robot and its environment. To address these challenges, this paper proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's ability to interpret ambiguous instructions and improve task planning, we design a structured Chain-of-Thought reasoning module that integrates high-level task understanding and planning, failed task feedback, and low-level imaginative reasoning about future object positions and robot actions. Additionally, we construct a real-time updatable 3D Pose-Object graph, which captures the spatial configuration of robot joints and the topological relationships between objects in 3D space, enabling the model to better understand and manipulate their interactions. We further integrates a dropout hybrid reasoning strategy to achieve efficient control outputs. Experimental results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA significantly outperforms existing methods in terms of task success rate and response speed, exhibiting strong generalization and robustness in open environments and under uncertain instructions.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-08-11T06:01:00Z",
          "authors": [
            "Helong Huang",
            "Min Cen",
            "Kai Tan",
            "Xingyue Quan",
            "Guowei Huang",
            "Hong Zhang"
          ],
          "arxiv_id": "2508.07650",
          "abstract": "Vision-language-action models have emerged as a crucial paradigm in robotic manipulation. However, existing VLA models exhibit notable limitations in handling ambiguous language instructions and unknown environmental states. Furthermore, their perception is largely constrained to static two-dimensional observations, lacking the capability to model three-dimensional interactions between the robot and its environment. To address these challenges, this paper proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's ability to interpret ambiguous instructions and improve task planning, we design a structured Chain-of-Thought reasoning module that integrates high-level task understanding and planning, failed task feedback, and low-level imaginative reasoning about future object positions and robot actions. Additionally, we construct a real-time updatable 3D Pose-Object graph, which captures the spatial configuration of robot joints and the topological relationships between objects in 3D space, enabling the model to better understand and manipulate their interactions. We further integrates a dropout hybrid reasoning strategy to achieve efficient control outputs. Experimental results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA significantly outperforms existing methods in terms of task success rate and response speed, exhibiting strong generalization and robustness in open environments and under uncertain instructions.",
          "abstract_zh": "视觉-语言-动作模型已成为机器人操作的重要范例。然而，现有的 VLA 模型在处理不明确的语言指令和未知的环境状态方面表现出明显的局限性。此外，它们的感知在很大程度上受限于静态二维观察，缺乏对机器人与其环境之间的三维交互进行建模的能力。为了应对这些挑战，本文提出了 GraphCoT-VLA，一种高效的端到端模型。为了增强模型解释模糊指令和改进任务规划的能力，我们设计了一个结构化的思想链推理模块，该模块集成了高级任务理解和规划、失败的任务反馈以及关于未来物体位置和机器人动作的低级想象推理。此外，我们构建了一个实时可更新的 3D 姿态-对象图，它捕获机器人关节的空间配置以及 3D 空间中对象之间的拓扑关系，使模型能够更好地理解和操纵它们的交互。我们进一步集成了 dropout 混合推理策略以实现高效的控制输出。多个现实世界机器人任务的实验结果表明，GraphCoT-VLA 在任务成功率和响应速度方面显着优于现有方法，在开放环境和不确定指令下表现出很强的泛化性和鲁棒性。"
        },
        {
          "title": "ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver",
          "url": "http://arxiv.org/abs/2508.10333v1",
          "snippet": "Recent advances in Vision-Language-Action (VLA) models have enabled robotic agents to integrate multimodal understanding with action execution. However, our empirical analysis reveals that current VLAs struggle to allocate visual attention to target regions. Instead, visual attention is always dispersed. To guide the visual attention grounding on the correct target, we propose ReconVLA, a reconstructive VLA model with an implicit grounding paradigm. Conditioned on the model's visual outputs, a diffusion transformer aims to reconstruct the gaze region of the image, which corresponds to the target manipulated objects. This process prompts the VLA model to learn fine-grained representations and accurately allocate visual attention, thus effectively leveraging task-specific visual information and conducting precise manipulation. Moreover, we curate a large-scale pretraining dataset comprising over 100k trajectories and 2 million data samples from open-source robotic datasets, further boosting the model's generalization in visual reconstruction. Extensive experiments in simulation and the real world demonstrate the superiority of our implicit grounding method, showcasing its capabilities of precise manipulation and generalization. Our project page is https://zionchow.github.io/ReconVLA/.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-08-14T04:20:19Z",
          "authors": [
            "Wenxuan Song",
            "Ziyang Zhou",
            "Han Zhao",
            "Jiayi Chen",
            "Pengxiang Ding",
            "Haodong Yan",
            "Yuxin Huang",
            "Feilong Tang",
            "Donglin Wang",
            "Haoang Li"
          ],
          "arxiv_id": "2508.10333",
          "abstract": "Recent advances in Vision-Language-Action (VLA) models have enabled robotic agents to integrate multimodal understanding with action execution. However, our empirical analysis reveals that current VLAs struggle to allocate visual attention to target regions. Instead, visual attention is always dispersed. To guide the visual attention grounding on the correct target, we propose ReconVLA, a reconstructive VLA model with an implicit grounding paradigm. Conditioned on the model's visual outputs, a diffusion transformer aims to reconstruct the gaze region of the image, which corresponds to the target manipulated objects. This process prompts the VLA model to learn fine-grained representations and accurately allocate visual attention, thus effectively leveraging task-specific visual information and conducting precise manipulation. Moreover, we curate a large-scale pretraining dataset comprising over 100k trajectories and 2 million data samples from open-source robotic datasets, further boosting the model's generalization in visual reconstruction. Extensive experiments in simulation and the real world demonstrate the superiority of our implicit grounding method, showcasing its capabilities of precise manipulation and generalization. Our project page is https://zionchow.github.io/ReconVLA/.",
          "abstract_zh": "视觉-语言-动作（VLA）模型的最新进展使机器人代理能够将多模式理解与动作执行相结合。然而，我们的实证分析表明，当前的 VLA 很难将视觉注意力分配到目标区域。相反，视觉注意力总是分散的。为了引导视觉注意扎根于正确的目标，我们提出了 ReconVLA，一种具有隐式扎根范式的重构 VLA 模型。根据模型的视觉输出，扩散变换器旨在重建图像的注视区域，该区域对应于目标操纵对象。这个过程促使VLA模型学习细粒度的表示并准确地分配视觉注意力，从而有效地利用特定任务的视觉信息并进行精确的操作。此外，我们还策划了一个大规模预训练数据集，其中包含来自开源机器人数据集的超过 10 万条轨迹和 200 万个数据样本，进一步提高了模型在视觉重建方面的泛化能力。模拟和现实世界中的大量实验证明了我们的隐式接地方法的优越性，展示了其精确操作和泛化的能力。我们的项目页面是 https://zionchow.github.io/ReconVLA/。"
        },
        {
          "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2508.19257v3",
          "snippet": "Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\\% vs 68.4\\% baseline), cross-environment validation on SimplerEnv (4.8\\% relative improvement), and 8.7\\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-08-15T12:03:34Z",
          "authors": [
            "Chenghao Liu",
            "Jiachen Zhang",
            "Chengxuan Li",
            "Zhimu Zhou",
            "Shixin Wu",
            "Songfang Huang",
            "Huiling Duan"
          ],
          "arxiv_id": "2508.19257",
          "abstract": "Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\\% vs 68.4\\% baseline), cross-environment validation on SimplerEnv (4.8\\% relative improvement), and 8.7\\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在每个时间步独立处理视觉输入，丢弃机器人操作任务中固有的有价值的时间信息。这种逐帧处理使模型容易受到视觉噪声的影响，同时忽略操作序列中连续帧之间的实质性一致性。我们提出了 Temporal Token Fusion (TTF)，这是一种无需训练的方法，可以智能地整合历史和当前的视觉表示，以提高 VLA 推理质量。我们的方法采用二维检测，将有效的灰度像素差异分析与基于注意的语义相关性评估相结合，通过硬融合策略和关键帧锚定实现选择性时间标记融合，以防止错误累积。LIBERO、SimplerEnv 和真实机器人任务的综合实验证明了一致的改进：LIBERO 平均提高 4.0 个百分点（72.4% vs 68.4% 基线），SimplerEnv 的跨环境验证（相对改进 4.8%），实际机器人任务相对改进 8.7%。事实证明，我们的方法与模型无关，可跨 OpenVLA 和 VLA-Cache 架构工作。值得注意的是，TTF 揭示了注意力机制中选择性查询矩阵重用可以增强而不是损害性能，这为直接 KQV 矩阵重用策略提出了有前途的方向，这些策略可以在提高任务成功率的同时实现计算加速。"
        },
        {
          "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation",
          "url": "http://arxiv.org/abs/2508.07770v2",
          "snippet": "We introduce AgentWorld, an interactive simulation platform for developing household mobile manipulation capabilities. Our platform combines automated scene construction that encompasses layout generation, semantic asset placement, visual material configuration, and physics simulation, with a dual-mode teleoperation system supporting both wheeled bases and humanoid locomotion policies for data collection. The resulting AgentWorld Dataset captures diverse tasks ranging from primitive actions (pick-and-place, push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.) across living rooms, bedrooms, and kitchens. Through extensive benchmarking of imitation learning methods including behavior cloning, action chunking transformers, diffusion policies, and vision-language-action models, we demonstrate the dataset's effectiveness for sim-to-real transfer. The integrated system provides a comprehensive solution for scalable robotic skill acquisition in complex home environments, bridging the gap between simulation-based training and real-world deployment. The code, datasets will be available at https://yizhengzhang1.github.io/agent_world/",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-08-11T08:56:19Z",
          "authors": [
            "Yizheng Zhang",
            "Zhenjun Yu",
            "Jiaxin Lai",
            "Cewu Lu",
            "Lei Han"
          ],
          "arxiv_id": "2508.07770",
          "abstract": "We introduce AgentWorld, an interactive simulation platform for developing household mobile manipulation capabilities. Our platform combines automated scene construction that encompasses layout generation, semantic asset placement, visual material configuration, and physics simulation, with a dual-mode teleoperation system supporting both wheeled bases and humanoid locomotion policies for data collection. The resulting AgentWorld Dataset captures diverse tasks ranging from primitive actions (pick-and-place, push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.) across living rooms, bedrooms, and kitchens. Through extensive benchmarking of imitation learning methods including behavior cloning, action chunking transformers, diffusion policies, and vision-language-action models, we demonstrate the dataset's effectiveness for sim-to-real transfer. The integrated system provides a comprehensive solution for scalable robotic skill acquisition in complex home environments, bridging the gap between simulation-based training and real-world deployment. The code, datasets will be available at https://yizhengzhang1.github.io/agent_world/",
          "abstract_zh": "我们推出 AgentWorld，一个用于开发家庭移动操控功能的交互式模拟平台。我们的平台将自动化场景构建（包括布局生成、语义资产放置、视觉材料配置和物理模拟）与支持轮式底座和用于数据收集的人形运动策略的双模式远程操作系统相结合。由此产生的 AgentWorld 数据集捕获了各种任务，从原始动作（拾放、推拉等）到客厅、卧室和厨房的多阶段活动（提供饮料、加热食物等）。通过对模仿学习方法（包括行为克隆、动作分块转换器、扩散策略和视觉-语言-动作模型）进行广泛的基准测试，我们证明了数据集在模拟到真实迁移方面的有效性。该集成系统为复杂家庭环境中可扩展的机器人技能获取提供了全面的解决方案，弥合了基于模拟的培训和实际部署之间的差距。代码、数据集可在 https://yizhengzhang1.github.io/agent_world/ 获取"
        },
        {
          "title": "Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy",
          "url": "http://arxiv.org/abs/2508.13103v1",
          "snippet": "Vision-Language-Action (VLA) models frequently encounter challenges in generalizing to real-world environments due to inherent discrepancies between observation and action spaces. Although training data are collected from diverse camera perspectives, the models typically predict end-effector poses within the robot base coordinate frame, resulting in spatial inconsistencies. To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA) framework, which grounds action predictions directly in the camera observation space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms end-effector poses from the robot base coordinate system into the camera coordinate system, thereby unifying prediction targets across heterogeneous viewpoints. This lightweight, plug-and-play strategy ensures robust alignment between perception and action, substantially improving model resilience to camera viewpoint variations. The proposed approach is readily compatible with existing VLA architectures, requiring no substantial modifications. Comprehensive evaluations on both simulated and real-world robotic manipulation tasks demonstrate that OC-VLA accelerates convergence, enhances task success rates, and improves cross-view generalization. The code will be publicly available.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-08-18T17:10:45Z",
          "authors": [
            "Tianyi Zhang",
            "Haonan Duan",
            "Haoran Hao",
            "Yu Qiao",
            "Jifeng Dai",
            "Zhi Hou"
          ],
          "arxiv_id": "2508.13103",
          "abstract": "Vision-Language-Action (VLA) models frequently encounter challenges in generalizing to real-world environments due to inherent discrepancies between observation and action spaces. Although training data are collected from diverse camera perspectives, the models typically predict end-effector poses within the robot base coordinate frame, resulting in spatial inconsistencies. To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA) framework, which grounds action predictions directly in the camera observation space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms end-effector poses from the robot base coordinate system into the camera coordinate system, thereby unifying prediction targets across heterogeneous viewpoints. This lightweight, plug-and-play strategy ensures robust alignment between perception and action, substantially improving model resilience to camera viewpoint variations. The proposed approach is readily compatible with existing VLA architectures, requiring no substantial modifications. Comprehensive evaluations on both simulated and real-world robotic manipulation tasks demonstrate that OC-VLA accelerates convergence, enhances task success rates, and improves cross-view generalization. The code will be publicly available.",
          "abstract_zh": "由于观察和行动空间之间固有的差异，视觉-语言-行动（VLA）模型在推广到现实世界环境时经常遇到挑战。尽管训练数据是从不同的相机角度收集的，但模型通常会预测机器人基础坐标系内的末端执行器姿势，从而导致空间不一致。为了缓解这一限制，我们引入了以观察为中心的 VLA (OC-VLA) 框架，该框架直接在摄像机观察空间中进行动作预测。利用相机的外部校准矩阵，OC-VLA 将末端执行器姿态从机器人基础坐标系转换到相机坐标系，从而统一跨异构视点的预测目标。这种轻量级、即插即用的策略可确保感知和动作之间的稳健对齐，从而显着提高模型对摄像机视点变化的适应能力。所提出的方法很容易与现有的 VLA 架构兼容，无需进行实质性修改。对模拟和现实世界机器人操作任务的综合评估表明，OC-VLA 可以加速收敛、提高任务成功率并提高跨视图泛化能力。该代码将公开。"
        },
        {
          "title": "GeoVLA: Empowering 3D Representations in Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2508.09071v2",
          "snippet": "Vision-Language-Action (VLA) models have emerged as a promising approach for enabling robots to follow language instructions and predict corresponding actions. However, current VLA models mainly rely on 2D visual inputs, neglecting the rich geometric information in the 3D physical world, which limits their spatial awareness and adaptability. In this paper, we present GeoVLA, a novel VLA framework that effectively integrates 3D information to advance robotic manipulation. It uses a vision-language model (VLM) to process images and language instructions,extracting fused vision-language embeddings. In parallel, it converts depth maps into point clouds and employs a customized point encoder, called Point Embedding Network, to generate 3D geometric embeddings independently. These produced embeddings are then concatenated and processed by our proposed spatial-aware action expert, called 3D-enhanced Action Expert, which combines information from different sensor modalities to produce precise action sequences. Through extensive experiments in both simulation and real-world environments, GeoVLA demonstrates superior performance and robustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2 simulation benchmarks and shows remarkable robustness in real-world tasks requiring height adaptability, scale awareness and viewpoint invariance.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-08-12T16:46:05Z",
          "authors": [
            "Lin Sun",
            "Bin Xie",
            "Yingfei Liu",
            "Hao Shi",
            "Tiancai Wang",
            "Jiale Cao"
          ],
          "arxiv_id": "2508.09071",
          "abstract": "Vision-Language-Action (VLA) models have emerged as a promising approach for enabling robots to follow language instructions and predict corresponding actions. However, current VLA models mainly rely on 2D visual inputs, neglecting the rich geometric information in the 3D physical world, which limits their spatial awareness and adaptability. In this paper, we present GeoVLA, a novel VLA framework that effectively integrates 3D information to advance robotic manipulation. It uses a vision-language model (VLM) to process images and language instructions,extracting fused vision-language embeddings. In parallel, it converts depth maps into point clouds and employs a customized point encoder, called Point Embedding Network, to generate 3D geometric embeddings independently. These produced embeddings are then concatenated and processed by our proposed spatial-aware action expert, called 3D-enhanced Action Expert, which combines information from different sensor modalities to produce precise action sequences. Through extensive experiments in both simulation and real-world environments, GeoVLA demonstrates superior performance and robustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2 simulation benchmarks and shows remarkable robustness in real-world tasks requiring height adaptability, scale awareness and viewpoint invariance.",
          "abstract_zh": "视觉-语言-动作（VLA）模型已成为一种有前途的方法，使机器人能够遵循语言指令并预测相应的动作。然而，当前的VLA模型主要依赖于2D视觉输入，忽略了3D物理世界中丰富的几何信息，这限制了它们的空间感知和适应性。在本文中，我们提出了 GeoVLA，这是一种新颖的 VLA 框架，可以有效地集成 3D 信息以推进机器人操作。它使用视觉语言模型（VLM）来处理图像和语言指令，提取融合的视觉语言嵌入。同时，它将深度图转换为点云，并采用称为点嵌入网络的定制点编码器来独立生成 3D 几何嵌入。然后，我们提出的空间感知动作专家（称为 3D 增强动作专家）对这些生成的嵌入进行连接和处理，它结合了来自不同传感器模式的信息以产生精确的动作序列。通过在模拟和现实环境中进行大量实验，GeoVLA 展示了卓越的性能和鲁棒性。它在 LIBERO 和 ManiSkill2 模拟基准中取得了最先进的结果，并在需要高度适应性、尺度感知和视点不变性的现实任务中表现出卓越的鲁棒性。"
        },
        {
          "title": "Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search",
          "url": "http://arxiv.org/abs/2508.12211v2",
          "snippet": "Pre-trained vision-language-action (VLA) models offer a promising foundation for generalist robot policies, but often produce brittle behaviors or unsafe failures when deployed zero-shot in out-of-distribution scenarios. We present Vision-Language-Action Planning & Search (VLAPS) -- a novel framework and accompanying algorithms that embed model-based search into the inference procedure of pre-trained VLA policies to improve their performance on robotic tasks. Specifically, our method biases a modified Monte Carlo Tree Search (MCTS) algorithm -- run using a model of the target environment -- using action priors defined by the VLA policy. By using VLA-derived abstractions and priors in model-based search, VLAPS efficiently explores language-conditioned robotics tasks whose search spaces would otherwise be intractably large. Conversely, by integrating model-based search with the VLA policy's inference procedure, VLAPS yields behaviors that are more performant than those obtained by directly following the VLA policy's action predictions. VLAPS offers a principled framework to: i) control test-time compute in VLA models, ii) leverage a priori knowledge of the robotic environment, and iii) integrate established planning and reinforcement learning techniques into the VLA inference process. Across all experiments, VLAPS significantly outperforms VLA-only baselines on language-specified tasks that would otherwise be intractable for uninformed search algorithms, increasing success rates by as much as 67 percentage points.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-08-17T02:59:42Z",
          "authors": [
            "Cyrus Neary",
            "Omar G. Younis",
            "Artur Kuramshin",
            "Ozgur Aslan",
            "Glen Berseth"
          ],
          "arxiv_id": "2508.12211",
          "abstract": "Pre-trained vision-language-action (VLA) models offer a promising foundation for generalist robot policies, but often produce brittle behaviors or unsafe failures when deployed zero-shot in out-of-distribution scenarios. We present Vision-Language-Action Planning & Search (VLAPS) -- a novel framework and accompanying algorithms that embed model-based search into the inference procedure of pre-trained VLA policies to improve their performance on robotic tasks. Specifically, our method biases a modified Monte Carlo Tree Search (MCTS) algorithm -- run using a model of the target environment -- using action priors defined by the VLA policy. By using VLA-derived abstractions and priors in model-based search, VLAPS efficiently explores language-conditioned robotics tasks whose search spaces would otherwise be intractably large. Conversely, by integrating model-based search with the VLA policy's inference procedure, VLAPS yields behaviors that are more performant than those obtained by directly following the VLA policy's action predictions. VLAPS offers a principled framework to: i) control test-time compute in VLA models, ii) leverage a priori knowledge of the robotic environment, and iii) integrate established planning and reinforcement learning techniques into the VLA inference process. Across all experiments, VLAPS significantly outperforms VLA-only baselines on language-specified tasks that would otherwise be intractable for uninformed search algorithms, increasing success rates by as much as 67 percentage points.",
          "abstract_zh": "预训练的视觉-语言-动作（VLA）模型为通用机器人策略提供了有前景的基础，但在分布外场景中零样本部署时通常会产生脆弱的行为或不安全的故障。我们提出了视觉-语言-动作规划和搜索（VLAPS）——一种新颖的框架和配套算法，它将基于模型的搜索嵌入到预先训练的 VLA 策略的推理过程中，以提高其在机器人任务上的性能。具体来说，我们的方法偏向修改后的蒙特卡罗树搜索 (MCTS) 算法（使用目标环境模型运行），使用 VLA 策略定义的操作先验。通过在基于模型的搜索中使用 VLA 派生的抽象和先验，VLAPS 有效地探索了语言条件机器人任务，否则其搜索空间将非常大。相反，通过将基于模型的搜索与 VLA 策略的推理过程相集成，VLAPS 产生的行为比直接遵循 VLA 策略的动作预测所获得的行为性能更高。VLAPS 提供了一个原则框架：i）控制 VLA 模型中的测试时计算，ii）利用机器人环境的先验知识，以及 iii）将已建立的规划和强化学习技术集成到 VLA 推理过程中。在所有实验中，VLAPS 在特定语言的任务上显着优于仅使用 VLA 的基线，否则这些任务对于不知情的搜索算法来说是很困难的，成功率提高了 67 个百分点。"
        },
        {
          "title": "Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning",
          "url": "http://arxiv.org/abs/2508.10399v1",
          "snippet": "Embodied AI aims to develop intelligent systems with physical forms capable of perceiving, decision-making, acting, and learning in real-world environments, providing a promising way to Artificial General Intelligence (AGI). Despite decades of explorations, it remains challenging for embodied agents to achieve human-level intelligence for general-purpose tasks in open dynamic environments. Recent breakthroughs in large models have revolutionized embodied AI by enhancing perception, interaction, planning and learning. In this article, we provide a comprehensive survey on large model empowered embodied AI, focusing on autonomous decision-making and embodied learning. We investigate both hierarchical and end-to-end decision-making paradigms, detailing how large models enhance high-level planning, low-level execution, and feedback for hierarchical decision-making, and how large models enhance Vision-Language-Action (VLA) models for end-to-end decision making. For embodied learning, we introduce mainstream learning methodologies, elaborating on how large models enhance imitation learning and reinforcement learning in-depth. For the first time, we integrate world models into the survey of embodied AI, presenting their design methods and critical roles in enhancing decision-making and learning. Though solid advances have been achieved, challenges still exist, which are discussed at the end of this survey, potentially as the further research directions.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-08-14T06:56:16Z",
          "authors": [
            "Wenlong Liang",
            "Rui Zhou",
            "Yang Ma",
            "Bing Zhang",
            "Songlin Li",
            "Yijia Liao",
            "Ping Kuang"
          ],
          "arxiv_id": "2508.10399",
          "abstract": "Embodied AI aims to develop intelligent systems with physical forms capable of perceiving, decision-making, acting, and learning in real-world environments, providing a promising way to Artificial General Intelligence (AGI). Despite decades of explorations, it remains challenging for embodied agents to achieve human-level intelligence for general-purpose tasks in open dynamic environments. Recent breakthroughs in large models have revolutionized embodied AI by enhancing perception, interaction, planning and learning. In this article, we provide a comprehensive survey on large model empowered embodied AI, focusing on autonomous decision-making and embodied learning. We investigate both hierarchical and end-to-end decision-making paradigms, detailing how large models enhance high-level planning, low-level execution, and feedback for hierarchical decision-making, and how large models enhance Vision-Language-Action (VLA) models for end-to-end decision making. For embodied learning, we introduce mainstream learning methodologies, elaborating on how large models enhance imitation learning and reinforcement learning in-depth. For the first time, we integrate world models into the survey of embodied AI, presenting their design methods and critical roles in enhancing decision-making and learning. Though solid advances have been achieved, challenges still exist, which are discussed at the end of this survey, potentially as the further research directions.",
          "abstract_zh": "嵌入式人工智能旨在开发具有物理形式的智能系统，能够在现实世界环境中感知、决策、行动和学习，为通用人工智能（AGI）提供一种有前景的方法。尽管经过了数十年的探索，实体智能体在开放动态环境中实现通用任务的人类水平智能仍然具有挑战性。最近大型模型的突破通过增强感知、交互、规划和学习，彻底改变了具体人工智能。在本文中，我们对大型模型赋能的具体人工智能进行了全面的调查，重点关注自主决策和具体学习。我们研究了分层决策范式和端到端决策范式，详细介绍了大型模型如何增强分层决策的高层规划、低层执行和反馈，以及大型模型如何增强端到端决策的视觉-语言-行动（VLA）模型。对于体现学习，我们介绍了主流的学习方法，深入阐述了大型模型如何增强模仿学习和强化学习。我们首次将世界模型融入到具体人工智能的调查中，展示它们的设计方法以及在增强决策和学习方面的关键作用。尽管已经取得了扎实的进展，但挑战仍然存在，这些挑战将在本次调查的最后进行讨论，可能作为进一步的研究方向。"
        },
        {
          "title": "Leveraging OS-Level Primitives for Robotic Action Management",
          "url": "http://arxiv.org/abs/2508.10259v1",
          "snippet": "End-to-end imitation learning frameworks (e.g., VLA) are increasingly prominent in robotics, as they enable rapid task transfer by learning directly from perception to control, eliminating the need for complex hand-crafted features. However, even when employing SOTA VLA-based models, they still exhibit limited generalization capabilities and suboptimal action efficiency, due to the constraints imposed by insufficient robotic training datasets. In addition to addressing this problem using model-based approaches, we observe that robotic action slices, which consist of contiguous action steps, exhibit strong analogies to the time slices of threads in traditional operating systems. This insight presents a novel opportunity to tackle the problem at the system level.\n  In this paper, we propose AMS, a robot action management system enhanced with OS-level primitives like exception, context switch and record-and-replay, that improves both execution efficiency and success rates of robotic tasks. AMS first introduces action exception, which facilitates the immediate interruption of robotic actions to prevent error propagation. Secondly, AMS proposes action context, which eliminates redundant computations for VLA-based models, thereby accelerating execution efficiency in robotic actions. Finally, AMS leverages action replay to facilitate repetitive or similar robotic tasks without the need for re-training efforts. We implement AMS in both an emulated environment and on a real robot platform. The evaluation results demonstrate that AMS significantly enhances the model's generalization ability and action efficiency, achieving task success rate improvements ranging from 7x to 24x and saving end-to-end execution time ranging from 29% to 74% compared to existing robotic system without AMS support.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-08-14T00:57:58Z",
          "authors": [
            "Wenxin Zheng",
            "Boyang Li",
            "Bin Xu",
            "Erhu Feng",
            "Jinyu Gu",
            "Haibo Chen"
          ],
          "arxiv_id": "2508.10259",
          "abstract": "End-to-end imitation learning frameworks (e.g., VLA) are increasingly prominent in robotics, as they enable rapid task transfer by learning directly from perception to control, eliminating the need for complex hand-crafted features. However, even when employing SOTA VLA-based models, they still exhibit limited generalization capabilities and suboptimal action efficiency, due to the constraints imposed by insufficient robotic training datasets. In addition to addressing this problem using model-based approaches, we observe that robotic action slices, which consist of contiguous action steps, exhibit strong analogies to the time slices of threads in traditional operating systems. This insight presents a novel opportunity to tackle the problem at the system level. In this paper, we propose AMS, a robot action management system enhanced with OS-level primitives like exception, context switch and record-and-replay, that improves both execution efficiency and success rates of robotic tasks. AMS first introduces action exception, which facilitates the immediate interruption of robotic actions to prevent error propagation. Secondly, AMS proposes action context, which eliminates redundant computations for VLA-based models, thereby accelerating execution efficiency in robotic actions. Finally, AMS leverages action replay to facilitate repetitive or similar robotic tasks without the need for re-training efforts. We implement AMS in both an emulated environment and on a real robot platform. The evaluation results demonstrate that AMS significantly enhances the model's generalization ability and action efficiency, achieving task success rate improvements ranging from 7x to 24x and saving end-to-end execution time ranging from 29% to 74% compared to existing robotic system without AMS support.",
          "abstract_zh": "端到端模仿学习框架（例如 VLA）在机器人技术中越来越重要，因为它们通过直接从感知到控制的学习来实现快速任务转移，从而消除了对复杂手工制作功能的需求。然而，即使采用基于 SOTA VLA 的模型，由于机器人训练数据集不足所带来的限制，它们仍然表现出有限的泛化能力和次优的动作效率。除了使用基于模型的方法解决这个问题之外，我们还观察到由连续动作步骤组成的机器人动作片与传统操作系统中线程的时间片表现出很强的相似性。这一见解提供了在系统级别解决问题的新机会。在本文中，我们提出了 AMS，这是一种通过操作系统级原语（如异常、上下文切换和记录和重放）增强的机器人动作管理系统，可提高机器人任务的执行效率和成功率。AMS首先引入了动作异常，便于立即中断机器人动作，防止错误传播。其次，AMS提出了动作上下文，消除了基于VLA的模型的冗余计算，从而加快了机器人动作的执行效率。最后，AMS 利用动作重播来促进重复或类似的机器人任务，而无需重新训练。我们在模拟环境和真实机器人平台上实施 AMS。评估结果表明，与没有AMS支持的现有机器人系统相比，AMS显着增强了模型的泛化能力和动作效率，任务成功率提高了7倍至24倍，端到端执行时间节省了29%至74%。"
        },
        {
          "title": "Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding",
          "url": "http://arxiv.org/abs/2508.09032v1",
          "snippet": "Vision-Language-Action models have demonstrated remarkable capabilities in predicting agent movements within virtual environments and real-world scenarios based on visual observations and textual instructions. Although recent research has focused on enhancing spatial and temporal understanding independently, this paper presents a novel approach that integrates both aspects through visual prompting. We introduce a method that projects visual traces of key points from observations onto depth maps, enabling models to capture both spatial and temporal information simultaneously. The experiments in SimplerEnv show that the mean number of tasks successfully solved increased for 4% compared to SpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this enhancement can be achieved with minimal training data, making it particularly valuable for real-world applications where data collection is challenging. The project page is available at https://ampiromax.github.io/ST-VLA.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-08-12T15:53:45Z",
          "authors": [
            "Maxim A. Patratskiy",
            "Alexey K. Kovalev",
            "Aleksandr I. Panov"
          ],
          "arxiv_id": "2508.09032",
          "abstract": "Vision-Language-Action models have demonstrated remarkable capabilities in predicting agent movements within virtual environments and real-world scenarios based on visual observations and textual instructions. Although recent research has focused on enhancing spatial and temporal understanding independently, this paper presents a novel approach that integrates both aspects through visual prompting. We introduce a method that projects visual traces of key points from observations onto depth maps, enabling models to capture both spatial and temporal information simultaneously. The experiments in SimplerEnv show that the mean number of tasks successfully solved increased for 4% compared to SpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this enhancement can be achieved with minimal training data, making it particularly valuable for real-world applications where data collection is challenging. The project page is available at https://ampiromax.github.io/ST-VLA.",
          "abstract_zh": "视觉-语言-动作模型在基于视觉观察和文本指令预测虚拟环境和现实场景中的代理运动方面表现出了卓越的能力。尽管最近的研究侧重于独立地增强空间和时间理解，但本文提出了一种通过视觉提示将这两个方面整合在一起的新颖方法。我们引入了一种方法，将观察到的关键点的视觉痕迹投影到深度图上，使模型能够同时捕获空间和时间信息。SimplerEnv 中的实验表明，与 SpatialVLA 相比，成功解决的任务的平均数量增加了 4%，与 TraceVLA 相比增加了 19%。此外，我们表明这种增强可以用最少的训练数据来实现，这使得它对于数据收集具有挑战性的实际应用程序特别有价值。该项目页面位于 https://ampiromax.github.io/ST-VLA。"
        },
        {
          "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model",
          "url": "http://arxiv.org/abs/2508.10416v1",
          "snippet": "Existing vision-and-language navigation models often deviate from the correct trajectory when executing instructions. However, these models lack effective error correction capability, hindering their recovery from errors. To address this challenge, we propose Self-correction Flywheel, a novel post-training paradigm. Instead of considering the model's error trajectories on the training set as a drawback, our paradigm emphasizes their significance as a valuable data source. We have developed a method to identify deviations in these error trajectories and devised innovative techniques to automatically generate self-correction data for perception and action. These self-correction data serve as fuel to power the model's continued training. The brilliance of our paradigm is revealed when we re-evaluate the model on the training set, uncovering new error trajectories. At this time, the self-correction flywheel begins to spin. Through multiple flywheel iterations, we progressively enhance our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2% and 16.4%. Real robot tests in various indoor and outdoor environments demonstrate \\method's superior capability of error correction, dynamic obstacle avoidance, and long instruction following.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-08-14T07:39:26Z",
          "authors": [
            "Zhuoyuan Yu",
            "Yuxing Long",
            "Zihan Yang",
            "Chengyan Zeng",
            "Hongwei Fan",
            "Jiyao Zhang",
            "Hao Dong"
          ],
          "arxiv_id": "2508.10416",
          "abstract": "Existing vision-and-language navigation models often deviate from the correct trajectory when executing instructions. However, these models lack effective error correction capability, hindering their recovery from errors. To address this challenge, we propose Self-correction Flywheel, a novel post-training paradigm. Instead of considering the model's error trajectories on the training set as a drawback, our paradigm emphasizes their significance as a valuable data source. We have developed a method to identify deviations in these error trajectories and devised innovative techniques to automatically generate self-correction data for perception and action. These self-correction data serve as fuel to power the model's continued training. The brilliance of our paradigm is revealed when we re-evaluate the model on the training set, uncovering new error trajectories. At this time, the self-correction flywheel begins to spin. Through multiple flywheel iterations, we progressively enhance our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2% and 16.4%. Real robot tests in various indoor and outdoor environments demonstrate \\method's superior capability of error correction, dynamic obstacle avoidance, and long instruction following.",
          "abstract_zh": "现有的视觉和语言导航模型在执行指令时经常偏离正确的轨迹。然而，这些模型缺乏有效的纠错能力，阻碍了它们从错误中恢复。为了应对这一挑战，我们提出了自我校正飞轮，一种新颖的训练后范例。我们的范式没有将训练集上的模型错误轨迹视为缺点，而是强调它们作为有价值的数据源的重要性。我们开发了一种方法来识别这些错误轨迹中的偏差，并设计了创新技术来自动生成感知和行动的自我校正数据。这些自我修正数据可以作为模型持续训练的燃料。当我们在训练集上重新评估模型并发现新的错误轨迹时，我们的范式的卓越之处就显现出来了。此时，自纠偏飞轮开始旋转。通过多次飞轮迭代，我们逐步增强了基于单目 RGB 的 VLA 导航模型 CorrectNav。R2R-CE 和 RxR-CE 基准测试表明 CorrectNav 取得了最先进的成功率，分别为 65.1% 和 69.3%，比之前最好的 VLA 导航模型高出 8.2% 和 16.4%。在各种室内外环境下的真实机器人测试证明了该方法具有卓越的纠错、动态避障和长时间指令跟随能力。"
        },
        {
          "title": "OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing",
          "url": "http://arxiv.org/abs/2508.08706v2",
          "snippet": "Recent vision-language-action (VLA) models build upon vision-language foundations, and have achieved promising results and exhibit the possibility of task generalization in robot manipulation. However, due to the heterogeneity of tactile sensors and the difficulty of acquiring tactile data, current VLA models significantly overlook the importance of tactile perception and fail in contact-rich tasks. To address this issue, this paper proposes OmniVTLA, a novel architecture involving tactile sensing. Specifically, our contributions are threefold. First, our OmniVTLA features a dual-path tactile encoder framework. This framework enhances tactile perception across diverse vision-based and force-based tactile sensors by using a pretrained vision transformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we introduce ObjTac, a comprehensive force-based tactile dataset capturing textual, visual, and tactile information for 56 objects across 10 categories. With 135K tri-modal samples, ObjTac supplements existing visuo-tactile datasets. Third, leveraging this dataset, we train a semantically-aligned tactile encoder to learn a unified tactile representation, serving as a better initialization for OmniVTLA. Real-world experiments demonstrate substantial improvements over state-of-the-art VLA baselines, achieving 96.9% success rates with grippers, (21.9% higher over baseline) and 100% success rates with dexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides, OmniVTLA significantly reduces task completion time and generates smoother trajectories through tactile sensing compared to existing VLA. Our ObjTac dataset can be found at https://readerek.github.io/Objtac.github.io",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-08-12T07:53:36Z",
          "authors": [
            "Zhengxue Cheng",
            "Yiqian Zhang",
            "Wenkang Zhang",
            "Haoyu Li",
            "Keyu Wang",
            "Li Song",
            "Hengdi Zhang"
          ],
          "arxiv_id": "2508.08706",
          "abstract": "Recent vision-language-action (VLA) models build upon vision-language foundations, and have achieved promising results and exhibit the possibility of task generalization in robot manipulation. However, due to the heterogeneity of tactile sensors and the difficulty of acquiring tactile data, current VLA models significantly overlook the importance of tactile perception and fail in contact-rich tasks. To address this issue, this paper proposes OmniVTLA, a novel architecture involving tactile sensing. Specifically, our contributions are threefold. First, our OmniVTLA features a dual-path tactile encoder framework. This framework enhances tactile perception across diverse vision-based and force-based tactile sensors by using a pretrained vision transformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we introduce ObjTac, a comprehensive force-based tactile dataset capturing textual, visual, and tactile information for 56 objects across 10 categories. With 135K tri-modal samples, ObjTac supplements existing visuo-tactile datasets. Third, leveraging this dataset, we train a semantically-aligned tactile encoder to learn a unified tactile representation, serving as a better initialization for OmniVTLA. Real-world experiments demonstrate substantial improvements over state-of-the-art VLA baselines, achieving 96.9% success rates with grippers, (21.9% higher over baseline) and 100% success rates with dexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides, OmniVTLA significantly reduces task completion time and generates smoother trajectories through tactile sensing compared to existing VLA. Our ObjTac dataset can be found at https://readerek.github.io/Objtac.github.io",
          "abstract_zh": "最近的视觉-语言-动作（VLA）模型建立在视觉-语言基础上，并取得了有希望的结果，并展示了机器人操作中任务泛化的可能性。然而，由于触觉传感器的异质性和获取触觉数据的困难，当前的 VLA 模型严重忽视了触觉感知的重要性，并且在接触丰富的任务中失败。为了解决这个问题，本文提出了 OmniVTLA，一种涉及触觉传感的新颖架构。具体来说，我们的贡献有三个方面。首先，我们的 OmniVTLA 具有双路径触觉编码器框架。该框架通过使用预训练的视觉变换器 (ViT) 和语义对齐的触觉 ViT (SA-ViT) 来增强各种基于视觉和基于力的触觉传感器的触觉感知。其次，我们介绍 ObjTac，这是一个基于力的综合触觉数据集，可捕获 10 个类别的 56 个物体的文本、视觉和触觉信息。ObjTac 拥有 135K 三模态样本，补充了现有的视觉触觉数据集。第三，利用该数据集，我们训练语义对齐的触觉编码器来学习统一的触觉表示，为 OmniVTLA 提供更好的初始化。现实世界的实验表明，与最先进的 VLA 基线相比有了显着的改进，在拾取和放置任务中，使用夹具实现了 96.9% 的成功率（比基线高出 21.9%），使用灵巧的手实现了 100% 的成功率（比基线高出 6.2%）。此外，与现有的 VLA 相比，OmniVTLA 显着缩短了任务完成时间，并通过触觉感应生成更平滑的轨迹。我们的 ObjTac 数据集可以在 https://readerek.github.io/Objtac.github.io 找到"
        },
        {
          "title": "Human Centric General Physical Intelligence for Agile Manufacturing Automation",
          "url": "http://arxiv.org/abs/2508.11960v2",
          "snippet": "Agile human-centric manufacturing increasingly requires resilient robotic solutions that are capable of safe and productive interactions within unstructured environments of modern factories. While multi-modal sensor fusion provides comprehensive situational awareness yet robots must also contextualize their reasoning to achieve deep semantic understanding of complex scenes. Foundation model particularly Vision-Language-Action (VLA) models have emerged as promising approach on integrating diverse perceptual modalities and spatio-temporal reasoning abilities to ground physical actions to realize General Physical Intelligence (GPI) across various robotic embodiments. Although GPI has been conceptually discussed in literature but its pivotal role and practical deployment in agile manufacturing remain underexplored. To address this gap, this practical review systematically surveys recent advances in VLA models through the lens of GPI by offering comparative analysis of leading implementations and evaluating their industrial readiness via structured ablation study. The state of the art is organized into six thematic pillars including multisensory representation learning, sim2real transfer, planning and control, uncertainty and safety measures and benchmarking. Finally, the review highlights open challenges and future directions for integrating GPI into industrial ecosystems to align with the vision of Industry 5.0 for intelligent, adaptive and collaborative manufacturing ecosystem.",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-08-16T07:43:11Z",
          "authors": [
            "Sandeep Kanta",
            "Mehrdad Tavassoli",
            "Varun Teja Chirkuri",
            "Venkata Akhil Kumar",
            "Santhi Bharath Punati",
            "Praveen Damacharla",
            "Sunny Katyara"
          ],
          "arxiv_id": "2508.11960",
          "abstract": "Agile human-centric manufacturing increasingly requires resilient robotic solutions that are capable of safe and productive interactions within unstructured environments of modern factories. While multi-modal sensor fusion provides comprehensive situational awareness yet robots must also contextualize their reasoning to achieve deep semantic understanding of complex scenes. Foundation model particularly Vision-Language-Action (VLA) models have emerged as promising approach on integrating diverse perceptual modalities and spatio-temporal reasoning abilities to ground physical actions to realize General Physical Intelligence (GPI) across various robotic embodiments. Although GPI has been conceptually discussed in literature but its pivotal role and practical deployment in agile manufacturing remain underexplored. To address this gap, this practical review systematically surveys recent advances in VLA models through the lens of GPI by offering comparative analysis of leading implementations and evaluating their industrial readiness via structured ablation study. The state of the art is organized into six thematic pillars including multisensory representation learning, sim2real transfer, planning and control, uncertainty and safety measures and benchmarking. Finally, the review highlights open challenges and future directions for integrating GPI into industrial ecosystems to align with the vision of Industry 5.0 for intelligent, adaptive and collaborative manufacturing ecosystem.",
          "abstract_zh": "以人为本的敏捷制造越来越需要有弹性的机器人解决方案，这些解决方案能够在现代工厂的非结构化环境中进行安全和高效的交互。虽然多模态传感器融合提供了全面的态势感知，但机器人还必须将其推理置于情境中，以实现对复杂场景的深入语义理解。基础模型，特别是视觉-语言-动作（VLA）模型已经成为一种有前景的方法，可以整合不同的感知模式和时空推理能力，为物理动作提供基础，从而在各种机器人实施例中实现通用物理智能（GPI）。尽管 GPI 已在文献中进行了概念性讨论，但其在敏捷制造中的关键作用和实际部署仍有待探索。为了弥补这一差距，本次实践综述通过 GPI 的视角系统地调查了 VLA 模型的最新进展，提供了对领先实施方案的比较分析，并通过结构化消融研究评估了其工业准备情况。最先进的技术分为六个主题支柱，包括多感官表征学习、sim2real 迁移、规划和控制、不确定性和安全措施以及基准测试。最后，该评论强调了将 GPI 集成到工业生态系统中的开放挑战和未来方向，以符合工业 5.0 智能、自适应和协作制造生态系统的愿景。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 10 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 10）。",
      "items": [
        {
          "title": "patrick-llgc/Learning-Deep-Learning",
          "url": "https://github.com/patrick-llgc/Learning-Deep-Learning",
          "snippet": "Paper reading notes on Deep Learning and Machine Learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "ReinFlow/ReinFlow",
          "url": "https://github.com/ReinFlow/ReinFlow",
          "snippet": "[NeurIPS 2025] Flow x RL. \"ReinFlow: Fine-tuning Flow Policy with Online Reinforcement Learning\". Support VLAs e.g., pi0, pi0.5. Fully open-sourced. ",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "NVIDIA/Isaac-GR00T",
          "url": "https://github.com/NVIDIA/Isaac-GR00T",
          "snippet": "NVIDIA Isaac GR00T N1.6 -  A Foundation Model for Generalist Robots.",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 9
        },
        {
          "title": "yang-zj1026/NaVILA-Bench",
          "url": "https://github.com/yang-zj1026/NaVILA-Bench",
          "snippet": "Vision-Language Navigation Benchmark in Isaac Lab",
          "site": "github.com",
          "rank": 10
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-08-11",
  "week_end": "2025-08-17",
  "last_updated": "2026-01-07"
}