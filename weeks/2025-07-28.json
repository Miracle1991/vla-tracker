{
  "generated_at": "2026-01-07T13:21:33.110065",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org ä¸Šå…±å‘ç° 8 æ¡ä¸ VLA ç›¸å…³çš„æ›´æ–°å†…å®¹ï¼ˆæŒ‰æœç´¢ç›¸å…³æ€§æ’åºï¼Œæ˜¾ç¤º Top 8ï¼‰ã€‚",
      "items": [
        {
          "title": "FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2508.02190v1",
          "snippet": "Vision-language-action (VLA) models have significantly advanced robotic manipulation by enabling robots to interpret language instructions for task execution. However, training these models often relies on large-scale user-specific data, raising concerns about privacy and security, which in turn limits their broader adoption. To address this, we propose FedVLA, the first federated VLA learning framework, enabling distributed model training that preserves data privacy without compromising performance. Our framework integrates task-aware representation learning, adaptive expert selection, and expert-driven federated aggregation, enabling efficient and privacy-preserving training of VLA models. Specifically, we introduce an Instruction Oriented Scene-Parsing mechanism, which decomposes and enhances object-level features based on task instructions, improving contextual understanding. To effectively learn diverse task patterns, we design a Dual Gating Mixture-of-Experts (DGMoE) mechanism, where not only input tokens but also self-aware experts adaptively decide their activation. Finally, we propose an Expert-Driven Aggregation strategy at the federated server, where model aggregation is guided by activated experts, ensuring effective cross-client knowledge transfer.Extensive simulations and real-world robotic experiments demonstrate the effectiveness of our proposals. Notably, DGMoE significantly improves computational efficiency compared to its vanilla counterpart, while FedVLA achieves task success rates comparable to centralized training, effectively preserving data privacy.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-08-04T08:39:43Z",
          "authors": [
            "Cui Miao",
            "Tao Chang",
            "Meihan Wu",
            "Hongbin Xu",
            "Chun Li",
            "Ming Li",
            "Xiaodong Wang"
          ],
          "arxiv_id": "2508.02190",
          "abstract": "Vision-language-action (VLA) models have significantly advanced robotic manipulation by enabling robots to interpret language instructions for task execution. However, training these models often relies on large-scale user-specific data, raising concerns about privacy and security, which in turn limits their broader adoption. To address this, we propose FedVLA, the first federated VLA learning framework, enabling distributed model training that preserves data privacy without compromising performance. Our framework integrates task-aware representation learning, adaptive expert selection, and expert-driven federated aggregation, enabling efficient and privacy-preserving training of VLA models. Specifically, we introduce an Instruction Oriented Scene-Parsing mechanism, which decomposes and enhances object-level features based on task instructions, improving contextual understanding. To effectively learn diverse task patterns, we design a Dual Gating Mixture-of-Experts (DGMoE) mechanism, where not only input tokens but also self-aware experts adaptively decide their activation. Finally, we propose an Expert-Driven Aggregation strategy at the federated server, where model aggregation is guided by activated experts, ensuring effective cross-client knowledge transfer.Extensive simulations and real-world robotic experiments demonstrate the effectiveness of our proposals. Notably, DGMoE significantly improves computational efficiency compared to its vanilla counterpart, while FedVLA achieves task success rates comparable to centralized training, effectively preserving data privacy.",
          "abstract_zh": "è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä½¿æœºå™¨äººèƒ½å¤Ÿè§£é‡Šè¯­è¨€æŒ‡ä»¤ä»¥æ‰§è¡Œä»»åŠ¡ï¼Œä»è€Œæ˜¾ç€æé«˜äº†æœºå™¨äººæ“ä½œçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿™äº›æ¨¡å‹é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡çš„ç”¨æˆ·ç‰¹å®šæ•°æ®ï¼Œå¼•å‘äº†äººä»¬å¯¹éšç§å’Œå®‰å…¨çš„æ‹…å¿§ï¼Œè¿›è€Œé™åˆ¶äº†å®ƒä»¬çš„æ›´å¹¿æ³›é‡‡ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† FedVLAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè”åˆ VLA å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒåˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒï¼Œåœ¨ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹ä¿æŠ¤æ•°æ®éšç§ã€‚æˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†ä»»åŠ¡æ„ŸçŸ¥è¡¨ç¤ºå­¦ä¹ ã€è‡ªé€‚åº”ä¸“å®¶é€‰æ‹©å’Œä¸“å®¶é©±åŠ¨çš„è”åˆèšåˆï¼Œä»è€Œå®ç°äº† VLA æ¨¡å‹çš„é«˜æ•ˆä¸”ä¿æŠ¤éšç§çš„è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é¢å‘æŒ‡ä»¤çš„åœºæ™¯è§£ææœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®ä»»åŠ¡æŒ‡ä»¤åˆ†è§£å’Œå¢å¼ºå¯¹è±¡çº§ç‰¹å¾ï¼Œä»è€Œæé«˜ä¸Šä¸‹æ–‡ç†è§£ã€‚ä¸ºäº†æœ‰æ•ˆåœ°å­¦ä¹ ä¸åŒçš„ä»»åŠ¡æ¨¡å¼ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŒé‡é—¨æ§ä¸“å®¶æ··åˆï¼ˆDGMoEï¼‰æœºåˆ¶ï¼Œå…¶ä¸­ä¸ä»…è¾“å…¥ä»¤ç‰Œï¼Œè€Œä¸”æœ‰è‡ªæˆ‘æ„è¯†çš„ä¸“å®¶è‡ªé€‚åº”åœ°å†³å®šå…¶æ¿€æ´»ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨è”åˆæœåŠ¡å™¨ä¸Šæå‡ºäº†ä¸“å®¶é©±åŠ¨çš„èšåˆç­–ç•¥ï¼Œå…¶ä¸­æ¨¡å‹èšåˆç”±æ¿€æ´»çš„ä¸“å®¶æŒ‡å¯¼ï¼Œç¡®ä¿æœ‰æ•ˆçš„è·¨å®¢æˆ·ç«¯çŸ¥è¯†ä¼ è¾“ã€‚å¹¿æ³›çš„æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œçš„æœºå™¨äººå®éªŒè¯æ˜äº†æˆ‘ä»¬å»ºè®®çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸æ™®é€šç‰ˆæœ¬ç›¸æ¯”ï¼ŒDGMoE æ˜¾ç€æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œè€Œ FedVLA çš„ä»»åŠ¡æˆåŠŸç‡ä¸é›†ä¸­å¼è®­ç»ƒç›¸å½“ï¼Œæœ‰æ•ˆä¿æŠ¤äº†æ•°æ®éšç§ã€‚"
        },
        {
          "title": "RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2508.02062v1",
          "snippet": "Multi-task ``vision-language-action'' (VLA) models have recently demonstrated increasing promise as generalist foundation models for robotics, achieving non-trivial performance out of the box on new tasks in new environments. However, for such models to be truly useful, an end user must have easy means to teach them to improve. For language and vision models, the emergent ability to perform in-context learning (ICL) has proven to be a versatile and highly useful interface to easily teach new tasks with no parameter finetuning. Unfortunately, VLAs pre-trained with imitation learning objectives do not naturally acquire ICL abilities. In this paper, we demonstrate that, with the right finetuning recipe and a small robot demonstration dataset, it is possible to inject in-context adaptability post hoc into such a VLA. After retraining for in-context learning (RICL), our system permits an end user to provide a small number (10-20) of demonstrations for a new task. RICL then fetches the most relevant portions of those demonstrations into the VLA context to exploit ICL, performing the new task and boosting task performance. We apply RICL to inject ICL into the $Ï€_{0}$-FAST VLA, and show that it permits large in-context improvements for a variety of new manipulation tasks with only 20 demonstrations per task, without any parameter updates. When parameter updates on the target task demonstrations is possible, RICL finetuning further boosts performance. We release code and model weights for RICL-$Ï€_{0}$-FAST alongside the paper to enable, for the first time, a simple in-context learning interface for new manipulation tasks. Website: https://ricl-vla.github.io.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-08-04T05:01:11Z",
          "authors": [
            "Kaustubh Sridhar",
            "Souradeep Dutta",
            "Dinesh Jayaraman",
            "Insup Lee"
          ],
          "arxiv_id": "2508.02062",
          "abstract": "Multi-task ``vision-language-action'' (VLA) models have recently demonstrated increasing promise as generalist foundation models for robotics, achieving non-trivial performance out of the box on new tasks in new environments. However, for such models to be truly useful, an end user must have easy means to teach them to improve. For language and vision models, the emergent ability to perform in-context learning (ICL) has proven to be a versatile and highly useful interface to easily teach new tasks with no parameter finetuning. Unfortunately, VLAs pre-trained with imitation learning objectives do not naturally acquire ICL abilities. In this paper, we demonstrate that, with the right finetuning recipe and a small robot demonstration dataset, it is possible to inject in-context adaptability post hoc into such a VLA. After retraining for in-context learning (RICL), our system permits an end user to provide a small number (10-20) of demonstrations for a new task. RICL then fetches the most relevant portions of those demonstrations into the VLA context to exploit ICL, performing the new task and boosting task performance. We apply RICL to inject ICL into the $Ï€_{0}$-FAST VLA, and show that it permits large in-context improvements for a variety of new manipulation tasks with only 20 demonstrations per task, without any parameter updates. When parameter updates on the target task demonstrations is possible, RICL finetuning further boosts performance. We release code and model weights for RICL-$Ï€_{0}$-FAST alongside the paper to enable, for the first time, a simple in-context learning interface for new manipulation tasks. Website: https://ricl-vla.github.io.",
          "abstract_zh": "å¤šä»»åŠ¡â€œè§†è§‰-è¯­è¨€-åŠ¨ä½œâ€ï¼ˆVLAï¼‰æ¨¡å‹æœ€è¿‘æ˜¾ç¤ºå‡ºä½œä¸ºæœºå™¨äººæŠ€æœ¯é€šç”¨åŸºç¡€æ¨¡å‹çš„å‰æ™¯è¶Šæ¥è¶Šå¤§ï¼Œåœ¨æ–°ç¯å¢ƒä¸­çš„æ–°ä»»åŠ¡ä¸­å®ç°äº†éå‡¡çš„å¼€ç®±å³ç”¨æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¸ºäº†è®©è¿™äº›æ¨¡å‹çœŸæ­£æœ‰ç”¨ï¼Œæœ€ç»ˆç”¨æˆ·å¿…é¡»æœ‰ç®€å•çš„æ–¹æ³•æ¥æ•™ä»–ä»¬æ”¹è¿›ã€‚å¯¹äºè¯­è¨€å’Œè§†è§‰æ¨¡å‹ï¼Œæ‰§è¡Œä¸Šä¸‹æ–‡å­¦ä¹  (ICL) çš„æ–°å…´èƒ½åŠ›å·²è¢«è¯æ˜æ˜¯ä¸€ç§å¤šåŠŸèƒ½ä¸”éå¸¸æœ‰ç”¨çš„ç•Œé¢ï¼Œå¯ä»¥è½»æ¾æ•™æˆæ–°ä»»åŠ¡ï¼Œæ— éœ€å‚æ•°å¾®è°ƒã€‚ä¸å¹¸çš„æ˜¯ï¼Œä»¥æ¨¡ä»¿å­¦ä¹ ä¸ºç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒçš„ VLA å¹¶ä¸èƒ½è‡ªç„¶è·å¾— ICL èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡æ­£ç¡®çš„å¾®è°ƒæ–¹æ³•å’Œå°å‹æœºå™¨äººæ¼”ç¤ºæ•°æ®é›†ï¼Œå¯ä»¥å°†ä¸Šä¸‹æ–‡é€‚åº”æ€§äº‹åæ³¨å…¥åˆ°è¿™æ ·çš„ VLA ä¸­ã€‚ç»è¿‡æƒ…å¢ƒå­¦ä¹  (RICL) çš„å†åŸ¹è®­åï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå…è®¸æœ€ç»ˆç”¨æˆ·ä¸ºæ–°ä»»åŠ¡æä¾›å°‘é‡ï¼ˆ10-20ï¼‰æ¬¡æ¼”ç¤ºã€‚ç„¶åï¼ŒRICL å°†è¿™äº›æ¼”ç¤ºä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†æå–åˆ° VLA ä¸Šä¸‹æ–‡ä¸­ä»¥åˆ©ç”¨ ICLï¼Œæ‰§è¡Œæ–°ä»»åŠ¡å¹¶æé«˜ä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬åº”ç”¨ RICL å°† ICL æ³¨å…¥ $Ï€_{0}$-FAST VLAï¼Œå¹¶è¡¨æ˜å®ƒå…è®¸å¯¹å„ç§æ–°æ“ä½œä»»åŠ¡è¿›è¡Œå¤§é‡ä¸Šä¸‹æ–‡æ”¹è¿›ï¼Œæ¯ä¸ªä»»åŠ¡ä»…è¿›è¡Œ 20 æ¬¡æ¼”ç¤ºï¼Œæ— éœ€ä»»ä½•å‚æ•°æ›´æ–°ã€‚å½“ç›®æ ‡ä»»åŠ¡æ¼”ç¤ºçš„å‚æ•°å¯ä»¥æ›´æ–°æ—¶ï¼ŒRICL å¾®è°ƒå¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬ä¸è®ºæ–‡ä¸€èµ·å‘å¸ƒäº† RICL-$Ï€_{0}$-FAST çš„ä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œé¦–æ¬¡ä¸ºæ–°çš„æ“ä½œä»»åŠ¡æä¾›äº†ä¸€ä¸ªç®€å•çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç•Œé¢ã€‚ç½‘ç«™ï¼šhttps://ricl-vla.github.ioã€‚"
        },
        {
          "title": "XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation",
          "url": "http://arxiv.org/abs/2508.00097v2",
          "snippet": "The rapid advancement of Vision-Language-Action models has created an urgent need for large-scale, high-quality robot demonstration datasets. Although teleoperation is the predominant method for data collection, current approaches suffer from limited scalability, complex setup procedures, and suboptimal data quality. This paper presents XRoboToolkit, a cross-platform framework for extended reality based robot teleoperation built on the OpenXR standard. The system features low-latency stereoscopic visual feedback, optimization-based inverse kinematics, and support for diverse tracking modalities including head, controller, hand, and auxiliary motion trackers. XRoboToolkit's modular architecture enables seamless integration across robotic platforms and simulation environments, spanning precision manipulators, mobile robots, and dexterous hands. We demonstrate the framework's effectiveness through precision manipulation tasks and validate data quality by training VLA models that exhibit robust autonomous performance.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-07-31T18:45:13Z",
          "authors": [
            "Zhigen Zhao",
            "Liuchuan Yu",
            "Ke Jing",
            "Ning Yang"
          ],
          "arxiv_id": "2508.00097",
          "abstract": "The rapid advancement of Vision-Language-Action models has created an urgent need for large-scale, high-quality robot demonstration datasets. Although teleoperation is the predominant method for data collection, current approaches suffer from limited scalability, complex setup procedures, and suboptimal data quality. This paper presents XRoboToolkit, a cross-platform framework for extended reality based robot teleoperation built on the OpenXR standard. The system features low-latency stereoscopic visual feedback, optimization-based inverse kinematics, and support for diverse tracking modalities including head, controller, hand, and auxiliary motion trackers. XRoboToolkit's modular architecture enables seamless integration across robotic platforms and simulation environments, spanning precision manipulators, mobile robots, and dexterous hands. We demonstrate the framework's effectiveness through precision manipulation tasks and validate data quality by training VLA models that exhibit robust autonomous performance.",
          "abstract_zh": "è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„å¿«é€Ÿå‘å±•è¿«åˆ‡éœ€è¦å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æœºå™¨äººæ¼”ç¤ºæ•°æ®é›†ã€‚å°½ç®¡è¿œç¨‹æ“ä½œæ˜¯æ•°æ®æ”¶é›†çš„ä¸»è¦æ–¹æ³•ï¼Œä½†å½“å‰çš„æ–¹æ³•å­˜åœ¨å¯æ‰©å±•æ€§æœ‰é™ã€è®¾ç½®è¿‡ç¨‹å¤æ‚å’Œæ•°æ®è´¨é‡æ¬ ä½³çš„é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº† XRoboToolkitï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº OpenXR æ ‡å‡†çš„åŸºäºæ‰©å±•ç°å®çš„æœºå™¨äººè¿œç¨‹æ“ä½œçš„è·¨å¹³å°æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿå…·æœ‰ä½å»¶è¿Ÿç«‹ä½“è§†è§‰åé¦ˆã€åŸºäºä¼˜åŒ–çš„é€†è¿åŠ¨å­¦ï¼Œå¹¶æ”¯æŒå¤šç§è·Ÿè¸ªæ¨¡å¼ï¼ŒåŒ…æ‹¬å¤´éƒ¨ã€æ§åˆ¶å™¨ã€æ‰‹éƒ¨å’Œè¾…åŠ©è¿åŠ¨è·Ÿè¸ªå™¨ã€‚XRoboToolkit çš„æ¨¡å—åŒ–æ¶æ„å¯å®ç°è·¨æœºå™¨äººå¹³å°å’Œæ¨¡æ‹Ÿç¯å¢ƒçš„æ— ç¼é›†æˆï¼Œæ¶µç›–ç²¾å¯†æœºæ¢°æ‰‹ã€ç§»åŠ¨æœºå™¨äººå’Œçµå·§çš„æ‰‹ã€‚æˆ‘ä»¬é€šè¿‡ç²¾ç¡®æ“ä½œä»»åŠ¡å±•ç¤ºäº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡è®­ç»ƒå…·æœ‰å¼ºå¤§è‡ªä¸»æ€§èƒ½çš„ VLA æ¨¡å‹æ¥éªŒè¯æ•°æ®è´¨é‡ã€‚"
        },
        {
          "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2507.23682v3",
          "snippet": "Vision-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent works have begun to explore the incorporation of latent actions, abstract representations of motion between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Vision-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. We demonstrate that villa-X can generate latent action plans in a zero-shot fashion, even for unseen embodiments and open-vocabulary symbolic understanding. This capability enables villa-X to achieve superior performance across diverse simulation tasks in SIMPLER and on two real-world robotic setups involving both gripper and dexterous hand manipulation. These results establish villa-X as a principled and scalable paradigm for learning generalizable robot manipulation policies. We believe it provides a strong foundation for future research.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-07-31T15:57:46Z",
          "authors": [
            "Xiaoyu Chen",
            "Hangxing Wei",
            "Pushi Zhang",
            "Chuheng Zhang",
            "Kaixin Wang",
            "Yanjiang Guo",
            "Rushuai Yang",
            "Yucen Wang",
            "Xinquan Xiao",
            "Li Zhao",
            "Jianyu Chen",
            "Jiang Bian"
          ],
          "arxiv_id": "2507.23682",
          "abstract": "Vision-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent works have begun to explore the incorporation of latent actions, abstract representations of motion between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Vision-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. We demonstrate that villa-X can generate latent action plans in a zero-shot fashion, even for unseen embodiments and open-vocabulary symbolic understanding. This capability enables villa-X to achieve superior performance across diverse simulation tasks in SIMPLER and on two real-world robotic setups involving both gripper and dexterous hand manipulation. These results establish villa-X as a principled and scalable paradigm for learning generalizable robot manipulation policies. We believe it provides a strong foundation for future research.",
          "abstract_zh": "è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹å·²æˆä¸ºå­¦ä¹ æœºå™¨äººæ“ä½œç­–ç•¥çš„æµè¡ŒèŒƒä¾‹ï¼Œå®ƒå¯ä»¥éµå¾ªè¯­è¨€æŒ‡ä»¤å¹¶æ³›åŒ–åˆ°æ–°çš„åœºæ™¯ã€‚æœ€è¿‘çš„å·¥ä½œå·²ç»å¼€å§‹æ¢ç´¢å°†æ½œåœ¨åŠ¨ä½œï¼ˆä¸¤å¸§ä¹‹é—´è¿åŠ¨çš„æŠ½è±¡è¡¨ç¤ºï¼‰çº³å…¥ VLA é¢„è®­ç»ƒä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† Villa-Xï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€-æ½œåœ¨-åŠ¨ä½œ (ViLLA) æ¡†æ¶ï¼Œå®ƒæ”¹è¿›äº†ç”¨äºå­¦ä¹ å¯æ¨å¹¿çš„æœºå™¨äººæ“ä½œç­–ç•¥çš„æ½œåœ¨åŠ¨ä½œå»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¹è¿›äº†æ½œåœ¨åŠ¨ä½œçš„å­¦ä¹ æ–¹å¼ä»¥åŠå°†å®ƒä»¬çº³å…¥ VLA é¢„è®­ç»ƒçš„æ–¹å¼ã€‚æˆ‘ä»¬è¯æ˜äº† Villa-X å¯ä»¥ä»¥é›¶å°„å‡»çš„æ–¹å¼ç”Ÿæˆæ½œåœ¨çš„è¡ŒåŠ¨è®¡åˆ’ï¼Œå³ä½¿å¯¹äºçœ‹ä¸è§çš„å®æ–½ä¾‹å’Œå¼€æ”¾è¯æ±‡ç¬¦å·ç†è§£ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤åŠŸèƒ½ä½¿ Villa-X èƒ½å¤Ÿåœ¨ SIMPLER ä¸­çš„å„ç§æ¨¡æ‹Ÿä»»åŠ¡ä»¥åŠæ¶‰åŠå¤¹å…·å’Œçµå·§æ‰‹æ“ä½œçš„ä¸¤ä¸ªç°å®ä¸–ç•Œæœºå™¨äººè®¾ç½®ä¸Šå®ç°å“è¶Šçš„æ€§èƒ½ã€‚è¿™äº›ç»“æœå°† Villa-X ç¡®ç«‹ä¸ºå­¦ä¹ é€šç”¨æœºå™¨äººæ“ä½œç­–ç•¥çš„æœ‰åŸåˆ™ä¸”å¯æ‰©å±•çš„èŒƒä¾‹ã€‚æˆ‘ä»¬ç›¸ä¿¡å®ƒä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚"
        },
        {
          "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving",
          "url": "http://arxiv.org/abs/2507.23540v1",
          "snippet": "Autonomous driving systems face significant challenges in achieving human-like adaptability, robustness, and interpretability in complex, open-world environments. These challenges stem from fragmented architectures, limited generalization to novel scenarios, and insufficient semantic extraction from perception. To address these limitations, we propose a unified Perception-Language-Action (PLA) framework that integrates multi-sensor fusion (cameras, LiDAR, radar) with a large language model (LLM)-augmented Vision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered reasoning core. This framework unifies low-level sensory processing with high-level contextual reasoning, tightly coupling perception with natural language-based semantic understanding and decision-making to enable context-aware, explainable, and safety-bounded autonomous driving. Evaluations on an urban intersection scenario with a construction zone demonstrate superior performance in trajectory tracking, speed prediction, and adaptive planning. The results highlight the potential of language-augmented cognitive frameworks for advancing the safety, interpretability, and scalability of autonomous driving systems.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-07-31T13:30:47Z",
          "authors": [
            "Yi Zhang",
            "Erik Leo HaÃŸ",
            "Kuo-Yi Chao",
            "Nenad Petrovic",
            "Yinglei Song",
            "Chengdong Wu",
            "Alois Knoll"
          ],
          "arxiv_id": "2507.23540",
          "abstract": "Autonomous driving systems face significant challenges in achieving human-like adaptability, robustness, and interpretability in complex, open-world environments. These challenges stem from fragmented architectures, limited generalization to novel scenarios, and insufficient semantic extraction from perception. To address these limitations, we propose a unified Perception-Language-Action (PLA) framework that integrates multi-sensor fusion (cameras, LiDAR, radar) with a large language model (LLM)-augmented Vision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered reasoning core. This framework unifies low-level sensory processing with high-level contextual reasoning, tightly coupling perception with natural language-based semantic understanding and decision-making to enable context-aware, explainable, and safety-bounded autonomous driving. Evaluations on an urban intersection scenario with a construction zone demonstrate superior performance in trajectory tracking, speed prediction, and adaptive planning. The results highlight the potential of language-augmented cognitive frameworks for advancing the safety, interpretability, and scalability of autonomous driving systems.",
          "abstract_zh": "è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚çš„å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®ç°ç±»äººçš„é€‚åº”æ€§ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜æºäºæ”¯ç¦»ç ´ç¢çš„æ¶æ„ã€å¯¹æ–°åœºæ™¯çš„æœ‰é™æ³›åŒ–ä»¥åŠä»æ„ŸçŸ¥ä¸­æå–çš„è¯­ä¹‰ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ„ŸçŸ¥-è¯­è¨€-åŠ¨ä½œï¼ˆPLAï¼‰æ¡†æ¶ï¼Œå®ƒå°†å¤šä¼ æ„Ÿå™¨èåˆï¼ˆç›¸æœºã€LiDARã€é›·è¾¾ï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¶æ„é›†æˆåœ¨ä¸€èµ·ï¼Œç‰¹åˆ«æ˜¯ç”± GPT-4.1 æ”¯æŒçš„æ¨ç†æ ¸å¿ƒã€‚è¯¥æ¡†æ¶å°†ä½çº§æ„ŸçŸ¥å¤„ç†ä¸é«˜çº§ä¸Šä¸‹æ–‡æ¨ç†ç›¸ç»“åˆï¼Œå°†æ„ŸçŸ¥ä¸åŸºäºè‡ªç„¶è¯­è¨€çš„è¯­ä¹‰ç†è§£å’Œå†³ç­–ç´§å¯†è€¦åˆï¼Œä»¥å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€å¯è§£é‡Šä¸”å®‰å…¨çš„è‡ªåŠ¨é©¾é©¶ã€‚å¯¹å¸¦æœ‰æ–½å·¥åŒºçš„åŸå¸‚äº¤å‰è·¯å£åœºæ™¯çš„è¯„ä¼°æ˜¾ç¤ºäº†åœ¨è½¨è¿¹è·Ÿè¸ªã€é€Ÿåº¦é¢„æµ‹å’Œè‡ªé€‚åº”è§„åˆ’æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚ç»“æœå‡¸æ˜¾äº†è¯­è¨€å¢å¼ºè®¤çŸ¥æ¡†æ¶åœ¨æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ã€å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„æ½œåŠ›ã€‚"
        },
        {
          "title": "CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning",
          "url": "http://arxiv.org/abs/2508.02219v1",
          "snippet": "Vision-Language-Action (VLA) models demonstrate significant potential for developing generalized policies in real-world robotic control. This progress inspires researchers to explore fine-tuning these models with Reinforcement Learning (RL). However, fine-tuning VLA models with RL still faces challenges related to sample efficiency, compatibility with action chunking, and training stability. To address these challenges, we explore the fine-tuning of VLA models through offline reinforcement learning incorporating action chunking. In this work, we propose Chunked RL, a novel reinforcement learning framework specifically designed for VLA models. Within this framework, we extend temporal difference (TD) learning to incorporate action chunking, a prominent characteristic of VLA models. Building upon this framework, we propose CO-RFT, an algorithm aimed at fine-tuning VLA models using a limited set of demonstrations (30 to 60 samples). Specifically, we first conduct imitation learning (IL) with full parameter fine-tuning to initialize both the backbone and the policy. Subsequently, we implement offline RL with action chunking to optimize the pretrained policy. Our empirical results in real-world environments demonstrate that CO-RFT outperforms previous supervised methods, achieving a 57% improvement in success rate and a 22.3% reduction in cycle time. Moreover, our method exhibits robust positional generalization capabilities, attaining a success rate of 44.3% in previously unseen positions.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-08-04T09:11:48Z",
          "authors": [
            "Dongchi Huang",
            "Zhirui Fang",
            "Tianle Zhang",
            "Yihang Li",
            "Lin Zhao",
            "Chunhe Xia"
          ],
          "arxiv_id": "2508.02219",
          "abstract": "Vision-Language-Action (VLA) models demonstrate significant potential for developing generalized policies in real-world robotic control. This progress inspires researchers to explore fine-tuning these models with Reinforcement Learning (RL). However, fine-tuning VLA models with RL still faces challenges related to sample efficiency, compatibility with action chunking, and training stability. To address these challenges, we explore the fine-tuning of VLA models through offline reinforcement learning incorporating action chunking. In this work, we propose Chunked RL, a novel reinforcement learning framework specifically designed for VLA models. Within this framework, we extend temporal difference (TD) learning to incorporate action chunking, a prominent characteristic of VLA models. Building upon this framework, we propose CO-RFT, an algorithm aimed at fine-tuning VLA models using a limited set of demonstrations (30 to 60 samples). Specifically, we first conduct imitation learning (IL) with full parameter fine-tuning to initialize both the backbone and the policy. Subsequently, we implement offline RL with action chunking to optimize the pretrained policy. Our empirical results in real-world environments demonstrate that CO-RFT outperforms previous supervised methods, achieving a 57% improvement in success rate and a 22.3% reduction in cycle time. Moreover, our method exhibits robust positional generalization capabilities, attaining a success rate of 44.3% in previously unseen positions.",
          "abstract_zh": "è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹å±•ç¤ºäº†åœ¨ç°å®ä¸–ç•Œæœºå™¨äººæ§åˆ¶ä¸­å¼€å‘é€šç”¨ç­–ç•¥çš„å·¨å¤§æ½œåŠ›ã€‚è¿™ä¸€è¿›å±•æ¿€åŠ±ç ”ç©¶äººå‘˜æ¢ç´¢åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œä½¿ç”¨ RL å¾®è°ƒ VLA æ¨¡å‹ä»ç„¶é¢ä¸´æ ·æœ¬æ•ˆç‡ã€åŠ¨ä½œåˆ†å—å…¼å®¹æ€§å’Œè®­ç»ƒç¨³å®šæ€§ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆåŠ¨ä½œåˆ†å—çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¢ç´¢ VLA æ¨¡å‹çš„å¾®è°ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† Chunked RLï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸º VLA æ¨¡å‹è®¾è®¡çš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚åœ¨æ­¤æ¡†æ¶å†…ï¼Œæˆ‘ä»¬æ‰©å±•äº†æ—¶é—´å·®å¼‚ (TD) å­¦ä¹ ï¼Œä»¥çº³å…¥åŠ¨ä½œåˆ†å—ï¼Œè¿™æ˜¯ VLA æ¨¡å‹çš„ä¸€ä¸ªæ˜¾ç€ç‰¹å¾ã€‚åœ¨æ­¤æ¡†æ¶çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº† CO-RFTï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä½¿ç”¨æœ‰é™çš„æ¼”ç¤ºé›†ï¼ˆ30 åˆ° 60 ä¸ªæ ·æœ¬ï¼‰å¾®è°ƒ VLA æ¨¡å‹çš„ç®—æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰å’Œå…¨å‚æ•°å¾®è°ƒï¼Œä»¥åˆå§‹åŒ–éª¨å¹²ç½‘å’Œç­–ç•¥ã€‚éšåï¼Œæˆ‘ä»¬é€šè¿‡åŠ¨ä½œåˆ†å—å®ç°ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œä»¥ä¼˜åŒ–é¢„è®­ç»ƒç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ç°å®ç¯å¢ƒä¸­çš„å®è¯ç»“æœè¡¨æ˜ï¼ŒCO-RFT ä¼˜äºä»¥å‰çš„ç›‘ç£æ–¹æ³•ï¼ŒæˆåŠŸç‡æé«˜äº† 57%ï¼Œå‘¨æœŸæ—¶é—´ç¼©çŸ­äº† 22.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„ä½ç½®æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä»¥å‰æœªè§è¿‡çš„ä½ç½®ä¸Šè¾¾åˆ°äº† 44.3% çš„æˆåŠŸç‡ã€‚"
        },
        {
          "title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning",
          "url": "http://arxiv.org/abs/2507.23318v4",
          "snippet": "Vision-Language-Action (VLA) models have demonstrated significant potential in complex scene understanding and action reasoning, leading to their increasing adoption in end-to-end autonomous driving systems. However, the long visual tokens of VLA models greatly increase computational costs. Current visual token pruning methods in Vision-Language Models (VLM) rely on either visual token similarity or visual-text attention, but both have shown poor performance in autonomous driving scenarios. Given that human drivers concentrate on relevant foreground areas while driving, we assert that retaining visual tokens containing this foreground information is essential for effective decision-making. Inspired by this, we propose FastDriveVLA, a novel reconstruction-based vision token pruning framework designed specifically for autonomous driving. FastDriveVLA includes a plug-and-play visual token pruner called ReconPruner, which prioritizes foreground information through MAE-style pixel reconstruction. A novel adversarial foreground-background reconstruction strategy is designed to train ReconPruner for the visual encoder of VLA models. Once trained, ReconPruner can be seamlessly applied to different VLA models with the same visual encoder without retraining. To train ReconPruner, we also introduce a large-scale dataset called nuScenes-FG, consisting of 241K image-mask pairs with annotated foreground regions. Our approach achieves state-of-the-art results on the nuScenes open-loop planning benchmark across different pruning ratios.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-07-31T07:55:56Z",
          "authors": [
            "Jiajun Cao",
            "Qizhe Zhang",
            "Peidong Jia",
            "Xuhui Zhao",
            "Bo Lan",
            "Xiaoan Zhang",
            "Zhuo Li",
            "Xiaobao Wei",
            "Sixiang Chen",
            "Liyun Li",
            "Xianming Liu",
            "Ming Lu",
            "Yang Wang",
            "Shanghang Zhang"
          ],
          "arxiv_id": "2507.23318",
          "abstract": "Vision-Language-Action (VLA) models have demonstrated significant potential in complex scene understanding and action reasoning, leading to their increasing adoption in end-to-end autonomous driving systems. However, the long visual tokens of VLA models greatly increase computational costs. Current visual token pruning methods in Vision-Language Models (VLM) rely on either visual token similarity or visual-text attention, but both have shown poor performance in autonomous driving scenarios. Given that human drivers concentrate on relevant foreground areas while driving, we assert that retaining visual tokens containing this foreground information is essential for effective decision-making. Inspired by this, we propose FastDriveVLA, a novel reconstruction-based vision token pruning framework designed specifically for autonomous driving. FastDriveVLA includes a plug-and-play visual token pruner called ReconPruner, which prioritizes foreground information through MAE-style pixel reconstruction. A novel adversarial foreground-background reconstruction strategy is designed to train ReconPruner for the visual encoder of VLA models. Once trained, ReconPruner can be seamlessly applied to different VLA models with the same visual encoder without retraining. To train ReconPruner, we also introduce a large-scale dataset called nuScenes-FG, consisting of 241K image-mask pairs with annotated foreground regions. Our approach achieves state-of-the-art results on the nuScenes open-loop planning benchmark across different pruning ratios.",
          "abstract_zh": "è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤æ‚çš„åœºæ™¯ç†è§£å’ŒåŠ¨ä½œæ¨ç†æ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œå¯¼è‡´å®ƒä»¬åœ¨ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­çš„é‡‡ç”¨è¶Šæ¥è¶Šå¤šã€‚ç„¶è€Œï¼ŒVLA æ¨¡å‹çš„é•¿è§†è§‰æ ‡è®°å¤§å¤§å¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­çš„è§†è§‰æ ‡è®°ä¿®å‰ªæ–¹æ³•ä¾èµ–äºè§†è§‰æ ‡è®°ç›¸ä¼¼æ€§æˆ–è§†è§‰æ–‡æœ¬æ³¨æ„åŠ›ï¼Œä½†è¿™ä¸¤ç§æ–¹æ³•åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚é‰´äºäººç±»é©¾é©¶å‘˜åœ¨é©¾é©¶æ—¶ä¸“æ³¨äºç›¸å…³çš„å‰æ™¯åŒºåŸŸï¼Œæˆ‘ä»¬æ–­è¨€ä¿ç•™åŒ…å«è¿™äº›å‰æ™¯ä¿¡æ¯çš„è§†è§‰æ ‡è®°å¯¹äºæœ‰æ•ˆçš„å†³ç­–è‡³å…³é‡è¦ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†FastDriveVLAï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡çš„æ–°å‹åŸºäºé‡å»ºçš„è§†è§‰ä»¤ç‰Œä¿®å‰ªæ¡†æ¶ã€‚FastDriveVLA åŒ…æ‹¬ä¸€ä¸ªåä¸º ReconPruner çš„å³æ’å³ç”¨è§†è§‰æ ‡è®°ä¿®å‰ªå™¨ï¼Œå®ƒé€šè¿‡ MAE é£æ ¼çš„åƒç´ é‡å»ºæ¥ä¼˜å…ˆè€ƒè™‘å‰æ™¯ä¿¡æ¯ã€‚è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„å¯¹æŠ—æ€§å‰æ™¯-èƒŒæ™¯é‡å»ºç­–ç•¥æ¥ä¸º VLA æ¨¡å‹çš„è§†è§‰ç¼–ç å™¨è®­ç»ƒ ReconPrunerã€‚ç»è¿‡è®­ç»ƒåï¼ŒReconPruner å¯ä»¥ä½¿ç”¨ç›¸åŒçš„è§†è§‰ç¼–ç å™¨æ— ç¼åº”ç”¨äºä¸åŒçš„ VLA æ¨¡å‹ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ä¸ºäº†è®­ç»ƒ ReconPrunerï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåä¸º nuScenes-FG çš„å¤§å‹æ•°æ®é›†ï¼Œç”± 241K ä¸ªå¸¦æœ‰æ³¨é‡Šå‰æ™¯åŒºåŸŸçš„å›¾åƒæ©æ¨¡å¯¹ç»„æˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒå‰ªæç‡çš„ nuScenes å¼€ç¯è§„åˆ’åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚"
        },
        {
          "title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming",
          "url": "http://arxiv.org/abs/2508.02549v4",
          "snippet": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth inputs to provide rich spatial cues for action planning, but these sensors can be costly or less accessible in real-world deployments. Recent approaches based on Vision-Language Action (VLA) models achieve strong results with monocular input, yet they still lag behind methods using panoramic RGB-D information. We present MonoDream, a lightweight VLA framework that enables monocular agents to learn a Unified Navigation Representation (UNR). This shared feature representation jointly aligns navigation-relevant visual semantics (e.g., global layout, depth, and future cues) and language-grounded action intent, enabling more reliable action prediction. MonoDream further introduces Latent Panoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to predict latent features of panoramic RGB and depth observations at both current and future steps based on only monocular input. Experiments on multiple VLN benchmarks show that MonoDream consistently improves monocular navigation performance and significantly narrows the gap with panoramic-based agents.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-08-04T16:01:30Z",
          "authors": [
            "Shuo Wang",
            "Yongcai Wang",
            "Zhaoxin Fan",
            "Yucheng Wang",
            "Maiyue Chen",
            "Kaihui Wang",
            "Zhizhong Su",
            "Wanting Li",
            "Xudong Cai",
            "Yeying Jin",
            "Deying Li"
          ],
          "arxiv_id": "2508.02549",
          "abstract": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth inputs to provide rich spatial cues for action planning, but these sensors can be costly or less accessible in real-world deployments. Recent approaches based on Vision-Language Action (VLA) models achieve strong results with monocular input, yet they still lag behind methods using panoramic RGB-D information. We present MonoDream, a lightweight VLA framework that enables monocular agents to learn a Unified Navigation Representation (UNR). This shared feature representation jointly aligns navigation-relevant visual semantics (e.g., global layout, depth, and future cues) and language-grounded action intent, enabling more reliable action prediction. MonoDream further introduces Latent Panoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to predict latent features of panoramic RGB and depth observations at both current and future steps based on only monocular input. Experiments on multiple VLN benchmarks show that MonoDream consistently improves monocular navigation performance and significantly narrows the gap with panoramic-based agents.",
          "abstract_zh": "è§†è§‰è¯­è¨€å¯¼èˆª (VLN) ä»»åŠ¡é€šå¸¸åˆ©ç”¨å…¨æ™¯ RGB å’Œæ·±åº¦è¾“å…¥æ¥ä¸ºè¡ŒåŠ¨è§„åˆ’æä¾›ä¸°å¯Œçš„ç©ºé—´çº¿ç´¢ï¼Œä½†è¿™äº›ä¼ æ„Ÿå™¨å¯èƒ½æˆæœ¬é«˜æ˜‚ï¼Œæˆ–è€…åœ¨å®é™…éƒ¨ç½²ä¸­ä¸æ˜“è®¿é—®ã€‚æœ€è¿‘åŸºäºè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ–¹æ³•é€šè¿‡å•çœ¼è¾“å…¥å–å¾—äº†å¾ˆå¥½çš„ç»“æœï¼Œä½†å®ƒä»¬ä»ç„¶è½åäºä½¿ç”¨å…¨æ™¯ RGB-D ä¿¡æ¯çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ¨å‡ºäº† MonoDreamï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ VLA æ¡†æ¶ï¼Œä½¿å•çœ¼æ™ºèƒ½ä½“èƒ½å¤Ÿå­¦ä¹ ç»Ÿä¸€å¯¼èˆªè¡¨ç¤º (UNR)ã€‚è¿™ç§å…±äº«çš„ç‰¹å¾è¡¨ç¤ºè”åˆå¯¹é½ä¸å¯¼èˆªç›¸å…³çš„è§†è§‰è¯­ä¹‰ï¼ˆä¾‹å¦‚ï¼Œå…¨å±€å¸ƒå±€ã€æ·±åº¦å’Œæœªæ¥çº¿ç´¢ï¼‰å’ŒåŸºäºè¯­è¨€çš„åŠ¨ä½œæ„å›¾ï¼Œä»è€Œå®ç°æ›´å¯é çš„åŠ¨ä½œé¢„æµ‹ã€‚MonoDream è¿›ä¸€æ­¥å¼•å…¥äº†æ½œåœ¨å…¨æ™¯æ¢¦ (LPD) ä»»åŠ¡æ¥ç›‘ç£ UNRï¼Œè®­ç»ƒæ¨¡å‹ä»…åŸºäºå•çœ¼è¾“å…¥æ¥é¢„æµ‹å½“å‰å’Œæœªæ¥æ­¥éª¤ä¸­å…¨æ™¯ RGB å’Œæ·±åº¦è§‚å¯Ÿçš„æ½œåœ¨ç‰¹å¾ã€‚åœ¨å¤šä¸ª VLN åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMonoDream æŒç»­æ”¹è¿›äº†å•ç›®å¯¼èˆªæ€§èƒ½ï¼Œå¹¶æ˜¾ç€ç¼©å°äº†ä¸åŸºäºå…¨æ™¯çš„æ™ºèƒ½ä½“çš„å·®è·ã€‚"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "å¤´éƒ¨ç©å®¶æœ¬å‘¨æ— æ›´æ–°ã€‚",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com ä¸Šå…±å‘ç° 5 æ¡ä¸ VLA ç›¸å…³çš„æ›´æ–°å†…å®¹ï¼ˆæŒ‰æœç´¢ç›¸å…³æ€§æ’åºï¼Œæ˜¾ç¤º Top 5ï¼‰ã€‚",
      "items": [
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "ReinFlow/ReinFlow",
          "url": "https://github.com/ReinFlow/ReinFlow",
          "snippet": "[NeurIPS 2025] Flow x RL. \"ReinFlow: Fine-tuning Flow Policy with Online Reinforcement Learning\". Support VLAs e.g., pi0, pi0.5. Fully open-sourced. ",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "RoboTwin-Platform/RoboTwin",
          "url": "https://github.com/RoboTwin-Platform/RoboTwin",
          "snippet": "RoboTwin 2.0 Offical Repo",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news å‘¨æŠ¥å†…å®¹ æ¯å‘¨äºŒ0ç‚¹å·¦å³æ›´æ–°",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. ğŸ”‰Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 5
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co æœ¬å‘¨æ— æ›´æ–°ã€‚",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com æœ¬å‘¨æ— æ›´æ–°ã€‚",
      "items": []
    }
  ],
  "week_start": "2025-07-28",
  "week_end": "2025-08-03",
  "last_updated": "2026-01-07"
}