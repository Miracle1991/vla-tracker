{
  "generated_at": "2026-01-07T13:24:12.445242",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org ä¸Šå…±å‘ç° 7 æ¡ä¸ VLA ç›¸å…³çš„æ›´æ–°å†…å®¹ï¼ˆæŒ‰æœç´¢ç›¸å…³æ€§æ’åºï¼Œæ˜¾ç¤º Top 7ï¼‰ã€‚",
      "items": [
        {
          "title": "Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey",
          "url": "http://arxiv.org/abs/2508.13073v2",
          "snippet": "Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-08-18T16:45:48Z",
          "authors": [
            "Rui Shao",
            "Wei Li",
            "Lingsen Zhang",
            "Renshan Zhang",
            "Zhiyang Liu",
            "Ran Chen",
            "Liqiang Nie"
          ],
          "arxiv_id": "2508.13073",
          "abstract": "Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation",
          "abstract_zh": "æœºå™¨äººæ“çºµæ˜¯æœºå™¨äººæŠ€æœ¯å’ŒåµŒå…¥å¼äººå·¥æ™ºèƒ½çš„å…³é”®å‰æ²¿ï¼Œéœ€è¦ç²¾ç¡®çš„è¿åŠ¨æ§åˆ¶å’Œå¤šæ¨¡æ€ç†è§£ï¼Œä½†ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ–¹æ³•æ— æ³•åœ¨éç»“æ„åŒ–çš„æ–°é¢–ç¯å¢ƒä¸­è¿›è¡Œæ‰©å±•æˆ–æ³›åŒ–ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºåœ¨å¤§é‡å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) æ„å»ºçš„è§†è§‰è¯­è¨€åŠ¨ä½œ (VLA) æ¨¡å‹å·²æˆä¸ºä¸€ç§å˜é©èŒƒå¼ã€‚è¿™é¡¹è°ƒæŸ¥é¦–æ¬¡å¯¹ç”¨äºæœºå™¨äººæ“ä½œçš„åŸºäº VLM çš„å¤§å‹ VLA æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿçš„ã€é¢å‘åˆ†ç±»çš„å®¡æŸ¥ã€‚æˆ‘ä»¬é¦–å…ˆæ˜ç¡®å®šä¹‰åŸºäº VLM çš„å¤§å‹ VLA æ¨¡å‹ï¼Œå¹¶æè¿°ä¸¤ä¸ªä¸»è¦æ¶æ„èŒƒä¾‹ï¼šï¼ˆ1ï¼‰æ•´ä½“æ¨¡å‹ï¼ŒåŒ…æ‹¬å…·æœ‰ä¸åŒé›†æˆçº§åˆ«çš„å•ç³»ç»Ÿå’ŒåŒç³»ç»Ÿè®¾è®¡ï¼›(2)åˆ†å±‚æ¨¡å‹ï¼Œå®ƒé€šè¿‡å¯è§£é‡Šçš„ä¸­é—´è¡¨ç¤ºæ˜ç¡®åœ°å°†è®¡åˆ’ä¸æ‰§è¡Œåˆ†ç¦»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯¹åŸºäº VLM çš„å¤§å‹ VLA æ¨¡å‹è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼šï¼ˆ1ï¼‰ä¸é«˜çº§é¢†åŸŸçš„é›†æˆï¼ŒåŒ…æ‹¬å¼ºåŒ–å­¦ä¹ ã€å…è®­ç»ƒä¼˜åŒ–ã€äººç±»è§†é¢‘å­¦ä¹ å’Œä¸–ç•Œæ¨¡å‹é›†æˆï¼›(2) ç»¼åˆç‹¬ç‰¹çš„ç‰¹å¾ï¼Œæ•´åˆæ¶æ„ç‰¹å¾ã€æ“ä½œä¼˜åŠ¿ä»¥åŠæ”¯æŒå…¶å‘å±•çš„æ•°æ®é›†å’ŒåŸºå‡†ï¼›ï¼ˆ3ï¼‰ç¡®å®šæœ‰å‰æ™¯çš„æ–¹å‘ï¼ŒåŒ…æ‹¬è®°å¿†æœºåˆ¶ã€4Dæ„ŸçŸ¥ã€é«˜æ•ˆé€‚åº”ã€å¤šæ™ºèƒ½ä½“åˆä½œå’Œå…¶ä»–æ–°å…´èƒ½åŠ›ã€‚è¿™é¡¹è°ƒæŸ¥æ•´åˆäº†æœ€æ–°çš„è¿›å±•ï¼Œä»¥è§£å†³ç°æœ‰åˆ†ç±»ä¸­çš„ä¸ä¸€è‡´é—®é¢˜ï¼Œå‡å°‘ç ”ç©¶ç¢ç‰‡åŒ–ï¼Œå¹¶é€šè¿‡å¤§å‹ VLM å’Œæœºå™¨äººæ“ä½œäº¤å‰ç‚¹çš„ç ”ç©¶çš„ç³»ç»Ÿæ•´åˆæ¥å¡«è¡¥å…³é”®ç©ºç™½ã€‚æˆ‘ä»¬æä¾›å®šæœŸæ›´æ–°çš„é¡¹ç›®é¡µé¢æ¥è®°å½•æŒç»­è¿›å±•ï¼šhttps://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation"
        },
        {
          "title": "Survey of Vision-Language-Action Models for Embodied Manipulation",
          "url": "http://arxiv.org/abs/2508.15201v2",
          "snippet": "Embodied intelligence systems, which enhance agent capabilities through continuous environment interactions, have garnered significant attention from both academia and industry. Vision-Language-Action models, inspired by advancements in large foundation models, serve as universal robotic control frameworks that substantially improve agent-environment interaction capabilities in embodied intelligence systems. This expansion has broadened application scenarios for embodied AI robots. This survey comprehensively reviews VLA models for embodied manipulation. Firstly, it chronicles the developmental trajectory of VLA architectures. Subsequently, we conduct a detailed analysis of current research across 5 critical dimensions: VLA model structures, training datasets, pre-training methods, post-training methods, and model evaluation. Finally, we synthesize key challenges in VLA development and real-world deployment, while outlining promising future research directions.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-08-21T03:30:04Z",
          "authors": [
            "Haoran Li",
            "Yuhui Chen",
            "Wenbo Cui",
            "Weiheng Liu",
            "Kai Liu",
            "Mingcai Zhou",
            "Zhengtao Zhang",
            "Dongbin Zhao"
          ],
          "arxiv_id": "2508.15201",
          "abstract": "Embodied intelligence systems, which enhance agent capabilities through continuous environment interactions, have garnered significant attention from both academia and industry. Vision-Language-Action models, inspired by advancements in large foundation models, serve as universal robotic control frameworks that substantially improve agent-environment interaction capabilities in embodied intelligence systems. This expansion has broadened application scenarios for embodied AI robots. This survey comprehensively reviews VLA models for embodied manipulation. Firstly, it chronicles the developmental trajectory of VLA architectures. Subsequently, we conduct a detailed analysis of current research across 5 critical dimensions: VLA model structures, training datasets, pre-training methods, post-training methods, and model evaluation. Finally, we synthesize key challenges in VLA development and real-world deployment, while outlining promising future research directions.",
          "abstract_zh": "å…·èº«æ™ºèƒ½ç³»ç»Ÿé€šè¿‡æŒç»­çš„ç¯å¢ƒäº¤äº’æ¥å¢å¼ºæ™ºèƒ½ä½“çš„èƒ½åŠ›ï¼Œå¼•èµ·äº†å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„é«˜åº¦å…³æ³¨ã€‚è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å—åˆ°å¤§å‹åŸºç¡€æ¨¡å‹è¿›æ­¥çš„å¯å‘ï¼Œä½œä¸ºé€šç”¨æœºå™¨äººæ§åˆ¶æ¡†æ¶ï¼Œå¯æ˜¾ç€æé«˜å®ä½“æ™ºèƒ½ç³»ç»Ÿä¸­çš„ä»£ç†ä¸ç¯å¢ƒäº¤äº’èƒ½åŠ›ã€‚è¿™ä¸€æ‰©å±•æ‹“å®½äº†å®ä½“äººå·¥æ™ºèƒ½æœºå™¨äººçš„åº”ç”¨åœºæ™¯ã€‚è¿™é¡¹è°ƒæŸ¥å…¨é¢å®¡æŸ¥äº†ä½“ç°æ“çºµçš„ VLA æ¨¡å‹ã€‚é¦–å…ˆï¼Œå®ƒè®°å½•äº†VLAæ¶æ„çš„å‘å±•è½¨è¿¹ã€‚éšåï¼Œæˆ‘ä»¬å¯¹å½“å‰ç ”ç©¶çš„ 5 ä¸ªå…³é”®ç»´åº¦è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼šVLA æ¨¡å‹ç»“æ„ã€è®­ç»ƒæ•°æ®é›†ã€è®­ç»ƒå‰æ–¹æ³•ã€è®­ç»ƒåæ–¹æ³•å’Œæ¨¡å‹è¯„ä¼°ã€‚æœ€åï¼Œæˆ‘ä»¬ç»¼åˆäº† VLA å¼€å‘å’Œå®é™…éƒ¨ç½²ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒæ—¶æ¦‚è¿°äº†æœ‰å‰æ™¯çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚"
        },
        {
          "title": "Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy",
          "url": "http://arxiv.org/abs/2508.13103v1",
          "snippet": "Vision-Language-Action (VLA) models frequently encounter challenges in generalizing to real-world environments due to inherent discrepancies between observation and action spaces. Although training data are collected from diverse camera perspectives, the models typically predict end-effector poses within the robot base coordinate frame, resulting in spatial inconsistencies. To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA) framework, which grounds action predictions directly in the camera observation space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms end-effector poses from the robot base coordinate system into the camera coordinate system, thereby unifying prediction targets across heterogeneous viewpoints. This lightweight, plug-and-play strategy ensures robust alignment between perception and action, substantially improving model resilience to camera viewpoint variations. The proposed approach is readily compatible with existing VLA architectures, requiring no substantial modifications. Comprehensive evaluations on both simulated and real-world robotic manipulation tasks demonstrate that OC-VLA accelerates convergence, enhances task success rates, and improves cross-view generalization. The code will be publicly available.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-08-18T17:10:45Z",
          "authors": [
            "Tianyi Zhang",
            "Haonan Duan",
            "Haoran Hao",
            "Yu Qiao",
            "Jifeng Dai",
            "Zhi Hou"
          ],
          "arxiv_id": "2508.13103",
          "abstract": "Vision-Language-Action (VLA) models frequently encounter challenges in generalizing to real-world environments due to inherent discrepancies between observation and action spaces. Although training data are collected from diverse camera perspectives, the models typically predict end-effector poses within the robot base coordinate frame, resulting in spatial inconsistencies. To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA) framework, which grounds action predictions directly in the camera observation space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms end-effector poses from the robot base coordinate system into the camera coordinate system, thereby unifying prediction targets across heterogeneous viewpoints. This lightweight, plug-and-play strategy ensures robust alignment between perception and action, substantially improving model resilience to camera viewpoint variations. The proposed approach is readily compatible with existing VLA architectures, requiring no substantial modifications. Comprehensive evaluations on both simulated and real-world robotic manipulation tasks demonstrate that OC-VLA accelerates convergence, enhances task success rates, and improves cross-view generalization. The code will be publicly available.",
          "abstract_zh": "ç”±äºè§‚å¯Ÿå’Œè¡ŒåŠ¨ç©ºé—´ä¹‹é—´å›ºæœ‰çš„å·®å¼‚ï¼Œè§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹åœ¨æ¨å¹¿åˆ°ç°å®ä¸–ç•Œç¯å¢ƒæ—¶ç»å¸¸é‡åˆ°æŒ‘æˆ˜ã€‚å°½ç®¡è®­ç»ƒæ•°æ®æ˜¯ä»ä¸åŒçš„ç›¸æœºè§’åº¦æ”¶é›†çš„ï¼Œä½†æ¨¡å‹é€šå¸¸ä¼šé¢„æµ‹æœºå™¨äººåŸºç¡€åæ ‡ç³»å†…çš„æœ«ç«¯æ‰§è¡Œå™¨å§¿åŠ¿ï¼Œä»è€Œå¯¼è‡´ç©ºé—´ä¸ä¸€è‡´ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»¥è§‚å¯Ÿä¸ºä¸­å¿ƒçš„ VLA (OC-VLA) æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥åœ¨æ‘„åƒæœºè§‚å¯Ÿç©ºé—´ä¸­è¿›è¡ŒåŠ¨ä½œé¢„æµ‹ã€‚åˆ©ç”¨ç›¸æœºçš„å¤–éƒ¨æ ¡å‡†çŸ©é˜µï¼ŒOC-VLA å°†æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€ä»æœºå™¨äººåŸºç¡€åæ ‡ç³»è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»ï¼Œä»è€Œç»Ÿä¸€è·¨å¼‚æ„è§†ç‚¹çš„é¢„æµ‹ç›®æ ‡ã€‚è¿™ç§è½»é‡çº§ã€å³æ’å³ç”¨çš„ç­–ç•¥å¯ç¡®ä¿æ„ŸçŸ¥å’ŒåŠ¨ä½œä¹‹é—´çš„ç¨³å¥å¯¹é½ï¼Œä»è€Œæ˜¾ç€æé«˜æ¨¡å‹å¯¹æ‘„åƒæœºè§†ç‚¹å˜åŒ–çš„é€‚åº”èƒ½åŠ›ã€‚æ‰€æå‡ºçš„æ–¹æ³•å¾ˆå®¹æ˜“ä¸ç°æœ‰çš„ VLA æ¶æ„å…¼å®¹ï¼Œæ— éœ€è¿›è¡Œå®è´¨æ€§ä¿®æ”¹ã€‚å¯¹æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡çš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOC-VLA å¯ä»¥åŠ é€Ÿæ”¶æ•›ã€æé«˜ä»»åŠ¡æˆåŠŸç‡å¹¶æé«˜è·¨è§†å›¾æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ä»£ç å°†å…¬å¼€ã€‚"
        },
        {
          "title": "CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2508.13446v1",
          "snippet": "Generalist robots should be able to understand and follow user instructions, but current vision-language-action (VLA) models struggle with following fine-grained commands despite providing a powerful architecture for mapping open-vocabulary natural language instructions to robot actions. One cause for this is a lack of semantic diversity and language grounding in existing robot datasets and, specifically, a lack of fine-grained task diversity for similar observations. To address this, we present a novel method to augment existing robot datasets by leveraging vision language models to create counterfactual labels. Our method improves the language-following capabilities of VLAs by increasing the diversity and granularity of language grounding for robot datasets by generating counterfactual language and actions. We evaluate the resulting model's ability to follow language instructions, ranging from simple object-centric commands to complex referential tasks, by conducting visual language navigation experiments in 3 different indoor and outdoor environments. Our experiments demonstrate that counterfactual relabeling, without any additional data collection, significantly improves instruction-following in VLA policies, making them competitive with state-of-the-art methods and increasing success rate by 27% on navigation tasks.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-08-19T02:01:06Z",
          "authors": [
            "Catherine Glossop",
            "William Chen",
            "Arjun Bhorkar",
            "Dhruv Shah",
            "Sergey Levine"
          ],
          "arxiv_id": "2508.13446",
          "abstract": "Generalist robots should be able to understand and follow user instructions, but current vision-language-action (VLA) models struggle with following fine-grained commands despite providing a powerful architecture for mapping open-vocabulary natural language instructions to robot actions. One cause for this is a lack of semantic diversity and language grounding in existing robot datasets and, specifically, a lack of fine-grained task diversity for similar observations. To address this, we present a novel method to augment existing robot datasets by leveraging vision language models to create counterfactual labels. Our method improves the language-following capabilities of VLAs by increasing the diversity and granularity of language grounding for robot datasets by generating counterfactual language and actions. We evaluate the resulting model's ability to follow language instructions, ranging from simple object-centric commands to complex referential tasks, by conducting visual language navigation experiments in 3 different indoor and outdoor environments. Our experiments demonstrate that counterfactual relabeling, without any additional data collection, significantly improves instruction-following in VLA policies, making them competitive with state-of-the-art methods and increasing success rate by 27% on navigation tasks.",
          "abstract_zh": "å¤šé¢æ‰‹æœºå™¨äººåº”è¯¥èƒ½å¤Ÿç†è§£å¹¶éµå¾ªç”¨æˆ·æŒ‡ä»¤ï¼Œä½†å½“å‰çš„è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹å°½ç®¡æä¾›äº†å¼ºå¤§çš„æ¶æ„æ¥å°†å¼€æ”¾è¯æ±‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°æœºå™¨äººåŠ¨ä½œï¼Œä½†ä»éš¾ä»¥éµå¾ªç»†ç²’åº¦çš„å‘½ä»¤ã€‚é€ æˆè¿™ç§æƒ…å†µçš„åŸå› ä¹‹ä¸€æ˜¯ç°æœ‰æœºå™¨äººæ•°æ®é›†ä¸­ç¼ºä¹è¯­ä¹‰å¤šæ ·æ€§å’Œè¯­è¨€åŸºç¡€ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹ç±»ä¼¼è§‚å¯Ÿçš„ç»†ç²’åº¦ä»»åŠ¡å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹åˆ›å»ºåäº‹å®æ ‡ç­¾æ¥å¢å¼ºç°æœ‰çš„æœºå™¨äººæ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç”Ÿæˆåäº‹å®è¯­è¨€å’ŒåŠ¨ä½œæ¥å¢åŠ æœºå™¨äººæ•°æ®é›†è¯­è¨€åŸºç¡€çš„å¤šæ ·æ€§å’Œç²’åº¦ï¼Œä»è€Œæé«˜ VLA çš„è¯­è¨€è·Ÿè¸ªèƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ 3 ä¸ªä¸åŒçš„å®¤å†…å’Œå®¤å¤–ç¯å¢ƒä¸­è¿›è¡Œè§†è§‰è¯­è¨€å¯¼èˆªå®éªŒï¼Œè¯„ä¼°ç”Ÿæˆçš„æ¨¡å‹éµå¾ªè¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œä»ç®€å•çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„å‘½ä»¤åˆ°å¤æ‚çš„å‚è€ƒä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸æ”¶é›†ä»»ä½•é¢å¤–æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåäº‹å®é‡æ–°æ ‡è®°å¯ä»¥æ˜¾ç€æ”¹å–„ VLA ç­–ç•¥ä¸­çš„æŒ‡ä»¤éµå¾ªæƒ…å†µï¼Œä½¿å…¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶å°†å¯¼èˆªä»»åŠ¡çš„æˆåŠŸç‡æé«˜ 27%ã€‚"
        },
        {
          "title": "FlowVLA: Visual Chain of Thought-based Motion Reasoning for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2508.18269v3",
          "snippet": "Many Vision-Language-Action (VLA) models are built upon an internal world model trained via next-frame prediction ``$v_t \\rightarrow v_{t+1}$''. However, this paradigm attempts to predict the future frame's appearance directly, without explicitly reasoning about the underlying dynamics. \\textbf{This lack of an explicit motion reasoning step} often leads to physically implausible visual forecasts and inefficient policy learning. To address this limitation, we introduce the \\textbf{Visual Chain of Thought (Visual CoT)}, a paradigm that compels the model to first reason about \\textbf{motion dynamics} before generating the future frame. We instantiate this paradigm by proposing \\textbf{FlowVLA}, an autoregressive Transformer that explicitly materializes this reasoning process as ``$v_t \\rightarrow f_t \\rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction that inherently encodes motion. By forcing the model to first follow the motion plan encoded by $f_t$, this process inherently \\textbf{aligns the pre-training objective of dynamics prediction with the downstream task of action generation.} We conduct experiments on challenging robotics manipulation benchmarks, as well as real-robot evaluations. Our FlowVLA not only generates \\textbf{more coherent and physically plausible visual predictions}, but also achieves state-of-the-art policy performance with \\textbf{substantially improved sample efficiency}, pointing toward a more principled foundation for world modeling in VLAs. Project page: https://irpn-lab.github.io/FlowVLA/",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-08-25T17:59:21Z",
          "authors": [
            "Zhide Zhong",
            "Haodong Yan",
            "Junfeng Li",
            "Xiangchen Liu",
            "Xin Gong",
            "Tianran Zhang",
            "Wenxuan Song",
            "Jiayi Chen",
            "Xinhu Zheng",
            "Hesheng Wang",
            "Haoang Li"
          ],
          "arxiv_id": "2508.18269",
          "abstract": "Many Vision-Language-Action (VLA) models are built upon an internal world model trained via next-frame prediction ``$v_t \\rightarrow v_{t+1}$''. However, this paradigm attempts to predict the future frame's appearance directly, without explicitly reasoning about the underlying dynamics. \\textbf{This lack of an explicit motion reasoning step} often leads to physically implausible visual forecasts and inefficient policy learning. To address this limitation, we introduce the \\textbf{Visual Chain of Thought (Visual CoT)}, a paradigm that compels the model to first reason about \\textbf{motion dynamics} before generating the future frame. We instantiate this paradigm by proposing \\textbf{FlowVLA}, an autoregressive Transformer that explicitly materializes this reasoning process as ``$v_t \\rightarrow f_t \\rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction that inherently encodes motion. By forcing the model to first follow the motion plan encoded by $f_t$, this process inherently \\textbf{aligns the pre-training objective of dynamics prediction with the downstream task of action generation.} We conduct experiments on challenging robotics manipulation benchmarks, as well as real-robot evaluations. Our FlowVLA not only generates \\textbf{more coherent and physically plausible visual predictions}, but also achieves state-of-the-art policy performance with \\textbf{substantially improved sample efficiency}, pointing toward a more principled foundation for world modeling in VLAs. Project page: https://irpn-lab.github.io/FlowVLA/",
          "abstract_zh": "è®¸å¤šè§†è§‰-è¯­è¨€-åŠ¨ä½œ (VLA) æ¨¡å‹éƒ½æ˜¯å»ºç«‹åœ¨é€šè¿‡ä¸‹ä¸€å¸§é¢„æµ‹â€œ$v_t \\rightarrow v_{t+1}$â€è®­ç»ƒçš„å†…éƒ¨ä¸–ç•Œæ¨¡å‹ä¹‹ä¸Šã€‚ç„¶è€Œï¼Œè¿™ç§èŒƒå¼è¯•å›¾ç›´æ¥é¢„æµ‹æœªæ¥æ¡†æ¶çš„å¤–è§‚ï¼Œè€Œä¸æ˜ç¡®æ¨ç†æ½œåœ¨çš„åŠ¨æ€ã€‚\\textbf{ç¼ºä¹æ˜ç¡®çš„è¿åŠ¨æ¨ç†æ­¥éª¤}é€šå¸¸ä¼šå¯¼è‡´ç‰©ç†ä¸Šä¸å¯ä¿¡çš„è§†è§‰é¢„æµ‹å’Œä½æ•ˆçš„ç­–ç•¥å­¦ä¹ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº† \\textbf{è§†è§‰æ€æƒ³é“¾ (Visual CoT)}ï¼Œè¿™æ˜¯ä¸€ç§èŒƒå¼ï¼Œè¿«ä½¿æ¨¡å‹åœ¨ç”Ÿæˆæœªæ¥å¸§ä¹‹å‰é¦–å…ˆæ¨ç† \\textbf{è¿åŠ¨åŠ¨åŠ›å­¦}ã€‚æˆ‘ä»¬é€šè¿‡æå‡º \\textbf{FlowVLA} æ¥å®ä¾‹åŒ–è¿™ä¸ªèŒƒå¼ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªå›å½’ Transformerï¼Œå®ƒæ˜ç¡®åœ°å°†è¿™ä¸ªæ¨ç†è¿‡ç¨‹å…·ä½“åŒ–ä¸ºâ€œ$v_t \\rightarrow f_t \\rightarrow v_{t+1}$â€ï¼Œå…¶ä¸­ $f_t$ æ˜¯æœ¬è´¨ä¸Šå¯¹è¿åŠ¨è¿›è¡Œç¼–ç çš„ä¸­é—´å…‰æµé¢„æµ‹ã€‚é€šè¿‡å¼ºåˆ¶æ¨¡å‹é¦–å…ˆéµå¾ª $f_t$ ç¼–ç çš„è¿åŠ¨è®¡åˆ’ï¼Œè¿™ä¸ªè¿‡ç¨‹æœ¬è´¨ä¸Š \\textbf{å°†åŠ¨æ€é¢„æµ‹çš„é¢„è®­ç»ƒç›®æ ‡ä¸åŠ¨ä½œç”Ÿæˆçš„ä¸‹æ¸¸ä»»åŠ¡å¯¹é½ã€‚}æˆ‘ä»¬å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æœºå™¨äººæ“ä½œåŸºå‡†ä»¥åŠçœŸå®çš„æœºå™¨äººè¯„ä¼°è¿›è¡Œäº†å®éªŒã€‚æˆ‘ä»¬çš„ FlowVLA ä¸ä»…ç”Ÿæˆ \\textbf{æ›´è¿è´¯ä¸”ç‰©ç†ä¸Šåˆç†çš„è§†è§‰é¢„æµ‹}ï¼Œè€Œä¸”è¿˜é€šè¿‡ \\textbf{æ˜¾ç€æé«˜çš„æ ·æœ¬æ•ˆç‡}å®ç°äº†æœ€å…ˆè¿›çš„ç­–ç•¥æ€§èƒ½ï¼Œä¸º VLA ä¸­çš„ä¸–ç•Œå»ºæ¨¡æä¾›äº†æ›´æœ‰åŸåˆ™çš„åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://irpn-lab.github.io/FlowVLA/"
        },
        {
          "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible",
          "url": "http://arxiv.org/abs/2508.16292v1",
          "snippet": "Recently, Vision-Language-Action (VLA) models have demonstrated strong performance on a range of robotic tasks. These models rely on multimodal inputs, with language instructions playing a crucial role -- not only in predicting actions, but also in robustly interpreting user intent, even when the requests are impossible to fulfill. In this work, we investigate how VLAs can recognize, interpret, and respond to false-premise instructions: natural language commands that reference objects or conditions absent from the environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that (i) detects when an instruction cannot be executed due to a false premise, (ii) engages in language-based clarification or correction, and (iii) grounds plausible alternatives in perception and action. Towards this end, we construct a large-scale instruction tuning setup with structured language prompts and train a VLA model capable of handling both accurate and erroneous requests. Our approach leverages a contextually augmented, semi-synthetic dataset containing paired positive and false-premise instructions, enabling robust detection and natural language correction. Our experiments show that IVA improves false premise detection accuracy by 97.56% over baselines, while increasing successful responses in false-premise scenarios by 50.78%.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-08-22T10:54:33Z",
          "authors": [
            "Wen-Han Hsieh",
            "Elvis Hsieh",
            "Dantong Niu",
            "Trevor Darrell",
            "Roei Herzig",
            "David M. Chan"
          ],
          "arxiv_id": "2508.16292",
          "abstract": "Recently, Vision-Language-Action (VLA) models have demonstrated strong performance on a range of robotic tasks. These models rely on multimodal inputs, with language instructions playing a crucial role -- not only in predicting actions, but also in robustly interpreting user intent, even when the requests are impossible to fulfill. In this work, we investigate how VLAs can recognize, interpret, and respond to false-premise instructions: natural language commands that reference objects or conditions absent from the environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that (i) detects when an instruction cannot be executed due to a false premise, (ii) engages in language-based clarification or correction, and (iii) grounds plausible alternatives in perception and action. Towards this end, we construct a large-scale instruction tuning setup with structured language prompts and train a VLA model capable of handling both accurate and erroneous requests. Our approach leverages a contextually augmented, semi-synthetic dataset containing paired positive and false-premise instructions, enabling robust detection and natural language correction. Our experiments show that IVA improves false premise detection accuracy by 97.56% over baselines, while increasing successful responses in false-premise scenarios by 50.78%.",
          "abstract_zh": "æœ€è¿‘ï¼Œè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨ä¸€ç³»åˆ—æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹ä¾èµ–äºå¤šæ¨¡å¼è¾“å…¥ï¼Œè¯­è¨€æŒ‡ä»¤å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨â€”â€”ä¸ä»…åœ¨é¢„æµ‹åŠ¨ä½œæ–¹é¢ï¼Œè€Œä¸”åœ¨å¼ºæœ‰åŠ›åœ°è§£é‡Šç”¨æˆ·æ„å›¾æ–¹é¢ï¼Œå³ä½¿è¯·æ±‚æ— æ³•æ»¡è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº† VLA å¦‚ä½•è¯†åˆ«ã€è§£é‡Šå’Œå“åº”é”™è¯¯å‰ææŒ‡ä»¤ï¼šå¼•ç”¨ç¯å¢ƒä¸­ä¸å­˜åœ¨çš„å¯¹è±¡æˆ–æ¡ä»¶çš„è‡ªç„¶è¯­è¨€å‘½ä»¤ã€‚æˆ‘ä»¬æå‡ºæŒ‡ä»¤éªŒè¯å’Œè¡ŒåŠ¨ï¼ˆIVAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå®ƒï¼ˆiï¼‰æ£€æµ‹ä½•æ—¶ç”±äºé”™è¯¯çš„å‰æè€Œæ— æ³•æ‰§è¡ŒæŒ‡ä»¤ï¼Œï¼ˆiiï¼‰è¿›è¡ŒåŸºäºè¯­è¨€çš„æ¾„æ¸…æˆ–çº æ­£ï¼Œä»¥åŠï¼ˆiiiï¼‰åœ¨æ„ŸçŸ¥å’Œè¡ŒåŠ¨ä¸­æä¾›åˆç†çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…·æœ‰ç»“æ„åŒ–è¯­è¨€æç¤ºçš„å¤§è§„æ¨¡æŒ‡ä»¤è°ƒä¼˜è®¾ç½®ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å‡†ç¡®å’Œé”™è¯¯è¯·æ±‚çš„ VLA æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸Šä¸‹æ–‡å¢å¼ºçš„åŠåˆæˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«é…å¯¹çš„è‚¯å®šå’Œé”™è¯¯å‰ææŒ‡ä»¤ï¼Œä»è€Œå®ç°ç¨³å¥çš„æ£€æµ‹å’Œè‡ªç„¶è¯­è¨€æ ¡æ­£ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒIVA å°†é”™è¯¯å‰ææ£€æµ‹å‡†ç¡®åº¦æé«˜äº† 97.56%ï¼ŒåŒæ—¶å°†é”™è¯¯å‰æåœºæ™¯ä¸­çš„æˆåŠŸå“åº”æé«˜äº† 50.78%ã€‚"
        },
        {
          "title": "4D Visual Pre-training for Robot Learning",
          "url": "http://arxiv.org/abs/2508.17230v2",
          "snippet": "General visual representations learned from web-scale datasets for robotics have achieved great success in recent years, enabling data-efficient robot learning on manipulation tasks; yet these pre-trained representations are mostly on 2D images, neglecting the inherent 3D nature of the world. However, due to the scarcity of large-scale 3D data, it is still hard to extract a universal 3D representation from web datasets. Instead, we are seeking a general visual pre-training framework that could improve all 3D representations as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training framework for real-world robot learning. FVP frames the visual pre-training objective as a next-point-cloud-prediction problem, models the prediction model as a diffusion model, and pre-trains the model on the larger public datasets directly. Across twelve real-world manipulation tasks, FVP boosts the average success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP pre-trained DP3 achieves state-of-the-art performance across imitation learning methods. Moreover, the efficacy of FVP adapts across various point cloud encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger Vision-Language-Action robotic model, enhancing its performance on various robot tasks. Our project page is available at: https://4d-visual-pretraining.github.io/",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-08-24T07:06:56Z",
          "authors": [
            "Chengkai Hou",
            "Yanjie Ze",
            "Yankai Fu",
            "Zeyu Gao",
            "Songbo Hu",
            "Yue Yu",
            "Shanghang Zhang",
            "Huazhe Xu"
          ],
          "arxiv_id": "2508.17230",
          "abstract": "General visual representations learned from web-scale datasets for robotics have achieved great success in recent years, enabling data-efficient robot learning on manipulation tasks; yet these pre-trained representations are mostly on 2D images, neglecting the inherent 3D nature of the world. However, due to the scarcity of large-scale 3D data, it is still hard to extract a universal 3D representation from web datasets. Instead, we are seeking a general visual pre-training framework that could improve all 3D representations as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training framework for real-world robot learning. FVP frames the visual pre-training objective as a next-point-cloud-prediction problem, models the prediction model as a diffusion model, and pre-trains the model on the larger public datasets directly. Across twelve real-world manipulation tasks, FVP boosts the average success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP pre-trained DP3 achieves state-of-the-art performance across imitation learning methods. Moreover, the efficacy of FVP adapts across various point cloud encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger Vision-Language-Action robotic model, enhancing its performance on various robot tasks. Our project page is available at: https://4d-visual-pretraining.github.io/",
          "abstract_zh": "è¿‘å¹´æ¥ï¼Œä»ç½‘ç»œè§„æ¨¡çš„æœºå™¨äººæ•°æ®é›†ä¸­å­¦ä¹ çš„é€šç”¨è§†è§‰è¡¨ç¤ºå–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œä½¿å¾—æœºå™¨äººèƒ½å¤Ÿåœ¨æ“ä½œä»»åŠ¡ä¸Šè¿›è¡Œæ•°æ®é«˜æ•ˆçš„å­¦ä¹ ï¼›ç„¶è€Œè¿™äº›é¢„å…ˆè®­ç»ƒçš„è¡¨ç¤ºå¤§å¤šæ˜¯åœ¨ 2D å›¾åƒä¸Šï¼Œå¿½ç•¥äº†ä¸–ç•Œå›ºæœ‰çš„ 3D æ€§è´¨ã€‚ç„¶è€Œï¼Œç”±äºå¤§è§„æ¨¡ 3D æ•°æ®çš„ç¨€ç¼ºï¼Œä»ç½‘ç»œæ•°æ®é›†ä¸­æå–é€šç”¨çš„ 3D è¡¨ç¤ºä»ç„¶å¾ˆå›°éš¾ã€‚ç›¸åï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ç§é€šç”¨çš„è§†è§‰é¢„è®­ç»ƒæ¡†æ¶ï¼Œä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒå¯ä»¥æ”¹å–„æ‰€æœ‰ 3D è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ¡†æ¶ç§°ä¸º FVPï¼Œæ˜¯ä¸€ç§æ–°é¢–çš„ 4D è§†è§‰é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºç°å®ä¸–ç•Œçš„æœºå™¨äººå­¦ä¹ ã€‚FVP å°†è§†è§‰é¢„è®­ç»ƒç›®æ ‡æ„å»ºä¸ºä¸‹ä¸€ä¸ªç‚¹äº‘é¢„æµ‹é—®é¢˜ï¼Œå°†é¢„æµ‹æ¨¡å‹å»ºæ¨¡ä¸ºæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ç›´æ¥åœ¨æ›´å¤§çš„å…¬å…±æ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ 12 ä¸ªç°å®ä¸–ç•Œçš„æ“ä½œä»»åŠ¡ä¸­ï¼ŒFVP å°†è¿™äº›ä»»åŠ¡çš„ 3D æ‰©æ•£ç­–ç•¥ (DP3) çš„å¹³å‡æˆåŠŸç‡æé«˜äº† 28%ã€‚ç»è¿‡ FVP é¢„è®­ç»ƒçš„ DP3 åœ¨æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFVP çš„åŠŸæ•ˆé€‚ç”¨äºå„ç§ç‚¹äº‘ç¼–ç å™¨å’Œæ•°æ®é›†ã€‚æœ€åï¼Œæˆ‘ä»¬å°† FVP åº”ç”¨äºæ›´å¤§çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæœºå™¨äººæ¨¡å‹ RDT-1Bï¼Œå¢å¼ºå…¶åœ¨å„ç§æœºå™¨äººä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ä½äºï¼šhttps://4d-visual-pretraining.github.io/"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "å¤´éƒ¨ç©å®¶æœ¬å‘¨æ— æ›´æ–°ã€‚",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com ä¸Šå…±å‘ç° 9 æ¡ä¸ VLA ç›¸å…³çš„æ›´æ–°å†…å®¹ï¼ˆæŒ‰æœç´¢ç›¸å…³æ€§æ’åºï¼Œæ˜¾ç¤º Top 9ï¼‰ã€‚",
      "items": [
        {
          "title": "patrick-llgc/Learning-Deep-Learning",
          "url": "https://github.com/patrick-llgc/Learning-Deep-Learning",
          "snippet": "Paper reading notes on Deep Learning and Machine Learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics ç¤¾åŒºå…·èº«æ™ºèƒ½å­¦ä¹ æŒ‡å—ï¼šæˆ‘ä»¬æŠŠâ€œå…·èº«ç»¼è¿°â†’å­¦ä¹ è·¯çº¿â†’ä»¿çœŸå­¦ä¹ â†’å¼€æºå®ç‰©â†’äººç‰©è®¿è°ˆâ†’å…¬å¸å›¾è°±â€ä¸²èµ·æ¥ï¼Œå¸®åŠ©æ–°æ‰‹å’Œå®æˆ˜è€…å¿«é€Ÿå®šä½è·¯å¾„ã€è½åœ°é¡¹ç›®ä¸å‚ä¸å¼€æºã€‚",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "NVIDIA/Isaac-GR00T",
          "url": "https://github.com/NVIDIA/Isaac-GR00T",
          "snippet": "NVIDIA Isaac GR00T N1.6 -  A Foundation Model for Generalist Robots.",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news å‘¨æŠ¥å†…å®¹ æ¯å‘¨äºŒ0ç‚¹å·¦å³æ›´æ–°",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. ğŸ”‰Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "yang-zj1026/NaVILA-Bench",
          "url": "https://github.com/yang-zj1026/NaVILA-Bench",
          "snippet": "Vision-Language Navigation Benchmark in Isaac Lab",
          "site": "github.com",
          "rank": 9
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co æœ¬å‘¨æ— æ›´æ–°ã€‚",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com æœ¬å‘¨æ— æ›´æ–°ã€‚",
      "items": []
    }
  ],
  "week_start": "2025-08-18",
  "week_end": "2025-08-24",
  "last_updated": "2026-01-07"
}