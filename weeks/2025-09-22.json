{
  "generated_at": "2026-01-07T13:31:18.354487",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 30 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 30）。",
      "items": [
        {
          "title": "Control Your Robot: A Unified System for Robot Control and Policy Deployment",
          "url": "http://arxiv.org/abs/2509.23823v2",
          "snippet": "Cross-platform robot control remains difficult because hardware interfaces, data formats, and control paradigms vary widely, which fragments toolchains and slows deployment. To address this, we present Control Your Robot, a modular, general-purpose framework that unifies data collection and policy deployment across diverse platforms. The system reduces fragmentation through a standardized workflow with modular design, unified APIs, and a closed-loop architecture. It supports flexible robot registration, dual-mode control with teleoperation and trajectory playback, and seamless integration from multimodal data acquisition to inference. Experiments on single-arm and dual-arm systems show efficient, low-latency data collection and effective support for policy learning with imitation learning and vision-language-action models. Policies trained on data gathered by Control Your Robot match expert demonstrations closely, indicating that the framework enables scalable and reproducible robot learning across platforms.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-09-28T11:51:29Z",
          "authors": [
            "Tian Nian",
            "Weijie Ke",
            "Shaolong Zhu",
            "Bingshan Hu"
          ],
          "arxiv_id": "2509.23823",
          "abstract": "Cross-platform robot control remains difficult because hardware interfaces, data formats, and control paradigms vary widely, which fragments toolchains and slows deployment. To address this, we present Control Your Robot, a modular, general-purpose framework that unifies data collection and policy deployment across diverse platforms. The system reduces fragmentation through a standardized workflow with modular design, unified APIs, and a closed-loop architecture. It supports flexible robot registration, dual-mode control with teleoperation and trajectory playback, and seamless integration from multimodal data acquisition to inference. Experiments on single-arm and dual-arm systems show efficient, low-latency data collection and effective support for policy learning with imitation learning and vision-language-action models. Policies trained on data gathered by Control Your Robot match expert demonstrations closely, indicating that the framework enables scalable and reproducible robot learning across platforms.",
          "abstract_zh": "跨平台机器人控制仍然很困难，因为硬件接口、数据格式和控制范式差异很大，这会导致工具链碎片化并减慢部署速度。为了解决这个问题，我们推出了控制你的机器人，这是一个模块化的通用框架，可以跨不同平台统一数据收集和策略部署。该系统通过模块化设计、统一API和闭环架构的标准化工作流程来减少碎片化。它支持灵活的机器人注册、远程操作和轨迹回放的双模控制，以及从多模态数据采集到推理的无缝集成。单臂和双臂系统的实验表明，通过模仿学习和视觉-语言-动作模型，可以实现高效、低延迟的数据收集和对政策学习的有效支持。根据“控制你的机器人”收集的数据训练的策略与专家演示非常匹配，表明该框架能够实现跨平台的可扩展和可重复的机器人学习。"
        },
        {
          "title": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models",
          "url": "http://arxiv.org/abs/2509.23655v1",
          "snippet": "Vision-Language-Action (VLA) models offer a pivotal approach to learning robotic manipulation at scale by repurposing large pre-trained Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs for robotic domains comes with an unnecessarily high computational cost, which we attribute to the tokenization scheme of visual inputs. In this work, we aim to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric Tokenization for VLAs. Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. As a result, we find that Oat-VLA can drastically reduce the number of visual tokens to just a few tokens without sacrificing performance. We reveal that Oat-VLA converges at least twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in diverse real-world pick and place tasks.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-09-28T05:42:53Z",
          "authors": [
            "Rokas Bendikas",
            "Daniel Dijkman",
            "Markus Peschl",
            "Sanjay Haresh",
            "Pietro Mazzaglia"
          ],
          "arxiv_id": "2509.23655",
          "abstract": "Vision-Language-Action (VLA) models offer a pivotal approach to learning robotic manipulation at scale by repurposing large pre-trained Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs for robotic domains comes with an unnecessarily high computational cost, which we attribute to the tokenization scheme of visual inputs. In this work, we aim to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric Tokenization for VLAs. Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. As a result, we find that Oat-VLA can drastically reduce the number of visual tokens to just a few tokens without sacrificing performance. We reveal that Oat-VLA converges at least twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in diverse real-world pick and place tasks.",
          "abstract_zh": "视觉语言动作（VLA）模型通过重新利用大型预训练视觉语言模型（VLM）来输出机器人动作，为大规模学习机器人操作提供了一种关键方法。然而，将 VLM 应用于机器人领域会带来不必要的高计算成本，我们将其归因于视觉输入的标记化方案。在这项工作中，我们的目标是通过提出 Oat-VLA（一种以对象代理为中心的 VLA 标记化）来实现高效的 VLA 训练。基于以对象为中心的表示学习的见解，我们的方法引入了对场景对象和代理自身视觉信息的归纳偏差。结果，我们发现 Oat-VLA 可以将视觉标记的数量大幅减少到几个标记，而不会牺牲性能。我们发现，Oat-VLA 在 LIBERO 套件上的收敛速度至少是 OpenVLA 的两倍，并且在各种现实世界的拾取和放置任务中优于 OpenVLA。"
        },
        {
          "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
          "url": "http://arxiv.org/abs/2509.19012v3",
          "snippet": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-09-23T13:53:52Z",
          "authors": [
            "Dapeng Zhang",
            "Jing Sun",
            "Chenghui Hu",
            "Xiaoyan Wu",
            "Zhenlong Yuan",
            "Rui Zhou",
            "Fei Shen",
            "Qingguo Zhou"
          ],
          "arxiv_id": "2509.19012",
          "abstract": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
          "abstract_zh": "视觉语言动作（VLA）模型的出现标志着从传统的基于策略的控制到广义机器人技术的范式转变，将视觉语言模型（VLM）从被动序列生成器重新构建为在复杂动态环境中进行操纵和决策的主动代理。这项调查深入研究了先进的 VLA 方法，旨在提供清晰的分类法并对现有研究进行系统、全面的回顾。它对不同场景下的 VLA 应用进行了全面分析，并将 VLA 方法分为几种范式：基于自回归、基于扩散、基于强化、混合和专门方法；同时详细检查他们的动机、核心战略和实施情况。此外，还介绍了基础数据集、基准测试和模拟平台。在当前 VLA 格局的基础上，该综述进一步提出了对关键挑战和未来方向的看法，以推进 VLA 模型和通用机器人技术的研究。通过综合最近三百多项研究的见解，本次调查描绘了这个快速发展的领域的轮廓，并强调了将塑造可扩展的通用 VLA 方法的发展的机遇和挑战。"
        },
        {
          "title": "EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer",
          "url": "http://arxiv.org/abs/2509.22407v1",
          "snippet": "Vision-language-action (VLA) models increasingly rely on diverse training data to achieve robust generalization. However, collecting large-scale real-world robot manipulation data across varied object appearances and environmental conditions remains prohibitively time-consuming and expensive. To overcome this bottleneck, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates a generative data engine with an effective training pipeline. We introduce DreamTransfer, a diffusion Transformer-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos. DreamTransfer enables text-controlled visual editing of robot videos, transforming foreground, background, and lighting conditions without compromising 3D structure or geometrical plausibility. Furthermore, we explore hybrid training with real and generated data, and introduce AdaMix, a hard-sample-aware training strategy that dynamically reweights training batches to focus optimization on perceptually or kinematically challenging samples. Extensive experiments show that videos generated by DreamTransfer significantly outperform prior video generation methods in multi-view consistency, geometric fidelity, and text-conditioning accuracy. Crucially, VLAs trained with generated data enable robots to generalize to unseen object categories and novel visual domains using only demonstrations from a single appearance. In real-world robotic manipulation tasks with zero-shot visual domains, our approach achieves over a 200% relative performance gain compared to training on real data alone, and further improves by 13% with AdaMix, demonstrating its effectiveness in boosting policy generalization.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-09-26T14:34:44Z",
          "authors": [
            "Zhehao Dong",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Yirui Wang",
            "Yang Wang",
            "Yukun Zhou",
            "Boyuan Wang",
            "Chaojun Ni",
            "Runqi Ouyang",
            "Wenkang Qin",
            "Xinze Chen",
            "Yun Ye",
            "Guan Huang"
          ],
          "arxiv_id": "2509.22407",
          "abstract": "Vision-language-action (VLA) models increasingly rely on diverse training data to achieve robust generalization. However, collecting large-scale real-world robot manipulation data across varied object appearances and environmental conditions remains prohibitively time-consuming and expensive. To overcome this bottleneck, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates a generative data engine with an effective training pipeline. We introduce DreamTransfer, a diffusion Transformer-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos. DreamTransfer enables text-controlled visual editing of robot videos, transforming foreground, background, and lighting conditions without compromising 3D structure or geometrical plausibility. Furthermore, we explore hybrid training with real and generated data, and introduce AdaMix, a hard-sample-aware training strategy that dynamically reweights training batches to focus optimization on perceptually or kinematically challenging samples. Extensive experiments show that videos generated by DreamTransfer significantly outperform prior video generation methods in multi-view consistency, geometric fidelity, and text-conditioning accuracy. Crucially, VLAs trained with generated data enable robots to generalize to unseen object categories and novel visual domains using only demonstrations from a single appearance. In real-world robotic manipulation tasks with zero-shot visual domains, our approach achieves over a 200% relative performance gain compared to training on real data alone, and further improves by 13% with AdaMix, demonstrating its effectiveness in boosting policy generalization.",
          "abstract_zh": "视觉-语言-动作（VLA）模型越来越依赖多样化的训练数据来实现强大的泛化。然而，在不同的物体外观和环境条件下收集大规模的现实世界机器人操作数据仍然非常耗时且昂贵。为了克服这一瓶颈，我们提出了嵌入操纵媒体适应（EMMA），这是一种 VLA 策略增强框架，它将生成数据引擎与有效的训练管道集成在一起。我们介绍 DreamTransfer，一个基于扩散 Transformer 的框架，用于生成多视图一致、基于几何的具体操作视频。DreamTransfer 可以对机器人视频进行文本控制的可视化编辑，变换前景、背景和照明条件，而不会影响 3D 结构或几何合理性。此外，我们探索了使用真实数据和生成数据的混合训练，并引入了 AdaMix，这是一种硬样本感知训练策略，可动态重新调整训练批次的权重，以将优化重点放在感知或运动学上具有挑战性的样本上。大量实验表明，DreamTransfer 生成的视频在多视图一致性、几何保真度和文本调节准确性方面显着优于先前的视频生成方法。至关重要的是，使用生成的数据训练的 VLA 使机器人能够仅使用单一外观的演示来概括看不见的物体类别和新颖的视觉领域。在具有零镜头视觉域的现实世界机器人操作任务中，与仅使用真实数据进行训练相比，我们的方法实现了超过 200% 的相对性能增益，并且使用 AdaMix 进一步提高了 13%，证明了其在促进策略泛化方面的有效性。"
        },
        {
          "title": "ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2509.20841v1",
          "snippet": "End-to-end robot manipulation policies offer significant potential for enabling embodied agents to understand and interact with the world. Unlike traditional modular pipelines, end-to-end learning mitigates key limitations such as information loss between modules and feature misalignment caused by isolated optimization targets. Despite these advantages, existing end-to-end neural networks for robotic manipulation--including those based on large VLM/VLA models--remain insufficiently performant for large-scale practical deployment. In this paper, we take a step towards an end-to-end manipulation policy that is generalizable, accurate and reliable. To achieve this goal, we propose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for robotic manipulation. Our formulation is used as the action representation of a neural policy, which can be trained in an end-to-end fashion. Such an action representation is general, as it extends the standard end-effector pose action representation and supports a diverse set of manipulation tasks in a unified manner. The oriented keypoint in our method enables natural generalization to objects with different shapes and sizes, while achieving sub-centimeter accuracy. Moreover, our formulation can easily handle multi-stage tasks, multi-modal robot behaviors, and deformable objects. Extensive simulated and hardware experiments demonstrate the effectiveness of our method.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-09-25T07:29:07Z",
          "authors": [
            "Dekun Lu",
            "Wei Gao",
            "Kui Jia"
          ],
          "arxiv_id": "2509.20841",
          "abstract": "End-to-end robot manipulation policies offer significant potential for enabling embodied agents to understand and interact with the world. Unlike traditional modular pipelines, end-to-end learning mitigates key limitations such as information loss between modules and feature misalignment caused by isolated optimization targets. Despite these advantages, existing end-to-end neural networks for robotic manipulation--including those based on large VLM/VLA models--remain insufficiently performant for large-scale practical deployment. In this paper, we take a step towards an end-to-end manipulation policy that is generalizable, accurate and reliable. To achieve this goal, we propose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for robotic manipulation. Our formulation is used as the action representation of a neural policy, which can be trained in an end-to-end fashion. Such an action representation is general, as it extends the standard end-effector pose action representation and supports a diverse set of manipulation tasks in a unified manner. The oriented keypoint in our method enables natural generalization to objects with different shapes and sizes, while achieving sub-centimeter accuracy. Moreover, our formulation can easily handle multi-stage tasks, multi-modal robot behaviors, and deformable objects. Extensive simulated and hardware experiments demonstrate the effectiveness of our method.",
          "abstract_zh": "端到端的机器人操纵策略为实体代理理解世界并与世界互动提供了巨大的潜力。与传统的模块化管道不同，端到端学习减轻了关键限制，例如模块之间的信息丢失以及由孤立的优化目标引起的特征错位。尽管有这些优点，现有的用于机器人操作的端到端神经网络（包括基于大型 VLM/VLA 模型的神经网络）对于大规模实际部署来说仍然性能不足。在本文中，我们朝着可推广、准确和可靠的端到端操纵策略迈出了一步。为了实现这一目标，我们提出了一种用于机器人操作的新型移动导向关键点链（CoMOK）公式。我们的公式用作神经策略的动作表示，可以以端到端的方式进行训练。这种动作表示是通用的，因为它扩展了标准末端执行器姿势动作表示，并以统一的方式支持一组不同的操作任务。我们的方法中的定向关键点可以自然地推广到不同形状和大小的物体，同时实现亚厘米级的精度。此外，我们的公式可以轻松处理多阶段任务、多模式机器人行为和可变形物体。大量的模拟和硬件实验证明了我们方法的有效性。"
        },
        {
          "title": "IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks",
          "url": "http://arxiv.org/abs/2509.24768v1",
          "snippet": "Vision-language-action models (VLAs) have become an increasingly popular approach for addressing robot manipulation problems in recent years. However, such models need to output actions at a rate suitable for robot control, which limits the size of the language model they can be based on, and consequently, their language understanding capabilities. Manipulation tasks may require complex language instructions, such as identifying target objects by their relative positions, to specify human intention. Therefore, we introduce IA-VLA, a framework that utilizes the extensive language understanding of a large vision language model as a pre-processing stage to generate improved context to augment the input of a VLA. We evaluate the framework on a set of semantically complex tasks which have been underexplored in VLA literature, namely tasks involving visual duplicates, i.e., visually indistinguishable objects. A dataset of three types of scenes with duplicate objects is used to compare a baseline VLA against two augmented variants. The experiments show that the VLA benefits from the augmentation scheme, especially when faced with language instructions that require the VLA to extrapolate from concepts it has seen in the demonstrations. For the code, dataset, and videos, see https://sites.google.com/view/ia-vla.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-09-29T13:34:59Z",
          "authors": [
            "Eric Hannus",
            "Miika Malin",
            "Tran Nguyen Le",
            "Ville Kyrki"
          ],
          "arxiv_id": "2509.24768",
          "abstract": "Vision-language-action models (VLAs) have become an increasingly popular approach for addressing robot manipulation problems in recent years. However, such models need to output actions at a rate suitable for robot control, which limits the size of the language model they can be based on, and consequently, their language understanding capabilities. Manipulation tasks may require complex language instructions, such as identifying target objects by their relative positions, to specify human intention. Therefore, we introduce IA-VLA, a framework that utilizes the extensive language understanding of a large vision language model as a pre-processing stage to generate improved context to augment the input of a VLA. We evaluate the framework on a set of semantically complex tasks which have been underexplored in VLA literature, namely tasks involving visual duplicates, i.e., visually indistinguishable objects. A dataset of three types of scenes with duplicate objects is used to compare a baseline VLA against two augmented variants. The experiments show that the VLA benefits from the augmentation scheme, especially when faced with language instructions that require the VLA to extrapolate from concepts it has seen in the demonstrations. For the code, dataset, and videos, see https://sites.google.com/view/ia-vla.",
          "abstract_zh": "近年来，视觉-语言-动作模型（VLA）已成为解决机器人操作问题的越来越流行的方法。然而，此类模型需要以适合机器人控制的速率输出动作，这限制了它们所基于的语言模型的大小，从而限制了它们的语言理解能力。操纵任务可能需要复杂的语言指令，例如通过目标对象的相对位置来识别目标对象，以指定人类意图。因此，我们引入了 IA-VLA，该框架利用大型视觉语言模型的广泛语言理解作为预处理阶段来生成改进的上下文以增强 VLA 的输入。我们在一组语义复杂的任务上评估该框架，这些任务在 VLA 文献中尚未得到充分探索，即涉及视觉重复的任务，即视觉上无法区分的对象。具有重复对象的三种类型场景的数据集用于将基线 VLA 与两个增强变体进行比较。实验表明，VLA 受益于增强方案，尤其是在面对需要 VLA 从演示中看到的概念进行推断的语言指令时。有关代码、数据集和视频，请参阅 https://sites.google.com/view/ia-vla。"
        },
        {
          "title": "MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training",
          "url": "http://arxiv.org/abs/2509.22199v2",
          "snippet": "Vision Language Action (VLA) models derive their generalization capability from diverse training data, yet collecting embodied robot interaction data remains prohibitively expensive. In contrast, human demonstration videos are far more scalable and cost-efficient to collect, and recent studies confirm their effectiveness in training VLA models. However, a significant domain gap persists between human videos and robot-executed videos, including unstable camera viewpoints, visual discrepancies between human hands and robotic arms, and differences in motion dynamics. To bridge this gap, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoint, and actions to directly support policy training. For visual alignment, we propose H2R Aligner, a video diffusion model that generates high-fidelity robot demonstration videos by transferring motion from human manipulation footage. For viewpoint stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos via homography and inpaints occlusions and distortions caused by warping. For action alignment, we map human hand trajectories to the robot frame and apply a constrained inverse kinematics solver to produce feasible, low-jitter joint commands with accurate pose tracking. Empirically, VLA models trained purely on our synthesized human-to-robot videos achieve few-shot execution on real robots. Moreover, scaling training with human data significantly boosts performance compared to models trained solely on real robot data; our approach improves the average success rate by 14.7\\% across six representative manipulation tasks.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-09-26T11:05:10Z",
          "authors": [
            "Haoyun Li",
            "Ivan Zhang",
            "Runqi Ouyang",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Zhiqin Yang",
            "Zhentao Zhang",
            "Boyuan Wang",
            "Chaojun Ni",
            "Wenkang Qin",
            "Xinze Chen",
            "Yun Ye",
            "Guan Huang",
            "Zhenbo Song",
            "Xingang Wang"
          ],
          "arxiv_id": "2509.22199",
          "abstract": "Vision Language Action (VLA) models derive their generalization capability from diverse training data, yet collecting embodied robot interaction data remains prohibitively expensive. In contrast, human demonstration videos are far more scalable and cost-efficient to collect, and recent studies confirm their effectiveness in training VLA models. However, a significant domain gap persists between human videos and robot-executed videos, including unstable camera viewpoints, visual discrepancies between human hands and robotic arms, and differences in motion dynamics. To bridge this gap, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoint, and actions to directly support policy training. For visual alignment, we propose H2R Aligner, a video diffusion model that generates high-fidelity robot demonstration videos by transferring motion from human manipulation footage. For viewpoint stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos via homography and inpaints occlusions and distortions caused by warping. For action alignment, we map human hand trajectories to the robot frame and apply a constrained inverse kinematics solver to produce feasible, low-jitter joint commands with accurate pose tracking. Empirically, VLA models trained purely on our synthesized human-to-robot videos achieve few-shot execution on real robots. Moreover, scaling training with human data significantly boosts performance compared to models trained solely on real robot data; our approach improves the average success rate by 14.7\\% across six representative manipulation tasks.",
          "abstract_zh": "视觉语言动作（VLA）模型从不同的训练数据中获得泛化能力，但收集具体的机器人交互数据仍然非常昂贵。相比之下，人类演示视频的可扩展性和收集成本效益要高得多，最近的研究证实了它们在训练 VLA 模型方面的有效性。然而，人类视频和机器人执行的视频之间仍然存在显着的领域差距，包括不稳定的摄像机视点、人手和机器人手臂之间的视觉差异以及运动动力学的差异。为了弥补这一差距，我们提出了 MimicDreamer，这是一个框架，通过联合调整愿景、观点和行动来直接支持政策培训，将快速、低成本的人类演示转变为机器人可用的监督。对于视觉对齐，我们提出了 H2R Aligner，这是一种视频扩散模型，可通过传输人类操作镜头的运动来生成高保真机器人演示视频。为了稳定视点，提出了 EgoStabilizer，它通过单应性规范化以自我为中心的视频，并修复由扭曲引起的遮挡和扭曲。对于动作对齐，我们将人手轨迹映射到机器人框架，并应用约束逆运动学解算器来生成具有精确姿态跟踪的可行、低抖动关节命令。根据经验，纯粹根据我们合成的人机视频训练的 VLA 模型可以在真实机器人上实现少量执行。此外，与仅基于真实机器人数据训练的模型相比，使用人类数据进行扩展训练可以显着提高性能；我们的方法将六个代表性操作任务的平均成功率提高了 14.7%。"
        },
        {
          "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
          "url": "http://arxiv.org/abs/2509.18953v1",
          "snippet": "Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-09-23T13:02:23Z",
          "authors": [
            "Hanqing Liu",
            "Jiahuan Long",
            "Junqi Wu",
            "Jiacheng Hou",
            "Huili Tang",
            "Tingsong Jiang",
            "Weien Zhou",
            "Wen Yao"
          ],
          "arxiv_id": "2509.18953",
          "abstract": "Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.",
          "abstract_zh": "视觉-语言-动作（VLA）模型已成为机器人操作的有前途的解决方案，但其对现实世界物理变化的鲁棒性仍然严重不足。为了弥补这一差距，我们提出了 Eva-VLA，这是第一个统一框架，通过将离散物理变化转化为连续优化问题来系统地评估 VLA 模型的鲁棒性。然而，全面评估 VLA 稳健性面临两个关键挑战：(1) 如何系统地表征现实世界部署中遇到的各种物理变化，同时保持评估的可重复性；(2) 如何在不产生过高的现实世界数据收集成本的情况下有效地发现最坏情况场景。为了解决第一个挑战，我们将现实世界的变化分解为三个关键领域：影响空间推理的对象 3D 变换、挑战视觉感知的照明变化以及破坏场景理解的对抗性补丁。对于第二个挑战，我们引入了连续的黑盒优化框架，将离散的物理变化转化为参数优化，从而能够系统地探索最坏的情况。在多个基准测试中对最先进的 OpenVLA 模型进行的广泛实验揭示了令人震惊的漏洞：所有变体类型都会触发超过 60% 的失败率，其中对象转换导致长期任务中高达 97.8% 的失败。我们的研究结果揭示了受控实验室成功与不可预测的部署准备之间的关键差距，而 Eva-VLA 框架为强化基于 VLA 的机器人操作模型应对现实世界的部署挑战提供了一条实用途径。"
        },
        {
          "title": "VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search",
          "url": "http://arxiv.org/abs/2509.22643v1",
          "snippet": "Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-09-26T17:59:40Z",
          "authors": [
            "Wenkai Guo",
            "Guanxing Lu",
            "Haoyuan Deng",
            "Zhenyu Wu",
            "Yansong Tang",
            "Ziwei Wang"
          ],
          "arxiv_id": "2509.22643",
          "abstract": "Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.",
          "abstract_zh": "视觉-语言-动作模型（VLA）通过扩展模仿学习，在一般机器人操作任务中取得了优异的性能。然而，现有的 VLA 仅限于预测短视的下一步行动，由于增量偏差，它们难以应对长期轨迹任务。为了解决这个问题，我们提出了一个名为 VLA-Reasoner 的插件框架，它有效地赋予现成的 VLA 通过测试时间扩展来预测未来状态的能力。具体来说，VLA-Reasoner 采样并推出可能的动作轨迹，其中所涉及的动作是通过世界模型生成未来状态的基本原理，这使得 VLA-Reasoner 能够预见和推理潜在结果并搜索最佳动作。我们进一步利用蒙特卡洛树搜索 (MCTS) 来提高大型动作空间中的搜索效率，其中逐步 VLA 预测播种根。同时，我们引入了基于核密度估计（KDE）的置信采样机制，以实现 MCTS 中的有效探索，而无需冗余的 VLA 查询。我们通过离线奖励塑造策略评估 MCTS 中的中间状态，以对预测的未来进行评分并通过长期反馈纠正偏差。我们在模拟器和现实世界中进行了广泛的实验，证明我们提出的 VLA-Reasoner 比最先进的 VLA 实现了显着改进。我们的方法突出了机器人操作的可扩展测试时间计算的潜在途径。"
        },
        {
          "title": "OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation",
          "url": "http://arxiv.org/abs/2509.19480v1",
          "snippet": "Humans can flexibly interpret and compose different goal specifications, such as language instructions, spatial coordinates, or visual references, when navigating to a destination. In contrast, most existing robotic navigation policies are trained on a single modality, limiting their adaptability to real-world scenarios where different forms of goal specification are natural and complementary. In this work, we present a training framework for robotic foundation models that enables omni-modal goal conditioning for vision-based navigation. Our approach leverages a high-capacity vision-language-action (VLA) backbone and trains with three primary goal modalities: 2D poses, egocentric images, and natural language, as well as their combinations, through a randomized modality fusion strategy. This design not only expands the pool of usable datasets but also encourages the policy to develop richer geometric, semantic, and visual representations. The resulting model, OmniVLA, achieves strong generalization to unseen environments, robustness to scarce modalities, and the ability to follow novel natural language instructions. We demonstrate that OmniVLA outperforms specialist baselines across modalities and offers a flexible foundation for fine-tuning to new modalities and tasks. We believe OmniVLA provides a step toward broadly generalizable and flexible navigation policies, and a scalable path for building omni-modal robotic foundation models. We present videos showcasing OmniVLA performance and will release its checkpoints and training code on our project page.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-09-23T18:40:29Z",
          "authors": [
            "Noriaki Hirose",
            "Catherine Glossop",
            "Dhruv Shah",
            "Sergey Levine"
          ],
          "arxiv_id": "2509.19480",
          "abstract": "Humans can flexibly interpret and compose different goal specifications, such as language instructions, spatial coordinates, or visual references, when navigating to a destination. In contrast, most existing robotic navigation policies are trained on a single modality, limiting their adaptability to real-world scenarios where different forms of goal specification are natural and complementary. In this work, we present a training framework for robotic foundation models that enables omni-modal goal conditioning for vision-based navigation. Our approach leverages a high-capacity vision-language-action (VLA) backbone and trains with three primary goal modalities: 2D poses, egocentric images, and natural language, as well as their combinations, through a randomized modality fusion strategy. This design not only expands the pool of usable datasets but also encourages the policy to develop richer geometric, semantic, and visual representations. The resulting model, OmniVLA, achieves strong generalization to unseen environments, robustness to scarce modalities, and the ability to follow novel natural language instructions. We demonstrate that OmniVLA outperforms specialist baselines across modalities and offers a flexible foundation for fine-tuning to new modalities and tasks. We believe OmniVLA provides a step toward broadly generalizable and flexible navigation policies, and a scalable path for building omni-modal robotic foundation models. We present videos showcasing OmniVLA performance and will release its checkpoints and training code on our project page.",
          "abstract_zh": "在导航到目的地时，人类可以灵活地解释和构成不同的目标规范，例如语言指令、空间坐标或视觉参考。相比之下，大多数现有的机器人导航策略都是在单一模式上进行训练的，这限制了它们对现实世界场景的适应性，在现实世界场景中，不同形式的目标规范是自然且互补的。在这项工作中，我们提出了一个机器人基础模型的训练框架，可以实现基于视觉的导航的全模式目标调节。我们的方法利用高容量视觉-语言-动作（VLA）主干，并通过随机模态融合策略训练三种主要目标模态：2D 姿势、自我中心图像和自然语言及其组合。这种设计不仅扩大了可用数据集的范围，而且还鼓励政策开发更丰富的几何、语义和视觉表示。由此产生的模型 OmniVLA 实现了对不可见环境的强大泛化、对稀缺模式的鲁棒性以及遵循新颖的自然语言指令的能力。我们证明 OmniVLA 的性能优于跨模式的专家基线，并为微调新模式和任务提供了灵活的基础。我们相信 OmniVLA 为实现广泛通用和灵活的导航策略迈出了一步，并为构建全模态机器人基础模型提供了可扩展的路径。我们展示展示 OmniVLA 性能的视频，并将在我们的项目页面上发布其检查点和训练代码。"
        },
        {
          "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies",
          "url": "http://arxiv.org/abs/2509.18282v1",
          "snippet": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: 1. end-effector paths specifying what actions to take, and 2. task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need--where, what, and how. Website at https://peek-robot.github.io/.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-09-22T18:10:14Z",
          "authors": [
            "Jesse Zhang",
            "Marius Memmel",
            "Kevin Kim",
            "Dieter Fox",
            "Jesse Thomason",
            "Fabio Ramos",
            "Erdem Bıyık",
            "Abhishek Gupta",
            "Anqi Li"
          ],
          "arxiv_id": "2509.18282",
          "abstract": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: 1. end-effector paths specifying what actions to take, and 2. task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need--where, what, and how. Website at https://peek-robot.github.io/.",
          "abstract_zh": "机器人操纵策略通常无法概括，因为它们必须同时学习去哪里参加、采取什么行动以及如何执行这些行动。我们认为，关于哪里和什么的高级推理可以转移到视觉语言模型（VLM），让政策专门研究如何采取行动。我们提出了 PEEK（与策略无关的基本关键点提取），它可以微调 VLM 以预测统一的基于点的中间表示：1. 末端执行器路径指定要采取的操作，2. 与任务相关的掩码指示要关注的位置。这些注释直接覆盖到机器人观察上，使得表示与策略无关并且可以跨架构转移。为了实现可扩展的训练，我们引入了自动注释管道，跨 9 个实施例的 20 多个机器人数据集生成标记数据。在现实世界的评估中，PEEK 始终如一地提高了零样本泛化能力，包括仅在模拟中训练的 3D 策略在现实世界中的改进为 41.4 倍，以及大型 VLA 和小型操纵策略的 2-3.5 倍的增益。通过让 VLM 吸收语义和视觉的复杂性，PEEK 为操作策略配备了所需的最少线索——地点、内容和方式。网站 https://peek-robot.github.io/。"
        },
        {
          "title": "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving",
          "url": "http://arxiv.org/abs/2509.20109v1",
          "snippet": "End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, with Vision-Language-Action (VLA) models representing a new paradigm that leverages pre-trained multimodal knowledge from Vision-Language Models (VLMs) to interpret and interact with complex real-world environments. However, these methods remain constrained by the limitations of imitation learning, which struggles to inherently encode physical rules during training. Existing approaches often rely on complex rule-based post-refinement, employ reinforcement learning that remains largely limited to simulation, or utilize diffusion guidance that requires computationally expensive gradient calculations. To address these challenges, we introduce ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. We first discretize the two-dimensional driving space to construct an action codebook, enabling the use of pre-trained Diffusion Language Models for planning tasks through fine-tuning. Central to our approach is a safety-aware reflection mechanism that performs iterative self-correction without gradient computation. Our method begins with goal-conditioned trajectory generation to model multi-modal driving behaviors. Based on this, we apply local search methods to identify unsafe tokens and determine feasible solutions, which then serve as safe anchors for inpainting-based regeneration. Evaluated on the NAVSIM benchmark, ReflectDrive demonstrates significant advantages in safety-critical trajectory generation, offering a scalable and reliable solution for autonomous driving systems.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-09-24T13:35:15Z",
          "authors": [
            "Pengxiang Li",
            "Yinan Zheng",
            "Yue Wang",
            "Huimin Wang",
            "Hang Zhao",
            "Jingjing Liu",
            "Xianyuan Zhan",
            "Kun Zhan",
            "Xianpeng Lang"
          ],
          "arxiv_id": "2509.20109",
          "abstract": "End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, with Vision-Language-Action (VLA) models representing a new paradigm that leverages pre-trained multimodal knowledge from Vision-Language Models (VLMs) to interpret and interact with complex real-world environments. However, these methods remain constrained by the limitations of imitation learning, which struggles to inherently encode physical rules during training. Existing approaches often rely on complex rule-based post-refinement, employ reinforcement learning that remains largely limited to simulation, or utilize diffusion guidance that requires computationally expensive gradient calculations. To address these challenges, we introduce ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. We first discretize the two-dimensional driving space to construct an action codebook, enabling the use of pre-trained Diffusion Language Models for planning tasks through fine-tuning. Central to our approach is a safety-aware reflection mechanism that performs iterative self-correction without gradient computation. Our method begins with goal-conditioned trajectory generation to model multi-modal driving behaviors. Based on this, we apply local search methods to identify unsafe tokens and determine feasible solutions, which then serve as safe anchors for inpainting-based regeneration. Evaluated on the NAVSIM benchmark, ReflectDrive demonstrates significant advantages in safety-critical trajectory generation, offering a scalable and reliable solution for autonomous driving systems.",
          "abstract_zh": "端到端（E2E）解决方案已成为自动驾驶系统的主流方法，视觉-语言-动作（VLA）模型代表了一种新范式，利用视觉-语言模型（VLM）中预先训练的多模态知识来解释复杂的现实世界环境并与之交互。然而，这些方法仍然受到模仿学习的局限性的限制，模仿学习很难在训练过程中固有地编码物理规则。现有的方法通常依赖于复杂的基于规则的后细化，采用很大程度上仅限于模拟的强化学习，或者利用需要计算昂贵的梯度计算的扩散指导。为了应对这些挑战，我们引入了 ReflectDrive，这是一种基于学习的新型框架，它集成了反射机制，通过离散扩散生成安全轨迹。我们首先离散化二维驾驶空间来构建动作密码本，从而能够使用预先训练的扩散语言模型通过微调来规划任务。我们方法的核心是一种安全意识反射机制，无需梯度计算即可执行迭代自我校正。我们的方法从目标条件轨迹生成开始，以模拟多模式驾驶行为。基于此，我们应用本地搜索方法来识别不安全的标记并确定可行的解决方案，然后将其作为基于修复的再生的安全锚点。根据 NAVSIM 基准进行评估，ReflectDrive 在安全关键轨迹生成方面展示了显着优势，为自动驾驶系统提供了可扩展且可靠的解决方案。"
        },
        {
          "title": "Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation",
          "url": "http://arxiv.org/abs/2509.22093v1",
          "snippet": "Robotic manipulation with Vision-Language-Action models requires efficient inference over long-horizon multi-modal context, where attention to dense visual tokens dominates computational cost. Existing methods optimize inference speed by reducing visual redundancy within VLA models, but they overlook the varying redundancy across robotic manipulation stages. We observe that the visual token redundancy is higher in coarse manipulation phase than in fine-grained operations, and is strongly correlated with the action dynamic. Motivated by this observation, we propose \\textbf{A}ction-aware \\textbf{D}ynamic \\textbf{P}runing (\\textbf{ADP}), a multi-modal pruning framework that integrates text-driven token selection with action-aware trajectory gating. Our method introduces a gating mechanism that conditions the pruning signal on recent action trajectories, using past motion windows to adaptively adjust token retention ratios in accordance with dynamics, thereby balancing computational efficiency and perceptual precision across different manipulation stages. Extensive experiments on the LIBERO suites and diverse real-world scenarios demonstrate that our method significantly reduces FLOPs and action inference latency (\\textit{e.g.} $1.35 \\times$ speed up on OpenVLA-OFT) while maintaining competitive success rates (\\textit{e.g.} 25.8\\% improvements with OpenVLA) compared to baselines, thereby providing a simple plug-in path to efficient robot policies that advances the efficiency and performance frontier of robotic manipulation. Our project website is: \\href{https://vla-adp.github.io/}{ADP.com}.",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-09-26T09:13:02Z",
          "authors": [
            "Xiaohuan Pei",
            "Yuxing Chen",
            "Siyu Xu",
            "Yunke Wang",
            "Yuheng Shi",
            "Chang Xu"
          ],
          "arxiv_id": "2509.22093",
          "abstract": "Robotic manipulation with Vision-Language-Action models requires efficient inference over long-horizon multi-modal context, where attention to dense visual tokens dominates computational cost. Existing methods optimize inference speed by reducing visual redundancy within VLA models, but they overlook the varying redundancy across robotic manipulation stages. We observe that the visual token redundancy is higher in coarse manipulation phase than in fine-grained operations, and is strongly correlated with the action dynamic. Motivated by this observation, we propose \\textbf{A}ction-aware \\textbf{D}ynamic \\textbf{P}runing (\\textbf{ADP}), a multi-modal pruning framework that integrates text-driven token selection with action-aware trajectory gating. Our method introduces a gating mechanism that conditions the pruning signal on recent action trajectories, using past motion windows to adaptively adjust token retention ratios in accordance with dynamics, thereby balancing computational efficiency and perceptual precision across different manipulation stages. Extensive experiments on the LIBERO suites and diverse real-world scenarios demonstrate that our method significantly reduces FLOPs and action inference latency (\\textit{e.g.} $1.35 \\times$ speed up on OpenVLA-OFT) while maintaining competitive success rates (\\textit{e.g.} 25.8\\% improvements with OpenVLA) compared to baselines, thereby providing a simple plug-in path to efficient robot policies that advances the efficiency and performance frontier of robotic manipulation. Our project website is: \\href{https://vla-adp.github.io/}{ADP.com}.",
          "abstract_zh": "使用视觉-语言-动作模型的机器人操作需要对长视野多模态上下文进行有效推理，其中对密集视觉标记的关注在计算成本中占主导地位。现有方法通过减少 VLA 模型内的视觉冗余来优化推理速度，但它们忽略了机器人操作阶段的不同冗余。我们观察到，粗粒度操作阶段的视觉标记冗余度高于细粒度操作阶段，并且与动作动态密切相关。受此观察的启发，我们提出 \\textbf{A}action-aware \\textbf{D}ynamic \\textbf{P}runing (\\textbf{ADP})，这是一种多模态修剪框架，它将文本驱动的标记选择与动作感知轨迹门控集成在一起。我们的方法引入了一种门控机制，该机制可以根据最近的动作轨迹调节修剪信号，使用过去的运动窗口根据动态自适应调整令牌保留率，从而平衡不同操作阶段的计算效率和感知精度。对 LIBERO 套件和各种现实场景的广泛实验表明，与基线相比，我们的方法显着减少了 FLOP 和动作推理延迟（\\textit{e.g.} 在 OpenVLA-OFT 上加速了 1.35 美元\\times$），同时保持了有竞争力的成功率（\\textit{e.g.} OpenVLA 提高了 25.8%），从而为高效的机器人策略提供了一个简单的插件路径，从而提高了机器人的效率和性能前沿机器人操纵。我们的项目网站是：\\href{https://vla-adp.github.io/}{ADP.com}。"
        },
        {
          "title": "UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation",
          "url": "http://arxiv.org/abs/2509.22441v1",
          "snippet": "This paper presents UnderwaterVLA, a novel framework for autonomous underwater navigation that integrates multimodal foundation models with embodied intelligence systems. Underwater operations remain difficult due to hydrodynamic disturbances, limited communication bandwidth, and degraded sensing in turbid waters. To address these challenges, we introduce three innovations. First, a dual-brain architecture decouples high-level mission reasoning from low-level reactive control, enabling robust operation under communication and computational constraints. Second, we apply Vision-Language-Action(VLA) models to underwater robotics for the first time, incorporating structured chain-of-thought reasoning for interpretable decision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC) scheme compensates for fluid effects in real time without costly task-specific training. Experimental results in field tests show that UnderwaterVLA reduces navigation errors in degraded visual conditions while maintaining higher task completion by 19% to 27% over baseline. By minimizing reliance on underwater-specific training data and improving adaptability across environments, UnderwaterVLA provides a scalable and cost-effective path toward the next generation of intelligent AUVs.",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-09-26T15:01:52Z",
          "authors": [
            "Zhangyuan Wang",
            "Yunpeng Zhu",
            "Yuqi Yan",
            "Xiaoyuan Tian",
            "Xinhao Shao",
            "Meixuan Li",
            "Weikun Li",
            "Guangsheng Su",
            "Weicheng Cui",
            "Dixia Fan"
          ],
          "arxiv_id": "2509.22441",
          "abstract": "This paper presents UnderwaterVLA, a novel framework for autonomous underwater navigation that integrates multimodal foundation models with embodied intelligence systems. Underwater operations remain difficult due to hydrodynamic disturbances, limited communication bandwidth, and degraded sensing in turbid waters. To address these challenges, we introduce three innovations. First, a dual-brain architecture decouples high-level mission reasoning from low-level reactive control, enabling robust operation under communication and computational constraints. Second, we apply Vision-Language-Action(VLA) models to underwater robotics for the first time, incorporating structured chain-of-thought reasoning for interpretable decision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC) scheme compensates for fluid effects in real time without costly task-specific training. Experimental results in field tests show that UnderwaterVLA reduces navigation errors in degraded visual conditions while maintaining higher task completion by 19% to 27% over baseline. By minimizing reliance on underwater-specific training data and improving adaptability across environments, UnderwaterVLA provides a scalable and cost-effective path toward the next generation of intelligent AUVs.",
          "abstract_zh": "本文提出了 UnderwaterVLA，这是一种将多模态基础模型与具体智能系统集成在一起的新型水下自主导航框架。由于水动力扰动、通信带宽有限以及浑浊水域中的传感性能下降，水下作业仍然很困难。为了应对这些挑战，我们引入了三项创新。首先，双脑架构将高级任务推理与低级反应控制解耦，从而在通信和计算限制下实现稳健的操作。其次，我们首次将视觉-语言-动作（VLA）模型应用于水下机器人技术，结合结构化思维链推理来进行可解释的决策。第三，基于流体动力学的模型预测控制（MPC）方案可以实时补偿流体效应，而无需昂贵的特定任务培训。现场测试的实验结果表明，UnderwaterVLA 可以减少视觉退化条件下的导航错误，同时保持比基线高 19% 至 27% 的任务完成率。通过最大限度地减少对水下特定训练数据的依赖并提高跨环境的适应性，UnderwaterVLA 为下一代智能 AUV 提供了一条可扩展且经济高效的路径。"
        },
        {
          "title": "Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action",
          "url": "http://arxiv.org/abs/2509.19571v1",
          "snippet": "Executing open-ended natural language queries is a core problem in robotics. While recent advances in imitation learning and vision-language-actions models (VLAs) have enabled promising end-to-end policies, these models struggle when faced with complex instructions and new scenes. An alternative is to design an explicit scene representation as a queryable interface between the robot and the world, using query results to guide downstream motion planning. In this work, we present Agentic Scene Policies (ASP), an agentic framework that leverages the advanced semantic, spatial, and affordance-based querying capabilities of modern scene representations to implement a capable language-conditioned robot policy. ASP can execute open-vocabulary queries in a zero-shot manner by explicitly reasoning about object affordances in the case of more complex skills. Through extensive experiments, we compare ASP with VLAs on tabletop manipulation problems and showcase how ASP can tackle room-level queries through affordance-guided navigation, and a scaled-up scene representation. (Project page: https://montrealrobotics.ca/agentic-scene-policies.github.io/)",
          "site": "arxiv.org",
          "rank": 15,
          "published": "2025-09-23T20:56:00Z",
          "authors": [
            "Sacha Morin",
            "Kumaraditya Gupta",
            "Mahtab Sandhu",
            "Charlie Gauthier",
            "Francesco Argenziano",
            "Kirsty Ellis",
            "Liam Paull"
          ],
          "arxiv_id": "2509.19571",
          "abstract": "Executing open-ended natural language queries is a core problem in robotics. While recent advances in imitation learning and vision-language-actions models (VLAs) have enabled promising end-to-end policies, these models struggle when faced with complex instructions and new scenes. An alternative is to design an explicit scene representation as a queryable interface between the robot and the world, using query results to guide downstream motion planning. In this work, we present Agentic Scene Policies (ASP), an agentic framework that leverages the advanced semantic, spatial, and affordance-based querying capabilities of modern scene representations to implement a capable language-conditioned robot policy. ASP can execute open-vocabulary queries in a zero-shot manner by explicitly reasoning about object affordances in the case of more complex skills. Through extensive experiments, we compare ASP with VLAs on tabletop manipulation problems and showcase how ASP can tackle room-level queries through affordance-guided navigation, and a scaled-up scene representation. (Project page: https://montrealrobotics.ca/agentic-scene-policies.github.io/)",
          "abstract_zh": "执行开放式自然语言查询是机器人技术的核心问题。虽然模仿学习和视觉语言动作模型（VLA）的最新进展已经实现了有前景的端到端策略，但这些模型在面对复杂的指令和新场景时却陷入困境。另一种方法是设计一个显式场景表示作为机器人和世界之间的可查询接口，使用查询结果来指导下游运动规划。在这项工作中，我们提出了代理场景策略（ASP），这是一个代理框架，它利用现代场景表示的高级语义、空间和基于可供性的查询功能来实现功能强大的语言条件机器人策略。ASP 可以通过在更复杂的技能的情况下显式推理对象可供性来以零次方式执行开放词汇表查询。通过大量实验，我们在桌面操作问题上将 ASP 与 VLA 进行比较，并展示 ASP 如何通过可供性引导的导航和放大的场景表示来处理房间级查询。（项目页面：https://montrealrobotics.ca/agentic-scene-policies.github.io/）"
        },
        {
          "title": "Developing Vision-Language-Action Model from Egocentric Videos",
          "url": "http://arxiv.org/abs/2509.21986v1",
          "snippet": "Egocentric videos capture how humans manipulate objects and tools, providing diverse motion cues for learning object manipulation. Unlike the costly, expert-driven manual teleoperation commonly used in training Vision-Language-Action models (VLAs), egocentric videos offer a scalable alternative. However, prior studies that leverage such videos for training robot policies typically rely on auxiliary annotations, such as detailed hand-pose recordings. Consequently, it remains unclear whether VLAs can be trained directly from raw egocentric videos. In this work, we address this challenge by leveraging EgoScaler, a framework that extracts 6DoF object manipulation trajectories from egocentric videos without requiring auxiliary recordings. We apply EgoScaler to four large-scale egocentric video datasets and automatically refine noisy or incomplete trajectories, thereby constructing a new large-scale dataset for VLA pre-training. Our experiments with a state-of-the-art $π_0$ architecture in both simulated and real-robot environments yield three key findings: (i) pre-training on our dataset improves task success rates by over 20\\% compared to training from scratch, (ii) the performance is competitive with that achieved using real-robot datasets, and (iii) combining our dataset with real-robot data yields further improvements. These results demonstrate that egocentric videos constitute a promising and scalable resource for advancing VLA research.",
          "site": "arxiv.org",
          "rank": 16,
          "published": "2025-09-26T07:09:33Z",
          "authors": [
            "Tomoya Yoshida",
            "Shuhei Kurita",
            "Taichi Nishimura",
            "Shinsuke Mori"
          ],
          "arxiv_id": "2509.21986",
          "abstract": "Egocentric videos capture how humans manipulate objects and tools, providing diverse motion cues for learning object manipulation. Unlike the costly, expert-driven manual teleoperation commonly used in training Vision-Language-Action models (VLAs), egocentric videos offer a scalable alternative. However, prior studies that leverage such videos for training robot policies typically rely on auxiliary annotations, such as detailed hand-pose recordings. Consequently, it remains unclear whether VLAs can be trained directly from raw egocentric videos. In this work, we address this challenge by leveraging EgoScaler, a framework that extracts 6DoF object manipulation trajectories from egocentric videos without requiring auxiliary recordings. We apply EgoScaler to four large-scale egocentric video datasets and automatically refine noisy or incomplete trajectories, thereby constructing a new large-scale dataset for VLA pre-training. Our experiments with a state-of-the-art $π_0$ architecture in both simulated and real-robot environments yield three key findings: (i) pre-training on our dataset improves task success rates by over 20\\% compared to training from scratch, (ii) the performance is competitive with that achieved using real-robot datasets, and (iii) combining our dataset with real-robot data yields further improvements. These results demonstrate that egocentric videos constitute a promising and scalable resource for advancing VLA research.",
          "abstract_zh": "以自我为中心的视频捕捉人类如何操纵物体和工具，为学习物体操纵提供不同的运动线索。与训练视觉-语言-动作模型 (VLA) 时常用的昂贵的专家驱动的手动远程操作不同，以自我为中心的视频提供了一种可扩展的替代方案。然而，之前利用此类视频来训练机器人策略的研究通常依赖于辅助注释，例如详细的手势记录。因此，目前尚不清楚 VLA 是否可以直接从原始的自我中心视频中进行训练。在这项工作中，我们通过利用 EgoScaler 来应对这一挑战，该框架可以从以自我为中心的视频中提取 6DoF 对象操作轨迹，而无需辅助录制。我们将 EgoScaler 应用于四个大规模以自我为中心的视频数据集，并自动细化噪声或不完整的轨迹，从而构建用于 VLA 预训练的新的大规模数据集。我们在模拟和真实机器人环境中使用最先进的 $π_0$ 架构进行的实验产生了三个关键发现：(i) 与从头开始训练相比，对我们的数据集进行预训练将任务成功率提高了 20% 以上，(ii) 性能与使用真实机器人数据集实现的性能具有竞争力，(iii) 将我们的数据集与真实机器人数据相结合可产生进一步的改进。这些结果表明，以自我为中心的视频是推进 VLA 研究的有前途且可扩展的资源。"
        },
        {
          "title": "FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2509.19870v1",
          "snippet": "Vision-Language-Action (VLA) models are driving rapid progress in robotics by enabling agents to interpret multimodal inputs and execute complex, long-horizon tasks. However, their safety and robustness against adversarial attacks remain largely underexplored. In this work, we identify and formalize a critical adversarial vulnerability in which adversarial images can \"freeze\" VLA models and cause them to ignore subsequent instructions. This threat effectively disconnects the robot's digital mind from its physical actions, potentially inducing inaction during critical interventions. To systematically study this vulnerability, we propose FreezeVLA, a novel attack framework that generates and evaluates action-freezing attacks via min-max bi-level optimization. Experiments on three state-of-the-art VLA models and four robotic benchmarks show that FreezeVLA attains an average attack success rate of 76.2%, significantly outperforming existing methods. Moreover, adversarial images generated by FreezeVLA exhibit strong transferability, with a single image reliably inducing paralysis across diverse language prompts. Our findings expose a critical safety risk in VLA models and highlight the urgent need for robust defense mechanisms.",
          "site": "arxiv.org",
          "rank": 17,
          "published": "2025-09-24T08:15:28Z",
          "authors": [
            "Xin Wang",
            "Jie Li",
            "Zejia Weng",
            "Yixu Wang",
            "Yifeng Gao",
            "Tianyu Pang",
            "Chao Du",
            "Yan Teng",
            "Yingchun Wang",
            "Zuxuan Wu",
            "Xingjun Ma",
            "Yu-Gang Jiang"
          ],
          "arxiv_id": "2509.19870",
          "abstract": "Vision-Language-Action (VLA) models are driving rapid progress in robotics by enabling agents to interpret multimodal inputs and execute complex, long-horizon tasks. However, their safety and robustness against adversarial attacks remain largely underexplored. In this work, we identify and formalize a critical adversarial vulnerability in which adversarial images can \"freeze\" VLA models and cause them to ignore subsequent instructions. This threat effectively disconnects the robot's digital mind from its physical actions, potentially inducing inaction during critical interventions. To systematically study this vulnerability, we propose FreezeVLA, a novel attack framework that generates and evaluates action-freezing attacks via min-max bi-level optimization. Experiments on three state-of-the-art VLA models and four robotic benchmarks show that FreezeVLA attains an average attack success rate of 76.2%, significantly outperforming existing methods. Moreover, adversarial images generated by FreezeVLA exhibit strong transferability, with a single image reliably inducing paralysis across diverse language prompts. Our findings expose a critical safety risk in VLA models and highlight the urgent need for robust defense mechanisms.",
          "abstract_zh": "视觉-语言-动作（VLA）模型使代理能够解释多模式输入并执行复杂的长期任务，从而推动机器人技术的快速进步。然而，它们针对对抗性攻击的安全性和稳健性在很大程度上仍未得到充分探索。在这项工作中，我们识别并形式化了一个关键的对抗性漏洞，其中对抗性图像可以“冻结”VLA 模型并导致它们忽略后续指令。这种威胁有效地断开了机器人的数字思维与其物理行为的连接，可能导致在关键干预期间不采取行动。为了系统地研究这个漏洞，我们提出了 FreezeVLA，一种新颖的攻击框架，它通过最小-最大双层优化生成和评估动作冻结攻击。对三个最先进的 VLA 模型和四个机器人基准测试的实验表明，FreezeVLA 的平均攻击成功率为 76.2%，明显优于现有方法。此外，FreezeVLA 生成的对抗性图像表现出很强的可转移性，单个图像可靠地导致跨不同语言提示的瘫痪。我们的研究结果暴露了 VLA 模型中的一个关键安全风险，并强调了对强大防御机制的迫切需要。"
        },
        {
          "title": "On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations",
          "url": "http://arxiv.org/abs/2510.00037v3",
          "snippet": "In Vision-Language-Action (VLA) models, robustness to real-world perturbations is critical for deployment. Existing methods target simple visual disturbances, overlooking the broader multi-modal perturbations that arise in actions, instructions, environments, and observations. Here, we first evaluate the robustness of mainstream VLAs under 17 perturbations across four modalities. We find (1) actions as the most fragile modality, (2) Existing visual-robust VLA do not gain robustness in other modality, and (3) pi0 demonstrates superior robustness with a diffusion-based action head. To build multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA inputs and outputs. For output robustness, we perform offline robust optimization against worst-case action noise that maximizes mismatch in flow matching objective. This can be seen as adversarial training, label smoothing, and outlier penalization. For input robustness, we enforce consistent actions across input variations that preserve task semantics. To account for multiple perturbations, we formulate robustness as a multi-armed bandit problem and apply an upper confidence bound algorithm to automatically identify the most harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations. Our RobustVLA is particularly effective on real-world FR5 robot with limited demonstrations, showing absolute gains by 65.6% under perturbations of four modalities.",
          "site": "arxiv.org",
          "rank": 18,
          "published": "2025-09-26T14:42:23Z",
          "authors": [
            "Jianing Guo",
            "Zhenhong Wu",
            "Chang Tu",
            "Yiyao Ma",
            "Xiangqi Kong",
            "Zhiqian Liu",
            "Jiaming Ji",
            "Shuning Zhang",
            "Yuanpei Chen",
            "Kai Chen",
            "Qi Dou",
            "Yaodong Yang",
            "Xianglong Liu",
            "Huijie Zhao",
            "Weifeng Lv",
            "Simin Li"
          ],
          "arxiv_id": "2510.00037",
          "abstract": "In Vision-Language-Action (VLA) models, robustness to real-world perturbations is critical for deployment. Existing methods target simple visual disturbances, overlooking the broader multi-modal perturbations that arise in actions, instructions, environments, and observations. Here, we first evaluate the robustness of mainstream VLAs under 17 perturbations across four modalities. We find (1) actions as the most fragile modality, (2) Existing visual-robust VLA do not gain robustness in other modality, and (3) pi0 demonstrates superior robustness with a diffusion-based action head. To build multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA inputs and outputs. For output robustness, we perform offline robust optimization against worst-case action noise that maximizes mismatch in flow matching objective. This can be seen as adversarial training, label smoothing, and outlier penalization. For input robustness, we enforce consistent actions across input variations that preserve task semantics. To account for multiple perturbations, we formulate robustness as a multi-armed bandit problem and apply an upper confidence bound algorithm to automatically identify the most harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations. Our RobustVLA is particularly effective on real-world FR5 robot with limited demonstrations, showing absolute gains by 65.6% under perturbations of four modalities.",
          "abstract_zh": "在视觉-语言-动作 (VLA) 模型中，对现实世界扰动的鲁棒性对于部署至关重要。现有方法针对简单的视觉干扰，忽略了动作、指令、环境和观察中出现的更广泛的多模态扰动。在这里，我们首先评估主流 VLA 在四种模式的 17 种扰动下的稳健性。我们发现（1）动作是最脆弱的模态，（2）现有的视觉鲁棒性 VLA 在其他模态中没有获得鲁棒性，（3）pi0 通过基于扩散的动作头表现出卓越的鲁棒性。为了构建多模态鲁棒 VLA，我们提出了针对 VLA 输入和输出扰动的 RobustVLA。为了保证输出的鲁棒性，我们针对最坏情况的动作噪声执行离线鲁棒优化，从而最大化流匹配目标中的不匹配。这可以看作是对抗性训练、标签平滑和异常值惩罚。为了确保输入的鲁棒性，我们在输入变化中强制执行一致的操作，以保留任务语义。为了考虑多重扰动，我们将鲁棒性表述为多臂老虎机问题，并应用置信上限算法来自动识别最有害的噪声。LIBERO 上的实验表明，我们的 RobustVLA 在所有 17 个扰动下，在 pi0 主干上的绝对增益超过 12.6%，在 OpenVLA 主干上的绝对增益超过 10.4%，推理速度比现有视觉鲁棒 VLA 快 50.6 倍，在混合扰动下增益为 10.4%。我们的 RobustVLA 对有限演示的现实世界 FR5 机器人特别有效，在四种模式的扰动下显示出 65.6% 的绝对增益。"
        },
        {
          "title": "World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training",
          "url": "http://arxiv.org/abs/2509.24948v3",
          "snippet": "Vision-Language-Action (VLA) models trained via imitation learning suffer from significant performance degradation in data-scarce scenarios due to their reliance on large-scale demonstration datasets. Although reinforcement learning (RL)-based post-training has proven effective in addressing data scarcity, its application to VLA models is hindered by the non-resettable nature of real-world environments. This limitation is particularly critical in high-risk domains such as industrial automation, where interactions often induce state changes that are costly or infeasible to revert. Furthermore, existing VLA approaches lack a reliable mechanism for detecting task completion, leading to redundant actions that reduce overall task success rates. To address these challenges, we propose World-Env, an RL-based post-training framework that replaces physical interaction with a low-cost, world model-based virtual simulator. World-Env consists of two key components: (1) a video-based world simulator that generates temporally consistent future visual observations, and (2) a vision-language model (VLM)-guided instant reflector that provides continuous reward signals and predicts action termination. This simulated environment enables VLA models to safely explore and generalize beyond their initial imitation learning distribution. Our method achieves notable performance gains with as few as five expert demonstrations per task. Experiments on complex robotic manipulation tasks demonstrate that World-Env effectively overcomes the data inefficiency, safety constraints, and inefficient execution of conventional VLA models that rely on real-world interaction, offering a practical and scalable solution for post-training in resource-constrained settings. Our code is available at https://github.com/amap-cvlab/world-env.",
          "site": "arxiv.org",
          "rank": 19,
          "published": "2025-09-29T15:45:19Z",
          "authors": [
            "Junjin Xiao",
            "Yandan Yang",
            "Xinyuan Chang",
            "Ronghan Chen",
            "Feng Xiong",
            "Mu Xu",
            "Wei-Shi Zheng",
            "Qing Zhang"
          ],
          "arxiv_id": "2509.24948",
          "abstract": "Vision-Language-Action (VLA) models trained via imitation learning suffer from significant performance degradation in data-scarce scenarios due to their reliance on large-scale demonstration datasets. Although reinforcement learning (RL)-based post-training has proven effective in addressing data scarcity, its application to VLA models is hindered by the non-resettable nature of real-world environments. This limitation is particularly critical in high-risk domains such as industrial automation, where interactions often induce state changes that are costly or infeasible to revert. Furthermore, existing VLA approaches lack a reliable mechanism for detecting task completion, leading to redundant actions that reduce overall task success rates. To address these challenges, we propose World-Env, an RL-based post-training framework that replaces physical interaction with a low-cost, world model-based virtual simulator. World-Env consists of two key components: (1) a video-based world simulator that generates temporally consistent future visual observations, and (2) a vision-language model (VLM)-guided instant reflector that provides continuous reward signals and predicts action termination. This simulated environment enables VLA models to safely explore and generalize beyond their initial imitation learning distribution. Our method achieves notable performance gains with as few as five expert demonstrations per task. Experiments on complex robotic manipulation tasks demonstrate that World-Env effectively overcomes the data inefficiency, safety constraints, and inefficient execution of conventional VLA models that rely on real-world interaction, offering a practical and scalable solution for post-training in resource-constrained settings. Our code is available at https://github.com/amap-cvlab/world-env.",
          "abstract_zh": "通过模仿学习训练的视觉-语言-动作（VLA）模型由于依赖大规模演示数据集，因此在数据稀缺场景中性能会显着下降。尽管基于强化学习 (RL) 的后训练已被证明可以有效解决数据稀缺问题，但其在 VLA 模型中的应用受到现实环境的不可重置特性的阻碍。这种限制在工业自动化等高风险领域尤其重要，其中交互通常会导致状态变化，而状态变化的恢复成本高昂或不可行。此外，现有的 VLA 方法缺乏可靠的机制来检测任务完成情况，从而导致冗余操作，从而降低总体任务成功率。为了应对这些挑战，我们提出了 World-Env，一种基于强化学习的后训练框架，用低成本、基于世界模型的虚拟模拟器取代物理交互。World-Env 由两个关键组件组成：(1) 基于视频的世界模拟器，可生成时间一致的未来视觉观察；(2) 视觉语言模型 (VLM) 引导的即时反射器，可提供连续奖励信号并预测动作终止。这种模拟环境使 VLA 模型能够安全地探索和推广超出其初始模仿学习分布的内容。我们的方法只需对每个任务进行五次专家演示即可实现显着的性能提升。对复杂机器人操作任务的实验表明，World-Env有效克服了依赖现实世界交互的传统VLA模型的数据效率低下、安全限制和执行效率低下的问题，为资源受限环境下的后期训练提供了实用且可扩展的解决方案。我们的代码可在 https://github.com/amap-cvlab/world-env 获取。"
        },
        {
          "title": "RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2509.21243v1",
          "snippet": "Recent Vision-Language-Action (VLA) models demonstrate remarkable generalization in robotics but are restricted by their substantial size and computational cost, limiting real-world deployment. However, conventional lightweighting methods often sacrifice critical capabilities, particularly spatial reasoning. This creates a trade-off between efficiency and performance. To address this challenge, our work reuses Register Tokens, which were introduced for artifact removal in Vision Transformers but subsequently discarded. We suppose that these tokens contain essential spatial information and propose RetoVLA, a novel architecture that reuses them directly by injecting them into the Action Expert.\n  RetoVLA maintains a lightweight structure while leveraging this repurposed spatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness through a series of comprehensive experiments. On our custom-built 7-DOF robot arm, the model achieves a 17.1%p absolute improvement in success rates for complex manipulation tasks. Our results confirm that reusing Register Tokens directly enhances spatial reasoning, demonstrating that what was previously discarded as an artifact is in fact a valuable, unexplored resource for robotic intelligence. A video demonstration is available at: https://youtu.be/2CseBR-snZg",
          "site": "arxiv.org",
          "rank": 20,
          "published": "2025-09-25T14:37:54Z",
          "authors": [
            "Jiyeon Koo",
            "Taewan Cho",
            "Hyunjoon Kang",
            "Eunseom Pyo",
            "Tae Gyun Oh",
            "Taeryang Kim",
            "Andrew Jaeyong Choi"
          ],
          "arxiv_id": "2509.21243",
          "abstract": "Recent Vision-Language-Action (VLA) models demonstrate remarkable generalization in robotics but are restricted by their substantial size and computational cost, limiting real-world deployment. However, conventional lightweighting methods often sacrifice critical capabilities, particularly spatial reasoning. This creates a trade-off between efficiency and performance. To address this challenge, our work reuses Register Tokens, which were introduced for artifact removal in Vision Transformers but subsequently discarded. We suppose that these tokens contain essential spatial information and propose RetoVLA, a novel architecture that reuses them directly by injecting them into the Action Expert. RetoVLA maintains a lightweight structure while leveraging this repurposed spatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness through a series of comprehensive experiments. On our custom-built 7-DOF robot arm, the model achieves a 17.1%p absolute improvement in success rates for complex manipulation tasks. Our results confirm that reusing Register Tokens directly enhances spatial reasoning, demonstrating that what was previously discarded as an artifact is in fact a valuable, unexplored resource for robotic intelligence. A video demonstration is available at: https://youtu.be/2CseBR-snZg",
          "abstract_zh": "最近的视觉-语言-动作（VLA）模型在机器人技术中展示了显着的通用性，但受到其巨大尺寸和计算成本的限制，限制了现实世界的部署。然而，传统的轻量化方法通常会牺牲关键能力，特别是空间推理。这会在效率和性能之间进行权衡。为了应对这一挑战，我们的工作重用了注册令牌，这些令牌最初是为了在 Vision Transformers 中去除伪影而引入的，但随后被丢弃。我们假设这些令牌包含必要的空间信息，并提出了 RetoVLA，这是一种新颖的架构，可以通过将它们注入到 Action Expert 中来直接重用它们。RetoVLA 保持轻量级结构，同时利用这种重新调整用途的空间上下文来增强推理。我们通过一系列综合实验证明了 RetoVLA 的有效性。在我们定制的 7 自由度机器人手臂上，该模型将复杂操作任务的成功率绝对提高了 17.1%p。我们的结果证实，重复使用寄存器令牌可以直接增强空间推理，这表明以前作为工件而丢弃的东西实际上是机器人智能的宝贵的、未经开发的资源。视频演示请访问：https://youtu.be/2CseBR-snZg"
        },
        {
          "title": "Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting",
          "url": "http://arxiv.org/abs/2509.22195v1",
          "snippet": "Fine-tuning vision-language models (VLMs) on robot teleoperation data to create vision-language-action (VLA) models is a promising paradigm for training generalist policies, but it suffers from a fundamental tradeoff: learning to produce actions often diminishes the VLM's foundational reasoning and multimodal understanding, hindering generalization to novel scenarios, instruction following, and semantic understanding. We argue that this catastrophic forgetting is due to a distribution mismatch between the VLM's internet-scale pretraining corpus and the robotics fine-tuning data. Inspired by this observation, we introduce VLM2VLA: a VLA training paradigm that first resolves this mismatch at the data level by representing low-level actions with natural language. This alignment makes it possible to train VLAs solely with Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and averting catastrophic forgetting. As a result, the VLM can be fine-tuned on robot teleoperation data without fundamentally altering the underlying architecture and without expensive co-training on internet-scale VLM datasets. Through extensive Visual Question Answering (VQA) studies and over 800 real-world robotics experiments, we demonstrate that VLM2VLA preserves the VLM's core capabilities, enabling zero-shot generalization to novel tasks that require open-world semantic reasoning and multilingual instruction following.",
          "site": "arxiv.org",
          "rank": 21,
          "published": "2025-09-26T10:54:04Z",
          "authors": [
            "Asher J. Hancock",
            "Xindi Wu",
            "Lihan Zha",
            "Olga Russakovsky",
            "Anirudha Majumdar"
          ],
          "arxiv_id": "2509.22195",
          "abstract": "Fine-tuning vision-language models (VLMs) on robot teleoperation data to create vision-language-action (VLA) models is a promising paradigm for training generalist policies, but it suffers from a fundamental tradeoff: learning to produce actions often diminishes the VLM's foundational reasoning and multimodal understanding, hindering generalization to novel scenarios, instruction following, and semantic understanding. We argue that this catastrophic forgetting is due to a distribution mismatch between the VLM's internet-scale pretraining corpus and the robotics fine-tuning data. Inspired by this observation, we introduce VLM2VLA: a VLA training paradigm that first resolves this mismatch at the data level by representing low-level actions with natural language. This alignment makes it possible to train VLAs solely with Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and averting catastrophic forgetting. As a result, the VLM can be fine-tuned on robot teleoperation data without fundamentally altering the underlying architecture and without expensive co-training on internet-scale VLM datasets. Through extensive Visual Question Answering (VQA) studies and over 800 real-world robotics experiments, we demonstrate that VLM2VLA preserves the VLM's core capabilities, enabling zero-shot generalization to novel tasks that require open-world semantic reasoning and multilingual instruction following.",
          "abstract_zh": "在机器人远程操作数据上微调视觉语言模型（VLM）以创建视觉语言动作（VLA）模型是训练通才策略的一种有前途的范例，但它面临着一个基本的权衡：学习产生动作通常会削弱VLM的基础推理和多模态理解，阻碍对新场景、指令遵循和语义理解的泛化。我们认为，这种灾难性遗忘是由于 VLM 的互联网规模预训练语料库和机器人微调数据之间的分布不匹配造成的。受这一观察的启发，我们引入了 VLM2VLA：一种 VLA 训练范例，它首先通过用自然语言表示低级动作来解决数据级别的这种不匹配问题。这种对齐方式使得仅使用低秩适应（LoRA）来训练 VLA 成为可能，从而最大限度地减少 VLM 主干的修改并避免灾难性遗忘。因此，VLM 可以根据机器人远程操作数据进行微调，而无需从根本上改变底层架构，也无需在互联网规模的 VLM 数据集上进行昂贵的协同训练。通过广泛的视觉问答 (VQA) 研究和 800 多个真实世界的机器人实验，我们证明 VLM2VLA 保留了 VLM 的核心功能，能够零样本泛化到需要开放世界语义推理和多语言指令遵循的新任务。"
        },
        {
          "title": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
          "url": "http://arxiv.org/abs/2509.23224v1",
          "snippet": "To improve efficiency and temporal coherence, Vision-Language-Action (VLA) models often predict action chunks; however, this action chunking harms reactivity under inference delay and long horizons. We introduce Asynchronous Action Chunk Correction (A2C2), which is a lightweight real-time chunk correction head that runs every control step and adds a time-aware correction to any off-the-shelf VLA's action chunk. The module combines the latest observation, the predicted action from VLA (base action), a positional feature that encodes the index of the base action within the chunk, and some features from the base policy, then outputs a per-step correction. This preserves the base model's competence while restoring closed-loop responsiveness. The approach requires no retraining of the base policy and is orthogonal to asynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic Kinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent success rate improvements across increasing delays and execution horizons (+23% point and +7% point respectively, compared to RTC), and also improves robustness for long horizons even with zero injected delay. Since the correction head is small and fast, there is minimal overhead compared to the inference of large VLA models. These results indicate that A2C2 is an effective, plug-in mechanism for deploying high-capacity chunking policies in real-time control.",
          "site": "arxiv.org",
          "rank": 22,
          "published": "2025-09-27T10:07:49Z",
          "authors": [
            "Kohei Sendai",
            "Maxime Alvarez",
            "Tatsuya Matsushima",
            "Yutaka Matsuo",
            "Yusuke Iwasawa"
          ],
          "arxiv_id": "2509.23224",
          "abstract": "To improve efficiency and temporal coherence, Vision-Language-Action (VLA) models often predict action chunks; however, this action chunking harms reactivity under inference delay and long horizons. We introduce Asynchronous Action Chunk Correction (A2C2), which is a lightweight real-time chunk correction head that runs every control step and adds a time-aware correction to any off-the-shelf VLA's action chunk. The module combines the latest observation, the predicted action from VLA (base action), a positional feature that encodes the index of the base action within the chunk, and some features from the base policy, then outputs a per-step correction. This preserves the base model's competence while restoring closed-loop responsiveness. The approach requires no retraining of the base policy and is orthogonal to asynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic Kinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent success rate improvements across increasing delays and execution horizons (+23% point and +7% point respectively, compared to RTC), and also improves robustness for long horizons even with zero injected delay. Since the correction head is small and fast, there is minimal overhead compared to the inference of large VLA models. These results indicate that A2C2 is an effective, plug-in mechanism for deploying high-capacity chunking policies in real-time control.",
          "abstract_zh": "为了提高效率和时间连贯性，视觉-语言-动作（VLA）模型通常预测动作块；然而，这种动作分块会损害推理延迟和长视野下的反应性。我们引入了异步动作块校正（A2​​C2），它是一个轻量级的实时块校正头，它运行每个控制步骤，并向任何现成的 VLA 动作块添加时间感知校正。该模块结合了最新的观察结果、VLA（基本动作）的预测动作、对块内基本动作的索引进行编码的位置特征以及来自基本策略的一些特征，然后输出每步校正。这保留了基础模型的能力，同时恢复了闭环响应能力。该方法不需要重新训练基本策略，并且与实时分块 (RTC) 等异步执行方案正交。在动态 Kinetix 任务套件（12 个任务）和 LIBERO Spatial 上，我们的方法在增加延迟和执行范围内实现了一致的成功率改进（与 RTC 相比，分别+23% 点和 +7% 点），并且即使在零注入延迟的情况下也提高了长范围的鲁棒性。由于校正头小且速度快，因此与大型 VLA 模型的推理相比，开销最小。这些结果表明，A2C2 是一种在实时控制中部署大容量分块策略的有效插件机制。"
        },
        {
          "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation",
          "url": "http://arxiv.org/abs/2509.18865v1",
          "snippet": "We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality. Bi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion. We validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning. Our Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility. Experimental results validate the effectiveness of Bi-VLA in real-world tasks. For additional material, please visit the website: https://mertcookimg.github.io/bi-vla/",
          "site": "arxiv.org",
          "rank": 23,
          "published": "2025-09-23T10:02:16Z",
          "authors": [
            "Masato Kobayashi",
            "Thanpimon Buamanee"
          ],
          "arxiv_id": "2509.18865",
          "abstract": "We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality. Bi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion. We validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning. Our Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility. Experimental results validate the effectiveness of Bi-VLA in real-world tasks. For additional material, please visit the website: https://mertcookimg.github.io/bi-vla/",
          "abstract_zh": "我们提出了通过视觉语言融合进行动作生成的基于双边控制的模仿学习（Bi-VLA），这是一种新颖的框架，它扩展了基于双边控制的模仿学习，以在单个模型中处理多个任务。传统的双边控制方法利用关节角度、速度、扭矩和视觉来进行精确操作，但需要特定于任务的模型，限制了其通用性。Bi-VLA 通过利用来自主从双边控制的机器人关节角度、速度和扭矩数据以及视觉特征和自然语言指令（通过基于 SigLIP 和 FiLM 的融合）克服了这一限制。我们在两种任务类型上验证了 Bi-VLA：一种需要补充语言提示，另一种仅通过视觉即可区分。真实机器人实验表明，与传统的基于双边控制的模仿学习相比，Bi-VLA 成功地解释了视觉语言组合并提高了任务成功率。我们的 Bi-VLA 解决了先前双边方法的单一任务限制，并提供了经验证据，证明视觉和语言的结合可以显着增强多功能性。实验结果验证了 Bi-VLA 在实际任务中的有效性。如需其他材料，请访问网站：https://mertcookimg.github.io/bi-vla/"
        },
        {
          "title": "Latent Action Pretraining Through World Modeling",
          "url": "http://arxiv.org/abs/2509.18428v1",
          "snippet": "Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and $π_{0}$, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging. In this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.",
          "site": "arxiv.org",
          "rank": 24,
          "published": "2025-09-22T21:19:10Z",
          "authors": [
            "Bahey Tharwat",
            "Yara Nasser",
            "Ali Abouzeid",
            "Ian Reid"
          ],
          "arxiv_id": "2509.18428",
          "abstract": "Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and $π_{0}$, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging. In this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.",
          "abstract_zh": "视觉-语言-动作（VLA）模型因学习遵循语言指令的机器人操作任务而受到欢迎。最先进的 VLA，例如 OpenVLA 和 $π_{0}$，是在通过远程操作收集的大规模、手动标记的动作数据集上进行训练的。最近的方法，包括 LAPA 和 Villa-X，引入了潜在动作表示，通过对帧之间的抽象视觉变化进行建模，可以对未标记的数据集进行无监督的预训练。尽管这些方法已经显示出强大的结果，但它们的模型规模较大，使得在现实环境中的部署具有挑战性。在这项工作中，我们提出了 LAWM，这是一种与模型无关的框架，通过世界建模从未标记的视频数据中学习潜在动作表示，以自我监督的方式预训练模仿学习模型。这些视频可以来自机器人录制的视频或人类对日常物体执行动作的视频。我们的框架旨在有效地跨任务、环境和实施例进行迁移。它的性能优于在 LIBERO 基准和现实世界设置上使用地面实况机器人动作和类似预训练方法训练的模型，同时对于现实世界设置而言更加高效和实用。"
        },
        {
          "title": "Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training",
          "url": "http://arxiv.org/abs/2509.19752v2",
          "snippet": "Vision-language-action (VLA) models have shown strong generalization across tasks and embodiments; however, their reliance on large-scale human demonstrations limits their scalability owing to the cost and effort of manual data collection. Reinforcement learning (RL) offers a potential alternative to generate demonstrations autonomously, yet conventional RL algorithms often struggle on long-horizon manipulation tasks with sparse rewards. In this paper, we propose a modified diffusion policy optimization algorithm to generate high-quality and low-variance trajectories, which contributes to a diffusion RL-powered VLA training pipeline. Our algorithm benefits from not only the high expressiveness of diffusion models to explore complex and diverse behaviors but also the implicit regularization of the iterative denoising process to yield smooth and consistent demonstrations. We evaluate our approach on the LIBERO benchmark, which includes 130 long-horizon manipulation tasks, and show that the generated trajectories are smoother and more consistent than both human demonstrations and those from standard Gaussian RL policies. Further, training a VLA model exclusively on the diffusion RL-generated data achieves an average success rate of 81.9%, which outperforms the model trained on human data by +5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight our diffusion RL as an effective alternative for generating abundant, high-quality, and low-variance demonstrations for VLA models.",
          "site": "arxiv.org",
          "rank": 25,
          "published": "2025-09-24T04:08:14Z",
          "authors": [
            "Rushuai Yang",
            "Hangxing Wei",
            "Ran Zhang",
            "Zhiyuan Feng",
            "Xiaoyu Chen",
            "Tong Li",
            "Chuheng Zhang",
            "Li Zhao",
            "Jiang Bian",
            "Xiu Su",
            "Yi Chen"
          ],
          "arxiv_id": "2509.19752",
          "abstract": "Vision-language-action (VLA) models have shown strong generalization across tasks and embodiments; however, their reliance on large-scale human demonstrations limits their scalability owing to the cost and effort of manual data collection. Reinforcement learning (RL) offers a potential alternative to generate demonstrations autonomously, yet conventional RL algorithms often struggle on long-horizon manipulation tasks with sparse rewards. In this paper, we propose a modified diffusion policy optimization algorithm to generate high-quality and low-variance trajectories, which contributes to a diffusion RL-powered VLA training pipeline. Our algorithm benefits from not only the high expressiveness of diffusion models to explore complex and diverse behaviors but also the implicit regularization of the iterative denoising process to yield smooth and consistent demonstrations. We evaluate our approach on the LIBERO benchmark, which includes 130 long-horizon manipulation tasks, and show that the generated trajectories are smoother and more consistent than both human demonstrations and those from standard Gaussian RL policies. Further, training a VLA model exclusively on the diffusion RL-generated data achieves an average success rate of 81.9%, which outperforms the model trained on human data by +5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight our diffusion RL as an effective alternative for generating abundant, high-quality, and low-variance demonstrations for VLA models.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在任务和实施例中表现出很强的泛化性；然而，由于手动数据收集的成本和工作量，它们对大规模人类演示的依赖限制了其可扩展性。强化学习 (RL) 提供了一种自动生成演示的潜在替代方案，但传统的 RL 算法常常难以处理奖励稀疏的长范围操作任务。在本文中，我们提出了一种改进的扩散策略优化算法来生成高质量和低方差的轨迹，这有助于扩散 RL 驱动的 VLA 训练流程。我们的算法不仅受益于扩散模型的高表达力来探索复杂多样的行为，而且受益于迭代去噪过程的隐式正则化以产生平滑且一致的演示。我们在 LIBERO 基准上评估了我们的方法，其中包括 130 个长视野操作任务，结果表明生成的轨迹比人类演示和标准高斯强化学习策略的轨迹更平滑、更一致。此外，仅在扩散强化学习生成的数据上训练 VLA 模型的平均成功率为 81.9%，比在人类数据上训练的模型高出 5.3%，在高斯强化学习生成的数据上训练的模型高出 12.6%。结果凸显了我们的扩散强化学习是为 VLA 模型生成丰富、高质量和低方差演示的有效替代方案。"
        },
        {
          "title": "PhysiAgent: An Embodied Agent Framework in Physical World",
          "url": "http://arxiv.org/abs/2509.24524v1",
          "snippet": "Vision-Language-Action (VLA) models have achieved notable success but often struggle with limited generalizations. To address this, integrating generalized Vision-Language Models (VLMs) as assistants to VLAs has emerged as a popular solution. However, current approaches often combine these models in rigid, sequential structures: using VLMs primarily for high-level scene understanding and task planning, and VLAs merely as executors of lower-level actions, leading to ineffective collaboration and poor grounding challenges. In this paper, we propose an embodied agent framework, PhysiAgent, tailored to operate effectively in physical environments. By incorporating monitor, memory, self-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent offers an autonomous scaffolding framework to prompt VLMs to organize different components based on real-time proficiency feedback from VLAs to maximally exploit VLAs' capabilities. Experimental results demonstrate significant improvements in task-solving performance on complex real-world robotic tasks, showcasing effective self-regulation of VLMs, coherent tool collaboration, and adaptive evolution of the framework during execution. PhysiAgent makes practical and pioneering efforts to integrate VLMs and VLAs, effectively grounding embodied agent frameworks in real-world settings.",
          "site": "arxiv.org",
          "rank": 26,
          "published": "2025-09-29T09:39:32Z",
          "authors": [
            "Zhihao Wang",
            "Jianxiong Li",
            "Jinliang Zheng",
            "Wencong Zhang",
            "Dongxiu Liu",
            "Yinan Zheng",
            "Haoyi Niu",
            "Junzhi Yu",
            "Xianyuan Zhan"
          ],
          "arxiv_id": "2509.24524",
          "abstract": "Vision-Language-Action (VLA) models have achieved notable success but often struggle with limited generalizations. To address this, integrating generalized Vision-Language Models (VLMs) as assistants to VLAs has emerged as a popular solution. However, current approaches often combine these models in rigid, sequential structures: using VLMs primarily for high-level scene understanding and task planning, and VLAs merely as executors of lower-level actions, leading to ineffective collaboration and poor grounding challenges. In this paper, we propose an embodied agent framework, PhysiAgent, tailored to operate effectively in physical environments. By incorporating monitor, memory, self-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent offers an autonomous scaffolding framework to prompt VLMs to organize different components based on real-time proficiency feedback from VLAs to maximally exploit VLAs' capabilities. Experimental results demonstrate significant improvements in task-solving performance on complex real-world robotic tasks, showcasing effective self-regulation of VLMs, coherent tool collaboration, and adaptive evolution of the framework during execution. PhysiAgent makes practical and pioneering efforts to integrate VLMs and VLAs, effectively grounding embodied agent frameworks in real-world settings.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型取得了显着的成功，但往往难以概括。为了解决这个问题，集成通用视觉语言模型 (VLM) 作为 VLA 的助手已成为一种流行的解决方案。然而，当前的方法通常将这些模型以严格的顺序结构结合起来：主要使用 VLM 进行高级场景理解和任务规划，而 VLA 仅作为较低级别操作的执行者，从而导致无效的协作和基础较差的挑战。在本文中，我们提出了一个具体的代理框架 PhysiAgent，专为在物理环境中有效运行而定制。通过整合监控、内存、自我反思机制和轻量级现成工具箱，PhysiAgent 提供了一个自主的脚手架框架，促使 VLM 根据 VLA 的实时熟练程度反馈来组织不同的组件，以最大限度地利用 VLA 的功能。实验结果表明，复杂的现实世界机器人任务的任务解决性能显着提高，展示了 VLM 的有效自我调节、连贯的工具协作以及执行过程中框架的自适应进化。PhysiAgent 在集成 VLM 和 VLA 方面做出了务实且开创性的努力，有效地将具体代理框架扎根于现实环境中。"
        },
        {
          "title": "AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation",
          "url": "http://arxiv.org/abs/2509.25032v1",
          "snippet": "As robots transition from controlled settings to unstructured human environments, building generalist agents that can reliably follow natural language instructions remains a central challenge. Progress in robust mobile manipulation requires large-scale multimodal datasets that capture contact-rich and long-horizon tasks, yet existing resources lack synchronized force-torque sensing, hierarchical annotations, and explicit failure cases. We address this gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset for mobile manipulation. It includes synchronized RGB images, joint states, six-axis wrist force-torque signals, and internal robot states, together with a novel two-layer annotation schema of sub-goals and primitive actions for hierarchical learning and error analysis. The initial dataset comprises 25,469 episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa provides a critical benchmark for advancing the next generation of Vision-Language-Action models. The first version of our dataset is now available at https://huggingface.co/datasets/airoa-org/airoa-moma .",
          "site": "arxiv.org",
          "rank": 27,
          "published": "2025-09-29T16:51:47Z",
          "authors": [
            "Ryosuke Takanami",
            "Petr Khrapchenkov",
            "Shu Morikuni",
            "Jumpei Arima",
            "Yuta Takaba",
            "Shunsuke Maeda",
            "Takuya Okubo",
            "Genki Sano",
            "Satoshi Sekioka",
            "Aoi Kadoya",
            "Motonari Kambara",
            "Naoya Nishiura",
            "Haruto Suzuki",
            "Takanori Yoshimoto",
            "Koya Sakamoto",
            "Shinnosuke Ono",
            "Hu Yang",
            "Daichi Yashima",
            "Aoi Horo",
            "Tomohiro Motoda",
            "Kensuke Chiyoma",
            "Hiroshi Ito",
            "Koki Fukuda",
            "Akihito Goto",
            "Kazumi Morinaga",
            "Yuya Ikeda",
            "Riko Kawada",
            "Masaki Yoshikawa",
            "Norio Kosuge",
            "Yuki Noguchi",
            "Kei Ota",
            "Tatsuya Matsushima",
            "Yusuke Iwasawa",
            "Yutaka Matsuo",
            "Tetsuya Ogata"
          ],
          "arxiv_id": "2509.25032",
          "abstract": "As robots transition from controlled settings to unstructured human environments, building generalist agents that can reliably follow natural language instructions remains a central challenge. Progress in robust mobile manipulation requires large-scale multimodal datasets that capture contact-rich and long-horizon tasks, yet existing resources lack synchronized force-torque sensing, hierarchical annotations, and explicit failure cases. We address this gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset for mobile manipulation. It includes synchronized RGB images, joint states, six-axis wrist force-torque signals, and internal robot states, together with a novel two-layer annotation schema of sub-goals and primitive actions for hierarchical learning and error analysis. The initial dataset comprises 25,469 episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa provides a critical benchmark for advancing the next generation of Vision-Language-Action models. The first version of our dataset is now available at https://huggingface.co/datasets/airoa-org/airoa-moma .",
          "abstract_zh": "随着机器人从受控环境过渡到非结构化人类环境，构建能够可靠地遵循自然语言指令的多面手代理仍然是一个核心挑战。鲁棒移动操纵的进展需要大规模多模态数据集来捕获接触丰富和长视野的任务，但现有资源缺乏同步的力-扭矩传感、分层注释和明确的故障案例。我们通过 AIRoA MoMa 数据集弥补了这一差距，这是一个用于移动操作的大规模现实世界多模式数据集。它包括同步 RGB 图像、关节状态、六轴手腕力扭矩信号和内部机器人状态，以及用于分层学习和错误分析的子目标和原始动作的新颖两层注释模式。初始数据集包含由人类支持机器人 (HSR) 收集的 25,469 个片段（约 94 小时），并以 LeRobot v2.1 格式完全标准化。通过独特地集成移动操作、丰富的接触交互和长视野结构，AIRoA MoMa 为推进下一代视觉-语言-动作模型提供了关键基准。我们数据集的第一个版本现已在 https://huggingface.co/datasets/airoa-org/airoa-moma 提供。"
        },
        {
          "title": "Prepare Before You Act: Learning From Humans to Rearrange Initial States",
          "url": "http://arxiv.org/abs/2509.18043v1",
          "snippet": "Imitation learning (IL) has proven effective across a wide range of manipulation tasks. However, IL policies often struggle when faced with out-of-distribution observations; for instance, when the target object is in a previously unseen position or occluded by other objects. In these cases, extensive demonstrations are needed for current IL methods to reach robust and generalizable behaviors. But when humans are faced with these sorts of atypical initial states, we often rearrange the environment for more favorable task execution. For example, a person might rotate a coffee cup so that it is easier to grasp the handle, or push a box out of the way so they can directly grasp their target object. In this work we seek to equip robot learners with the same capability: enabling robots to prepare the environment before executing their given policy. We propose ReSET, an algorithm that takes initial states -- which are outside the policy's distribution -- and autonomously modifies object poses so that the restructured scene is similar to training data. Theoretically, we show that this two step process (rearranging the environment before rolling out the given policy) reduces the generalization gap. Practically, our ReSET algorithm combines action-agnostic human videos with task-agnostic teleoperation data to i) decide when to modify the scene, ii) predict what simplifying actions a human would take, and iii) map those predictions into robot action primitives. Comparisons with diffusion policies, VLAs, and other baselines show that using ReSET to prepare the environment enables more robust task execution with equal amounts of total training data. See videos at our project website: https://reset2025paper.github.io/",
          "site": "arxiv.org",
          "rank": 28,
          "published": "2025-09-22T17:18:52Z",
          "authors": [
            "Yinlong Dai",
            "Andre Keyser",
            "Dylan P. Losey"
          ],
          "arxiv_id": "2509.18043",
          "abstract": "Imitation learning (IL) has proven effective across a wide range of manipulation tasks. However, IL policies often struggle when faced with out-of-distribution observations; for instance, when the target object is in a previously unseen position or occluded by other objects. In these cases, extensive demonstrations are needed for current IL methods to reach robust and generalizable behaviors. But when humans are faced with these sorts of atypical initial states, we often rearrange the environment for more favorable task execution. For example, a person might rotate a coffee cup so that it is easier to grasp the handle, or push a box out of the way so they can directly grasp their target object. In this work we seek to equip robot learners with the same capability: enabling robots to prepare the environment before executing their given policy. We propose ReSET, an algorithm that takes initial states -- which are outside the policy's distribution -- and autonomously modifies object poses so that the restructured scene is similar to training data. Theoretically, we show that this two step process (rearranging the environment before rolling out the given policy) reduces the generalization gap. Practically, our ReSET algorithm combines action-agnostic human videos with task-agnostic teleoperation data to i) decide when to modify the scene, ii) predict what simplifying actions a human would take, and iii) map those predictions into robot action primitives. Comparisons with diffusion policies, VLAs, and other baselines show that using ReSET to prepare the environment enables more robust task execution with equal amounts of total training data. See videos at our project website: https://reset2025paper.github.io/",
          "abstract_zh": "模仿学习 (IL) 已被证明在各种操作任务中都有效。然而，IL 政策在面对分布外的观察结果时常常会陷入困境。例如，当目标对象处于先前未见过的位置或被其他对象遮挡时。在这些情况下，当前的 IL 方法需要进行广泛的演示才能实现稳健且可推广的行为。但是，当人类面临这些非典型的初始状态时，我们经常会重新安排环境以实现更有利的任务执行。例如，人们可能会旋转咖啡杯，以便更容易抓住把手，或者将盒子推开，以便他们可以直接抓住目标物体。在这项工作中，我们寻求为机器人学习者配备相同的能力：使机器人能够在执行给定策略之前准备好环境。我们提出了 ReSET，这是一种采用初始状态（在策略分布之外）的算法，并自动修改对象姿势，以便重构的场景与训练数据相似。从理论上讲，我们表明这两个步骤的过程（在推出给定策略之前重新安排环境）减少了泛化差距。实际上，我们的 ReSET 算法将与动作无关的人类视频与与任务无关的遥操作数据相结合，以 i) 决定何时修改场景，ii) 预测人类将采取哪些简化动作，以及 iii) 将这些预测映射到机器人动作原语中。与扩散策略、VLA 和其他基线的比较表明，使用 ReSET 准备环境可以使用等量的总训练数据实现更稳健的任务执行。在我们的项目网站上观看视频：https://reset2025paper.github.io/"
        },
        {
          "title": "AutoPrune: Each Complexity Deserves a Pruning Policy",
          "url": "http://arxiv.org/abs/2509.23931v2",
          "snippet": "The established redundancy in visual tokens within large vision-language models allows pruning to effectively reduce their substantial computational demands. Previous methods typically employ heuristic layer-specific pruning strategies where, although the number of tokens removed may differ across decoder layers, the overall pruning schedule is fixed and applied uniformly to all input samples and tasks, failing to align token elimination with the model's holistic reasoning trajectory. Cognitive science indicates that human visual processing often begins with broad exploration to accumulate evidence before narrowing focus as the target becomes distinct. Our experiments reveal an analogous pattern in these models. This observation suggests that neither a fixed pruning schedule nor a heuristic layer-wise strategy can optimally accommodate the diverse complexities inherent in different inputs. To overcome this limitation, we introduce Complexity-Adaptive Pruning (AutoPrune), a training-free, plug-and-play framework that tailors pruning policies to varying sample and task complexities. Specifically, AutoPrune quantifies the mutual information between visual and textual tokens, then projects this signal to a budget-constrained logistic retention curve. Each such logistic curve, defined by its unique shape, corresponds to the specific complexity of different tasks and can guarantee adherence to predefined computational constraints. We evaluate AutoPrune on standard vision-language tasks and on Vision-Language-Action models for autonomous driving. Notably, when applied to LLaVA-1.5-7B, our method prunes 89% of visual tokens and reduces inference FLOPs by 76.8% while retaining 96.7% of the original accuracy averaged over all tasks. This corresponds to a 9.1% improvement over the recent work PDrop, demonstrating the effectiveness. Code is available at https://github.com/AutoLab-SAI-SJTU/AutoPrune.",
          "site": "arxiv.org",
          "rank": 29,
          "published": "2025-09-28T15:09:00Z",
          "authors": [
            "Hanshi Wang",
            "Yuhao Xu",
            "Zekun Xu",
            "Jin Gao",
            "Yufan Liu",
            "Weiming Hu",
            "Ke Wang",
            "Zhipeng Zhang"
          ],
          "arxiv_id": "2509.23931",
          "abstract": "The established redundancy in visual tokens within large vision-language models allows pruning to effectively reduce their substantial computational demands. Previous methods typically employ heuristic layer-specific pruning strategies where, although the number of tokens removed may differ across decoder layers, the overall pruning schedule is fixed and applied uniformly to all input samples and tasks, failing to align token elimination with the model's holistic reasoning trajectory. Cognitive science indicates that human visual processing often begins with broad exploration to accumulate evidence before narrowing focus as the target becomes distinct. Our experiments reveal an analogous pattern in these models. This observation suggests that neither a fixed pruning schedule nor a heuristic layer-wise strategy can optimally accommodate the diverse complexities inherent in different inputs. To overcome this limitation, we introduce Complexity-Adaptive Pruning (AutoPrune), a training-free, plug-and-play framework that tailors pruning policies to varying sample and task complexities. Specifically, AutoPrune quantifies the mutual information between visual and textual tokens, then projects this signal to a budget-constrained logistic retention curve. Each such logistic curve, defined by its unique shape, corresponds to the specific complexity of different tasks and can guarantee adherence to predefined computational constraints. We evaluate AutoPrune on standard vision-language tasks and on Vision-Language-Action models for autonomous driving. Notably, when applied to LLaVA-1.5-7B, our method prunes 89% of visual tokens and reduces inference FLOPs by 76.8% while retaining 96.7% of the original accuracy averaged over all tasks. This corresponds to a 9.1% improvement over the recent work PDrop, demonstrating the effectiveness. Code is available at https://github.com/AutoLab-SAI-SJTU/AutoPrune.",
          "abstract_zh": "大型视觉语言模型中视觉标记中建立的冗余允许修剪以有效地减少其大量的计算需求。以前的方法通常采用启发式的特定于层的剪枝策略，尽管在解码器层之间删除的令牌数量可能有所不同，但总体剪枝时间表是固定的并统一应用于所有输入样本和任务，无法将令牌消除与模型的整体推理轨迹保持一致。认知科学表明，人类视觉处理通常从广泛探索开始，以积累证据，然后随着目标变得清晰而缩小焦点。我们的实验揭示了这些模型中的类似模式。这一观察结果表明，固定的修剪计划和启发式分层策略都无法最佳地适应不同输入中固有的不同复杂性。为了克服这一限制，我们引入了复杂性自适应剪枝（AutoPrune），这是一种免训练、即插即用的框架，可以根据不同的样本和任务复杂性定制剪枝策略。具体来说，AutoPrune 量化视觉和文本标记之间的相互信息，然后将该信号投射到预算受限的逻辑保留曲线上。每条这样的逻辑曲线由其独特的形状定义，对应于不同任务的特定复杂性，并且可以保证遵守预定义的计算约束。我们在标准视觉语言任务和自动驾驶视觉语言动作模型上评估 AutoPrune。值得注意的是，当应用于 LLaVA-1.5-7B 时，我们的方法修剪了 89% 的视觉标记，并将推理失败次数减少了 76.8%，同时保留了所有任务平均原始准确度的 96.7%。这相当于比最近的工作 PDrop 提高了 9.1%，证明了有效性。代码可在 https://github.com/AutoLab-SAI-SJTU/AutoPrune 获取。"
        },
        {
          "title": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation",
          "url": "http://arxiv.org/abs/2509.21006v1",
          "snippet": "We address natural language pick-and-place in unseen, unpredictable indoor environments with AnywhereVLA, a modular framework for mobile manipulation. A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy. An approach planner then selects visibility and reachability aware pre grasp base poses. For interaction, a compact SmolVLA manipulation head is fine tuned on platform pick and place trajectories for the SO-101 by TheRobotStudio, grounding local visual context and sub-goals into grasp and place proposals. The full system runs fully onboard on consumer-level hardware, with Jetson Orin NX for perception and VLA and an Intel NUC for SLAM, exploration, and control, sustaining real-time operation. We evaluated AnywhereVLA in a multi-room lab under static scenes and normal human motion. In this setting, the system achieves a $46\\%$ overall task success rate while maintaining throughput on embedded compute. By combining a classical stack with a fine-tuned VLA manipulation, the system inherits the reliability of geometry-based navigation with the agility and task generalization of language-conditioned manipulation.",
          "site": "arxiv.org",
          "rank": 30,
          "published": "2025-09-25T11:04:44Z",
          "authors": [
            "Konstantin Gubernatorov",
            "Artem Voronov",
            "Roman Voronov",
            "Sergei Pasynkov",
            "Stepan Perminov",
            "Ziang Guo",
            "Dzmitry Tsetserukou"
          ],
          "arxiv_id": "2509.21006",
          "abstract": "We address natural language pick-and-place in unseen, unpredictable indoor environments with AnywhereVLA, a modular framework for mobile manipulation. A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy. An approach planner then selects visibility and reachability aware pre grasp base poses. For interaction, a compact SmolVLA manipulation head is fine tuned on platform pick and place trajectories for the SO-101 by TheRobotStudio, grounding local visual context and sub-goals into grasp and place proposals. The full system runs fully onboard on consumer-level hardware, with Jetson Orin NX for perception and VLA and an Intel NUC for SLAM, exploration, and control, sustaining real-time operation. We evaluated AnywhereVLA in a multi-room lab under static scenes and normal human motion. In this setting, the system achieves a $46\\%$ overall task success rate while maintaining throughput on embedded compute. By combining a classical stack with a fine-tuned VLA manipulation, the system inherits the reliability of geometry-based navigation with the agility and task generalization of language-conditioned manipulation.",
          "abstract_zh": "我们使用 AnywhereVLA（一种用于移动操作的模块化框架）解决看不见、不可预测的室内环境中的自然语言拾取和放置问题。用户文本提示作为入口点，被解析为结构化任务图，该图利用 LiDAR 和摄像头、度量语义映射和任务感知前沿探索策略来调节经典 SLAM。然后，进近规划者选择可见性和可达性感知的预抓握基本姿势。对于交互，紧凑型 SmolVLA 操纵头通过 TheRobotStudio 在 SO-101 的平台拾取和放置轨迹上进行微调，将本地视觉背景和子目标融入抓取和放置建议中。整个系统完全在消费级硬件上运行，包括用于感知和 VLA 的 Jetson Orin NX 以及用于 SLAM、探索和控制的英特尔 NUC，以维持实时操作。我们在多房间实验室中的静态场景和正常人体运动下评估了 AnywhereVLA。在此设置下，系统的总体任务成功率达到 $46\\%$，同时保持嵌入式计算的吞吐量。通过将经典堆栈与微调的 VLA 操作相结合，该系统继承了基于几何的导航的可靠性以及语言条件操作的敏捷性和任务泛化。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 7 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 7）。",
      "items": [
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "HCPLab-SYSU/Embodied_AI_Paper_List",
          "url": "https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List",
          "snippet": "[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 7
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-09-22",
  "week_end": "2025-09-28",
  "last_updated": "2026-01-07"
}