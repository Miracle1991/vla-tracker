{
  "generated_at": "2026-01-07T13:26:55.742785",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 11 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 11）。",
      "items": [
        {
          "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model",
          "url": "http://arxiv.org/abs/2509.09372v2",
          "snippet": "Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training a large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose a lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only a 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of a powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-09-11T11:42:21Z",
          "authors": [
            "Yihao Wang",
            "Pengxiang Ding",
            "Lingxiao Li",
            "Can Cui",
            "Zirui Ge",
            "Xinyang Tong",
            "Wenxuan Song",
            "Han Zhao",
            "Wei Zhao",
            "Pengxu Hou",
            "Siteng Huang",
            "Yifan Tang",
            "Wenhui Wang",
            "Ru Zhang",
            "Jianyi Liu",
            "Donglin Wang"
          ],
          "arxiv_id": "2509.09372",
          "abstract": "Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training a large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose a lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only a 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of a powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型通常通过在机器人数据上预训练大规模视觉-语言模型 (VLM) 来弥合感知空间和动作空间之间的差距。虽然这种方法极大地提高了性能，但也会产生大量的培训成本。在本文中，我们研究了如何有效地将视觉语言（VL）表示与动作（A）联系起来。我们引入了 VLA-Adapter，这是一种新颖的范式，旨在减少 VLA 模型对大规模 VLM 和广泛预训练的依赖。为此，我们首先系统地分析了各种 VL 条件的有效性，并提出了哪些条件对于桥接感知和行动空间至关重要的关键发现。基于这些见解，我们提出了一个具有 Bridge Attention 的轻量级策略模块，它可以自动将最佳条件注入到动作空间中。通过这种方式，我们的方法仅使用 0.5B 参数主干就实现了高性能，无需任何机器人数据预训练。对模拟和现实世界机器人基准的大量实验表明，VLA-Adapter 不仅实现了最先进的性能水平，而且还提供了迄今为止报道的快速推理速度。此外，得益于所提出的先进桥接范例，VLA-Adapter 能够在单个消费级 GPU 上仅用 8 小时训练出强大的 VLA 模型，大大降低了部署 VLA 模型的门槛。项目页面：https://vla-adapter.github.io/。"
        },
        {
          "title": "TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2509.07962v1",
          "snippet": "Many robotic manipulation tasks require sensing and responding to force signals such as torque to assess whether the task has been successfully completed and to enable closed-loop control. However, current Vision-Language-Action (VLA) models lack the ability to integrate such subtle physical feedback. In this work, we explore Torque-aware VLA models, aiming to bridge this gap by systematically studying the design space for incorporating torque signals into existing VLA architectures. We identify and evaluate several strategies, leading to three key findings. First, introducing torque adapters into the decoder consistently outperforms inserting them into the encoder.Third, inspired by joint prediction and planning paradigms in autonomous driving, we propose predicting torque as an auxiliary output, which further improves performance. This strategy encourages the model to build a physically grounded internal representation of interaction dynamics. Extensive quantitative and qualitative experiments across contact-rich manipulation benchmarks validate our findings.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-09-09T17:50:37Z",
          "authors": [
            "Zongzheng Zhang",
            "Haobo Xu",
            "Zhuo Yang",
            "Chenghao Yue",
            "Zehao Lin",
            "Huan-ang Gao",
            "Ziwei Wang",
            "Hao Zhao"
          ],
          "arxiv_id": "2509.07962",
          "abstract": "Many robotic manipulation tasks require sensing and responding to force signals such as torque to assess whether the task has been successfully completed and to enable closed-loop control. However, current Vision-Language-Action (VLA) models lack the ability to integrate such subtle physical feedback. In this work, we explore Torque-aware VLA models, aiming to bridge this gap by systematically studying the design space for incorporating torque signals into existing VLA architectures. We identify and evaluate several strategies, leading to three key findings. First, introducing torque adapters into the decoder consistently outperforms inserting them into the encoder.Third, inspired by joint prediction and planning paradigms in autonomous driving, we propose predicting torque as an auxiliary output, which further improves performance. This strategy encourages the model to build a physically grounded internal representation of interaction dynamics. Extensive quantitative and qualitative experiments across contact-rich manipulation benchmarks validate our findings.",
          "abstract_zh": "许多机器人操纵任务需要感测和响应扭矩等力信号，以评估任务是否已成功完成并实现闭环控制。然而，当前的视觉-语言-动作（VLA）模型缺乏整合这种微妙的物理反馈的能力。在这项工作中，我们探索了扭矩感知 VLA 模型，旨在通过系统地研究将扭矩信号合并到现有 VLA 架构中的设计空间来弥补这一差距。我们确定并评估了几种策略，得出了三个关键发现。首先，将扭矩适配器引入解码器始终优于将其插入编码器。第三，受到自动驾驶中联合预测和规划范例的启发，我们建议将预测扭矩作为辅助输出，这进一步提高了性能。该策略鼓励模型构建交互动态的物理基础内部表示。跨接触丰富的操纵基准的广泛定量和定性实验验证了我们的发现。"
        },
        {
          "title": "Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation",
          "url": "http://arxiv.org/abs/2509.07957v1",
          "snippet": "Acquiring dexterous robotic skills from human video demonstrations remains a significant challenge, largely due to conventional reliance on low-level trajectory replication, which often fails to generalize across varying objects, spatial layouts, and manipulator configurations. To address this limitation, we introduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB-D human demonstrations. GF-VLA employs an information-theoretic approach to extract task-relevant cues, selectively highlighting critical hand-object and object-object interactions. These cues are structured into temporally ordered scene graphs, which are subsequently integrated with a language-conditioned transformer to produce hierarchical behavior trees and interpretable Cartesian motion primitives. To enhance efficiency in bimanual execution, we propose a cross-arm allocation strategy that autonomously determines gripper assignment without requiring explicit geometric modeling. We validate GF-VLA on four dual-arm block assembly benchmarks involving symbolic structure construction and spatial generalization. Empirical results demonstrate that the proposed representation achieves over 95% graph accuracy and 93% subtask segmentation, enabling the language-action planner to generate robust, interpretable task policies. When deployed on a dual-arm robot, these policies attain 94% grasp reliability, 89% placement accuracy, and 90% overall task success across stacking, letter-formation, and geometric reconfiguration tasks, evidencing strong generalization and robustness under diverse spatial and semantic variations.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-09-09T17:44:36Z",
          "authors": [
            "Shunlei Li",
            "Longsen Gao",
            "Jiuwen Cao",
            "Yingbai Hu"
          ],
          "arxiv_id": "2509.07957",
          "abstract": "Acquiring dexterous robotic skills from human video demonstrations remains a significant challenge, largely due to conventional reliance on low-level trajectory replication, which often fails to generalize across varying objects, spatial layouts, and manipulator configurations. To address this limitation, we introduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB-D human demonstrations. GF-VLA employs an information-theoretic approach to extract task-relevant cues, selectively highlighting critical hand-object and object-object interactions. These cues are structured into temporally ordered scene graphs, which are subsequently integrated with a language-conditioned transformer to produce hierarchical behavior trees and interpretable Cartesian motion primitives. To enhance efficiency in bimanual execution, we propose a cross-arm allocation strategy that autonomously determines gripper assignment without requiring explicit geometric modeling. We validate GF-VLA on four dual-arm block assembly benchmarks involving symbolic structure construction and spatial generalization. Empirical results demonstrate that the proposed representation achieves over 95% graph accuracy and 93% subtask segmentation, enabling the language-action planner to generate robust, interpretable task policies. When deployed on a dual-arm robot, these policies attain 94% grasp reliability, 89% placement accuracy, and 90% overall task success across stacking, letter-formation, and geometric reconfiguration tasks, evidencing strong generalization and robustness under diverse spatial and semantic variations.",
          "abstract_zh": "从人类视频演示中获得灵巧的机器人技能仍然是一个重大挑战，这主要是由于传统上对低级轨迹复制的依赖，而这种复制往往无法在不同的物体、空间布局和操纵器配置之间进行泛化。为了解决这一限制，我们引入了图形融合视觉语言动作（GF-VLA），这是一个统一的框架，使双臂机器人系统能够直接从 RGB-D 人类演示中执行任务级推理和执行。GF-VLA 采用信息论方法来提取与任务相关的线索，有选择地突出关键的手-物体和物体-物体交互。这些线索被构造成按时间顺序排列的场景图，随后与语言条件转换器集成以生成分层行为树和可解释的笛卡尔运动基元。为了提高双手执行的效率，我们提出了一种横臂分配策略，可以自主确定夹具分配，而不需要显式的几何建模。我们在涉及符号结构构建和空间泛化的四个双臂块组装基准上验证 GF-VLA。实证结果表明，所提出的表示实现了超过 95% 的图形准确性和 93% 的子任务分割，使语言-动作规划器能够生成强大的、可解释的任务策略。当部署在双臂机器人上时，这些策略在堆叠、字母形成和几何重新配置​​任务中实现了 94% 的抓取可靠性、89% 的放置准确性和 90% 的总体任务成功率，证明了在不同空间和语义变化下的强大泛化性和鲁棒性。"
        },
        {
          "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions",
          "url": "http://arxiv.org/abs/2509.06951v2",
          "snippet": "Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-09-08T17:58:30Z",
          "authors": [
            "Qi Lv",
            "Weijie Kong",
            "Hao Li",
            "Jia Zeng",
            "Zherui Qiu",
            "Delin Qu",
            "Haoming Song",
            "Qizhi Chen",
            "Xiang Deng",
            "Jiangmiao Pang"
          ],
          "arxiv_id": "2509.06951",
          "abstract": "Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.",
          "abstract_zh": "在动态视觉环境中执行语言条件任务仍然是实体人工智能的核心挑战。现有的视觉-语言-动作（VLA）模型主要采用反应式状态到动作映射，往往导致动态场景中的短视行为和鲁棒性差。在本文中，我们介绍了 F1，一个预训练的 VLA 框架，它将视觉预见生成集成到决策流程中。F1 采用 Mixture-of-Transformer 架构，配备感知、预见生成和控制专用模块，从而架起理解、生成和行动的桥梁。F1 的核心是采用下一个规模的预测机制来综合目标条件的视觉远见作为明确的规划目标。通过预测未来可能的视觉状态，F1 将动作生成重新表述为一个有远见引导的逆动力学问题，从而使动作能够隐式实现视觉目标。为了赋予 F1 强大且可泛化的能力，我们在包含 136 个不同任务的超过 330k 轨迹的广泛数据集上提出了一个三阶段训练方案。该训练方案增强了模块化推理，并为模型配备了可转移的视觉远见，这对于复杂和动态的环境至关重要。对现实世界任务和模拟基准的广泛评估表明，F1 始终优于现有方法，在任务成功率和泛化能力方面都取得了显着的进步。"
        },
        {
          "title": "Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations",
          "url": "http://arxiv.org/abs/2509.11417v2",
          "snippet": "Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments. However, direct fine-tuning on robot data often disrupts these representations and limits generalization. We present a framework that better preserves pretrained features while adapting them for robot manipulation. Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances. Evaluations in simulation and on real robots show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-09-14T20:08:56Z",
          "authors": [
            "Shresth Grover",
            "Akshay Gopalkrishnan",
            "Bo Ai",
            "Henrik I. Christensen",
            "Hao Su",
            "Xuanlin Li"
          ],
          "arxiv_id": "2509.11417",
          "abstract": "Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments. However, direct fine-tuning on robot data often disrupts these representations and limits generalization. We present a framework that better preserves pretrained features while adapting them for robot manipulation. Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances. Evaluations in simulation and on real robots show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.",
          "abstract_zh": "从视觉语言模型 (VLM) 微调而来的视觉语言动作 (VLA) 模型有望利用丰富的预训练表示来构建跨不同任务和环境的通用机器人。然而，对机器人数据的直接微调通常会破坏这些表示并限制泛化。我们提出了一个框架，可以更好地保留预训练的特征，同时使它们适应机器人操作。我们的方法引入了三个组件：（i）双编码器设计，其中一个冻结视觉编码器用于保留预训练特征，另一个可训练用于任务适应，（ii）基于字符串的动作分词器，将连续动作转换为与模型预训练域对齐的字符序列，以及（iii）联合训练策略，将机器人演示与强调空间推理和可供性的视觉语言数据集相结合。模拟和真实机器人的评估表明，与基线相比，我们的方法提高了对视觉扰动的鲁棒性、对新指令和环境的泛化以及总体任务成功率。"
        },
        {
          "title": "LLaDA-VLA: Vision Language Diffusion Action Models",
          "url": "http://arxiv.org/abs/2509.06932v2",
          "snippet": "The rapid progress of auto-regressive vision-language models (VLMs) has inspired growing interest in vision-language-action models (VLA) for robotic manipulation. Recently, masked diffusion models, a paradigm distinct from autoregressive models, have begun to demonstrate competitive performance in text generation and multimodal applications, leading to the development of a series of diffusion-based VLMs (d-VLMs). However, leveraging such models for robot policy learning remains largely unexplored. In this work, we present LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to robotic domain, we introduce two key designs: (1) a localized special-token classification strategy that replaces full-vocabulary classification with special action token classification, reducing adaptation difficulty; (2) a hierarchical action-structured decoding strategy that decodes action sequences hierarchically considering the dependencies within and across actions. Extensive experiments demonstrate that LLaDA-VLA significantly outperforms state-of-the-art VLAs on both simulation and real-world robots.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-09-08T17:45:40Z",
          "authors": [
            "Yuqing Wen",
            "Hebei Li",
            "Kefan Gu",
            "Yucheng Zhao",
            "Tiancai Wang",
            "Xiaoyan Sun"
          ],
          "arxiv_id": "2509.06932",
          "abstract": "The rapid progress of auto-regressive vision-language models (VLMs) has inspired growing interest in vision-language-action models (VLA) for robotic manipulation. Recently, masked diffusion models, a paradigm distinct from autoregressive models, have begun to demonstrate competitive performance in text generation and multimodal applications, leading to the development of a series of diffusion-based VLMs (d-VLMs). However, leveraging such models for robot policy learning remains largely unexplored. In this work, we present LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to robotic domain, we introduce two key designs: (1) a localized special-token classification strategy that replaces full-vocabulary classification with special action token classification, reducing adaptation difficulty; (2) a hierarchical action-structured decoding strategy that decodes action sequences hierarchically considering the dependencies within and across actions. Extensive experiments demonstrate that LLaDA-VLA significantly outperforms state-of-the-art VLAs on both simulation and real-world robots.",
          "abstract_zh": "自回归视觉语言模型（VLM）的快速发展激发了人们对用于机器人操作的视觉语言动作模型（VLA）的兴趣。最近，掩蔽扩散模型（一种不同于自回归模型的范例）已经开始在文本生成和多模态应用中展现出有竞争力的性能，从而导致了一系列基于扩散的 VLM（d-VLM）的开发。然而，利用此类模型进行机器人策略学习在很大程度上仍未得到探索。在这项工作中，我们提出了 LLaDA-VLA，这是第一个基于预训练的 d-VLM 构建的视觉-语言-扩散-动作模型，用于机器人操作。为了有效地将d-VLM适应机器人领域，我们引入了两个关键设计：（1）本地化的特殊标记分类策略，用特殊动作标记分类代替全词汇分类，降低了适应难度；（2）分层动作结构解码策略，考虑动作内部和动作之间的依赖性，分层解码动作序列。大量实验表明，LLaDA-VLA 在模拟和现实世界的机器人上都显着优于最先进的 VLA。"
        },
        {
          "title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs",
          "url": "http://arxiv.org/abs/2509.11480v1",
          "snippet": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-09-15T00:00:37Z",
          "authors": [
            "Amir Taherin",
            "Juyi Lin",
            "Arash Akbari",
            "Arman Akbari",
            "Pu Zhao",
            "Weiwei Chen",
            "David Kaeli",
            "Yanzhi Wang"
          ],
          "arxiv_id": "2509.11480",
          "abstract": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.",
          "abstract_zh": "视觉-语言-动作（VLA）模型已经成为机器人控制的强大通用策略，但它们在模型架构和硬件平台上的性能扩展以及相关的功率预算仍然知之甚少。这项工作对五种代表性 VLA 模型进行了评估，涵盖最先进的基线和两种新提出的架构，针对边缘和数据中心 GPU 平台。使用 LIBERO 基准，我们在不同的边缘功率限制和高性能数据中心 GPU 配置下测量准确性以及系统级指标，包括延迟、吞吐量和峰值内存使用情况。我们的结果确定了不同的扩展趋势：（1）架构选择，例如动作标记化和模型骨干大小，强烈影响吞吐量和内存占用；(2) 功率受限的边缘设备表现出非线性性能下降，某些配置匹配或超过旧数据中心 GPU；(3) 可以实现高通量变体，而不会显着损失准确性。这些发现为跨一系列部署限制选择和优化 VLA 提供了可行的见解。我们的工作挑战了当前关于数据中心硬件在机器人推理方面的优越性的假设。"
        },
        {
          "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
          "url": "http://arxiv.org/abs/2509.09674v1",
          "snippet": "Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $π_0$ on RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-09-11T17:59:17Z",
          "authors": [
            "Haozhan Li",
            "Yuxin Zuo",
            "Jiale Yu",
            "Yuhao Zhang",
            "Zhaohui Yang",
            "Kaiyan Zhang",
            "Xuekai Zhu",
            "Yuchen Zhang",
            "Tianxing Chen",
            "Ganqu Cui",
            "Dehui Wang",
            "Dingxiang Luo",
            "Yuchen Fan",
            "Youbang Sun",
            "Jia Zeng",
            "Jiangmiao Pang",
            "Shanghang Zhang",
            "Yu Wang",
            "Yao Mu",
            "Bowen Zhou",
            "Ning Ding"
          ],
          "arxiv_id": "2509.09674",
          "abstract": "Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $π_0$ on RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
          "abstract_zh": "视觉-语言-动作（VLA）模型最近已成为机器人操作的强大范例。尽管大规模预训练和监督微调 (SFT) 取得了实质性进展，但这些模型面临两个基本挑战：(i) SFT 扩展所需的大规模人类操作机器人轨迹的稀缺性和高成本，以及 (ii) 对涉及分布转移的任务的泛化有限。最近大型推理模型 (LRM) 的突破表明，强化学习 (RL) 可以显着增强分步推理能力，这就提出了一个自然的问题：RL 是否可以类似地改进 VLA 的长期分步行动规划？在这项工作中，我们介绍了 SimpleVLA-RL，这是一种专为 VLA 模型量身定制的高效 RL 框架。在 veRL 的基础上，我们引入了 VLA 特定的轨迹采样、可扩展的并行化、多环境渲染和优化的损失计算。当应用于 OpenVLA-OFT 时，SimpleVLA-RL 在 LIBERO 上实现了 SoTA 性能，甚至通过我们引入的探索增强策略在 RoboTwin 1.0\\&2.0 上优于 $π_0$。SimpleVLA-RL 不仅减少了对大规模数据的依赖并实现了稳健的泛化，而且在实际任务中也显着超越了 SFT。此外，我们在强化学习训练期间发现了一种新的现象“pushcut”，其中策略发现了之前训练过程中未见过的模式。Github：https://github.com/PRIME-RL/SimpleVLA-RL"
        },
        {
          "title": "RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation",
          "url": "http://arxiv.org/abs/2509.08820v1",
          "snippet": "Robotic chemists promise to both liberate human experts from repetitive tasks and accelerate scientific discovery, yet remain in their infancy. Chemical experiments involve long-horizon procedures over hazardous and deformable substances, where success requires not only task completion but also strict compliance with experimental norms. To address these challenges, we propose \\textit{RoboChemist}, a dual-loop framework that integrates Vision-Language Models (VLMs) with Vision-Language-Action (VLA) models. Unlike prior VLM-based systems (e.g., VoxPoser, ReKep) that rely on depth perception and struggle with transparent labware, and existing VLA systems (e.g., RDT, pi0) that lack semantic-level feedback for complex tasks, our method leverages a VLM to serve as (1) a planner to decompose tasks into primitive actions, (2) a visual prompt generator to guide VLA models, and (3) a monitor to assess task success and regulatory compliance. Notably, we introduce a VLA interface that accepts image-based visual targets from the VLM, enabling precise, goal-conditioned control. Our system successfully executes both primitive actions and complete multi-step chemistry protocols. Results show 23.57% higher average success rate and a 0.298 average increase in compliance rate over state-of-the-art VLA baselines, while also demonstrating strong generalization to objects and tasks.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-09-10T17:52:09Z",
          "authors": [
            "Zongzheng Zhang",
            "Chenghao Yue",
            "Haobo Xu",
            "Minwen Liao",
            "Xianglin Qi",
            "Huan-ang Gao",
            "Ziwei Wang",
            "Hao Zhao"
          ],
          "arxiv_id": "2509.08820",
          "abstract": "Robotic chemists promise to both liberate human experts from repetitive tasks and accelerate scientific discovery, yet remain in their infancy. Chemical experiments involve long-horizon procedures over hazardous and deformable substances, where success requires not only task completion but also strict compliance with experimental norms. To address these challenges, we propose \\textit{RoboChemist}, a dual-loop framework that integrates Vision-Language Models (VLMs) with Vision-Language-Action (VLA) models. Unlike prior VLM-based systems (e.g., VoxPoser, ReKep) that rely on depth perception and struggle with transparent labware, and existing VLA systems (e.g., RDT, pi0) that lack semantic-level feedback for complex tasks, our method leverages a VLM to serve as (1) a planner to decompose tasks into primitive actions, (2) a visual prompt generator to guide VLA models, and (3) a monitor to assess task success and regulatory compliance. Notably, we introduce a VLA interface that accepts image-based visual targets from the VLM, enabling precise, goal-conditioned control. Our system successfully executes both primitive actions and complete multi-step chemistry protocols. Results show 23.57% higher average success rate and a 0.298 average increase in compliance rate over state-of-the-art VLA baselines, while also demonstrating strong generalization to objects and tasks.",
          "abstract_zh": "机器人化学家承诺将人类专家从重复性任务中解放出来，并加速科学发现，但仍处于起步阶段。化学实验涉及危险和可变形物质的长期程序，成功不仅需要完成任务，还需要严格遵守实验规范。为了应对这些挑战，我们提出了 \\textit{RoboChemist}，这是一个将视觉语言模型（VLM）与视觉语言动作（VLA）模型集成的双循环框架。与之前基于 VLM 的系统（例如，VoxPoser、ReKep）依赖于深度感知并与透明实验室设备作斗争，以及现有的 VLA 系统（例如，RDT、pi0）缺乏复杂任务的语义级反馈，我们的方法利用 VLM 充当（1）将任务分解为原始动作的规划器，（2）指导 VLA 模型的视觉提示生成器，以及（3）评估任务成功和法规遵从性的监视器。值得注意的是，我们引入了一个 VLA 接口，它接受来自 VLM 的基于图像的视觉目标，从而实现精确的目标条件控制。我们的系统成功执行了原始操作和完整的多步骤化学协议。结果显示，与最先进的 VLA 基线相比，平均成功率提高了 23.57%，合规率平均提高了 0.298，同时还展示了对对象和任务的强大泛化能力。"
        },
        {
          "title": "TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning",
          "url": "http://arxiv.org/abs/2509.11839v2",
          "snippet": "Recent Vision-Language-Action models show potential to generalize across embodiments but struggle to quickly align with a new robot's action space when high-quality demonstrations are scarce, especially for bipedal humanoids. We present TrajBooster, a cross-embodiment framework that leverages abundant wheeled-humanoid data to boost bipedal VLA. Our key idea is to use end-effector trajectories as a morphology-agnostic interface. TrajBooster (i) extracts 6D dual-arm end-effector trajectories from real-world wheeled humanoids, (ii) retargets them in simulation to Unitree G1 with a whole-body controller trained via a heuristic-enhanced harmonized online DAgger to lift low-dimensional trajectory references into feasible high-dimensional whole-body actions, and (iii) forms heterogeneous triplets that couple source vision/language with target humanoid-compatible actions to post-pre-train a VLA, followed by only 10 minutes of teleoperation data collection on the target humanoid domain. Deployed on Unitree G1, our policy achieves beyond-tabletop household tasks, enabling squatting, cross-height manipulation, and coordinated whole-body motion with markedly improved robustness and generalization. Results show that TrajBooster allows existing wheeled-humanoid data to efficiently strengthen bipedal humanoid VLA performance, reducing reliance on costly same-embodiment data while enhancing action space understanding and zero-shot skill transfer capabilities. For more details, For more details, please refer to our \\href{https://jiachengliu3.github.io/TrajBooster/}.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-09-15T12:25:39Z",
          "authors": [
            "Jiacheng Liu",
            "Pengxiang Ding",
            "Qihang Zhou",
            "Yuxuan Wu",
            "Da Huang",
            "Zimian Peng",
            "Wei Xiao",
            "Weinan Zhang",
            "Lixin Yang",
            "Cewu Lu",
            "Donglin Wang"
          ],
          "arxiv_id": "2509.11839",
          "abstract": "Recent Vision-Language-Action models show potential to generalize across embodiments but struggle to quickly align with a new robot's action space when high-quality demonstrations are scarce, especially for bipedal humanoids. We present TrajBooster, a cross-embodiment framework that leverages abundant wheeled-humanoid data to boost bipedal VLA. Our key idea is to use end-effector trajectories as a morphology-agnostic interface. TrajBooster (i) extracts 6D dual-arm end-effector trajectories from real-world wheeled humanoids, (ii) retargets them in simulation to Unitree G1 with a whole-body controller trained via a heuristic-enhanced harmonized online DAgger to lift low-dimensional trajectory references into feasible high-dimensional whole-body actions, and (iii) forms heterogeneous triplets that couple source vision/language with target humanoid-compatible actions to post-pre-train a VLA, followed by only 10 minutes of teleoperation data collection on the target humanoid domain. Deployed on Unitree G1, our policy achieves beyond-tabletop household tasks, enabling squatting, cross-height manipulation, and coordinated whole-body motion with markedly improved robustness and generalization. Results show that TrajBooster allows existing wheeled-humanoid data to efficiently strengthen bipedal humanoid VLA performance, reducing reliance on costly same-embodiment data while enhancing action space understanding and zero-shot skill transfer capabilities. For more details, For more details, please refer to our \\href{https://jiachengliu3.github.io/TrajBooster/}.",
          "abstract_zh": "最近的视觉-语言-动作模型显示出跨实施例泛化的潜力，但在高质量演示稀缺的情况下，很难快速与新机器人的动作空间保持一致，尤其是对于双足类人机器人。我们提出了 TrajBooster，一个跨实体框架，利用丰富的轮式人形数据来增强双足 VLA。我们的关键想法是使用末端执行器轨迹作为形态不可知的接口。TrajBooster (i) 从现实世界的轮式人形机器人中提取 6D 双臂末端执行器轨迹，(ii) 在模拟中将它们重新定位到 Unitree G1，并使用通过启发式增强的协调在线 DAgger 进行训练的全身控制器，将低维轨迹参考提升为可行的高维全身动作，以及 (iii) 形成异构三元组，将源视觉/语言与目标人形兼容动作结合起来，以进行预训练VLA，随后仅用 10 分钟就对目标人形领域进行了远程操作数据收集。部署在 Unitree G1 上，我们的策略实现了超越桌面的家庭任务，实现了蹲下、跨高度操纵和协调全身运动，并且鲁棒性和泛化性显着提高。结果表明，TrajBooster 允许现有的轮式人形数据有效增强双足人形 VLA 性能，减少对昂贵的相同实施例数据的依赖，同时增强动作空间理解和零镜头技能转移能力。更多详情，请参考我们的\\href{https://jia Chengliu3.github.io/TrajBooster/}。"
        },
        {
          "title": "CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation",
          "url": "http://arxiv.org/abs/2509.06819v1",
          "snippet": "Learning-based controllers, such as diffusion policies and vision-language action models, often generate low-frequency or discontinuous robot state changes. Achieving smooth reference tracking requires a low-level controller that converts high-level targets commands into joint torques, enabling compliant behavior during contact interactions. We present CRISP, a lightweight C++ implementation of compliant Cartesian and joint-space controllers for the ROS2 control standard, designed for seamless integration with high-level learning-based policies as well as teleoperation. The controllers are compatible with any manipulator that exposes a joint-torque interface. Through our Python and Gymnasium interfaces, CRISP provides a unified pipeline for recording data from hardware and simulation and deploying high-level learning-based policies seamlessly, facilitating rapid experimentation. The system has been validated on hardware with the Franka Robotics FR3 and in simulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid integration, flexible deployment, and real-time performance, our implementation provides a unified pipeline for data collection and policy execution, lowering the barrier to applying learning-based methods on ROS2-compatible manipulators. Detailed documentation is available at the project website - https://utiasDSL.github.io/crisp_controllers.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-09-08T15:55:50Z",
          "authors": [
            "Daniel San José Pro",
            "Oliver Hausdörfer",
            "Ralf Römer",
            "Maximilian Dösch",
            "Martin Schuck",
            "Angela P. Schöllig"
          ],
          "arxiv_id": "2509.06819",
          "abstract": "Learning-based controllers, such as diffusion policies and vision-language action models, often generate low-frequency or discontinuous robot state changes. Achieving smooth reference tracking requires a low-level controller that converts high-level targets commands into joint torques, enabling compliant behavior during contact interactions. We present CRISP, a lightweight C++ implementation of compliant Cartesian and joint-space controllers for the ROS2 control standard, designed for seamless integration with high-level learning-based policies as well as teleoperation. The controllers are compatible with any manipulator that exposes a joint-torque interface. Through our Python and Gymnasium interfaces, CRISP provides a unified pipeline for recording data from hardware and simulation and deploying high-level learning-based policies seamlessly, facilitating rapid experimentation. The system has been validated on hardware with the Franka Robotics FR3 and in simulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid integration, flexible deployment, and real-time performance, our implementation provides a unified pipeline for data collection and policy execution, lowering the barrier to applying learning-based methods on ROS2-compatible manipulators. Detailed documentation is available at the project website - https://utiasDSL.github.io/crisp_controllers.",
          "abstract_zh": "基于学习的控制器，例如扩散策略和视觉语言动作模型，通常会产生低频或不连续的机器人状态变化。实现平滑的参考跟踪需要一个低级控制器，将高级目标命令转换为关节扭矩，从而在接触交互过程中实现顺从行为。我们推出了 CRISP，它是符合 ROS2 控制标准的笛卡尔和联合空间控制器的轻量级 C++ 实现，旨在与高级基于学习的策略以及远程操作无缝集成。该控制器与任何具有关节扭矩接口的机械手兼容。通过我们的 Python 和 Gymnasium 接口，CRISP 提供了一个统一的管道，用于记录来自硬件和模拟的数据，并无缝部署基于高级学习的策略，从而促进快速实验。该系统已通过 Franka Robotics FR3 的硬件验证以及 Kuka IIWA14 和 Kinova Gen3 的仿真验证。我们的实现专为快速集成、灵活部署和实时性能而设计，为数据收集和策略执行提供了统一的管道，降低了在 ROS2 兼容的操纵器上应用基于学习的方法的障碍。详细文档可在项目网站上找到 - https://utiasDSL.github.io/crisp_controllers。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 10 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 10）。",
      "items": [
        {
          "title": "patrick-llgc/Learning-Deep-Learning",
          "url": "https://github.com/patrick-llgc/Learning-Deep-Learning",
          "snippet": "Paper reading notes on Deep Learning and Machine Learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "Vector-Wangel/XLeRobot",
          "url": "https://github.com/Vector-Wangel/XLeRobot",
          "snippet": "XLeRobot: Practical Dual-Arm Mobile Home Robot for $660",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "HCPLab-SYSU/Embodied_AI_Paper_List",
          "url": "https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List",
          "snippet": "[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 9
        },
        {
          "title": "yang-zj1026/NaVILA-Bench",
          "url": "https://github.com/yang-zj1026/NaVILA-Bench",
          "snippet": "Vision-Language Navigation Benchmark in Isaac Lab",
          "site": "github.com",
          "rank": 10
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 上共发现 1 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 1）。",
      "items": [
        {
          "title": "InternRobotics/VLAC",
          "url": "https://huggingface.co/InternRobotics/VLAC",
          "snippet": "InternRobotics/VLAC",
          "site": "huggingface.co",
          "rank": 2,
          "published": "2025-09-15T13:13:33.000Z"
        }
      ]
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-09-08",
  "week_end": "2025-09-14",
  "last_updated": "2026-01-07"
}