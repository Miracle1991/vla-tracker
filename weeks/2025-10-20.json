{
  "generated_at": "2026-01-07T13:39:51.256861",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 22 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 22）。",
      "items": [
        {
          "title": "Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos",
          "url": "http://arxiv.org/abs/2510.21571v1",
          "snippet": "This paper presents a novel approach for pretraining robotic manipulation Vision-Language-Action (VLA) models using a large corpus of unscripted real-life video recordings of human hand activities. Treating human hand as dexterous robot end-effector, we show that \"in-the-wild\" egocentric human videos without any annotations can be transformed into data formats fully aligned with existing robotic V-L-A training data in terms of task granularity and labels. This is achieved by the development of a fully-automated holistic human activity analysis approach for arbitrary human hand videos. This approach can generate atomic-level hand activity segments and their language descriptions, each accompanied with framewise 3D hand motion and camera motion. We process a large volume of egocentric videos and create a hand-VLA training dataset containing 1M episodes and 26M frames. This training data covers a wide range of objects and concepts, dexterous manipulation tasks, and environment variations in real life, vastly exceeding the coverage of existing robot data. We design a dexterous hand VLA model architecture and pretrain the model on this dataset. The model exhibits strong zero-shot capabilities on completely unseen real-world observations. Additionally, fine-tuning it on a small amount of real robot action data significantly improves task success rates and generalization to novel objects in real robotic experiments. We also demonstrate the appealing scaling behavior of the model's task performance with respect to pretraining data scale. We believe this work lays a solid foundation for scalable VLA pretraining, advancing robots toward truly generalizable embodied intelligence.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-10-24T15:39:31Z",
          "authors": [
            "Qixiu Li",
            "Yu Deng",
            "Yaobo Liang",
            "Lin Luo",
            "Lei Zhou",
            "Chengtang Yao",
            "Lingqi Zeng",
            "Zhiyuan Feng",
            "Huizhi Liang",
            "Sicheng Xu",
            "Yizhong Zhang",
            "Xi Chen",
            "Hao Chen",
            "Lily Sun",
            "Dong Chen",
            "Jiaolong Yang",
            "Baining Guo"
          ],
          "arxiv_id": "2510.21571",
          "abstract": "This paper presents a novel approach for pretraining robotic manipulation Vision-Language-Action (VLA) models using a large corpus of unscripted real-life video recordings of human hand activities. Treating human hand as dexterous robot end-effector, we show that \"in-the-wild\" egocentric human videos without any annotations can be transformed into data formats fully aligned with existing robotic V-L-A training data in terms of task granularity and labels. This is achieved by the development of a fully-automated holistic human activity analysis approach for arbitrary human hand videos. This approach can generate atomic-level hand activity segments and their language descriptions, each accompanied with framewise 3D hand motion and camera motion. We process a large volume of egocentric videos and create a hand-VLA training dataset containing 1M episodes and 26M frames. This training data covers a wide range of objects and concepts, dexterous manipulation tasks, and environment variations in real life, vastly exceeding the coverage of existing robot data. We design a dexterous hand VLA model architecture and pretrain the model on this dataset. The model exhibits strong zero-shot capabilities on completely unseen real-world observations. Additionally, fine-tuning it on a small amount of real robot action data significantly improves task success rates and generalization to novel objects in real robotic experiments. We also demonstrate the appealing scaling behavior of the model's task performance with respect to pretraining data scale. We believe this work lays a solid foundation for scalable VLA pretraining, advancing robots toward truly generalizable embodied intelligence.",
          "abstract_zh": "本文提出了一种使用人类手部活动的无脚本现实视频记录的大型语料库来预训练机器人操作视觉-语言-动作（VLA）模型的新方法。将人手视为灵巧的机器人末端执行器，我们证明，没有任何注释的“野外”以自我为中心的人类视频可以转换为在任务粒度和标签方面与现有机器人 V-L-A 训练数据完全一致的数据格式。这是通过开发针对任意人手视频的全自动整体人类活动分析方法来实现的。这种方法可以生成原子级手部活动片段及其语言描述，每个片段都伴随着逐帧 3D 手部运动和相机运动。我们处理大量以自我为中心的视频，并创建包含 1M 集和 26M 帧的手动 VLA 训练数据集。这些训练数据涵盖了广泛的物体和概念、灵巧的操作任务以及现实生活中的环境变化，远远超出了现有机器人数据的覆盖范围。我们设计了一个灵巧的手 VLA 模型架构，并在此数据集上预训练模型。该模型在完全看不见的现实世界观测中表现出强大的零样本能力。此外，根据少量真实机器人动作数据对其进行微调，可以显着提高任务成功率以及对真实机器人实验中新物体的泛化能力。我们还展示了模型任务性能相对于预训练数据规模的有吸引力的扩展行为。我们相信这项工作为可扩展的 VLA 预训练奠定了坚实的基础，推动机器人走向真正通用的体现智能。"
        },
        {
          "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2510.17640v2",
          "snippet": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance on complex robotic manipulation tasks through imitation learning. However, existing imitation learning datasets contain only successful trajectories and lack failure or recovery data, especially for out-of-distribution (OOD) states where the robot deviates from the main policy due to minor perturbations or errors, leading VLA models to struggle with states deviating from the training distribution. To this end, we propose an automated OOD data augmentation framework named RESample through exploratory sampling. Specifically, we first leverage offline reinforcement learning to obtain an action-value network that accurately identifies sub-optimal actions under the current manipulation policy. We further sample potential OOD states from trajectories via rollout, and design an exploratory sampling mechanism that adaptively incorporates these action proxies into the training dataset to ensure efficiency. Subsequently, our framework explicitly encourages the VLAs to recover from OOD states and enhances their robustness against distributional shifts. We conduct extensive experiments on the LIBERO benchmark as well as real-world robotic manipulation tasks, demonstrating that RESample consistently improves the stability and generalization ability of VLA models.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-10-20T15:21:12Z",
          "authors": [
            "Yuquan Xue",
            "Guanxing Lu",
            "Zhenyu Wu",
            "Chuanrui Zhang",
            "Bofang Jia",
            "Zhengyi Gu",
            "Yansong Tang",
            "Ziwei Wang"
          ],
          "arxiv_id": "2510.17640",
          "abstract": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance on complex robotic manipulation tasks through imitation learning. However, existing imitation learning datasets contain only successful trajectories and lack failure or recovery data, especially for out-of-distribution (OOD) states where the robot deviates from the main policy due to minor perturbations or errors, leading VLA models to struggle with states deviating from the training distribution. To this end, we propose an automated OOD data augmentation framework named RESample through exploratory sampling. Specifically, we first leverage offline reinforcement learning to obtain an action-value network that accurately identifies sub-optimal actions under the current manipulation policy. We further sample potential OOD states from trajectories via rollout, and design an exploratory sampling mechanism that adaptively incorporates these action proxies into the training dataset to ensure efficiency. Subsequently, our framework explicitly encourages the VLAs to recover from OOD states and enhances their robustness against distributional shifts. We conduct extensive experiments on the LIBERO benchmark as well as real-world robotic manipulation tasks, demonstrating that RESample consistently improves the stability and generalization ability of VLA models.",
          "abstract_zh": "视觉-语言-动作模型（VLA）通过模仿学习在复杂的机器人操作任务中表现出了卓越的性能。然而，现有的模仿学习数据集仅包含成功的轨迹，缺乏失败或恢复数据，特别是对于机器人由于微小扰动或错误而偏离主要策略的分布外（OOD）状态，导致VLA模型与偏离训练分布的状态作斗争。为此，我们通过探索性采样提出了一个名为 RESample 的自动化 OOD 数据增强框架。具体来说，我们首先利用离线强化学习来获得一个动作价值网络，该网络可以准确识别当前操纵策略下的次优动作。我们通过推出进一步从轨迹中采样潜在的 OOD 状态，并设计一种探索性采样机制，自适应地将这些动作代理合并到训练数据集中以确保效率。随后，我们的框架明确鼓励 VLA 从 OOD 状态中恢复，并增强其针对分配变化的鲁棒性。我们对 LIBERO 基准以及现实世界的机器人操作任务进行了广泛的实验，证明 RESample 持续提高了 VLA 模型的稳定性和泛化能力。"
        },
        {
          "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots",
          "url": "http://arxiv.org/abs/2510.17369v1",
          "snippet": "Robotic systems are increasingly expected to operate in human-centered, unstructured environments where safety, adaptability, and generalization are essential. Vision-Language-Action (VLA) models have been proposed as a language guided generalized control framework for real robots. However, their deployment has been limited to conventional serial link manipulators. Coupled by their rigidity and unpredictability of learning based control, the ability to safely interact with the environment is missing yet critical. In this work, we present the deployment of a VLA model on a soft continuum manipulator to demonstrate autonomous safe human-robot interaction. We present a structured finetuning and deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and $π_0$) across representative manipulation tasks, and show while out-of-the-box policies fail due to embodiment mismatch, through targeted finetuning the soft robot performs equally to the rigid counterpart. Our findings highlight the necessity of finetuning for bridging embodiment gaps, and demonstrate that coupling VLA models with soft robots enables safe and flexible embodied AI in human-shared environments.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-10-20T10:06:39Z",
          "authors": [
            "Haochen Su",
            "Cristian Meo",
            "Francesco Stella",
            "Andrea Peirone",
            "Kai Junge",
            "Josie Hughes"
          ],
          "arxiv_id": "2510.17369",
          "abstract": "Robotic systems are increasingly expected to operate in human-centered, unstructured environments where safety, adaptability, and generalization are essential. Vision-Language-Action (VLA) models have been proposed as a language guided generalized control framework for real robots. However, their deployment has been limited to conventional serial link manipulators. Coupled by their rigidity and unpredictability of learning based control, the ability to safely interact with the environment is missing yet critical. In this work, we present the deployment of a VLA model on a soft continuum manipulator to demonstrate autonomous safe human-robot interaction. We present a structured finetuning and deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and $π_0$) across representative manipulation tasks, and show while out-of-the-box policies fail due to embodiment mismatch, through targeted finetuning the soft robot performs equally to the rigid counterpart. Our findings highlight the necessity of finetuning for bridging embodiment gaps, and demonstrate that coupling VLA models with soft robots enables safe and flexible embodied AI in human-shared environments.",
          "abstract_zh": "人们越来越期望机器人系统能够在以人为中心的非结构化环境中运行，在这些环境中，安全性、适应性和通用性至关重要。视觉-语言-动作（VLA）模型已被提议作为真实机器人的语言引导广义控制框架。然而，它们的部署仅限于传统的串行链路操纵器。再加上基于学习的控制的刚性和不可预测性，与环境安全交互的能力缺失但至关重要。在这项工作中，我们提出了在软连续体机械臂上部署 VLA 模型，以演示自主安全的人机交互。我们提出了一个结构化的微调和部署管道，评估两个最先进的 VLA 模型（OpenVLA-OFT 和 $π_0$）在代表性操作任务中的表现，并表明，虽然开箱即用的策略由于实施例不匹配而失败，但通过有针对性的微调，软机器人的性能与刚性机器人相同。我们的研究结果强调了进行微调以弥补体现差距的必要性，并证明将 VLA 模型与软机器人相结合可以在人类共享环境中实现安全、灵活的体现人工智能。"
        },
        {
          "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
          "url": "http://arxiv.org/abs/2510.23763v3",
          "snippet": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-10-27T18:49:03Z",
          "authors": [
            "Siyin Wang",
            "Jinlan Fu",
            "Feihong Liu",
            "Xinzhe He",
            "Huangxuan Wu",
            "Junhao Shi",
            "Kexin Huang",
            "Zhaoye Fei",
            "Jingjing Gong",
            "Zuxuan Wu",
            "Yu-Gang Jiang",
            "See-Kiong Ng",
            "Tat-Seng Chua",
            "Xipeng Qiu"
          ],
          "arxiv_id": "2510.23763",
          "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.",
          "abstract_zh": "多模态大语言模型 (MLLM) 的最新进展推动了机器人操作的视觉-语言-动作 (VLA) 模型的快速进步。尽管在许多场景中有效，但当前的方法很大程度上依赖于显式指令，而在现实世界的交互中，人类很少直接发出指令。有效的协作需要机器人主动推断用户意图。在这项工作中，我们引入了跨模式上下文指令，这是一种新的设置，其中意图源自口头对话、环境声音和视觉提示，而不是明确的命令。为了应对这一新环境，我们推出了 RoboOmni，这是一个基于端到端全模态法学硕士的感知器-思考者-说话者-执行器框架，它统一了意图识别、交互确认和动作执行。RoboOmni 融合听觉和视觉信号，实现强大的意图识别，同时支持直接语音交互。为了解决机器人操作中主动意图识别训练数据的缺乏问题，我们构建了 OmniAction，其中包括 140k 个片段、5k+ 个扬声器、2.4k 个事件声音、640 个背景和六种上下文指令类型。模拟和现实环境中的实验表明，RoboOmni 在成功率、推理速度、意图识别和主动协助方面超越了基于文本和 ASR 的基线。"
        },
        {
          "title": "MemER: Scaling Up Memory for Robot Control via Experience Retrieval",
          "url": "http://arxiv.org/abs/2510.20328v1",
          "snippet": "Humans routinely rely on memory to perform tasks, yet most robot policies lack this capability; our goal is to endow robot policies with the same ability. Naively conditioning on long observation histories is computationally expensive and brittle under covariate shift, while indiscriminate subsampling of history leads to irrelevant or redundant information. We propose a hierarchical policy framework, where the high-level policy is trained to select and track previous relevant keyframes from its experience. The high-level policy uses selected keyframes and the most recent frames when generating text instructions for a low-level policy to execute. This design is compatible with existing vision-language-action (VLA) models and enables the system to efficiently reason over long-horizon dependencies. In our experiments, we finetune Qwen2.5-VL-7B-Instruct and $π_{0.5}$ as the high-level and low-level policies respectively, using demonstrations supplemented with minimal language annotations. Our approach, MemER, outperforms prior methods on three real-world long-horizon robotic manipulation tasks that require minutes of memory. Videos and code can be found at https://jen-pan.github.io/memer/.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-10-23T08:26:17Z",
          "authors": [
            "Ajay Sridhar",
            "Jennifer Pan",
            "Satvik Sharma",
            "Chelsea Finn"
          ],
          "arxiv_id": "2510.20328",
          "abstract": "Humans routinely rely on memory to perform tasks, yet most robot policies lack this capability; our goal is to endow robot policies with the same ability. Naively conditioning on long observation histories is computationally expensive and brittle under covariate shift, while indiscriminate subsampling of history leads to irrelevant or redundant information. We propose a hierarchical policy framework, where the high-level policy is trained to select and track previous relevant keyframes from its experience. The high-level policy uses selected keyframes and the most recent frames when generating text instructions for a low-level policy to execute. This design is compatible with existing vision-language-action (VLA) models and enables the system to efficiently reason over long-horizon dependencies. In our experiments, we finetune Qwen2.5-VL-7B-Instruct and $π_{0.5}$ as the high-level and low-level policies respectively, using demonstrations supplemented with minimal language annotations. Our approach, MemER, outperforms prior methods on three real-world long-horizon robotic manipulation tasks that require minutes of memory. Videos and code can be found at https://jen-pan.github.io/memer/.",
          "abstract_zh": "人类通常依靠记忆来执行任务，但大多数机器人策略缺乏这种能力；我们的目标是赋予机器人策略同样的能力。在协变量平移下，对长期观测历史的天真调节在计算上是昂贵且脆弱的，而对历史的不加区别的子采样会导致不相关或冗余的信息。我们提出了一个分层策略框架，其中高级策略经过训练，可以根据其经验选择和跟踪先前的相关关键帧。高级策略在生成要执行的低级策略的文本指令时使用选定的关键帧和最新帧。该设计与现有的视觉-语言-动作（VLA）模型兼容，并使系统能够有效地推理长范围依赖性。在我们的实验中，我们使用辅以最少语言注释的演示，分别将 Qwen2.5-VL-7B-Instruct 和 $π_{0.5}$ 微调为高级和低级策略。我们的方法 MemER 在三个需要几分钟内存的现实世界长视野机器人操作任务中优于先前的方法。视频和代码可以在 https://jen-pan.github.io/memer/ 找到。"
        },
        {
          "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation",
          "url": "http://arxiv.org/abs/2510.23571v1",
          "snippet": "The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining \"success\" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-10-27T17:41:38Z",
          "authors": [
            "Yash Jangir",
            "Yidi Zhang",
            "Kashu Yamazaki",
            "Chenyu Zhang",
            "Kuan-Hsun Tu",
            "Tsung-Wei Ke",
            "Lei Ke",
            "Yonatan Bisk",
            "Katerina Fragkiadaki"
          ],
          "arxiv_id": "2510.23571",
          "abstract": "The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining \"success\" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.",
          "abstract_zh": "追求机器人通才——能够在不同环境中执行不同任务的可指导代理——需要严格且可扩展的评估。然而，机器人政策的现实测试仍然受到根本限制：它是劳动密集型的、缓慢的、大规模不安全的，并且难以复制。现有的模拟基准同样受到限制，因为它们在同一合成领域内训练和测试策略，并且无法评估从现实世界演示或替代模拟环境中训练的模型。随着政策范围和复杂性的扩大，这些障碍只会加剧，因为机器人技术“成功”的定义往往取决于人类对执行质量的细致判断。在本文中，我们介绍了一种新的基准测试框架，该框架通过将 VLA 评估转移到通过在线人类反馈增强的大规模模拟环境中来克服这些挑战。利用视觉语言模型、2D 到 3D 生成建模和可微分渲染方面的进步，我们的方法自动将广泛使用的机器人数据集的视频演示转换为模拟的对应数据。在这些数字孪生中，我们使用自动 VLM 引导评分和从众包工作者收集的可扩展人类偏好判断来评估 VLA 策略，将人类参与从繁琐的场景设置、重置和安全监督转变为轻量级偏好比较。为了衡量鲁棒性，我们沿着多个轴系统地扰动模拟环境，例如纹理和对象放置、受控变化下的压力测试策略泛化。其结果是为现实世界中训练有素的机器人操作策略提供了一个不断发展、可重复和可扩展的基准，解决了当今机器人领域中关键的缺失能力。"
        },
        {
          "title": "A Survey on Efficient Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.24795v1",
          "snippet": "Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-10-27T17:57:33Z",
          "authors": [
            "Zhaoshu Yu",
            "Bo Wang",
            "Pengpeng Zeng",
            "Haonan Zhang",
            "Ji Zhang",
            "Lianli Gao",
            "Jingkuan Song",
            "Nicu Sebe",
            "Heng Tao Shen"
          ],
          "arxiv_id": "2510.24795",
          "abstract": "Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/",
          "abstract_zh": "视觉-语言-动作模型（VLA）代表了体现智能的重要前沿，旨在架起数字知识与物理世界交互的桥梁。虽然这些模型表现出了卓越的通才能力，但其部署却受到其底层大规模基础模型固有的大量计算和数据要求的严重阻碍。出于应对这些挑战的迫切需要，本次调查首次对整个数据模型训练过程中的高效视觉-语言-行动模型（高效 VLA）进行了全面审查。具体来说，我们引入了一个统一的分类法来系统地组织该领域的不同工作，将当前技术分为三个核心支柱：（1）高效模型设计，重点关注高效架构和模型压缩；（2）高效训练，减少模型学习过程中的计算负担；(3)高效数据采集，解决机器人数据获取和利用的瓶颈。通过在此框架内对最先进的方法进行批判性审查，本次调查不仅为社区建立了基础参考，还总结了代表性应用，描绘了关键挑战，并为未来的研究制定了路线图。我们维护一个不断更新的项目页面来跟踪我们的最新进展：https://evla-survey.github.io/"
        },
        {
          "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
          "url": "http://arxiv.org/abs/2510.19430v3",
          "snippet": "Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-10-22T09:57:13Z",
          "authors": [
            "GigaBrain Team",
            "Angen Ye",
            "Boyuan Wang",
            "Chaojun Ni",
            "Guan Huang",
            "Guosheng Zhao",
            "Haoyun Li",
            "Jie Li",
            "Jiagang Zhu",
            "Lv Feng",
            "Peng Li",
            "Qiuping Deng",
            "Runqi Ouyang",
            "Wenkang Qin",
            "Xinze Chen",
            "Xiaofeng Wang",
            "Yang Wang",
            "Yifan Li",
            "Yilong Li",
            "Yiran Ding",
            "Yuan Xu",
            "Yun Ye",
            "Yukun Zhou",
            "Zhehao Dong",
            "Zhenan Wang",
            "Zhichao Liu",
            "Zheng Zhu"
          ],
          "arxiv_id": "2510.19430",
          "abstract": "Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin.",
          "abstract_zh": "训练通用机器人的视觉-语言-动作 (VLA) 模型通常需要大规模的现实世界机器人数据，而收集这些数据既昂贵又耗时。物理数据收集的低效率严重限制了当前 VLA 系统的可扩展性和泛化能力。为了应对这一挑战，我们引入了 GigaBrain-0，这是一种新颖的 VLA 基础模型，由世界模型生成的数据（例如视频生成、real2real 传输、人类传输、视图传输、sim2real 传输数据）提供支持。通过利用世界模型大规模生成不同的数据，GigaBrain-0 显着减少了对真实机器人数据的依赖，同时提高了跨任务泛化能力。我们的方法通过 RGBD 输入建模和体现的思想链 (CoT) 监督进一步提高了策略的稳健性，使模型能够在任务执行期间推理空间几何、对象状态和长范围依赖关系。这使得在灵巧、长视野和移动操作任务的实际性能方面取得了显着的进步。大量实验表明，GigaBrain-0 在外观（例如纹理、颜色）、对象放置和相机视点的变化方面实现了卓越的泛化。此外，我们还推出了 GigaBrain-0-Small，这是一种优化的轻量级变体，旨在在 NVIDIA Jetson AGX Orin 等设备上高效运行。"
        },
        {
          "title": "Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes",
          "url": "http://arxiv.org/abs/2510.19400v1",
          "snippet": "Vision-language models (VLMs) are essential to Embodied AI, enabling robots to perceive, reason, and act in complex environments. They also serve as the foundation for the recent Vision-Language-Action (VLA) models. Yet most evaluations of VLMs focus on single-view settings, leaving their ability to integrate multi-view information underexplored. At the same time, multi-camera setups are increasingly standard in robotic platforms, as they provide complementary perspectives to mitigate occlusion and depth ambiguity. Whether VLMs can effectively leverage such multi-view inputs for robotic reasoning therefore remains an open question. To bridge this gap, we introduce MV-RoboBench, a benchmark specifically designed to evaluate the multi-view spatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench consists of 1.7k manually curated QA items across eight subtasks, divided into two primary categories: spatial understanding and robotic execution. We evaluate a diverse set of existing VLMs, including both open-source and closed-source models, along with enhanced versions incorporating CoT-inspired techniques. The results show that state-of-the-art models remain far below human performance, underscoring the substantial challenges VLMs face in multi-view robotic perception. Additionally, our analysis uncovers two key findings: (i) spatial intelligence and robotic task execution are positively correlated in multi-view robotic scenarios; and (ii) strong performance on existing general-purpose single-view spatial understanding benchmarks does not reliably translate to success in the robotic spatial tasks assessed by our benchmark. We release MV-RoboBench as an open resource to foster progress in spatially grounded VLMs and VLAs, providing not only data but also a standardized evaluation protocol for multi-view embodied reasoning.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-10-22T09:20:09Z",
          "authors": [
            "Zhiyuan Feng",
            "Zhaolu Kang",
            "Qijie Wang",
            "Zhiying Du",
            "Jiongrui Yan",
            "Shubin Shi",
            "Chengbo Yuan",
            "Huizhi Liang",
            "Yu Deng",
            "Qixiu Li",
            "Rushuai Yang",
            "Arctanx An",
            "Leqi Zheng",
            "Weijie Wang",
            "Shawn Chen",
            "Sicheng Xu",
            "Yaobo Liang",
            "Jiaolong Yang",
            "Baining Guo"
          ],
          "arxiv_id": "2510.19400",
          "abstract": "Vision-language models (VLMs) are essential to Embodied AI, enabling robots to perceive, reason, and act in complex environments. They also serve as the foundation for the recent Vision-Language-Action (VLA) models. Yet most evaluations of VLMs focus on single-view settings, leaving their ability to integrate multi-view information underexplored. At the same time, multi-camera setups are increasingly standard in robotic platforms, as they provide complementary perspectives to mitigate occlusion and depth ambiguity. Whether VLMs can effectively leverage such multi-view inputs for robotic reasoning therefore remains an open question. To bridge this gap, we introduce MV-RoboBench, a benchmark specifically designed to evaluate the multi-view spatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench consists of 1.7k manually curated QA items across eight subtasks, divided into two primary categories: spatial understanding and robotic execution. We evaluate a diverse set of existing VLMs, including both open-source and closed-source models, along with enhanced versions incorporating CoT-inspired techniques. The results show that state-of-the-art models remain far below human performance, underscoring the substantial challenges VLMs face in multi-view robotic perception. Additionally, our analysis uncovers two key findings: (i) spatial intelligence and robotic task execution are positively correlated in multi-view robotic scenarios; and (ii) strong performance on existing general-purpose single-view spatial understanding benchmarks does not reliably translate to success in the robotic spatial tasks assessed by our benchmark. We release MV-RoboBench as an open resource to foster progress in spatially grounded VLMs and VLAs, providing not only data but also a standardized evaluation protocol for multi-view embodied reasoning.",
          "abstract_zh": "视觉语言模型 (VLM) 对于 Embodied AI 至关重要，它使机器人能够在复杂的环境中感知、推理和行动。它们也是最近的视觉-语言-行动（VLA）模型的基础。然而，大多数对 VLM 的评估都集中在单视图设置上，而其集成多视图信息的能力尚未得到充分开发。与此同时，多摄像头设置在机器人平台中日益成为标准，因为它们提供了互补的视角，以减轻遮挡和深度模糊。因此，VLM 是否能够有效利用这种多视图输入进行机器人推理仍然是一个悬而未决的问题。为了弥补这一差距，我们引入了 MV-RoboBench，这是一个专门设计用于评估 VLM 在机器人操作中的多视图空间推理能力的基准。MV-RoboBench 包含 1,700 个手动策划的 QA 项目，涉及八个子任务，分为两个主要类别：空间理解和机器人执行。我们评估了各种现有的 VLM，包括开源和闭源模型，以及包含 CoT 启发技术的增强版本。结果表明，最先进的模型仍然远远低于人类的表现，这凸显了 VLM 在多视图机器人感知方面面临的巨大挑战。此外，我们的分析揭示了两个关键发现：（i）空间智能和机器人任务执行在多视图机器人场景中呈正相关；(ii) 现有通用单视图空间理解基准的强劲表现并不能可靠地转化为我们的基准评估的机器人空间任务的成功。我们发布 MV-RoboBench 作为开放资源，以促进空间接地 VLM 和 VLA 的进步，不仅提供数据，还提供多视图体现推理的标准化评估协议。"
        },
        {
          "title": "Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence",
          "url": "http://arxiv.org/abs/2510.21860v1",
          "snippet": "We present Butter-Bench, a benchmark evaluating large language model (LLM) controlled robots for practical intelligence, defined as the ability to navigate the messiness of the physical world. Current state-of-the-art robotic systems use a hierarchical architecture with LLMs in charge of high-level reasoning, and a Vision Language Action (VLA) model for low-level control. Butter-Bench evaluates the LLM part in isolation from the VLA. Although LLMs have repeatedly surpassed humans in evaluations requiring analytical intelligence, we find humans still outperform LLMs on Butter-Bench. The best LLMs score 40% on Butter-Bench, while the mean human score is 95%. LLMs struggled the most with multi-step spatial planning and social understanding. We also evaluate LLMs that are fine-tuned for embodied reasoning and conclude that this training does not improve their score on Butter-Bench.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-10-23T07:28:28Z",
          "authors": [
            "Callum Sharrock",
            "Lukas Petersson",
            "Hanna Petersson",
            "Axel Backlund",
            "Axel Wennström",
            "Kristoffer Nordström",
            "Elias Aronsson"
          ],
          "arxiv_id": "2510.21860",
          "abstract": "We present Butter-Bench, a benchmark evaluating large language model (LLM) controlled robots for practical intelligence, defined as the ability to navigate the messiness of the physical world. Current state-of-the-art robotic systems use a hierarchical architecture with LLMs in charge of high-level reasoning, and a Vision Language Action (VLA) model for low-level control. Butter-Bench evaluates the LLM part in isolation from the VLA. Although LLMs have repeatedly surpassed humans in evaluations requiring analytical intelligence, we find humans still outperform LLMs on Butter-Bench. The best LLMs score 40% on Butter-Bench, while the mean human score is 95%. LLMs struggled the most with multi-step spatial planning and social understanding. We also evaluate LLMs that are fine-tuned for embodied reasoning and conclude that this training does not improve their score on Butter-Bench.",
          "abstract_zh": "我们推出了 Butter-Bench，这是一个评估大语言模型 (LLM) 控制的机器人实用智能的基准，其定义为驾驭混乱的物理世界的能力。当前最先进的机器人系统使用分层架构，其中法学硕士负责高级推理，视觉语言动作（VLA）模型用于低级控制。Butter-Bench 独立于 VLA 评估 LLM 部分。尽管法学硕士在需要分析智能的评估中一再超越人类，但我们发现人类在黄油台上的表现仍然优于法学硕士。最好的法学硕士在 Butter-Bench 上的得分为 40%，而人类的平均得分为 95%。法学硕士在多步骤空间规划和社会理解方面最困难。我们还评估了针对具体推理进行微调的法学硕士，并得出结论认为，这种培训不会提高他们在 Butter-Bench 上的分数。"
        },
        {
          "title": "RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies",
          "url": "http://arxiv.org/abs/2510.17950v1",
          "snippet": "Testing on real machines is indispensable for robotic control algorithms. In the context of learning-based algorithms, especially VLA models, demand for large-scale evaluation, i.e. testing a large number of models on a large number of tasks, is becoming increasingly urgent. However, doing this right is highly non-trivial, especially when scalability and reproducibility is taken into account. In this report, we describe our methodology for constructing RoboChallenge, an online evaluation system to test robotic control algorithms, and our survey of recent state-of-the-art VLA models using our initial benchmark Table30.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-10-20T17:59:14Z",
          "authors": [
            "Adina Yakefu",
            "Bin Xie",
            "Chongyang Xu",
            "Enwen Zhang",
            "Erjin Zhou",
            "Fan Jia",
            "Haitao Yang",
            "Haoqiang Fan",
            "Haowei Zhang",
            "Hongyang Peng",
            "Jing Tan",
            "Junwen Huang",
            "Kai Liu",
            "Kaixin Liu",
            "Kefan Gu",
            "Qinglun Zhang",
            "Ruitao Zhang",
            "Saike Huang",
            "Shen Cheng",
            "Shuaicheng Liu",
            "Tiancai Wang",
            "Tiezhen Wang",
            "Wei Sun",
            "Wenbin Tang",
            "Yajun Wei",
            "Yang Chen",
            "Youqiang Gui",
            "Yucheng Zhao",
            "Yunchao Ma",
            "Yunfei Wei",
            "Yunhuan Yang",
            "Yutong Guo",
            "Ze Chen",
            "Zhengyuan Du",
            "Ziheng Zhang",
            "Ziming Liu",
            "Ziwei Yan"
          ],
          "arxiv_id": "2510.17950",
          "abstract": "Testing on real machines is indispensable for robotic control algorithms. In the context of learning-based algorithms, especially VLA models, demand for large-scale evaluation, i.e. testing a large number of models on a large number of tasks, is becoming increasingly urgent. However, doing this right is highly non-trivial, especially when scalability and reproducibility is taken into account. In this report, we describe our methodology for constructing RoboChallenge, an online evaluation system to test robotic control algorithms, and our survey of recent state-of-the-art VLA models using our initial benchmark Table30.",
          "abstract_zh": "对于机器人控制算法来说，真机测试是必不可少的。在基于学习的算法，特别是VLA模型的背景下，大规模评估的需求，即在大量任务上测试大量模型，变得越来越迫切。然而，正确地做到这一点非常重要，特别是考虑到可扩展性和可重复性时。在本报告中，我们描述了构建 RoboChallenge（一个用于测试机器人控制算法的在线评估系统）的方法，以及我们使用初始基准 Table30 对最新最先进的 VLA 模型进行的调查。"
        },
        {
          "title": "Learning Affordances at Inference-Time for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.19752v1",
          "snippet": "Solving complex real-world control tasks often takes multiple tries: if we fail at first, we reflect on what went wrong, and change our strategy accordingly to avoid making the same mistake. In robotics, Vision-Language-Action models (VLAs) offer a promising path towards solving complex control tasks, but lack the ability to contextually and dynamically readjust behavior when they fail to accomplish a task. In this work, we introduce Learning from Inference-Time Execution (LITEN), which connects a VLA low-level policy to a high-level VLM that conditions on past experiences by including them in-context, allowing it to learn the affordances and capabilities of the low-level VLA. Our approach iterates between a reasoning phase that generates and executes plans for the low-level VLA, and an assessment phase that reflects on the resulting execution and draws useful conclusions to be included in future reasoning contexts. Unlike similar approaches to self-refinement in non-robotics domains, LITEN must reflect on unstructured real-world robot trajectories (e.g., raw videos), which requires structured guiderails during assessment. Our experimental results demonstrate LITEN is able to effectively learn from past experience to generate plans that use high-affordance instructions to accomplish long-horizon tasks.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-10-22T16:43:29Z",
          "authors": [
            "Ameesh Shah",
            "William Chen",
            "Adwait Godbole",
            "Federico Mora",
            "Sanjit A. Seshia",
            "Sergey Levine"
          ],
          "arxiv_id": "2510.19752",
          "abstract": "Solving complex real-world control tasks often takes multiple tries: if we fail at first, we reflect on what went wrong, and change our strategy accordingly to avoid making the same mistake. In robotics, Vision-Language-Action models (VLAs) offer a promising path towards solving complex control tasks, but lack the ability to contextually and dynamically readjust behavior when they fail to accomplish a task. In this work, we introduce Learning from Inference-Time Execution (LITEN), which connects a VLA low-level policy to a high-level VLM that conditions on past experiences by including them in-context, allowing it to learn the affordances and capabilities of the low-level VLA. Our approach iterates between a reasoning phase that generates and executes plans for the low-level VLA, and an assessment phase that reflects on the resulting execution and draws useful conclusions to be included in future reasoning contexts. Unlike similar approaches to self-refinement in non-robotics domains, LITEN must reflect on unstructured real-world robot trajectories (e.g., raw videos), which requires structured guiderails during assessment. Our experimental results demonstrate LITEN is able to effectively learn from past experience to generate plans that use high-affordance instructions to accomplish long-horizon tasks.",
          "abstract_zh": "解决复杂的现实世界控制任务通常需要多次尝试：如果我们一开始失败了，我们就会反思哪里出了问题，并相应地改变我们的策略，以避免犯同样的错误。在机器人技术中，视觉-语言-动作模型（VLA）为解决复杂控制任务提供了一条有希望的途径，但缺乏在无法完成任务时根据上下文动态重新调整行为的能力。在这项工作中，我们引入了从推理时间执行中学习（LITEN），它将 VLA 低级策略与高级 VLM 连接起来，该高级 VLM 通过将过去的经验纳入上下文中来进行调节，使其能够学习低级 VLA 的可供性和功能。我们的方法在推理阶段和评估阶段之间进行迭代，推理阶段生成并执行低级 VLA 的计划，评估阶段反映执行结果并得出有用的结论以包含在未来的推理上下文中。与非机器人领域类似的自我完善方法不同，LITEN 必须反思非结构化的现实世界机器人轨迹（例如原始视频），这在评估过程中需要结构化的导轨。我们的实验结果表明，LITEN 能够有效地从过去的经验中学习，生成使用高可供性指令来完成长期任务的计划。"
        },
        {
          "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
          "url": "http://arxiv.org/abs/2510.17111v3",
          "snippet": "Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. Despite their capabilities, VLA systems face significant challenges due to their massive computational and memory demands, which conflict with the constraints of edge platforms such as on-board mobile manipulators that require real-time performance. Addressing this tension has become a central focus of recent research. In light of the growing efforts toward more efficient and scalable VLA systems, this survey provides a systematic review of approaches for improving VLA efficiency, with an emphasis on reducing latency, memory footprint, and training and inference costs. We categorize existing solutions into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category. Finally, we discuss future trends and open challenges, highlighting directions for advancing efficient embodied intelligence.",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-10-20T02:59:45Z",
          "authors": [
            "Weifan Guan",
            "Qinghao Hu",
            "Aosheng Li",
            "Jian Cheng"
          ],
          "arxiv_id": "2510.17111",
          "abstract": "Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. Despite their capabilities, VLA systems face significant challenges due to their massive computational and memory demands, which conflict with the constraints of edge platforms such as on-board mobile manipulators that require real-time performance. Addressing this tension has become a central focus of recent research. In light of the growing efforts toward more efficient and scalable VLA systems, this survey provides a systematic review of approaches for improving VLA efficiency, with an emphasis on reducing latency, memory footprint, and training and inference costs. We categorize existing solutions into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category. Finally, we discuss future trends and open challenges, highlighting directions for advancing efficient embodied intelligence.",
          "abstract_zh": "视觉语言动作（VLA）模型通过将自然语言指令和视觉观察映射到机器人动作，将视觉语言模型扩展到具体控制。尽管 VLA 系统功能强大，但由于其大量的计算和内存需求，它面临着巨大的挑战，这与需要实时性能的板载移动机械手等边缘平台的限制相冲突。解决这种紧张局势已成为近期研究的焦点。鉴于人们越来越多地致力于提高 VLA 系统的效率和可扩展性，本次调查对提高 VLA 效率的方法进行了系统回顾，重点是减少延迟、内存占用以及训练和推理成本。我们将现有的解决方案分为四个维度：模型架构、感知特征、动作生成和训练/推理策略，总结了每个类别中的代表性技术。最后，我们讨论未来的趋势和开放的挑战，强调推进高效的体现智能的方向。"
        },
        {
          "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
          "url": "http://arxiv.org/abs/2510.23576v1",
          "snippet": "Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties.",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-10-27T17:46:43Z",
          "authors": [
            "Anqi Li",
            "Zhiyong Wang",
            "Jiazhao Zhang",
            "Minghan Li",
            "Yunpeng Qi",
            "Zhibo Chen",
            "Zhizheng Zhang",
            "He Wang"
          ],
          "arxiv_id": "2510.23576",
          "abstract": "Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties.",
          "abstract_zh": "城市微移动应用（例如送货机器人）需要在大规模城市环境中进行可靠导航，同时遵循长视距路线指令。由于现实城市地区的动态和非结构化性质，这项任务特别具有挑战性，但大多数现有的导航方法仍然是针对小规模和可控场景的。有效的城市微交通需要两个互补级别的导航技能：低级能力（例如点目标到达和避障）和高级能力（例如路线视觉对齐）。为此，我们提出了 UrbanVLA，这是一个专为可扩展的城市导航而设计的路线条件视觉-语言-行动（VLA）框架。我们的方法在执行过程中明确地将噪声路径点与视觉观察对齐，然后规划驱动机器人的轨迹。为了使 UrbanVLA 能够掌握两个级别的导航，我们采用了两阶段训练流程。该过程首先使用模拟环境和从网络视频解析的轨迹进行监督微调 (SFT)。接下来是对模拟和现实世界数据的混合进行强化微调（RFT），这增强了模型在现实世界环境中的安全性和适应性。实验表明，UrbanVLA 在 MetaUrban 上的 SocialNav 任务中超出了强基线 55% 以上。此外，UrbanVLA 实现了可靠的现实世界导航，展示了大规模城市环境的可扩展性和针对现实世界不确定性的鲁棒性。"
        },
        {
          "title": "MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning",
          "url": "http://arxiv.org/abs/2510.18337v3",
          "snippet": "Integrating visual-language instructions into visuomotor policies is gaining momentum in robot learning for enhancing open-world generalization. Despite promising advances, existing approaches face two challenges: limited language steerability when no generated reasoning is used as a condition, or significant inference latency when reasoning is incorporated. In this work, we introduce MoTVLA, a mixture-of-transformers (MoT)-based vision-language-action (VLA) model that integrates fast-slow unified reasoning with behavior policy learning. MoTVLA preserves the general intelligence of pre-trained VLMs (serving as the generalist) for tasks such as perception, scene understanding, and semantic planning, while incorporating a domain expert, a second transformer that shares knowledge with the pretrained VLM, to generate domain-specific fast reasoning (e.g., robot motion decomposition), thereby improving policy execution efficiency. By conditioning the action expert on decomposed motion instructions, MoTVLA can learn diverse behaviors and substantially improve language steerability. Extensive evaluations across natural language processing benchmarks, robotic simulation environments, and real-world experiments confirm the superiority of MoTVLA in both fast-slow reasoning and manipulation task performance.",
          "site": "arxiv.org",
          "rank": 15,
          "published": "2025-10-21T06:39:34Z",
          "authors": [
            "Wenhui Huang",
            "Changhe Chen",
            "Han Qi",
            "Chen Lv",
            "Yilun Du",
            "Heng Yang"
          ],
          "arxiv_id": "2510.18337",
          "abstract": "Integrating visual-language instructions into visuomotor policies is gaining momentum in robot learning for enhancing open-world generalization. Despite promising advances, existing approaches face two challenges: limited language steerability when no generated reasoning is used as a condition, or significant inference latency when reasoning is incorporated. In this work, we introduce MoTVLA, a mixture-of-transformers (MoT)-based vision-language-action (VLA) model that integrates fast-slow unified reasoning with behavior policy learning. MoTVLA preserves the general intelligence of pre-trained VLMs (serving as the generalist) for tasks such as perception, scene understanding, and semantic planning, while incorporating a domain expert, a second transformer that shares knowledge with the pretrained VLM, to generate domain-specific fast reasoning (e.g., robot motion decomposition), thereby improving policy execution efficiency. By conditioning the action expert on decomposed motion instructions, MoTVLA can learn diverse behaviors and substantially improve language steerability. Extensive evaluations across natural language processing benchmarks, robotic simulation environments, and real-world experiments confirm the superiority of MoTVLA in both fast-slow reasoning and manipulation task performance.",
          "abstract_zh": "将视觉语言指令集成到视觉运动策略中正在机器人学习中获得动力，以增强开放世界的泛化能力。尽管取得了有希望的进展，但现有方法面临两个挑战：当不使用生成推理作为条件时，语言可操纵性有限；或者当合并推理时，推理延迟显着。在这项工作中，我们介绍了 MoTVLA，这是一种基于混合变压器 (MoT) 的视觉语言动作 (VLA) 模型，它将快慢统一推理与行为策略学习相结合。MoTVLA保留了预训练VLM（充当多面手）的一般智能，用于感知、场景理解和语义规划等任务，同时结合了领域专家，即与预训练VLM共享知识的第二个变压器，以生成特定于领域的快速推理（例如机器人运动分解），从而提高策略执行效率。通过根据分解的动作指令来调节动作专家，MoTVLA 可以学习不同的行为并显着提高语言的可操控性。对自然语言处理基准、机器人模拟环境和现实世界实验的广泛评估证实了 MoTVLA 在快慢推理和操作任务性能方面的优越性。"
        },
        {
          "title": "ACG: Action Coherence Guidance for Flow-based VLA models",
          "url": "http://arxiv.org/abs/2510.22201v1",
          "snippet": "Diffusion and flow matching models have emerged as powerful robot policies, enabling Vision-Language-Action (VLA) models to generalize across diverse scenes and instructions. Yet, when trained via imitation learning, their high generative capacity makes them sensitive to noise in human demonstrations: jerks, pauses, and jitter which reduce action coherence. Reduced action coherence causes instability and trajectory drift during deployment, failures that are catastrophic in fine-grained manipulation where precision is crucial. In this paper, we present Action Coherence Guidance (ACG) for VLA models, a training-free test-time guidance algorithm that improves action coherence and thereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and real-world SO-101 tasks, ACG consistently improves action coherence and boosts success rates across diverse manipulation tasks. Code and project page are available at https://github.com/DAVIAN-Robotics/ACG and https://DAVIAN-Robotics.github.io/ACG , respectively.",
          "site": "arxiv.org",
          "rank": 16,
          "published": "2025-10-25T07:44:33Z",
          "authors": [
            "Minho Park",
            "Kinam Kim",
            "Junha Hyung",
            "Hyojin Jang",
            "Hoiyeong Jin",
            "Jooyeol Yun",
            "Hojoon Lee",
            "Jaegul Choo"
          ],
          "arxiv_id": "2510.22201",
          "abstract": "Diffusion and flow matching models have emerged as powerful robot policies, enabling Vision-Language-Action (VLA) models to generalize across diverse scenes and instructions. Yet, when trained via imitation learning, their high generative capacity makes them sensitive to noise in human demonstrations: jerks, pauses, and jitter which reduce action coherence. Reduced action coherence causes instability and trajectory drift during deployment, failures that are catastrophic in fine-grained manipulation where precision is crucial. In this paper, we present Action Coherence Guidance (ACG) for VLA models, a training-free test-time guidance algorithm that improves action coherence and thereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and real-world SO-101 tasks, ACG consistently improves action coherence and boosts success rates across diverse manipulation tasks. Code and project page are available at https://github.com/DAVIAN-Robotics/ACG and https://DAVIAN-Robotics.github.io/ACG , respectively.",
          "abstract_zh": "扩散和流动匹配模型已成为强大的机器人策略，使视觉-语言-动作（VLA）模型能够泛化到不同的场景和指令。然而，当通过模仿学习进行训练时，它们的高生成能力使它们对人类演示中的噪音敏感：抽动、停顿和抖动，这些都会降低动作的连贯性。动作一致性的降低会导致部署过程中的不稳定和轨迹漂移，这在精度至关重要的细粒度操作中会造成灾难性的故障。在本文中，我们提出了 VLA 模型的动作连贯性指导（ACG），这是一种无需训练的测试时指导算法，可以提高动作连贯性，从而提高性能。经过对 RoboCasa、DexMimicGen 和现实世界 SO-101 任务的评估，ACG 不断提高动作连贯性并提高各种操作任务的成功率。代码和项目页面分别位于 https://github.com/DAVIAN-Robotics/ACG 和 https://DAVIAN-Robotics.github.io/ACG 。"
        },
        {
          "title": "Dexbotic: Open-Source Vision-Language-Action Toolbox",
          "url": "http://arxiv.org/abs/2510.23511v1",
          "snippet": "In this paper, we present Dexbotic, an open-source Vision-Language-Action (VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLA research service for professionals in the field of embodied intelligence. It offers a codebase that supports multiple mainstream VLA policies simultaneously, allowing users to reproduce various VLA methods with just a single environment setup. The toolbox is experiment-centric, where the users can quickly develop new VLA experiments by simply modifying the Exp script. Moreover, we provide much stronger pretrained models to achieve great performance improvements for state-of-the-art VLA policies. Dexbotic will continuously update to include more of the latest pre-trained foundation models and cutting-edge VLA models in the industry.",
          "site": "arxiv.org",
          "rank": 17,
          "published": "2025-10-27T16:47:24Z",
          "authors": [
            "Bin Xie",
            "Erjin Zhou",
            "Fan Jia",
            "Hao Shi",
            "Haoqiang Fan",
            "Haowei Zhang",
            "Hebei Li",
            "Jianjian Sun",
            "Jie Bin",
            "Junwen Huang",
            "Kai Liu",
            "Kaixin Liu",
            "Kefan Gu",
            "Lin Sun",
            "Meng Zhang",
            "Peilong Han",
            "Ruitao Hao",
            "Ruitao Zhang",
            "Saike Huang",
            "Songhan Xie",
            "Tiancai Wang",
            "Tianle Liu",
            "Wenbin Tang",
            "Wenqi Zhu",
            "Yang Chen",
            "Yingfei Liu",
            "Yizhuang Zhou",
            "Yu Liu",
            "Yucheng Zhao",
            "Yunchao Ma",
            "Yunfei Wei",
            "Yuxiang Chen",
            "Ze Chen",
            "Zeming Li",
            "Zhao Wu",
            "Ziheng Zhang",
            "Ziming Liu",
            "Ziwei Yan",
            "Ziyu Zhang"
          ],
          "arxiv_id": "2510.23511",
          "abstract": "In this paper, we present Dexbotic, an open-source Vision-Language-Action (VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLA research service for professionals in the field of embodied intelligence. It offers a codebase that supports multiple mainstream VLA policies simultaneously, allowing users to reproduce various VLA methods with just a single environment setup. The toolbox is experiment-centric, where the users can quickly develop new VLA experiments by simply modifying the Exp script. Moreover, we provide much stronger pretrained models to achieve great performance improvements for state-of-the-art VLA policies. Dexbotic will continuously update to include more of the latest pre-trained foundation models and cutting-edge VLA models in the industry.",
          "abstract_zh": "在本文中，我们提出了 Dexbotic，一个基于 PyTorch 的开源视觉-语言-动作（VLA）模型工具箱。旨在为具身智能领域的专业人士提供一站式VLA研究服务。它提供了同时支持多种主流VLA策略的代码库，允许用户仅通过单个环境设置即可重现各种VLA方法。该工具箱以实验为中心，用户只需修改Exp脚本即可快速开发新的VLA实验。此外，我们提供了更强大的预训练模型，以实现最先进的 VLA 策略的巨大性能改进。Dexbotic 将不断更新，纳入更多最新的预训练基础模型和业界最前沿的 VLA 模型。"
        },
        {
          "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors",
          "url": "http://arxiv.org/abs/2510.17439v1",
          "snippet": "Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.",
          "site": "arxiv.org",
          "rank": 18,
          "published": "2025-10-20T11:26:45Z",
          "authors": [
            "Zhengshen Zhang",
            "Hao Li",
            "Yalun Dai",
            "Zhengbang Zhu",
            "Lei Zhou",
            "Chenchen Liu",
            "Dong Wang",
            "Francis E. H. Tay",
            "Sijin Chen",
            "Ziwei Liu",
            "Yuxiao Liu",
            "Xinghang Li",
            "Pan Zhou"
          ],
          "arxiv_id": "2510.17439",
          "abstract": "Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.",
          "abstract_zh": "现有的视觉-语言-动作 (VLA) 模型在 3D 现实世界中运行，但通常构建在 2D 编码器上，留下了限制泛化和适应性的空间推理差距。最近的 VLA 3D 集成技术要么需要专门的传感器并且跨模态传输效果不佳，要么注入缺乏几何形状的微弱线索并降低视觉语言对齐。在这项工作中，我们介绍了 FALCON（从空间到动作），这是一种将丰富的 3D 空间标记注入动作头的新颖范例。FALCON 利用空间基础模型仅从 RGB 提供强大的几何先验，并包括一个体现空间模型，该模型可以选择融合深度，或在可用时提供更高的保真度，而无需重新训练或架构更改。为了保留语言推理，空间标记由空间增强动作头消耗，而不是连接到视觉语言主干中。这些设计使 FALCON 能够解决空间表示、模态可转移性和对齐方面的限制。在对三个模拟基准和十一个现实世界任务的综合评估中，我们提出的 FALCON 实现了最先进的性能，始终超越竞争基线，并且在杂乱、空间提示调节以及物体尺度和高度变化的情况下保持鲁棒性。"
        },
        {
          "title": "VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation",
          "url": "http://arxiv.org/abs/2510.20818v1",
          "snippet": "A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robot's physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https://vamos-vla.github.io/",
          "site": "arxiv.org",
          "rank": 19,
          "published": "2025-10-23T17:59:45Z",
          "authors": [
            "Mateo Guaman Castro",
            "Sidharth Rajagopal",
            "Daniel Gorbatov",
            "Matt Schmittle",
            "Rohan Baijal",
            "Octi Zhang",
            "Rosario Scalise",
            "Sidharth Talia",
            "Emma Romig",
            "Celso de Melo",
            "Byron Boots",
            "Abhishek Gupta"
          ],
          "arxiv_id": "2510.20818",
          "abstract": "A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robot's physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https://vamos-vla.github.io/",
          "abstract_zh": "机器人导航的一个基本挑战在于学习策略，这些策略可以在不同的环境中推广，同时符合特定实施例的独特物理约束和能力（例如，四足动物可以走上楼梯，但漫游者不能）。我们提出了 VAMOS，一种分层 VLA，它将语义规划与实施例基础解耦：通才规划器从多样化的开放世界数据中学习，而专业可供性模型则在安全、低成本模拟中学习机器人的物理约束和能力。我们通过精心设计一个界面来实现这种分离，该界面让高级规划人员直接在图像空间中提出候选路径，然后可供性模型对其进行评估和重新排序。我们的真实实验表明，与最先进的基于模型的端到端学习方法相比，VAMOS 在室内和复杂的室外导航中取得了更高的成功率。我们还表明，我们的分层设计可以在腿式和轮式机器人之间进行跨实体导航，并且可以使用自然语言轻松操纵。现实世界的消融证实，专业模型是实施落地的关键，使单个高级规划器能够部署在物理上不同的轮式和腿式机器人上。最后，该模型显着增强了单个机器人的可靠性，通过拒绝物理上不可行的计划，将成功率提高了 3 倍。网站：https://vamos-vla.github.io/"
        },
        {
          "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment",
          "url": "http://arxiv.org/abs/2510.17148v4",
          "snippet": "Conventional end-to-end (E2E) driving models are effective at generating physically plausible trajectories, but often fail to generalize to long-tail scenarios due to the lack of essential world knowledge to understand and reason about surrounding environments. In contrast, Vision-Language-Action (VLA) models leverage world knowledge to handle challenging cases, but their limited 3D reasoning capability can lead to physically infeasible actions. In this work we introduce DiffVLA++, an enhanced autonomous driving framework that explicitly bridges cognitive reasoning and E2E planning through metric-guided alignment. First, we build a VLA module directly generating semantically grounded driving trajectories. Second, we design an E2E module with a dense trajectory vocabulary that ensures physical feasibility. Third, and most critically, we introduce a metric-guided trajectory scorer that guides and aligns the outputs of the VLA and E2E modules, thereby integrating their complementary strengths. The experiment on the ICCV 2025 Autonomous Grand Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.",
          "site": "arxiv.org",
          "rank": 20,
          "published": "2025-10-20T04:49:14Z",
          "authors": [
            "Yu Gao",
            "Anqing Jiang",
            "Yiru Wang",
            "Wang Jijun",
            "Hao Jiang",
            "Zhigang Sun",
            "Heng Yuwen",
            "Wang Shuo",
            "Hao Zhao",
            "Sun Hao"
          ],
          "arxiv_id": "2510.17148",
          "abstract": "Conventional end-to-end (E2E) driving models are effective at generating physically plausible trajectories, but often fail to generalize to long-tail scenarios due to the lack of essential world knowledge to understand and reason about surrounding environments. In contrast, Vision-Language-Action (VLA) models leverage world knowledge to handle challenging cases, but their limited 3D reasoning capability can lead to physically infeasible actions. In this work we introduce DiffVLA++, an enhanced autonomous driving framework that explicitly bridges cognitive reasoning and E2E planning through metric-guided alignment. First, we build a VLA module directly generating semantically grounded driving trajectories. Second, we design an E2E module with a dense trajectory vocabulary that ensures physical feasibility. Third, and most critically, we introduce a metric-guided trajectory scorer that guides and aligns the outputs of the VLA and E2E modules, thereby integrating their complementary strengths. The experiment on the ICCV 2025 Autonomous Grand Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.",
          "abstract_zh": "传统的端到端（E2E）驾驶模型可以有效地生成物理上合理的轨迹，但由于缺乏理解和推理周围环境的基本世界知识，通常无法推广到长尾场景。相比之下，视觉-语言-动作 (VLA) 模型利用世界知识来处理具有挑战性的案例，但其有限的 3D 推理能力可能会导致物理上不可行的动作。在这项工作中，我们介绍了 DiffVLA++，这是一种增强的自动驾驶框架，它通过度量引导的对齐方式明确地连接认知推理和 E2E 规划。首先，我们构建一个 VLA 模块，直接生成基于语义的驾驶轨迹。其次，我们设计了一个具有密集轨迹词汇的 E2E 模块，以确保物理可行性。第三，也是最关键的，我们引入了一个度量引导的轨迹评分器，它可以引导和调整 VLA 和 E2E 模块的输出，从而整合它们的互补优势。ICCV 2025 自主挑战赛排行榜上的实验表明，DiffVLA++ 的 EPDMS 达到了 49.12。"
        },
        {
          "title": "SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing",
          "url": "http://arxiv.org/abs/2510.20965v1",
          "snippet": "Robotic suturing is a prototypical long-horizon dexterous manipulation task, requiring coordinated needle grasping, precise tissue penetration, and secure knot tying. Despite numerous efforts toward end-to-end autonomy, a fully autonomous suturing pipeline has yet to be demonstrated on physical hardware. We introduce SutureBot: an autonomous suturing benchmark on the da Vinci Research Kit (dVRK), spanning needle pickup, tissue insertion, and knot tying. To ensure repeatability, we release a high-fidelity dataset comprising 1,890 suturing demonstrations. Furthermore, we propose a goal-conditioned framework that explicitly optimizes insertion-point precision, improving targeting accuracy by 59\\%-74\\% over a task-only baseline. To establish this task as a benchmark for dexterous imitation learning, we evaluate state-of-the-art vision-language-action (VLA) models, including $π_0$, GR00T N1, OpenVLA-OFT, and multitask ACT, each augmented with a high-level task-prediction policy. Autonomous suturing is a key milestone toward achieving robotic autonomy in surgery. These contributions support reproducible evaluation and development of precision-focused, long-horizon dexterous manipulation policies necessary for end-to-end suturing. Dataset is available at: https://huggingface.co/datasets/jchen396/suturebot",
          "site": "arxiv.org",
          "rank": 21,
          "published": "2025-10-23T19:50:17Z",
          "authors": [
            "Jesse Haworth",
            "Juo-Tung Chen",
            "Nigel Nelson",
            "Ji Woong Kim",
            "Masoud Moghani",
            "Chelsea Finn",
            "Axel Krieger"
          ],
          "arxiv_id": "2510.20965",
          "abstract": "Robotic suturing is a prototypical long-horizon dexterous manipulation task, requiring coordinated needle grasping, precise tissue penetration, and secure knot tying. Despite numerous efforts toward end-to-end autonomy, a fully autonomous suturing pipeline has yet to be demonstrated on physical hardware. We introduce SutureBot: an autonomous suturing benchmark on the da Vinci Research Kit (dVRK), spanning needle pickup, tissue insertion, and knot tying. To ensure repeatability, we release a high-fidelity dataset comprising 1,890 suturing demonstrations. Furthermore, we propose a goal-conditioned framework that explicitly optimizes insertion-point precision, improving targeting accuracy by 59\\%-74\\% over a task-only baseline. To establish this task as a benchmark for dexterous imitation learning, we evaluate state-of-the-art vision-language-action (VLA) models, including $π_0$, GR00T N1, OpenVLA-OFT, and multitask ACT, each augmented with a high-level task-prediction policy. Autonomous suturing is a key milestone toward achieving robotic autonomy in surgery. These contributions support reproducible evaluation and development of precision-focused, long-horizon dexterous manipulation policies necessary for end-to-end suturing. Dataset is available at: https://huggingface.co/datasets/jchen396/suturebot",
          "abstract_zh": "机器人缝合是一项典型的长视野灵巧操作任务，需要协调的抓针、精确的组织穿透和安全的打结。尽管在端到端自主方面做出了许多努力，但完全自主的缝合管道尚未在物理硬件上得到演示。我们介绍 SutureBot：达芬奇研究套件 (dVRK) 的自主缝合基准，可实现跨针拾取、组织插入和打结。为了确保可重复性，我们发布了包含 1,890 个缝合演示的高保真数据集。此外，我们提出了一个以目标为条件的框架，可以显式优化插入点精度，与仅任务基线相比，将目标准确度提高 59\\%-74\\%。为了将该任务建立为灵巧模仿学习的基准，我们评估了最先进的视觉语言动作（VLA）模型，包括 $π_0$、GR00T N1、OpenVLA-OFT 和多任务 ACT，每个模型都增强了高级任务预测策略。自主缝合是实现机器人手术自主性的一个重要里程碑。这些贡献支持可重复的评估和开发端到端缝合所需的精确、长期灵巧的操纵策略。数据集位于：https://huggingface.co/datasets/jchen396/suturebot"
        },
        {
          "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting",
          "url": "http://arxiv.org/abs/2510.21817v1",
          "snippet": "Current Vision-Language-Action (VLA) models are often constrained by a rigid, static interaction paradigm, which lacks the ability to see, hear, speak, and act concurrently as well as handle real-time user interruptions dynamically. This hinders seamless embodied collaboration, resulting in an inflexible and unresponsive user experience. To address these limitations, we introduce VITA-E, a novel embodied interaction framework designed for both behavioral concurrency and nearly real-time interruption. The core of our approach is a dual-model architecture where two parallel VLA instances operate as an ``Active Model'' and a ``Standby Model'', allowing the embodied agent to observe its environment, listen to user speech, provide verbal responses, and execute actions, all concurrently and interruptibly, mimicking human-like multitasking capabilities. We further propose a ``model-as-controller'' paradigm, where we fine-tune the VLM to generate special tokens that serve as direct system-level commands, coupling the model's reasoning with the system's behavior. Experiments conducted on a physical humanoid platform demonstrate that VITA-E can reliably handle complex interactive scenarios. Our framework is compatible with various dual-system VLA models, achieving an extremely high success rate on emergency stops and speech interruptions while also successfully performing concurrent speech and action. This represents a significant step towards more natural and capable embodied assistants.",
          "site": "arxiv.org",
          "rank": 22,
          "published": "2025-10-21T17:59:56Z",
          "authors": [
            "Xiaoyu Liu",
            "Chaoyou Fu",
            "Chi Yan",
            "Chu Wu",
            "Haihan Gao",
            "Yi-Fan Zhang",
            "Shaoqi Dong",
            "Cheng Qian",
            "Bin Luo",
            "Xiuyong Yang",
            "Guanwu Li",
            "Yusheng Cai",
            "Yunhang Shen",
            "Deqiang Jiang",
            "Haoyu Cao",
            "Xing Sun",
            "Caifeng Shan",
            "Ran He"
          ],
          "arxiv_id": "2510.21817",
          "abstract": "Current Vision-Language-Action (VLA) models are often constrained by a rigid, static interaction paradigm, which lacks the ability to see, hear, speak, and act concurrently as well as handle real-time user interruptions dynamically. This hinders seamless embodied collaboration, resulting in an inflexible and unresponsive user experience. To address these limitations, we introduce VITA-E, a novel embodied interaction framework designed for both behavioral concurrency and nearly real-time interruption. The core of our approach is a dual-model architecture where two parallel VLA instances operate as an ``Active Model'' and a ``Standby Model'', allowing the embodied agent to observe its environment, listen to user speech, provide verbal responses, and execute actions, all concurrently and interruptibly, mimicking human-like multitasking capabilities. We further propose a ``model-as-controller'' paradigm, where we fine-tune the VLM to generate special tokens that serve as direct system-level commands, coupling the model's reasoning with the system's behavior. Experiments conducted on a physical humanoid platform demonstrate that VITA-E can reliably handle complex interactive scenarios. Our framework is compatible with various dual-system VLA models, achieving an extremely high success rate on emergency stops and speech interruptions while also successfully performing concurrent speech and action. This represents a significant step towards more natural and capable embodied assistants.",
          "abstract_zh": "当前的视觉-语言-动作（VLA）模型通常受到严格的静态交互范例的限制，缺乏同时看、听、说和行动以及动态处理实时用户中断的能力。这阻碍了无缝的具体协作，导致不灵活且反应迟钝的用户体验。为了解决这些限制，我们引入了 VITA-E，这是一种新颖的体现交互框架，专为行为并发和近实时中断而设计。我们方法的核心是双模型架构，其中两个并行的 VLA 实例作为“活动模型”和“备用模型”运行，允许实体代理观察其环境、聆听用户语音、提供口头响应并执行操作，所有这些都是同时且可中断的，模仿类人的多任务处理能力。我们进一步提出了“模型即控制器”范例，其中我们对 VLM 进行微调以生成用作直接系统级命令的特殊令牌，将模型的推理与系统的行为耦合起来。在物理人形平台上进行的实验表明，VITA-E能够可靠地处理复杂的交互场景。我们的框架与各种双系统VLA模型兼容，在紧急停止和语音中断方面实现了极高的成功率，同时还成功地执行了并发语音和动作。这代表着向更自然、更有能力的实体助理迈出了重要一步。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 15 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 15）。",
      "items": [
        {
          "title": "TianxingChen/Embodied-AI-Guide",
          "url": "https://github.com/TianxingChen/Embodied-AI-Guide",
          "snippet": "[Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "thu-ml/Motus",
          "url": "https://github.com/thu-ml/Motus",
          "snippet": "Official code of Motus: A Unified Latent Action World Model",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "IliaLarchenko/behavior-1k-solution",
          "url": "https://github.com/IliaLarchenko/behavior-1k-solution",
          "snippet": "1st place solution of 2025 BEHAVIOR Challenge",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "thu-ml/RDT2",
          "url": "https://github.com/thu-ml/RDT2",
          "snippet": "Official code of RDT 2",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "OpenDriveLab/UniVLA",
          "url": "https://github.com/OpenDriveLab/UniVLA",
          "snippet": "[RSS 2025] Learning to Act Anywhere with Task-centric Latent Actions",
          "site": "github.com",
          "rank": 9
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 10
        },
        {
          "title": "BridgeVLA/BridgeVLA",
          "url": "https://github.com/BridgeVLA/BridgeVLA",
          "snippet": "✨✨【NeurIPS 2025】Official implementation of BridgeVLA",
          "site": "github.com",
          "rank": 11
        },
        {
          "title": "SalvatoreRa/ML-news-of-the-week",
          "url": "https://github.com/SalvatoreRa/ML-news-of-the-week",
          "snippet": "A collection of the the best ML and AI news every week (research, news, resources)",
          "site": "github.com",
          "rank": 12
        },
        {
          "title": "52CV/CVPR-2025-Papers",
          "url": "https://github.com/52CV/CVPR-2025-Papers",
          "snippet": "CVPR-2025-Papers",
          "site": "github.com",
          "rank": 13
        },
        {
          "title": "Hub-Tian/UAVs_Meet_LLMs",
          "url": "https://github.com/Hub-Tian/UAVs_Meet_LLMs",
          "snippet": "UAVs_Meet_LLMs",
          "site": "github.com",
          "rank": 14
        },
        {
          "title": "52CV/ECCV-2024-Papers",
          "url": "https://github.com/52CV/ECCV-2024-Papers",
          "snippet": "ECCV-2024-Papers",
          "site": "github.com",
          "rank": 15
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-10-20",
  "week_end": "2025-10-26",
  "last_updated": "2026-01-07"
}