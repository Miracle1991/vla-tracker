{
  "generated_at": "2026-01-07T13:44:24.686813",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 13 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 13）。",
      "items": [
        {
          "title": "MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation",
          "url": "http://arxiv.org/abs/2511.09516v1",
          "snippet": "Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-11-12T17:56:13Z",
          "authors": [
            "Runhao Li",
            "Wenkai Guo",
            "Zhenyu Wu",
            "Changyuan Wang",
            "Haoyuan Deng",
            "Zhenyu Weng",
            "Yap-Peng Tan",
            "Ziwei Wang"
          ],
          "arxiv_id": "2511.09516",
          "abstract": "Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.",
          "abstract_zh": "预训练的视觉-语言-动作（VLA）模型在提高端到端机器人操作的鲁棒性和泛化性方面取得了显着的成功。然而，由于缺乏记忆并且仅依赖于即时感官输入，这些模型难以应对长期任务。为了解决这一限制，我们提出了视觉-语言-动作模型的记忆增强提示（MAP-VLA），这是一种新颖的框架，它为预先训练的 VLA 模型提供了演示衍生的记忆提示，以增强长视野机器人操作任务的动作生成。为了实现这一目标，MAP-VLA 首先根据历史演示构建一个内存库，其中每个内存单元捕获有关任务特定阶段的信息。这些记忆单元被实现为通过提示调整优化的可学习软提示。然后，在实时任务执行过程中，MAP-VLA通过轨迹相似性匹配检索相关记忆，并将其动态集成到VLA模型中以增强动作生成。重要的是，这种即时调整和检索增强方法可作为冻结 VLA 模型的即插即用模块，提供轻量级且灵活的解决方案来提高任务性能。实验结果表明，MAP-VLA 在模拟基准测试中提供高达 7.0% 的绝对性能提升，在长视野任务的真实机器人评估中提供高达 25.0% 的绝对性能提升，超越了当前最先进的方法。"
        },
        {
          "title": "Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2511.09958v1",
          "snippet": "The Vision-Language-Action models (VLA) have achieved significant advances in robotic manipulation recently. However, vision-only VLA models create fundamental limitations, particularly in perceiving interactive and manipulation dynamic processes. This paper proposes Audio-VLA, a multimodal manipulation policy that leverages contact audio to perceive contact events and dynamic process feedback. Audio-VLA overcomes the vision-only constraints of VLA models. Additionally, this paper introduces the Task Completion Rate (TCR) metric to systematically evaluate dynamic operational processes. Audio-VLA employs pre-trained DINOv2 and SigLIP as visual encoders, AudioCLIP as the audio encoder, and Llama2 as the large language model backbone. We apply LoRA fine-tuning to these pre-trained modules to achieve robust cross-modal understanding of both visual and acoustic inputs. A multimodal projection layer aligns features from different modalities into the same feature space. Moreover RLBench and LIBERO simulation environments are enhanced by adding collision-based audio generation to provide realistic sound feedback during object interactions. Since current robotic manipulation evaluations focus on final outcomes rather than providing systematic assessment of dynamic operational processes, the proposed TCR metric measures how well robots perceive dynamic processes during manipulation, creating a more comprehensive evaluation metric. Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate Audio-VLA's superior performance over vision-only comparative methods, while the TCR metric effectively quantifies dynamic process perception capabilities.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-11-13T04:39:56Z",
          "authors": [
            "Xiangyi Wei",
            "Haotian Zhang",
            "Xinyi Cao",
            "Siyu Xie",
            "Weifeng Ge",
            "Yang Li",
            "Changbo Wang"
          ],
          "arxiv_id": "2511.09958",
          "abstract": "The Vision-Language-Action models (VLA) have achieved significant advances in robotic manipulation recently. However, vision-only VLA models create fundamental limitations, particularly in perceiving interactive and manipulation dynamic processes. This paper proposes Audio-VLA, a multimodal manipulation policy that leverages contact audio to perceive contact events and dynamic process feedback. Audio-VLA overcomes the vision-only constraints of VLA models. Additionally, this paper introduces the Task Completion Rate (TCR) metric to systematically evaluate dynamic operational processes. Audio-VLA employs pre-trained DINOv2 and SigLIP as visual encoders, AudioCLIP as the audio encoder, and Llama2 as the large language model backbone. We apply LoRA fine-tuning to these pre-trained modules to achieve robust cross-modal understanding of both visual and acoustic inputs. A multimodal projection layer aligns features from different modalities into the same feature space. Moreover RLBench and LIBERO simulation environments are enhanced by adding collision-based audio generation to provide realistic sound feedback during object interactions. Since current robotic manipulation evaluations focus on final outcomes rather than providing systematic assessment of dynamic operational processes, the proposed TCR metric measures how well robots perceive dynamic processes during manipulation, creating a more comprehensive evaluation metric. Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate Audio-VLA's superior performance over vision-only comparative methods, while the TCR metric effectively quantifies dynamic process perception capabilities.",
          "abstract_zh": "视觉-语言-动作模型（VLA）最近在机器人操作方面取得了重大进展。然而，仅视觉的 VLA 模型存在根本性的局限性，特别是在感知交互和操纵动态过程方面。本文提出了 Audio-VLA，这是一种多模态操纵策略，利用接触音频来感知接触事件和动态过程反馈。Audio-VLA 克服了 VLA 模型的仅视觉限制。此外，本文还引入了任务完成率（TCR）指标来系统地评估动态操作流程。Audio-VLA 采用预先训练的 DINOv2 和 SigLIP 作为视觉编码器，AudioCLIP 作为音频编码器，以及 Llama2 作为大型语言模型主干。我们对这些预训练模块应用 LoRA 微调，以实现对视觉和听觉输入的强大跨模式理解。多模态投影层将不同模态的特征对齐到相同的特征空间中。此外，通过添加基于碰撞的音频生成来增强 RLBench 和 LIBERO 模拟环境，以在对象交互期间提供逼真的声音反馈。由于当前的机器人操作评估侧重于最终结果，而不是提供动态操作过程的系统评估，因此提出的 TCR 指标衡量机器人在操作过程中感知动态过程的程度，从而创建更全面的评估指标。对 LIBERO、RLBench 和两个现实世界任务的大量实验证明了 Audio-VLA 比仅视觉比较方法具有优越的性能，而 TCR 指标有效地量化了动态过程感知能力。"
        },
        {
          "title": "Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2511.11298v1",
          "snippet": "Foundation models applied in robotics, particularly \\textbf{Vision--Language--Action (VLA)} models, hold great promise for achieving general-purpose manipulation. Yet, systematic real-world evaluations and cross-model comparisons remain scarce. This paper reports our \\textbf{empirical experiences} from benchmarking four representative VLAs -- \\textbf{ACT}, \\textbf{OpenVLA--OFT}, \\textbf{RDT-1B}, and \\boldmath{$π_0$} -- across four manipulation tasks conducted in both simulation and on the \\textbf{ALOHA Mobile} platform. We establish a \\textbf{standardized evaluation framework} that measures performance along three key dimensions: (1) \\textit{accuracy and efficiency} (success rate and time-to-success), (2) \\textit{adaptability} across in-distribution, spatial out-of-distribution, and instance-plus-spatial out-of-distribution settings, and (3) \\textit{language instruction-following accuracy}. Through this process, we observe that \\boldmath{$π_0$} demonstrates superior adaptability in out-of-distribution scenarios, while \\textbf{ACT} provides the highest stability in-distribution. Further analysis highlights differences in computational demands, data-scaling behavior, and recurring failure modes such as near-miss grasps, premature releases, and long-horizon state drift. These findings reveal practical trade-offs among VLA model architectures in balancing precision, generalization, and deployment cost, offering actionable insights for selecting and deploying VLAs in real-world robotic manipulation tasks.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-11-14T13:35:30Z",
          "authors": [
            "Yihao Zhang",
            "Yuankai Qi",
            "Xi Zheng"
          ],
          "arxiv_id": "2511.11298",
          "abstract": "Foundation models applied in robotics, particularly \\textbf{Vision--Language--Action (VLA)} models, hold great promise for achieving general-purpose manipulation. Yet, systematic real-world evaluations and cross-model comparisons remain scarce. This paper reports our \\textbf{empirical experiences} from benchmarking four representative VLAs -- \\textbf{ACT}, \\textbf{OpenVLA--OFT}, \\textbf{RDT-1B}, and \\boldmath{$π_0$} -- across four manipulation tasks conducted in both simulation and on the \\textbf{ALOHA Mobile} platform. We establish a \\textbf{standardized evaluation framework} that measures performance along three key dimensions: (1) \\textit{accuracy and efficiency} (success rate and time-to-success), (2) \\textit{adaptability} across in-distribution, spatial out-of-distribution, and instance-plus-spatial out-of-distribution settings, and (3) \\textit{language instruction-following accuracy}. Through this process, we observe that \\boldmath{$π_0$} demonstrates superior adaptability in out-of-distribution scenarios, while \\textbf{ACT} provides the highest stability in-distribution. Further analysis highlights differences in computational demands, data-scaling behavior, and recurring failure modes such as near-miss grasps, premature releases, and long-horizon state drift. These findings reveal practical trade-offs among VLA model architectures in balancing precision, generalization, and deployment cost, offering actionable insights for selecting and deploying VLAs in real-world robotic manipulation tasks.",
          "abstract_zh": "应用在机器人技术中的基础模型，特别是 \\textbf{Vision--Language--Action (VLA)} 模型，为实现通用操作带来了巨大的希望。然而，系统的现实世界评估和跨模型比较仍然很少。本文报告了我们对四个代表性 VLA（\\textbf{ACT}、\\textbf{OpenVLA--OFT}、\\textbf{RDT-1B} 和 \\boldmath{$π_0$} 进行基准测试的经验经验），涉及在模拟和 \\textbf{ALOHA Mobile} 平台上执行的四个操作任务。我们建立了一个\\textbf{标准化评估框架}，用于衡量三个关键维度的性能：（1）\\textit{准确性和效率}（成功率和成功时间），（2）\\textit{分布内、空间分布外和实例加空间分布外设置的适应性}，以及（3）\\textit{语言指令遵循准确性}。通过这个过程，我们观察到 \\boldmath{$π_0$} 在分布外场景中表现出卓越的适应性，而 \\textbf{ACT} 在分布内提供最高的稳定性。进一步的分析强调了计算需求、数据扩展行为和反复出现的故障模式（例如差点抓住、过早释放和长期状态漂移）方面的差异。这些发现揭示了 VLA 模型架构在平衡精度、泛化性和部署成本方面的实际权衡，为在现实世界的机器人操作任务中选择和部署 VLA 提供了可行的见解。"
        },
        {
          "title": "VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving",
          "url": "http://arxiv.org/abs/2511.12405v1",
          "snippet": "Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-11-16T00:55:28Z",
          "authors": [
            "Hyunki Seong",
            "Seongwoo Moon",
            "Hojin Ahn",
            "Jehun Kang",
            "David Hyunchul Shim"
          ],
          "arxiv_id": "2511.12405",
          "abstract": "Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.",
          "abstract_zh": "由于需要强大的泛化能力，以端到端的方式探索开放世界的情况是一项有前途但具有挑战性的任务。特别是，在非结构化室外环境中的端到端自动驾驶经常会遇到训练期间不熟悉的条件。在这项工作中，我们提出了视觉语言动作检索（VLA-R），这是一种开放世界的端到端自动驾驶（OW-E2EAD）框架，它将开放世界感知与新颖的视觉动作检索范例相结合。我们利用冻结的视觉语言模型进行开放世界检测和分割，以获得多尺度、提示引导和可解释的感知特征，而无需进行特定领域的调整。Q-Former 瓶颈将细粒度的视觉表示与语言一致的视觉特征聚合在一起，连接感知和动作领域。为了学习可转移的驾驶行为，我们引入了一种视觉-动作对比学习方案，该方案将视觉-语言和动作嵌入结合起来，以实现有效的开放世界推理和动作检索。我们在现实世界的机器人平台上进行的实验表明，即使数据有限，在非结构化、不可见的环境中也具有很强的泛化性和探索性性能。补充材料中提供了演示视频。"
        },
        {
          "title": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation",
          "url": "http://arxiv.org/abs/2511.10518v1",
          "snippet": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-11-13T17:24:37Z",
          "authors": [
            "Wei Li",
            "Renshan Zhang",
            "Rui Shao",
            "Zhijian Fang",
            "Kaiwen Zhou",
            "Zhuotao Tian",
            "Liqiang Nie"
          ],
          "arxiv_id": "2511.10518",
          "abstract": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA",
          "abstract_zh": "视觉-语言-动作（VLA）模型在机器人操作方面取得了进步，但实际部署仍然受到两个关键限制的阻碍：1）感知冗余，不相关的视觉输入处理效率低下；2）肤浅的指令-视觉对齐，这阻碍了动作的语义基础。在本文中，我们提出了 SemanticVLA，这是一种新颖的 VLA 框架，可以执行语义对齐稀疏化和增强以实现高效的机器人操作。具体来说：1）为了在保持语义对齐的同时稀疏冗余感知，语义引导双视觉剪枝器（SD-Pruner）执行：指令驱动剪枝器（ID-Pruner）在SigLIP中提取全局动作线索和局部语义锚；空间聚合剪枝器 (SA-Pruner) 将丰富的几何特征压缩为 DINOv2 中的任务自适应标记。2）为了利用稀疏特征并将语义与空间几何集成，语义互补分层融合器（SH-Fuser）融合了 SigLIP 和 DINOv2 上的密集补丁和稀疏标记，以实现连贯表示。3）为了增强从感知到行动的转变，语义条件动作耦合器（SA-Coupler）取代了传统的观察到自由度方法，为操作任务产生了更有效和可解释的行为建模。对模拟和现实世界任务的大量实验表明，SemanticVLA 在性能和效率方面都树立了新的 SOTA。SemanticVLA 在 LIBERO 基准上超越 OpenVLA，成功率提高了 21.1%，同时将训练成本和推理延迟分别降低了 3.0 倍和 2.7 倍。SemanticVLA 是开源的，可在 https://github.com/JiuTian-VL/SemanticVLA 上公开获取"
        },
        {
          "title": "Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective",
          "url": "http://arxiv.org/abs/2511.11478v3",
          "snippet": "As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSM's baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-11-14T16:56:01Z",
          "authors": [
            "Nhat Chung",
            "Taisei Hanyu",
            "Toan Nguyen",
            "Huy Le",
            "Frederick Bumgarner",
            "Duy Minh Ho Nguyen",
            "Khoa Vo",
            "Kashu Yamazaki",
            "Chase Rainwater",
            "Tung Kieu",
            "Anh Nguyen",
            "Ngan Le"
          ],
          "arxiv_id": "2511.11478",
          "abstract": "As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSM's baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies.",
          "abstract_zh": "随着具体代理在日益复杂的环境中运行，随着时间的推移感知、跟踪和推理单个对象实例的能力变得至关重要，特别是在需要与视觉上相似的对象进行有序交互的任务中。在这些非马尔可夫环境中，关键决策线索通常隐藏在特定于对象的历史中，而不是当前场景中。如果没有对先前交互的持久记忆（交互过什么、在哪里交互或如何改变），视觉运动策略可能会失败、重复过去的动作或忽略已完成的动作。为了应对这一挑战，我们引入了 LIBERO-Mem，这是一个非马尔可夫任务套件，用于在对象级部分可观测性下对机器人操作进行压力测试。它将短视野和长视野目标跟踪与时间排序的子目标结合起来，需要超出当前帧的推理。然而，视觉-语言-动作（VLA）模型通常在这种情况下陷入困境，即使对于仅跨越几百帧的任务，令牌缩放也很快变得棘手。我们提出了 Embodied-SlotSSM，一个以时隙为中心的 VLA 框架，专为时间可扩展性而构建。它维护时空一致的槽标识，并通过两种机制利用它们：（1）用于重建短期历史的槽状态空间建模，以及（2）关系编码器将输入标记与动作解码对齐。这些组件共同实现了基于时间的、上下文感知的动作预测。实验显示了 Embodied-SlotSSM 在 LIBERO-Mem 和一般任务上的基准性能，为以对象为中心的机器人策略中的非马尔可夫推理提供了可扩展的解决方案。"
        },
        {
          "title": "MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror",
          "url": "http://arxiv.org/abs/2511.08865v1",
          "snippet": "In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-11-12T01:09:20Z",
          "authors": [
            "Cong Tai",
            "Hansheng Wu",
            "Haixu Long",
            "Zhengbin Long",
            "Zhaoyu Zheng",
            "Haodong Xiang",
            "Tao Shen"
          ],
          "arxiv_id": "2511.08865",
          "abstract": "In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.",
          "abstract_zh": "在这项工作中，我们提出了一种基于PICO的机器人远程操作框架，能够低成本、实时采集手部运动和姿势数据，在成本效益方面优于主流视觉跟踪和运动捕捉解决方案。该框架与 RealMirror 生态系统原生兼容，为 Isaac 模拟环境中稳定且精确的机器人轨迹记录提供即用型功能，从而促进视觉-语言-动作 (VLA) 数据集的构建。此外，该系统支持各种配备末端执行器的机器人的实时远程操作，包括灵巧的手和机器人抓手。该工作旨在降低上肢机器人操控研究的技术壁垒，从而加速VLA相关研究的进展。"
        },
        {
          "title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2511.09515v1",
          "snippet": "Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the \"imagined\" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-11-12T17:54:09Z",
          "authors": [
            "Fangqi Zhu",
            "Zhengyang Yan",
            "Zicong Hong",
            "Quanxin Shou",
            "Xiao Ma",
            "Song Guo"
          ],
          "arxiv_id": "2511.09515",
          "abstract": "Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the \"imagined\" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在通用机器人操作方面显示出强大的潜力，但它们对专家演示的依赖限制了它们从失败中学习和进行自我纠正的能力。强化学习（RL）通过与物理环境的自我改进交互来解决这些问题，但在真实机器人上却面临着高样本复杂性的问题。我们引入了基于世界模型的策略优化（WMPO），这是一种无需与真实环境交互的策略 VLA RL 的原则框架。与广泛使用的潜在世界模型相比，WMPO 专注于基于像素的预测，将“想象的”轨迹与使用网络规模图像预训练的 VLA 特征对齐。至关重要的是，WMPO 使策略能够执行策略内 GRPO，其性能比常用的离策略方法更强。在模拟和真实机器人环境中进行的大量实验表明，WMPO (i) 显着提高了样本效率，(ii) 实现了更强的整体性能，(iii) 表现出自我纠正等紧急行为，(iv) 表现出强大的泛化和终身学习能力。"
        },
        {
          "title": "How Do VLAs Effectively Inherit from VLMs?",
          "url": "http://arxiv.org/abs/2511.06619v1",
          "snippet": "Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-11-10T01:58:02Z",
          "authors": [
            "Chuheng Zhang",
            "Rushuai Yang",
            "Xiaoyu Chen",
            "Kaixin Wang",
            "Li Zhao",
            "Yi Chen",
            "Jiang Bian"
          ],
          "arxiv_id": "2511.06619",
          "abstract": "Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems.",
          "abstract_zh": "视觉-语言-动作（VLA）模型有望实现普遍化的具体控制。为了实现这一目标，一个普遍的范例是利用大型视觉语言模型（VLM）丰富的视觉语义先验。然而，基本问题仍然存在：VLA 如何有效地继承 VLM 的先验知识？为了解决这个关键问题，我们引入了一个诊断基准 GrinningFace，这是一项表情符号桌面操作任务，其中要求机器人手臂将对象放置到与语言指令相对应的打印表情符号上。这个任务设计特别具有启发性——与表情符号相关的知识在用于 VLM 预训练的互联网规模数据集中无处不在，但表情符号本身在标准机器人数据集中却基本上不存在。因此，它们提供了一个干净的代理：成功的任务完成表明 VLM 在具体控制之前的有效转移。我们在模拟环境和真实机器人中实施此诊断任务，并比较各种有前途的知识转移技术。具体来说，我们研究了参数高效微调、VLM 冻结、协同训练、预测离散动作和预测潜在动作的效果。通过系统评估，我们的工作不仅证明了保留 VLM 先验对于 VLA 泛化的至关重要性，而且还为开发真正可泛化的具体人工智能系统的未来研究制定了指导方针。"
        },
        {
          "title": "Phantom Menace: Exploring and Enhancing the Robustness of VLA Models Against Physical Sensor Attacks",
          "url": "http://arxiv.org/abs/2511.10008v2",
          "snippet": "Vision-Language-Action (VLA) models revolutionize robotic systems by enabling end-to-end perception-to-action pipelines that integrate multiple sensory modalities, such as visual signals processed by cameras and auditory signals captured by microphones. This multi-modality integration allows VLA models to interpret complex, real-world environments using diverse sensor data streams. Given the fact that VLA-based systems heavily rely on the sensory input, the security of VLA models against physical-world sensor attacks remains critically underexplored. To address this gap, we present the first systematic study of physical sensor attacks against VLAs, quantifying the influence of sensor attacks and investigating the defenses for VLA models. We introduce a novel \"Real-Sim-Real\" framework that automatically simulates physics-based sensor attack vectors, including six attacks targeting cameras and two targeting microphones, and validates them on real robotic systems. Through large-scale evaluations across various VLA architectures and tasks under varying attack parameters, we demonstrate significant vulnerabilities, with susceptibility patterns that reveal critical dependencies on task types and model designs. We further develop an adversarial-training-based defense that enhances VLA robustness against out-of-distribution physical perturbations caused by sensor attacks while preserving model performance. Our findings expose an urgent need for standardized robustness benchmarks and mitigation strategies to secure VLA deployments in safety-critical environments.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-11-13T06:24:28Z",
          "authors": [
            "Xuancun Lu",
            "Jiaxiang Chen",
            "Shilin Xiao",
            "Zizhi Jin",
            "Zhangrui Chen",
            "Hanwen Yu",
            "Bohan Qian",
            "Ruochen Zhou",
            "Xiaoyu Ji",
            "Wenyuan Xu"
          ],
          "arxiv_id": "2511.10008",
          "abstract": "Vision-Language-Action (VLA) models revolutionize robotic systems by enabling end-to-end perception-to-action pipelines that integrate multiple sensory modalities, such as visual signals processed by cameras and auditory signals captured by microphones. This multi-modality integration allows VLA models to interpret complex, real-world environments using diverse sensor data streams. Given the fact that VLA-based systems heavily rely on the sensory input, the security of VLA models against physical-world sensor attacks remains critically underexplored. To address this gap, we present the first systematic study of physical sensor attacks against VLAs, quantifying the influence of sensor attacks and investigating the defenses for VLA models. We introduce a novel \"Real-Sim-Real\" framework that automatically simulates physics-based sensor attack vectors, including six attacks targeting cameras and two targeting microphones, and validates them on real robotic systems. Through large-scale evaluations across various VLA architectures and tasks under varying attack parameters, we demonstrate significant vulnerabilities, with susceptibility patterns that reveal critical dependencies on task types and model designs. We further develop an adversarial-training-based defense that enhances VLA robustness against out-of-distribution physical perturbations caused by sensor attacks while preserving model performance. Our findings expose an urgent need for standardized robustness benchmarks and mitigation strategies to secure VLA deployments in safety-critical environments.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型通过启用端到端感知到动作管道来彻底改变机器人系统，该管道集成了多种感官模式，例如摄像头处理的视觉信号和麦克风捕获的听觉信号。这种多模态集成允许 VLA 模型使用不同的传感器数据流来解释复杂的现实环境。鉴于基于 VLA 的系统严重依赖感官输入，VLA 模型针对物理世界传感器攻击的安全性仍然没有得到充分的研究。为了弥补这一差距，我们首次系统地研究了针对 VLA 的物理传感器攻击，量化了传感器攻击的影响并研究了 VLA 模型的防御。我们引入了一种新颖的“Real-Sim-Real”框架，该框架可自动模拟基于物理的传感器攻击向量，包括针对摄像机和两个目标麦克风的六次攻击，并在真实的机器人系统上对其进行验证。通过在不同攻击参数下对各种 VLA 架构和任务进行大规模评估，我们展示了重大漏洞，其易感性模式揭示了对任务类型和模型设计的关键依赖性。我们进一步开发了一种基于对抗训练的防御，增强了 VLA 的鲁棒性，以应对传感器攻击引起的分布外物理扰动，同时保持模型性能。我们的研究结果表明，迫切需要标准化的稳健性基准和缓解策略，以确保安全关键环境中的 VLA 部署。"
        },
        {
          "title": "AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2511.12149v1",
          "snippet": "Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-11-15T10:30:46Z",
          "authors": [
            "Jiayu Li",
            "Yunhan Zhao",
            "Xiang Zheng",
            "Zonghuan Xu",
            "Yige Li",
            "Xingjun Ma",
            "Yu-Gang Jiang"
          ],
          "arxiv_id": "2511.12149",
          "abstract": "Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.",
          "abstract_zh": "视觉-语言-动作（VLA）模型使机器人能够解释自然语言指令并执行不同的任务，但其感知、语言和控制的集成引入了新的安全漏洞。尽管人们对攻击此类模型的兴趣日益浓厚，但由于缺乏统一的评估框架，现有技术的有效性仍不清楚。一个主要问题是 VLA 架构中动作标记器的差异阻碍了可重复性和公平比较。更重要的是，大多数现有攻击尚未在现实​​场景中得到验证。为了应对这些挑战，我们提出了AttackVLA，这是一个与VLA开发生命周期保持一致的统一框架，涵盖数据构建、模型训练和推理。在此框架内，我们实施了一系列广泛的攻击，包括针对 VLA 的所有现有攻击以及最初为视觉语言模型开发的多种改编攻击，并在模拟和现实环境中对其进行评估。我们对现有攻击的分析揭示了一个关键差距：当前的方法往往会引发非目标性故障或静态动作状态，而导致驱动 VLA 执行精确的长范围动作序列的目标攻击在很大程度上尚未被探索。为了填补这一空白，我们引入了 BackdoorVLA，这是一种有针对性的后门攻击，只要存在触发器，就会迫使 VLA 执行攻击者指定的长范围操作序列。我们在模拟基准和真实机器人设置中评估 BackdoorVLA，实现了 58.4% 的平均目标成功率，并在选定任务上达到 100%。我们的工作为评估 VLA 漏洞提供了一个标准化框架，并展示了精确对抗性操纵的潜力，从而推动了对基于 VLA 的具体系统的保护的进一步研究。"
        },
        {
          "title": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer",
          "url": "http://arxiv.org/abs/2511.10560v2",
          "snippet": "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-11-13T17:59:01Z",
          "authors": [
            "Haosong Peng",
            "Hao Li",
            "Yalun Dai",
            "Yushi Lan",
            "Yihang Luo",
            "Tianyu Qi",
            "Zhengshen Zhang",
            "Yufeng Zhan",
            "Junfei Zhang",
            "Wenchao Xu",
            "Ziwei Liu"
          ],
          "arxiv_id": "2511.10560",
          "abstract": "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.",
          "abstract_zh": "通用 3D 基础模型已开始引领统一不同视觉任务的趋势，但大多数模型都假设仅 RGB 输入并忽略现成的几何线索（例如相机内在特征、姿势和深度图）。为了解决这个问题，我们引入了 OmniVGGT，这是一种新颖的框架，可以在训练和推理过程中有效地受益于任意数量的辅助几何模态。在我们的框架中，提出了一个 GeoAdapter 将深度和相机内在/外在编码到空间基础模型中。它采用零初始化卷积逐步注入几何信息，而不会破坏基础模型的表示空间。这种设计可确保稳定的优化，且开销可以忽略不计，即使在多个额外输入的情况下，也能保持与 VGGT 相当的推理速度。此外，还提出了一种随机多模态融合方案，该方案在训练期间对每个实例的模态子集进行随机采样。这使得在测试过程中能够实现任意数量的模态输入，并促进学习鲁棒的空间表示，而不是过度拟合辅助线索。单目/多视图深度估计、多视图立体和相机姿态估计的综合实验表明，OmniVGGT 的性能优于带有辅助输入的现有方法，即使仅使用 RGB 输入也能实现最先进的结果。为了进一步突出其实用性，我们将 OmniVGGT 集成到视觉-语言-动作 (VLA) 模型中。OmniVGGT 的增强型 VLA 模型不仅在主流基准测试中优于基于点云的基线，而且还有效地利用可访问的辅助输入来实现机器人任务的一致增益。"
        },
        {
          "title": "SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control",
          "url": "http://arxiv.org/abs/2511.07820v2",
          "snippet": "Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited set of behaviors, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leveraging dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-11-11T04:37:40Z",
          "authors": [
            "Zhengyi Luo",
            "Ye Yuan",
            "Tingwu Wang",
            "Chenran Li",
            "Sirui Chen",
            "Fernando Castañeda",
            "Zi-Ang Cao",
            "Jiefeng Li",
            "David Minor",
            "Qingwei Ben",
            "Xingye Da",
            "Runyu Ding",
            "Cyrus Hogg",
            "Lina Song",
            "Edy Lim",
            "Eugene Jeong",
            "Tairan He",
            "Haoru Xue",
            "Wenli Xiao",
            "Zi Wang",
            "Simon Yuen",
            "Jan Kautz",
            "Yan Chang",
            "Umar Iqbal",
            "Linxi \"Jim\" Fan",
            "Yuke Zhu"
          ],
          "arxiv_id": "2511.07820",
          "abstract": "Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited set of behaviors, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leveraging dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.",
          "abstract_zh": "尽管在数千个 GPU 上训练的数十亿参数基础模型不断涌现，但人形控制尚未显示出类似的缩放增益。目前的人形机器人神经控制器的规模仍然不大，只针对有限的行为，并且需要在少数 GPU 上进行数天的训练。我们证明，扩大模型容量、数据和计算量可以产生一个通用的人形控制器，能够创建自然且强大的全身运动。具体来说，我们将运动跟踪视为人形控制的一项自然且可扩展的任务，利用来自不同运动捕捉数据的密集监督来获取人体运动先验，而无需手动奖励工程。我们通过沿三个轴扩展来构建运动跟踪的基础模型：网络大小（从 1.2M 到 42M 参数）、数据集容量（超过 100M 帧、700 小时的高质量运动数据）和计算（9k GPU 小时）。除了展示规模的好处之外，我们还通过两种机制展示了我们模型的实用性：(1) 实时通用运动规划器，将运动跟踪与下游任务执行联系起来，实现自然和交互式控制；(2) 统一的令牌空间，支持各种运动输入接口，例如 VR 远程操作设备、人类视频和视觉语言动作 (VLA) 模型，所有这些都使用相同的策略。缩放运动跟踪表现出有利的特性：随着计算和数据多样性的增加，性能稳步提高，并且学习的表示泛化到看不见的运动，将大规模运动跟踪建立为人形控制的实用基础。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 15 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 15）。",
      "items": [
        {
          "title": "TianxingChen/Embodied-AI-Guide",
          "url": "https://github.com/TianxingChen/Embodied-AI-Guide",
          "snippet": "[Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "patrick-llgc/Learning-Deep-Learning",
          "url": "https://github.com/patrick-llgc/Learning-Deep-Learning",
          "snippet": "Paper reading notes on Deep Learning and Machine Learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "Dexmal/dexbotic",
          "url": "https://github.com/Dexmal/dexbotic",
          "snippet": "Dexbotic: Open-Source Vision-Language-Action Toolbox",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "IliaLarchenko/behavior-1k-solution",
          "url": "https://github.com/IliaLarchenko/behavior-1k-solution",
          "snippet": "1st place solution of 2025 BEHAVIOR Challenge",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "NVIDIA/Isaac-GR00T",
          "url": "https://github.com/NVIDIA/Isaac-GR00T",
          "snippet": "NVIDIA Isaac GR00T N1.6 -  A Foundation Model for Generalist Robots.",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "HCPLab-SYSU/Embodied_AI_Paper_List",
          "url": "https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List",
          "snippet": "[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 9
        },
        {
          "title": "OpenHelix-Team/VLA-Adapter",
          "url": "https://github.com/OpenHelix-Team/VLA-Adapter",
          "snippet": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model",
          "site": "github.com",
          "rank": 10
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 11
        },
        {
          "title": "BridgeVLA/BridgeVLA",
          "url": "https://github.com/BridgeVLA/BridgeVLA",
          "snippet": "✨✨【NeurIPS 2025】Official implementation of BridgeVLA",
          "site": "github.com",
          "rank": 12
        },
        {
          "title": "SalvatoreRa/ML-news-of-the-week",
          "url": "https://github.com/SalvatoreRa/ML-news-of-the-week",
          "snippet": "A collection of the the best ML and AI news every week (research, news, resources)",
          "site": "github.com",
          "rank": 13
        },
        {
          "title": "52CV/CVPR-2025-Papers",
          "url": "https://github.com/52CV/CVPR-2025-Papers",
          "snippet": "CVPR-2025-Papers",
          "site": "github.com",
          "rank": 14
        },
        {
          "title": "52CV/ECCV-2024-Papers",
          "url": "https://github.com/52CV/ECCV-2024-Papers",
          "snippet": "ECCV-2024-Papers",
          "site": "github.com",
          "rank": 15
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-11-10",
  "week_end": "2025-11-16",
  "last_updated": "2026-01-07"
}