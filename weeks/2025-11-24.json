{
  "generated_at": "2026-01-07T13:49:20.678831",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 30 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 30）。",
      "items": [
        {
          "title": "Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving",
          "url": "http://arxiv.org/abs/2511.19912v1",
          "snippet": "Vision-Language-Action (VLA) models have recently shown strong decision-making capabilities in autonomous driving. However, existing VLAs often struggle with achieving efficient inference and generalizing to novel autonomous vehicle configurations and driving scenarios. In this paper, we propose Reasoning-VLA, a general and fast action-generation VLA framework. The proposed model employs a set of learnable action queries, initialized via Gaussian sampling from ground-truth trajectories within the training corpus. These learnable queries interact with reasoning-enhanced vision-language features to generate continuous action trajectories in parallel. To promote robust generalization, we consolidate eight publicly available autonomous driving datasets into a standardized, Chain-of-Thought reasoning-based, and easy-to-use data format for model training. Leveraging both supervised learning and reinforcement learning fine-tuning, extensive empirical evaluations across multiple benchmarks demonstrate that Reasoning-VLA achieves state-of-the-art performance, superior generalization capability, and the excellent inference speed reported to date.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-11-25T04:40:11Z",
          "authors": [
            "Dapeng Zhang",
            "Zhenlong Yuan",
            "Zhangquan Chen",
            "Chih-Ting Liao",
            "Yinda Chen",
            "Fei Shen",
            "Qingguo Zhou",
            "Tat-Seng Chua"
          ],
          "arxiv_id": "2511.19912",
          "abstract": "Vision-Language-Action (VLA) models have recently shown strong decision-making capabilities in autonomous driving. However, existing VLAs often struggle with achieving efficient inference and generalizing to novel autonomous vehicle configurations and driving scenarios. In this paper, we propose Reasoning-VLA, a general and fast action-generation VLA framework. The proposed model employs a set of learnable action queries, initialized via Gaussian sampling from ground-truth trajectories within the training corpus. These learnable queries interact with reasoning-enhanced vision-language features to generate continuous action trajectories in parallel. To promote robust generalization, we consolidate eight publicly available autonomous driving datasets into a standardized, Chain-of-Thought reasoning-based, and easy-to-use data format for model training. Leveraging both supervised learning and reinforcement learning fine-tuning, extensive empirical evaluations across multiple benchmarks demonstrate that Reasoning-VLA achieves state-of-the-art performance, superior generalization capability, and the excellent inference speed reported to date.",
          "abstract_zh": "视觉-语言-动作（VLA）模型最近在自动驾驶方面表现出了强大的决策能力。然而，现有的 VLA 常常难以实现有效的推理并推广到新颖的自动驾驶车辆配置和驾驶场景。在本文中，我们提出 Reasoning-VLA，一种通用且快速的动作生成 VLA 框架。所提出的模型采用一组可学习的动作查询，通过从训练语料库内的地面真实轨迹进行高斯采样来初始化。这些可学习的查询与推理增强的视觉语言功能交互，以并行生成连续的动作轨迹。为了促进稳健的泛化，我们将八个公开的自动驾驶数据集整合为标准化、基于思想链推理且易于使用的数据格式，用于模型训练。利用监督学习和强化学习微调，跨多个基准的广泛实证评估表明，Reasoning-VLA 实现了迄今为止最先进的性能、卓越的泛化能力和出色的推理速度。"
        },
        {
          "title": "VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation",
          "url": "http://arxiv.org/abs/2511.21557v1",
          "snippet": "Vision Language Action models have significantly advanced general purpose robotic manipulation by harnessing large scale pretrained vision and language representations. Among existing approaches, a majority of current VLA systems employ parallel two finger grippers as their default end effectors. However, such grippers face inherent limitations in handling certain real world tasks such as wiping glass surfaces or opening drawers without handles due to insufficient contact area or lack of adhesion. To overcome these challenges, we present a low cost, integrated hardware design that combines a mechanical two finger gripper with a vacuum suction unit, enabling dual mode manipulation within a single end effector. Our system supports flexible switching or synergistic use of both modalities, expanding the range of feasible tasks. We validate the efficiency and practicality of our design within two state of the art VLA frameworks: DexVLA and Pi0. Experimental results demonstrate that with the proposed hybrid end effector, robots can successfully perform multiple complex tasks that are infeasible for conventional two finger grippers alone. All hardware designs and controlling systems will be released.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-11-26T16:29:24Z",
          "authors": [
            "Hui Zhou",
            "Siyuan Huang",
            "Minxing Li",
            "Hao Zhang",
            "Lue Fan",
            "Shaoshuai Shi"
          ],
          "arxiv_id": "2511.21557",
          "abstract": "Vision Language Action models have significantly advanced general purpose robotic manipulation by harnessing large scale pretrained vision and language representations. Among existing approaches, a majority of current VLA systems employ parallel two finger grippers as their default end effectors. However, such grippers face inherent limitations in handling certain real world tasks such as wiping glass surfaces or opening drawers without handles due to insufficient contact area or lack of adhesion. To overcome these challenges, we present a low cost, integrated hardware design that combines a mechanical two finger gripper with a vacuum suction unit, enabling dual mode manipulation within a single end effector. Our system supports flexible switching or synergistic use of both modalities, expanding the range of feasible tasks. We validate the efficiency and practicality of our design within two state of the art VLA frameworks: DexVLA and Pi0. Experimental results demonstrate that with the proposed hybrid end effector, robots can successfully perform multiple complex tasks that are infeasible for conventional two finger grippers alone. All hardware designs and controlling systems will be released.",
          "abstract_zh": "视觉语言动作模型通过利用大规模预训练视觉和语言表示，显着改进了通用机器人操作。在现有的方法中，大多数当前的 VLA 系统都采用平行的两个手指夹具作为默认的末端执行器。然而，由于接触面积不足或缺乏粘附力，这种夹具在处理某些现实世界任务时面临固有的局限性，例如擦拭玻璃表面或打开没有把手的抽屉。为了克服这些挑战，我们提出了一种低成本的集成硬件设计，将机械两指夹具与真空抽吸装置相结合，从而在单个末端执行器内实现双模式操作。我们的系统支持两种模式的灵活切换或协同使用，扩大了可行任务的范围。我们在两个最先进的 VLA 框架中验证了我们设计的效率和实用性：DexVLA 和 Pi0。实验结果表明，利用所提出的混合末端执行器，机器人可以成功地执行多种复杂的任务，而这些任务仅靠传统的两指夹持器是无法完成的。所有硬件设计和控制系统都将被发布。"
        },
        {
          "title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation",
          "url": "http://arxiv.org/abs/2511.18950v1",
          "snippet": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-11-24T10:06:41Z",
          "authors": [
            "Juntao Gao",
            "Feiyang Ye",
            "Jing Zhang",
            "Wenjing Qian"
          ],
          "arxiv_id": "2511.18950",
          "abstract": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.",
          "abstract_zh": "视觉-语言-动作（VLA）模型已成为嵌入式人工智能中的强大范例。然而，处理冗余视觉标记的大量计算开销仍然是实时机器人部署的关键瓶颈。虽然标准的标记修剪技术可以缓解这种情况，但这些与任务无关的方法很难保留任务关键的视觉信息。为了应对这一挑战，同时保留整体上下文和细粒度细节以实现精确操作，我们提出了 Compressor-VLA，这是一种新颖的混合指令条件令牌压缩框架，旨在对 VLA 模型中的视觉信息进行高效、面向任务的压缩。所提出的 Compressor-VLA 框架由两个令牌压缩模块组成：一个语义任务压缩器（STC），用于提取整体的、与任务相关的上下文；以及一个空间细化压缩器（SRC），用于保留细粒度的空间细节。这种压缩由自然语言指令动态调节，允许自适应压缩与任务相关的视觉信息。实验上，广泛的评估表明，与基准相比，Compressor-VLA 在 LIBERO 基准上实现了具有竞争力的成功率，同时将 FLOP 减少了 59%，并将视觉标记计数减少了 3 倍以上。双臂机器人平台上的真实机器人部署验证了该模型的仿真到真实的可移植性和实际适用性。此外，定性分析表明，我们的指导指导动态地将模型的感知焦点转向与任务相关的对象，从而验证了我们方法的有效性。"
        },
        {
          "title": "Distracted Robot: How Visual Clutter Undermine Robotic Manipulation",
          "url": "http://arxiv.org/abs/2511.22780v1",
          "snippet": "In this work, we propose an evaluation protocol for examining the performance of robotic manipulation policies in cluttered scenes. Contrary to prior works, we approach evaluation from a psychophysical perspective, therefore we use a unified measure of clutter that accounts for environmental factors as well as the distractors quantity, characteristics, and arrangement. Using this measure, we systematically construct evaluation scenarios in both hyper-realistic simulation and real-world and conduct extensive experimentation on manipulation policies, in particular vision-language-action (VLA) models. Our experiments highlight the significant impact of scene clutter, lowering the performance of the policies, by as much as 34% and show that despite achieving similar average performance across the tasks, different VLA policies have unique vulnerabilities and a relatively low agreement on success scenarios. We further show that our clutter measure is an effective indicator of performance degradation and analyze the impact of distractors in terms of their quantity and occluding influence. At the end, we show that finetuning on enhanced data, although effective, does not equally remedy all negative impacts of clutter on performance.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-11-27T22:13:13Z",
          "authors": [
            "Amir Rasouli",
            "Montgomery Alban",
            "Sajjad Pakdamansavoji",
            "Zhiyuan Li",
            "Zhanguang Zhang",
            "Aaron Wu",
            "Xuan Zhao"
          ],
          "arxiv_id": "2511.22780",
          "abstract": "In this work, we propose an evaluation protocol for examining the performance of robotic manipulation policies in cluttered scenes. Contrary to prior works, we approach evaluation from a psychophysical perspective, therefore we use a unified measure of clutter that accounts for environmental factors as well as the distractors quantity, characteristics, and arrangement. Using this measure, we systematically construct evaluation scenarios in both hyper-realistic simulation and real-world and conduct extensive experimentation on manipulation policies, in particular vision-language-action (VLA) models. Our experiments highlight the significant impact of scene clutter, lowering the performance of the policies, by as much as 34% and show that despite achieving similar average performance across the tasks, different VLA policies have unique vulnerabilities and a relatively low agreement on success scenarios. We further show that our clutter measure is an effective indicator of performance degradation and analyze the impact of distractors in terms of their quantity and occluding influence. At the end, we show that finetuning on enhanced data, although effective, does not equally remedy all negative impacts of clutter on performance.",
          "abstract_zh": "在这项工作中，我们提出了一种评估协议，用于检查机器人操纵策略在杂乱场景中的性能。与之前的工作相反，我们从心理物理学的角度进行评估，因此我们使用统一的杂乱测量方法，考虑环境因素以及干扰因素的数量、特征和排列。使用这种方法，我们在超现实模拟和现实世界中系统地构建评估场景，并对操纵策略，特别是视觉语言动作（VLA）模型进行广泛的实验。我们的实验强调了场景混乱的显着影响，使策略的性能降低了 34% 之多，并表明，尽管在各个任务中实现了相似的平均性能，但不同的 VLA 策略具有独特的漏洞，并且在成功场景上的一致性相对较低。我们进一步表明，我们的杂波测量是性能下降的有效指标，并根据干扰物的数量和遮挡影响来分析干扰物的影响。最后，我们表明，对增强数据进行微调虽然有效，但并不能同等地补救杂波对性能的所有负面影响。"
        },
        {
          "title": "Improving Robotic Manipulation Robustness via NICE Scene Surgery",
          "url": "http://arxiv.org/abs/2511.22777v1",
          "snippet": "Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets.\n  Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-11-27T22:02:02Z",
          "authors": [
            "Sajjad Pakdamansavoji",
            "Mozhgan Pourkeshavarz",
            "Adam Sigal",
            "Zhiyuan Li",
            "Rui Heng Yang",
            "Amir Rasouli"
          ],
          "arxiv_id": "2511.22777",
          "abstract": "Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets. Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.",
          "abstract_zh": "在现实世界中，学习用于机器人操作的强大视觉运动策略仍然是一个挑战，其中视觉干扰因素会显着降低性能和安全性。在这项工作中，我们提出了一个有效且可扩展的框架，即上下文增强的自然修复（NICE）。我们的方法通过使用现有演示构建新体验来增加视觉多样性，从而最大限度地减少模仿学习中的分布外（OOD）差距。通过利用图像生成框架和大型语言模型，NICE 执行三种编辑操作：对象替换、重新设计样式以及删除分散注意力的（非目标）对象。这些变化保留了空间关系，而不妨碍目标对象，并保持了动作标签的一致性。与以前的方法不同，NICE 不需要额外的机器人数据收集、模拟器访问或自定义模型训练，因此可以轻松应用于现有的机器人数据集。使用真实世界的场景，我们展示了我们的框架在生成照片般逼真的场景增强方面的能力。对于下游任务，我们使用 NICE 数据来微调用于空间可供性预测的视觉语言模型（VLM）和用于对象操作的视觉语言动作（VLA）策略。我们的评估表明，NICE 成功地最大限度地减少了 OOD 差距，从而使高度混乱场景中可供性预测的准确性提高了 20% 以上。对于操纵任务，在充满不同数量干扰物的环境中进行测试时，成功率平均提高 11%。此外，我们还表明，我们的方法提高了视觉鲁棒性，将目标混乱降低了 6%，并通过将碰撞率降低了 7% 来增强了安全性。"
        },
        {
          "title": "AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention",
          "url": "http://arxiv.org/abs/2511.18960v2",
          "snippet": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-11-24T10:22:28Z",
          "authors": [
            "Lei Xiao",
            "Jifeng Li",
            "Juntao Gao",
            "Feiyang Ye",
            "Yan Jin",
            "Jingjing Qian",
            "Jing Zhang",
            "Yong Wu",
            "Xiaoyuan Yu"
          ],
          "arxiv_id": "2511.18960",
          "abstract": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在具体的人工智能任务中表现出了卓越的能力。然而，现有的 VLA 模型通常基于视觉语言模型 (VLM) 构建，通常在每个时间步独立处理密集的视觉输入。这种方法将任务隐式建模为马尔可夫决策过程 (MDP)。然而，这种与历史无关的设计对于动态顺序决策中的有效视觉标记处理而言并不是最佳的，因为它无法利用历史背景。为了解决这个限制，我们从部分可观察马尔可夫决策过程（POMDP）的角度重新表述了这个问题，并提出了一个名为 AVA-VLA 的新框架。受到 POMDP 的启发，行动的生成应该以信念状态为条件。AVA-VLA 引入主动视觉注意（AVA）来动态调节视觉处理。它通过利用循环状态来实现这一点，循环状态是从先前决策步骤得出的代理信念状态的神经近似。具体来说，AVA 模块使用循环状态来计算软权重，以根据其历史上下文主动处理与任务相关的视觉标记。综合评估表明，AVA-VLA 在流行的机器人基准测试中实现了最先进的性能，包括 LIBERO 和 CALVIN。此外，双臂机器人平台上的实际部署验证了该框架的实际适用性和强大的模拟到真实的可迁移性。"
        },
        {
          "title": "Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention",
          "url": "http://arxiv.org/abs/2511.22555v1",
          "snippet": "Vision-Language-Action (VLA) models have enabled notable progress in general-purpose robotic manipulation, yet their learned policies often exhibit variable execution quality. We attribute this variability to the mixed-quality nature of human demonstrations, where the implicit principles that govern how actions should be carried out are only partially satisfied. To address this challenge, we introduce the LIBERO-Elegant benchmark with explicit criteria for evaluating execution quality. Using these criteria, we develop a decoupled refinement framework that improves execution quality without modifying or retraining the base VLA policy. We formalize Elegant Execution as the satisfaction of Implicit Task Constraints (ITCs) and train an Elegance Critic via offline Calibrated Q-Learning to estimate the expected quality of candidate actions. At inference time, a Just-in-Time Intervention (JITI) mechanism monitors critic confidence and intervenes only at decision-critical moments, providing selective, on-demand refinement. Experiments on LIBERO-Elegant and real-world manipulation tasks show that the learned Elegance Critic substantially improves execution quality, even on unseen tasks. The proposed model enables robotic control that values not only whether tasks succeed, but also how they are performed.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-11-27T15:45:25Z",
          "authors": [
            "Yanbo Mao",
            "Jianlong Fu",
            "Ruoxuan Zhang",
            "Hongxia Xie",
            "Meibao Yao"
          ],
          "arxiv_id": "2511.22555",
          "abstract": "Vision-Language-Action (VLA) models have enabled notable progress in general-purpose robotic manipulation, yet their learned policies often exhibit variable execution quality. We attribute this variability to the mixed-quality nature of human demonstrations, where the implicit principles that govern how actions should be carried out are only partially satisfied. To address this challenge, we introduce the LIBERO-Elegant benchmark with explicit criteria for evaluating execution quality. Using these criteria, we develop a decoupled refinement framework that improves execution quality without modifying or retraining the base VLA policy. We formalize Elegant Execution as the satisfaction of Implicit Task Constraints (ITCs) and train an Elegance Critic via offline Calibrated Q-Learning to estimate the expected quality of candidate actions. At inference time, a Just-in-Time Intervention (JITI) mechanism monitors critic confidence and intervenes only at decision-critical moments, providing selective, on-demand refinement. Experiments on LIBERO-Elegant and real-world manipulation tasks show that the learned Elegance Critic substantially improves execution quality, even on unseen tasks. The proposed model enables robotic control that values not only whether tasks succeed, but also how they are performed.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在通用机器人操作方面取得了显着进展，但其学习策略通常表现出不同的执行质量。我们将这种可变性归因于人类演示的质量参差不齐，其中控制如何执行行动的隐含原则仅得到部分满足。为了应对这一挑战，我们引入了 LIBERO-Elegant 基准，其中包含评估执行质量的明确标准。使用这些标准，我们开发了一个解耦的细化框架，该框架可以提高执行质量，而无需修改或重新训练基本 VLA 策略。我们将优雅执行形式化为隐式任务约束 (ITC) 的满足，并通过离线校准 Q 学习来训练优雅批评家，以估计候选操作的预期质量。在推理时，即时干预 (JITI) 机制会监控批评家的信心，并仅在决策关键时刻进行干预，从而提供选择性的按需改进。对 LIBERO-Elegant 和现实世界操作任务的实验表明，学习的 Elegance Critic 大大提高了执行质量，即使是在看不见的任务上也是如此。所提出的模型使机器人控制不仅重视任务是否成功，而且重视任务的执行方式。"
        },
        {
          "title": "DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving",
          "url": "http://arxiv.org/abs/2511.20720v1",
          "snippet": "Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-11-25T07:00:26Z",
          "authors": [
            "Haibo HU",
            "Lianming Huang",
            "Nan Guan",
            "Chun Jason Xue"
          ],
          "arxiv_id": "2511.20720",
          "abstract": "Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.",
          "abstract_zh": "视觉语言动作 (VLA) 模型统一了自动驾驶的感知、推理和轨迹生成，但由于深度变压器堆栈而存在显着的推理延迟。我们提出了 DeeAD，这是一种免训练、以行动为导向的提前退出框架，可通过评估中间轨迹的物理可行性来加速 VLA 规划。当预测轨迹与轻量级规划先验（例如导航或低精度规划）在可容忍偏差（<2m）内一致时，DeeAD 不依赖置信度分数，而是终止推理。为了提高效率，我们引入了一个多跳控制器，它根据分数的变化率自适应地跳过冗余层。DeeAD 集成到现有的 VLA 模型（例如 ORION）中，无需重新训练。Bench2Drive 基准测试表明，变压器层稀疏性高达 28%，延迟减少了 29%，同时保持了规划质量和安全性。"
        },
        {
          "title": "CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving",
          "url": "http://arxiv.org/abs/2511.22532v1",
          "snippet": "Vision-Language-Action (VLA) models have recently attracted growing attention in end-to-end autonomous driving for their strong reasoning capabilities and rich world knowledge. However, existing VLAs often suffer from limited numerical reasoning ability and overly simplified input-output mappings, which hinder their performance in complex driving scenarios requiring step-by-step causal reasoning. To address these challenges, we propose CoT4AD, a novel VLA framework that introduces Chain-of-Thought (CoT) reasoning for autonomous driving to enhance both numerical and causal reasoning in Vision-Language Models (VLMs). CoT4AD integrates visual observations and language instructions to perform semantic reasoning, scene understanding, and trajectory planning. During training, it explicitly models a perception-question-prediction-action CoT to align the reasoning space with the action space across multiple driving tasks. During inference, it performs implicit CoT reasoning to enable consistent numerical reasoning and robust decision-making in dynamic environments. Extensive experiments on both real-world and simulated benchmarks, including nuScenes and Bench2Drive, demonstrate that CoT4AD achieves state-of-the-art performance in both open-loop and closed-loop evaluations. Code will be released upon paper acceptance.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-11-27T15:13:13Z",
          "authors": [
            "Zhaohui Wang",
            "Tengbo Yu",
            "Hao Tang"
          ],
          "arxiv_id": "2511.22532",
          "abstract": "Vision-Language-Action (VLA) models have recently attracted growing attention in end-to-end autonomous driving for their strong reasoning capabilities and rich world knowledge. However, existing VLAs often suffer from limited numerical reasoning ability and overly simplified input-output mappings, which hinder their performance in complex driving scenarios requiring step-by-step causal reasoning. To address these challenges, we propose CoT4AD, a novel VLA framework that introduces Chain-of-Thought (CoT) reasoning for autonomous driving to enhance both numerical and causal reasoning in Vision-Language Models (VLMs). CoT4AD integrates visual observations and language instructions to perform semantic reasoning, scene understanding, and trajectory planning. During training, it explicitly models a perception-question-prediction-action CoT to align the reasoning space with the action space across multiple driving tasks. During inference, it performs implicit CoT reasoning to enable consistent numerical reasoning and robust decision-making in dynamic environments. Extensive experiments on both real-world and simulated benchmarks, including nuScenes and Bench2Drive, demonstrate that CoT4AD achieves state-of-the-art performance in both open-loop and closed-loop evaluations. Code will be released upon paper acceptance.",
          "abstract_zh": "视觉-语言-动作（VLA）模型最近因其强大的推理能力和丰富的世界知识而在端到端自动驾驶领域引起了越来越多的关注。然而，现有的 VLA 往往受到有限的数值推理能力和过于简化的输入输出映射的影响，这阻碍了它们在需要逐步因果推理的复杂驾驶场景中的性能。为了应对这些挑战，我们提出了 CoT4AD，这是一种新颖的 VLA 框架，它引入了自动驾驶的思想链 (CoT) 推理，以增强视觉语言模型 (VLM) 中的数值和因果推理。CoT4AD 集成了视觉观察和语言指令来执行语义推理、场景理解和轨迹规划。在训练过程中，它明确地建模了感知-问题-预测-动作 CoT，以将多个驾驶任务中的推理空间与动作空间保持一致。在推理过程中，它执行隐式 CoT 推理，以在动态环境中实现一致的数值推理和稳健的决策。对真实世界和模拟基准（包括 nuScenes 和 Bench2Drive）的大量实验表明，CoT4AD 在开环和闭环评估中均实现了最先进的性能。代码将在论文接受后发布。"
        },
        {
          "title": "ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation",
          "url": "http://arxiv.org/abs/2512.02013v1",
          "snippet": "Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the \"how\" process from the \"what\" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-12-01T18:59:50Z",
          "authors": [
            "Chenyang Gu",
            "Jiaming Liu",
            "Hao Chen",
            "Runzhong Huang",
            "Qingpo Wuwu",
            "Zhuoyang Liu",
            "Xiaoqi Li",
            "Ying Li",
            "Renrui Zhang",
            "Peng Jia",
            "Pheng-Ann Heng",
            "Shanghang Zhang"
          ],
          "arxiv_id": "2512.02013",
          "abstract": "Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the \"how\" process from the \"what\" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.",
          "abstract_zh": "视觉-语言-动作（VLA）模型最近出现，展示了机器人场景理解和操作的强大通用性。然而，当面临需要明确目标状态的长期任务时，例如乐高组装或对象重新排列，现有的 VLA 模型仍然面临着协调高层规划与精确操作的挑战。因此，我们的目标是赋予 VLA 模型从“什么”结果推断“如何”过程的能力，将目标状态转化为可执行的过程。在本文中，我们介绍了 ManualVLA，这是一个基于 Mixture-of-Transformers (MoT) 架构构建的统一 VLA 框架，可实现多模式手动生成和操作执行之间的连贯协作。与之前直接将感官输入映射到动作的 VLA 模型不同，我们首先为 ManualVLA 配备了规划专家，该专家可以生成由图像、位置提示和文本指令组成的中间手册。在这些多模式手册的基础上，我们设计了一个手动思维链（ManualCoT）推理过程，将它们输入到行动专家中，其中每个手动步骤提供了明确的控制条件，而其潜在表示为准确操作提供了隐式指导。为了减轻数据收集的负担，我们开发了基于 3D Gaussian Splatting 的高保真数字孪生工具包，它可以自动生成用于规划专家培训的手动数据。ManualVLA 展示了强大的现实性能，在乐高组装和对象重新排列任务上的平均成功率比之前的分层 SOTA 基线高出 32%。"
        },
        {
          "title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation",
          "url": "http://arxiv.org/abs/2512.01801v3",
          "snippet": "We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundation models to specialize into reliable real-world experts.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-12-01T15:33:59Z",
          "authors": [
            "Yunfei Li",
            "Xiao Ma",
            "Jiafeng Xu",
            "Yu Cui",
            "Zhongren Cui",
            "Zhigang Han",
            "Liqun Huang",
            "Tao Kong",
            "Yuxiao Liu",
            "Hao Niu",
            "Wanli Peng",
            "Jingchao Qiao",
            "Zeyu Ren",
            "Haixin Shi",
            "Zhi Su",
            "Jiawen Tian",
            "Yuyang Xiao",
            "Shenyu Zhang",
            "Liwei Zheng",
            "Hang Li",
            "Yonghui Wu"
          ],
          "arxiv_id": "2512.01801",
          "abstract": "We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundation models to specialize into reliable real-world experts.",
          "abstract_zh": "我们提出了 GR-RL，一种机器人学习框架，它将通用视觉语言动作（VLA）策略转变为长视界灵巧操作的高能力专家。假设人类示范的最优性是现有 VLA 政策的核心。然而，我们声称，在高度灵巧和精确的操作任务中，人类的演示是嘈杂且次优的。GR-RL 提出了一个多阶段训练管道，通过强化学习来过滤、增强和强化演示。首先，GR-RL 学习视觉语言条件下的任务进度，过滤演示轨迹，只保留对进度有积极贡献的转换。具体来说，我们表明，通过直接应用具有稀疏奖励的离线强化学习，所得的 $Q$ 值可以被视为稳健的进度函数。接下来，我们引入形态对称增强，它极大地提高了 GR-RL 的泛化能力和性能。最后，为了更好地调整 VLA 策略与其部署行为以实现高精度控制，我们通过学习潜在空间噪声预测器来执行在线强化学习。据我们所知，通过这条流程，GR-RL 是第一个基于学习的策略，可以通过将鞋带穿过多个孔眼来自动系鞋带，成功率达到 83.3%，这项任务需要长视野推理、毫米级精度和兼容的软体交互。我们希望 GR-RL 朝着使通用机器人基础模型专门化为可靠的现实世界专家迈出了一步。"
        },
        {
          "title": "When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2511.21192v2",
          "snippet": "Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-11-26T09:16:32Z",
          "authors": [
            "Hui Lu",
            "Yi Yu",
            "Yiming Yang",
            "Chenyu Yi",
            "Qixin Zhang",
            "Bingquan Shen",
            "Alex C. Kot",
            "Xudong Jiang"
          ],
          "arxiv_id": "2511.21192",
          "abstract": "Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.",
          "abstract_zh": "视觉-语言-动作（VLA）模型很容易受到对抗性攻击，但通用和可转移的攻击仍未得到充分探索，因为大多数现有补丁过度适合单一模型并在黑盒设置中失败。为了解决这一差距，我们对未知架构、微调变体和模拟到真实转换下的 VLA 驱动机器人的通用、可转移对抗补丁进行了系统研究。我们引入了UPA-RFAS（通过鲁棒特征、注意力和语义的通用补丁攻击），这是一个统一的框架，可以在共享特征空间中学习单个物理补丁，同时促进跨模型迁移。UPA-RFAS 结合了 (i) 具有 $\\ell_1$ 偏差先验和排斥性 InfoNCE 损失的特征空间目标，以引起可转移的表示偏移，(ii) 鲁棒性增强的两阶段最小-最大过程，其中内循环学习不可见的样本扰动，外循环针对这个硬化邻域优化通用补丁，以及 (iii) 两个 VLA 特定损失：补丁注意力优势劫持文本$\\to$视觉注意力和修补语义错位以导致没有标签的图像文本不匹配。跨不同 VLA 模型、操作套件和物理执行的实验表明，UPA-RFAS 能够一致地跨模型、任务和视点进行传输，暴露出实用的基于补丁的攻击面，并为未来的防御建立了强大的基线。"
        },
        {
          "title": "GigaWorld-0: World Models as Data Engine to Empower Embodied AI",
          "url": "http://arxiv.org/abs/2511.19861v2",
          "snippet": "World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-11-25T03:00:42Z",
          "authors": [
            "GigaWorld Team",
            "Angen Ye",
            "Boyuan Wang",
            "Chaojun Ni",
            "Guan Huang",
            "Guosheng Zhao",
            "Haoyun Li",
            "Jiagang Zhu",
            "Kerui Li",
            "Mengyuan Xu",
            "Qiuping Deng",
            "Siting Wang",
            "Wenkang Qin",
            "Xinze Chen",
            "Xiaofeng Wang",
            "Yankai Wang",
            "Yu Cao",
            "Yifan Chang",
            "Yuan Xu",
            "Yun Ye",
            "Yang Wang",
            "Yukun Zhou",
            "Zhengyuan Zhang",
            "Zhehao Dong",
            "Zheng Zhu"
          ],
          "arxiv_id": "2511.19861",
          "abstract": "World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.",
          "abstract_zh": "世界模型正在成为可扩展、数据高效的具体人工智能的基础范例。在这项工作中，我们提出了 GigaWorld-0，一个统一的世界模型框架，明确设计为视觉-语言-动作（VLA）学习的数据引擎。GigaWorld-0 集成了两个协同组件： GigaWorld-0-Video，它利用大规模视频生成，在外观、相机视点和动作语义的细粒度控制下产生多样化、纹理丰富且时间连贯的体现序列；GigaWorld-0-3D，它结合了 3D 生成建模、3D 高斯喷射重建、物理可微分系统识别和可执行运动规划，以确保几何一致性和物理真实性。它们的联合优化能够实现视觉上引人注目、空间连贯、物理上合理且指令一致的具体交互数据的可扩展合成。通过我们高效的 GigaTrain 框架，大规模训练变得可行，该框架利用 FP8 精度和稀疏注意力来大幅减少内存和计算需求。我们进行的综合评估表明，GigaWorld-0在多个维度上生成了高质量、多样化、可控的数据。至关重要的是，在 GigaWorld-0 生成的数据上训练的 VLA 模型（例如 GigaBrain-0）实现了强大的现实世界性能，显着提高了物理机器人的泛化能力和任务成功率，而无需在训练期间进行任何现实世界交互。"
        },
        {
          "title": "Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation",
          "url": "http://arxiv.org/abs/2511.19859v1",
          "snippet": "Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\\%, 9.6\\% and 12.1\\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-11-25T02:43:20Z",
          "authors": [
            "Xiangkai Ma",
            "Lekai Xing",
            "Han Zhang",
            "Wenzhong Li",
            "Sanglu Lu"
          ],
          "arxiv_id": "2511.19859",
          "abstract": "Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\\%, 9.6\\% and 12.1\\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.",
          "abstract_zh": "基于思想链（CoT）构建的视觉-语言-动作（VLA）模型由于其显着的感知理解能力，在推进通用机器人代理方面取得了显着的成功。最近，由于纯文本 CoT 难以在复杂的空间环境中充分捕捉场景细节，因此一种非常有前途的策略是利用视觉先验来指导机器人动作生成。然而，这些策略面临两个固有的挑战：（i）视觉观察和低级动作之间的模态差距，以及（ii）由于视觉预测和动作生成之间的目标相互竞争而导致训练不稳定。为了应对这些挑战，我们提出了一种视觉集成轨迹对齐（VITA）框架，该框架学习视觉和动作的共享离散潜在空间，从而实现感知和运动控制的联合建模。VITA 引入了隐式视觉 CoT：自回归生成的标记同时解码为未来帧预测和机器人动作，从而将视觉动态内化为运动规划的归纳偏差。在模拟和现实环境中进行的大量实验展示了最先进的性能。VITA 比 CALVIN、LIBERO 和 SimplerEnv 的现有基线提高了 14.5\\%、9.6\\% 和 12.1\\%。此外，VITA 在六项现实世界任务中的平均成功率达到 80.5%，展示了其作为通用机器人操作模型的潜力。"
        },
        {
          "title": "Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations",
          "url": "http://arxiv.org/abs/2511.22697v1",
          "snippet": "Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.",
          "site": "arxiv.org",
          "rank": 15,
          "published": "2025-11-27T18:50:21Z",
          "authors": [
            "Chancharik Mitra",
            "Yusen Luo",
            "Raj Saravanan",
            "Dantong Niu",
            "Anirudh Pai",
            "Jesse Thomason",
            "Trevor Darrell",
            "Abrar Anwar",
            "Deva Ramanan",
            "Roei Herzig"
          ],
          "arxiv_id": "2511.22697",
          "abstract": "Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.",
          "abstract_zh": "视觉语言动作（VLA）模型有望将视觉语言模型（VLM）的巨大成功扩展到机器人领域。然而，与视觉语言领域的 VLM 不同，机器人的 VLA 需要进行微调，以应对不同的物理因素，例如机器人实施例、环境特征和每个任务的空间关系。现有的微调方法缺乏特异性，无论任务的视觉、语言和物理特征如何，都采用相同的参数集。受神经科学功能特异性的启发，我们假设微调特定于给定任务的稀疏模型表示更为有效。在这项工作中，我们介绍了机器人转向，这是一种基于机械可解释性的微调方法，利用少量演示来识别和选择性地微调特定于任务的注意力头，使其与机器人任务的物理、视觉和语言要求保持一致。通过使用 Franka Emika 机器人手臂进行全面的机器人评估，我们证明了机器人转向优于 LoRA，同时在任务变化下实现了卓越的鲁棒性，降低了计算成本，并增强了使 VLA 适应不同机器人任务的可解释性。"
        },
        {
          "title": "RobotSeg: A Model and Dataset for Segmenting Robots in Image and Video",
          "url": "http://arxiv.org/abs/2511.22950v1",
          "snippet": "Accurate robot segmentation is a fundamental capability for robotic perception. It enables precise visual servoing for VLA systems, scalable robot-centric data augmentation, accurate real-to-sim transfer, and reliable safety monitoring in dynamic human-robot environments. Despite the strong capabilities of modern segmentation models, surprisingly it remains challenging to segment robots. This is due to robot embodiment diversity, appearance ambiguity, structural complexity, and rapid shape changes. Embracing these challenges, we introduce RobotSeg, a foundation model for robot segmentation in image and video. RobotSeg is built upon the versatile SAM 2 foundation model but addresses its three limitations for robot segmentation, namely the lack of adaptation to articulated robots, reliance on manual prompts, and the need for per-frame training mask annotations, by introducing a structure-enhanced memory associator, a robot prompt generator, and a label-efficient training strategy. These innovations collectively enable a structure-aware, automatic, and label-efficient solution. We further construct the video robot segmentation (VRS) dataset comprising over 2.8k videos (138k frames) with diverse robot embodiments and environments. Extensive experiments demonstrate that RobotSeg achieves state-of-the-art performance on both images and videos, establishing a strong foundation for future advances in robot perception.",
          "site": "arxiv.org",
          "rank": 16,
          "published": "2025-11-28T07:51:02Z",
          "authors": [
            "Haiyang Mei",
            "Qiming Huang",
            "Hai Ci",
            "Mike Zheng Shou"
          ],
          "arxiv_id": "2511.22950",
          "abstract": "Accurate robot segmentation is a fundamental capability for robotic perception. It enables precise visual servoing for VLA systems, scalable robot-centric data augmentation, accurate real-to-sim transfer, and reliable safety monitoring in dynamic human-robot environments. Despite the strong capabilities of modern segmentation models, surprisingly it remains challenging to segment robots. This is due to robot embodiment diversity, appearance ambiguity, structural complexity, and rapid shape changes. Embracing these challenges, we introduce RobotSeg, a foundation model for robot segmentation in image and video. RobotSeg is built upon the versatile SAM 2 foundation model but addresses its three limitations for robot segmentation, namely the lack of adaptation to articulated robots, reliance on manual prompts, and the need for per-frame training mask annotations, by introducing a structure-enhanced memory associator, a robot prompt generator, and a label-efficient training strategy. These innovations collectively enable a structure-aware, automatic, and label-efficient solution. We further construct the video robot segmentation (VRS) dataset comprising over 2.8k videos (138k frames) with diverse robot embodiments and environments. Extensive experiments demonstrate that RobotSeg achieves state-of-the-art performance on both images and videos, establishing a strong foundation for future advances in robot perception.",
          "abstract_zh": "准确的机器人分割是机器人感知的基本能力。它能够为 VLA 系统提供精确的视觉伺服、可扩展的以机器人为中心的数据增强、准确的实模传输以及动态人机环境中的可靠安全监控。尽管现代分割模型功能强大，但令人惊讶的是，分割机器人仍然具有挑战性。这是由于机器人实施方式的多样性、外观的模糊性、结构的复杂性和快速的形状变化。面对这些挑战，我们推出了 RobotSeg，这是图像和视频中机器人分割的基础模型。RobotSeg 基于多功能 SAM 2 基础模型构建，但通过引入结构增强型记忆关联器、机器人提示生成器和标签高效训练策略，解决了机器人分割的三个局限性，即缺乏对铰接式机器人的适应、依赖手动提示以及需要每帧训练掩模注释。这些创新共同实现了结构感知、自动化和标签高效的解决方案。我们进一步构建了视频机器人分割（VRS）数据集，其中包含超过 2.8k 个视频（138k 帧），具有不同的机器人实施例和环境。大量实验表明，RobotSeg 在图像和视频方面均实现了最先进的性能，为机器人感知的未来发展奠定了坚实的基础。"
        },
        {
          "title": "$\\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion",
          "url": "http://arxiv.org/abs/2511.21542v1",
          "snippet": "Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.",
          "site": "arxiv.org",
          "rank": 17,
          "published": "2025-11-26T16:14:20Z",
          "authors": [
            "Zhihao Zhan",
            "Jiaying Zhou",
            "Likui Zhang",
            "Qinhan Lv",
            "Hao Liu",
            "Jusheng Zhang",
            "Weizheng Li",
            "Ziliang Chen",
            "Tianshui Chen",
            "Keze Wang",
            "Liang Lin",
            "Guangrun Wang"
          ],
          "arxiv_id": "2511.21542",
          "abstract": "Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型通过集成视觉感知、语言理解和控制生成，为机器人操作提供统一的框架。然而，现有的 VLA 模型仍然难以泛化不同的任务、场景和摄像机视点，并且经常产生粗糙或不稳定的动作。我们引入 E0，一个连续离散扩散框架，它将动作生成表示为对量化动作标记的迭代去噪。与连续扩散策略相比，E0 具有两个关键优势：（1）离散动作标记与预训练的 VLM/VLA 主干的符号结构自然对齐，从而实现更强的语义调节；2.离散扩散与现实世界机器人控制的真实量化性质相匹配，其硬件约束（例如编码器分辨率、控制频率、驱动延迟）本质上离散连续信号，因此受益于贝叶斯最优降噪器，该降噪器可以对正确的离散动作分布进行建模，从而实现更强的泛化。与离散自回归和基于掩码的离散扩散模型相比，E0 支持更大、更细粒度的动作词汇，并避免了基于掩码的损坏引入的分布不匹配，从而产生更准确的细粒度动作控制。我们进一步引入了球面视点扰动增强方法，以提高相机移动的鲁棒性，而无需额外的数据。在 LIBERO、VLABench 和 ManiSkill 上进行的实验表明，E0 在 14 个不同的环境中实现了最先进的性能，平均比强大的基线高出 10.7%。对 Franka 臂的真实世界评估证实，E0 提供精确、稳健且可转移的操作，将离散扩散确立为可推广的 VLA 策略学习的一个有前途的方向。"
        },
        {
          "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
          "url": "http://arxiv.org/abs/2512.01715v1",
          "snippet": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.",
          "site": "arxiv.org",
          "rank": 18,
          "published": "2025-12-01T14:21:15Z",
          "authors": [
            "Wanpeng Zhang",
            "Ye Wang",
            "Hao Luo",
            "Haoqi Yuan",
            "Yicheng Feng",
            "Sipeng Zheng",
            "Qin Jin",
            "Zongqing Lu"
          ],
          "arxiv_id": "2512.01715",
          "abstract": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.",
          "abstract_zh": "经过流匹配训练的视觉-语言-动作 (VLA) 模型在机器人操作任务中表现出了令人印象深刻的能力。然而，它们的性能通常会在分布转移和复杂的多步骤任务中下降，这表明学习到的表示可能无法稳健地捕获与任务相关的语义。我们引入 DiG-Flow，这是一个通过几何正则化增强 VLA 鲁棒性的原则框架。我们的主要见解是，观察和动作嵌入之间的分布差异提供了有意义的几何信号：较低的运输成本表明兼容的表示，而较高的成本表明潜在的错位。DiG-Flow 计算观察和动作嵌入的经验分布之间的差异度量，通过单调函数将其映射到调制权重，并在流匹配之前对观察嵌入应用残差更新。至关重要的是，这种干预在表示级别上运行，而无需修改流匹配路径或目标矢量场。我们提供的理论保证表明，差异引导训练可证明会降低训练目标，并且引导推理细化会随着收缩而收敛。根据经验，DiG-Flow 集成到现有的 VLA 架构中，开销可以忽略不计，并持续提高性能，在复杂的多步骤任务和有限的训练数据下，性能提升尤其明显。"
        },
        {
          "title": "Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration",
          "url": "http://arxiv.org/abs/2512.00797v1",
          "snippet": "Foundation models have become central to unifying perception and planning in robotics, yet real-world deployment exposes a mismatch between their monolithic assumption that a single model can handle all cognitive functions and the distributed, dynamic nature of practical service workflows. Vision-language models offer strong semantic understanding but lack embodiment-aware action capabilities while relying on hand-crafted skills. Vision-Language-Action policies enable reactive manipulation but remain brittle across embodiments, weak in geometric grounding, and devoid of proactive collaboration mechanisms. These limitations indicate that scaling a single model alone cannot deliver reliable autonomy for service robots operating in human-populated settings. To address this gap, we present InteractGen, an LLM-powered multi-agent framework that decomposes robot intelligence into specialized agents for continuous perception, dependency-aware planning, decision and verification, failure reflection, and dynamic human delegation, treating foundation models as regulated components within a closed-loop collective. Deployed on a heterogeneous robot team and evaluated in a three-month open-use study, InteractGen improves task success, adaptability, and human-robot collaboration, providing evidence that multi-agent orchestration offers a more feasible path toward socially grounded service autonomy than further scaling standalone models.",
          "site": "arxiv.org",
          "rank": 19,
          "published": "2025-11-30T09:15:21Z",
          "authors": [
            "Nan Sun",
            "Bo Mao",
            "Yongchang Li",
            "Chenxu Wang",
            "Di Guo",
            "Huaping Liu"
          ],
          "arxiv_id": "2512.00797",
          "abstract": "Foundation models have become central to unifying perception and planning in robotics, yet real-world deployment exposes a mismatch between their monolithic assumption that a single model can handle all cognitive functions and the distributed, dynamic nature of practical service workflows. Vision-language models offer strong semantic understanding but lack embodiment-aware action capabilities while relying on hand-crafted skills. Vision-Language-Action policies enable reactive manipulation but remain brittle across embodiments, weak in geometric grounding, and devoid of proactive collaboration mechanisms. These limitations indicate that scaling a single model alone cannot deliver reliable autonomy for service robots operating in human-populated settings. To address this gap, we present InteractGen, an LLM-powered multi-agent framework that decomposes robot intelligence into specialized agents for continuous perception, dependency-aware planning, decision and verification, failure reflection, and dynamic human delegation, treating foundation models as regulated components within a closed-loop collective. Deployed on a heterogeneous robot team and evaluated in a three-month open-use study, InteractGen improves task success, adaptability, and human-robot collaboration, providing evidence that multi-agent orchestration offers a more feasible path toward socially grounded service autonomy than further scaling standalone models.",
          "abstract_zh": "基础模型已成为统一机器人感知和规划的核心，但现实世界的部署暴露了其单一模型可以处理所有认知功能的整体假设与实际服务工作流程的分布式、动态本质之间的不匹配。视觉语言模型提供了强大的语义理解，但缺乏具体感知的动作能力，同时依赖于手工技能。视觉-语言-行动策略支持反应性操作，但在不同实施例中仍然脆弱，几何基础薄弱，并且缺乏主动协作机制。这些限制表明，仅扩展单个模型无法为在人类居住环境中运行的服务机器人提供可靠的自主性。为了解决这一差距，我们提出了 InteractGen，这是一个由法学硕士支持的多智能体框架，它将机器人智能分解为专门的智能体，用于连续感知、依赖性感知规划、决策和验证、故障反射和动态人类委托，将基础模型视为闭环集体中的受监管组件。InteractGen 部署在异构机器人团队上，并在为期三个月的开放使用研究中进行了评估，它提高了任务成功率、适应性和人机协作能力，证明多代理编排为实现基于社会的服务自治提供了一条比进一步扩展独立模型更可行的途径。"
        },
        {
          "title": "Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment",
          "url": "http://arxiv.org/abs/2512.00783v2",
          "snippet": "To address the gap in humanoid robot cognitive systems regarding the lack of a time-updable mediating thought space between semantics and continuous control, this study constructs and trains a VLA model named \"Sigma\" that runs on a single RTX 4090. It uses the open-source pi05_base model as a foundation and preprocesses svla_so101_pickplace into a training dataset. The researcher independently designed an architecture for a vision-language-action model that combines deep semantic understanding and association to achieve telepathic communication. The training process involved repeated optimizations of data preprocessing, LoRA fine-tuning, and the inference-stage adapter. The experiment employed offline closed-loop replay, comparing Sigma with the untuned pure pi05_base model under data conditions. Results showed that Sigma exhibited a stable decrease in control MSE across vector, fragment, and entire trajectory timescales, while maintaining the telepathy norm and semantic-text alignment quality unchanged. It demonstrates that mind-responsive alignment control is quantified through an architecture that combines deep understanding of semantics and association without retraining the base model, which provides reproducible experience for semantic alignment and intention-driven behavior in humanoid robots.",
          "site": "arxiv.org",
          "rank": 20,
          "published": "2025-11-30T08:37:01Z",
          "authors": [
            "Libo Wang"
          ],
          "arxiv_id": "2512.00783",
          "abstract": "To address the gap in humanoid robot cognitive systems regarding the lack of a time-updable mediating thought space between semantics and continuous control, this study constructs and trains a VLA model named \"Sigma\" that runs on a single RTX 4090. It uses the open-source pi05_base model as a foundation and preprocesses svla_so101_pickplace into a training dataset. The researcher independently designed an architecture for a vision-language-action model that combines deep semantic understanding and association to achieve telepathic communication. The training process involved repeated optimizations of data preprocessing, LoRA fine-tuning, and the inference-stage adapter. The experiment employed offline closed-loop replay, comparing Sigma with the untuned pure pi05_base model under data conditions. Results showed that Sigma exhibited a stable decrease in control MSE across vector, fragment, and entire trajectory timescales, while maintaining the telepathy norm and semantic-text alignment quality unchanged. It demonstrates that mind-responsive alignment control is quantified through an architecture that combines deep understanding of semantics and association without retraining the base model, which provides reproducible experience for semantic alignment and intention-driven behavior in humanoid robots.",
          "abstract_zh": "为了解决人形机器人认知系统中语义和连续控制之间缺乏可时间更新的中介思维空间的差距，本研究构建并训练了一个名为“Sigma”的 VLA 模型，该模型在单个 RTX 4090 上运行。它使用开源 pi05_base 模型作为基础，并将 svla_so101_pickplace 预处理为训练数据集。研究人员独立设计了视觉-语言-动作模型的架构，结合深度语义理解和联想来实现心灵感应交流。训练过程涉及数据预处理、LoRA 微调和推理阶段适配器的重复优化。实验采用离线闭环回放，将Sigma与数据条件下未调整的纯pi05_base模型进行比较。结果表明，Sigma 在矢量、片段和整个轨迹时间尺度上表现出控制 MSE 的稳定下降，同时保持心灵感应规范和语义文本对齐质量不变。它表明，心灵响应对齐控制是通过一种架构来量化的，该架构结合了对语义和关联的深入理解，而无需重新训练基本模型，这为人形机器人中的语义对齐和意图驱动行为提供了可重复的体验。"
        },
        {
          "title": "CoC-VLA: Delving into Adversarial Domain Transfer for Explainable Autonomous Driving via Chain-of-Causality Visual-Language-Action Model",
          "url": "http://arxiv.org/abs/2511.19914v1",
          "snippet": "Autonomous driving represents a prominent application of artificial intelligence. Recent approaches have shifted from focusing solely on common scenarios to addressing complex, long-tail situations such as subtle human behaviors, traffic accidents, and non-compliant driving patterns. Given the demonstrated capabilities of large language models (LLMs) in understanding visual and natural language inputs and following instructions, recent methods have integrated LLMs into autonomous driving systems to enhance reasoning, interpretability, and performance across diverse scenarios. However, existing methods typically rely either on real-world data, which is suitable for industrial deployment, or on simulation data tailored to rare or hard case scenarios. Few approaches effectively integrate the complementary advantages of both data sources. To address this limitation, we propose a novel VLM-guided, end-to-end adversarial transfer framework for autonomous driving that transfers long-tail handling capabilities from simulation to real-world deployment, named CoC-VLA. The framework comprises a teacher VLM model, a student VLM model, and a discriminator. Both the teacher and student VLM models utilize a shared base architecture, termed the Chain-of-Causality Visual-Language Model (CoC VLM), which integrates temporal information via an end-to-end text adapter. This architecture supports chain-of-thought reasoning to infer complex driving logic. The teacher and student VLM models are pre-trained separately on simulated and real-world datasets. The discriminator is trained adversarially to facilitate the transfer of long-tail handling capabilities from simulated to real-world environments by the student VLM model, using a novel backpropagation strategy.",
          "site": "arxiv.org",
          "rank": 21,
          "published": "2025-11-25T04:46:30Z",
          "authors": [
            "Dapeng Zhang",
            "Fei Shen",
            "Rui Zhao",
            "Yinda Chen",
            "Peng Zhi",
            "Chenyang Li",
            "Rui Zhou",
            "Qingguo Zhou"
          ],
          "arxiv_id": "2511.19914",
          "abstract": "Autonomous driving represents a prominent application of artificial intelligence. Recent approaches have shifted from focusing solely on common scenarios to addressing complex, long-tail situations such as subtle human behaviors, traffic accidents, and non-compliant driving patterns. Given the demonstrated capabilities of large language models (LLMs) in understanding visual and natural language inputs and following instructions, recent methods have integrated LLMs into autonomous driving systems to enhance reasoning, interpretability, and performance across diverse scenarios. However, existing methods typically rely either on real-world data, which is suitable for industrial deployment, or on simulation data tailored to rare or hard case scenarios. Few approaches effectively integrate the complementary advantages of both data sources. To address this limitation, we propose a novel VLM-guided, end-to-end adversarial transfer framework for autonomous driving that transfers long-tail handling capabilities from simulation to real-world deployment, named CoC-VLA. The framework comprises a teacher VLM model, a student VLM model, and a discriminator. Both the teacher and student VLM models utilize a shared base architecture, termed the Chain-of-Causality Visual-Language Model (CoC VLM), which integrates temporal information via an end-to-end text adapter. This architecture supports chain-of-thought reasoning to infer complex driving logic. The teacher and student VLM models are pre-trained separately on simulated and real-world datasets. The discriminator is trained adversarially to facilitate the transfer of long-tail handling capabilities from simulated to real-world environments by the student VLM model, using a novel backpropagation strategy.",
          "abstract_zh": "自动驾驶是人工智能的突出应用。最近的方法已经从仅关注常见场景转向解决复杂的长尾情况，例如微妙的人类行为、交通事故和不合规的驾驶模式。鉴于大语言模型 (LLM) 在理解视觉和自然语言输入以及遵循指令方面所展示的能力，最近的方法已将 LLM 集成到自动驾驶系统中，以增强跨不同场景的推理、可解释性和性能。然而，现有方法通常依赖于适合工业部署的真实数据，或者依赖于针对罕见或困难情况场景定制的模拟数据。很少有方法能够有效地整合两个数据源的互补优势。为了解决这一限制，我们提出了一种新颖的 VLM 引导的端到端对抗性传输框架，用于自动驾驶，将长尾处理能力从模拟转移到现实世界的部署，名为 CoC-VLA。该框架包括教师 VLM 模型、学生 VLM 模型和判别器。教师和学生 VLM 模型都使用共享基础架构，称为因果链视觉语言模型 (CoC VLM)，它通过端到端文本适配器集成时间信息。该架构支持思想链推理来推断复杂的驱动逻辑。教师和学生 VLM 模型分别在模拟和真实数据集上进行预训练。鉴别器经过对抗性训练，以促进学生 VLM 模型使用新颖的反向传播策略将长尾处理能力从模拟环境转移到现实环境。"
        },
        {
          "title": "MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent",
          "url": "http://arxiv.org/abs/2511.18810v1",
          "snippet": "Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.",
          "site": "arxiv.org",
          "rank": 22,
          "published": "2025-11-24T06:30:04Z",
          "authors": [
            "Yuxia Fu",
            "Zhizhen Zhang",
            "Yuqi Zhang",
            "Zijian Wang",
            "Zi Huang",
            "Yadan Luo"
          ],
          "arxiv_id": "2511.18810",
          "abstract": "Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.",
          "abstract_zh": "最近的视觉-语言-动作（VLA）模型通过数百万个机器人演示进行调整，重新构建了视觉-语言模型。虽然它们在针对单个实施例或任务系列进行微调时表现良好，但将它们扩展到多技能设置仍然具有挑战性：直接合并受过不同任务训练的 VLA 专家会导致成功率接近于零。这就提出了一个基本问题：是什么阻止 VLA 在一个模型中掌握多种技能？通过在 VLA 微调期间对可学习参数进行经验分解，我们确定了不可合并性的两个关键来源：（1）微调驱动 VLM 主干中的 LoRA 适配器朝着不同的、特定于任务的方向发展，超出了现有合并方法的统一能力。（2）行动专家通过自注意力反馈形成块间依赖关系，导致任务信息跨层传播并防止模块重组。为了应对这些挑战，我们提出了 MergeVLA，这是一种面向合并的 VLA 架构，通过设计保留了可合并性。MergeVLA 通过任务掩码引入稀疏激活的 LoRA 适配器，以保留一致的参数并减少 VLM 中不可调和的冲突。其行动专家用仅交叉注意的块取代了自注意，以保持专业化的本地化和可组合性。当任务未知时，它使用测试时任务路由器从初始观察中自适应地选择适当的任务掩码和专家头，从而实现无监督任务推理。在 LIBERO、LIBERO-Plus、RoboTwin 和真实 SO101 机械臂上的多任务实验中，MergeVLA 实现了与单独微调的专家相当甚至超过的性能，展示了跨任务、实施例和环境的强大泛化能力。"
        },
        {
          "title": "From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings",
          "url": "http://arxiv.org/abs/2511.21428v1",
          "snippet": "We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel \"Latent Action Energy\" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.",
          "site": "arxiv.org",
          "rank": 23,
          "published": "2025-11-26T14:19:44Z",
          "authors": [
            "Jiajie Zhang",
            "Sören Schwertfeger",
            "Alexander Kleiner"
          ],
          "arxiv_id": "2511.21428",
          "abstract": "We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel \"Latent Action Energy\" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.",
          "abstract_zh": "我们提出了一种新颖的无监督框架，可以从连续工业视频流中解锁大量未标记的人类演示数据，用于视觉-语言-动作（VLA）模型预训练。我们的方法首先训练一个轻量级运动分词器来编码运动动态，然后采用一个无监督的动作分割器，利用一种新颖的“潜在动作能量”度量来发现和分割语义上连贯的动作原语。该管道输出分段视频剪辑及其相应的潜在动作序列，提供直接适合 VLA 预训练的结构化数据。对公共基准和专有电动机装配数据集的评估表明，对人类在工作站执行的关键任务进行了有效的细分。通过视觉语言模型的进一步聚类和定量评估证实了所发现的动作原语的语义一致性。据我们所知，这是第一个全自动端到端系统，用于从非结构化工业视频中提取和组织 VLA 预训练数据，为制造中的嵌入式 AI 集成提供可扩展的解决方案。"
        },
        {
          "title": "MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization",
          "url": "http://arxiv.org/abs/2511.19878v1",
          "snippet": "Vision-Language-Action (VLA) models inherit strong priors from pretrained Vision-Language Models (VLMs), but naive fine-tuning often disrupts these representations and harms generalization. Existing fixes -- freezing modules or applying uniform regularization -- either overconstrain adaptation or ignore the differing roles of VLA components. We present MAPS (Module-Wise Proximity Scheduling), the first robust fine-tuning framework for VLAs. Through systematic analysis, we uncover an empirical order in which proximity constraints should be relaxed to balance stability and flexibility. MAPS linearly schedules this relaxation, enabling visual encoders to stay close to their pretrained priors while action-oriented language layers adapt more freely. MAPS introduces no additional parameters or data, and can be seamlessly integrated into existing VLAs. Across MiniVLA-VQ, MiniVLA-OFT, OpenVLA-OFT, and challenging benchmarks such as SimplerEnv, CALVIN, LIBERO, as well as real-world evaluations on the Franka Emika Panda platform, MAPS consistently boosts both in-distribution and out-of-distribution performance (up to +30%). Our findings highlight empirically guided proximity to pretrained VLMs as a simple yet powerful principle for preserving broad generalization in VLM-to-VLA transfer.",
          "site": "arxiv.org",
          "rank": 24,
          "published": "2025-11-25T03:39:37Z",
          "authors": [
            "Chengyue Huang",
            "Mellon M. Zhang",
            "Robert Azarcon",
            "Glen Chou",
            "Zsolt Kira"
          ],
          "arxiv_id": "2511.19878",
          "abstract": "Vision-Language-Action (VLA) models inherit strong priors from pretrained Vision-Language Models (VLMs), but naive fine-tuning often disrupts these representations and harms generalization. Existing fixes -- freezing modules or applying uniform regularization -- either overconstrain adaptation or ignore the differing roles of VLA components. We present MAPS (Module-Wise Proximity Scheduling), the first robust fine-tuning framework for VLAs. Through systematic analysis, we uncover an empirical order in which proximity constraints should be relaxed to balance stability and flexibility. MAPS linearly schedules this relaxation, enabling visual encoders to stay close to their pretrained priors while action-oriented language layers adapt more freely. MAPS introduces no additional parameters or data, and can be seamlessly integrated into existing VLAs. Across MiniVLA-VQ, MiniVLA-OFT, OpenVLA-OFT, and challenging benchmarks such as SimplerEnv, CALVIN, LIBERO, as well as real-world evaluations on the Franka Emika Panda platform, MAPS consistently boosts both in-distribution and out-of-distribution performance (up to +30%). Our findings highlight empirically guided proximity to pretrained VLMs as a simple yet powerful principle for preserving broad generalization in VLM-to-VLA transfer.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型继承了预训练的视觉-语言模型 (VLM) 的强大先验，但幼稚的微调通常会破坏这些表示并损害泛化。现有的修复——冻结模块或应用统一正则化——要么过度限制适应，要么忽略 VLA 组件的不同角色。我们提出了 MAPS（模块级邻近调度），这是第一个强大的 VLA 微调框架。通过系统分析，我们揭示了一个应放宽邻近约束以平衡稳定性和灵活性的经验顺序。MAPS 线性地安排这种放松，使视觉编码器能够保持接近其预先训练的先验，同时面向动作的语言层更自由地适应。MAPS 不引入额外的参数或数据，并且可以无缝集成到现有的 VLA 中。在 MiniVLA-VQ、MiniVLA-OFT、OpenVLA-OFT 以及 SimplerEnv、CALVIN、LIBERO 等具有挑战性的基准测试以及 Franka Emika Panda 平台上的实际评估中，MAPS 持续提升了分布内和分布外性能（高达 +30%）。我们的研究结果强调，以经验为指导的接近预训练 VLM 是保持 VLM 到 VLA 传输广泛泛化的简单而强大的原则。"
        },
        {
          "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
          "url": "http://arxiv.org/abs/2512.01031v1",
          "snippet": "Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash",
          "site": "arxiv.org",
          "rank": 25,
          "published": "2025-11-30T18:59:24Z",
          "authors": [
            "Jiaming Tang",
            "Yufei Sun",
            "Yilong Zhao",
            "Shang Yang",
            "Yujun Lin",
            "Zhuoyang Zhang",
            "James Hou",
            "Yao Lu",
            "Zhijian Liu",
            "Song Han"
          ],
          "arxiv_id": "2512.01031",
          "abstract": "Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash",
          "abstract_zh": "视觉-语言-动作模型（VLA）在处理各种机器人任务方面的能力越来越强。然而，它们在现实世界中的部署仍然缓慢且低效：演示视频通常会加速 5-10 倍才能显得流畅，但会出现明显的动作停顿和对环境变化的延迟反应。异步推理提供了一种有前途的解决方案，通过使机器人能够同时执行动作和执行推理来实现连续和低延迟的控制。然而，由于机器人和环境在推理过程中不断发展，预测和执行间隔之间会出现时间错位。这会导致严重的操作不稳定，而现有方法要么会降低准确性，要么会引入运行时开销来缓解这种不稳定。我们提出了 VLASH，这是一种用于 VLA 的通用异步推理框架，可以提供平滑、准确和快速的反应控制，而无需额外的开销或架构更改。VLASH 通过使用先前生成的动作块向前滚动机器人状态来估计未来的执行时间状态，从而弥合预测和执行之间的差距。实验表明，与同步推理相比，VLASH 实现了高达 2.03 倍的加速，并减少了高达 17.4 倍的反应延迟，同时完全保留了原始精度。此外，它使 VLA 能够处理快速反应、高精度的任务，例如打乒乓球和打地鼠，而传统同步推理无法解决这些问题。代码可在 https://github.com/mit-han-lab/vlash 获取"
        },
        {
          "title": "LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2511.23034v1",
          "snippet": "Learning transferable latent actions from large-scale object manipulation videos can significantly enhance generalization in downstream robotics tasks, as such representations are agnostic to different robot embodiments. Existing approaches primarily rely on visual reconstruction objectives while neglecting physical priors, leading to sub-optimal performance in learning universal representations. To address these challenges, we propose a Universal Latent Action Learning framework that takes task instructions and multiple frames as inputs, and optimizes both future frame reconstruction and action sequence prediction. Unlike prior works, incorporating action predictions (e.g., gripper or hand trajectories and orientations) allows the model to capture richer physical priors such as real-world distances and orientations, thereby enabling seamless transferability to downstream tasks. We further decompose the latent actions into learnable motion and scene tokens to distinguish the robot's active movements from environmental changes, thus filtering out irrelevant dynamics. By distilling the learned latent actions into the latest VLA models, we achieve strong performance across both simulated (SIMPLER and LIBERO) and real-world robot settings. Notably, with only 10 real-world trajectories per task collected on a Franka robot, our approach successfully completes all five challenging tasks, demonstrating strong few-shot transferability in robotic manipulation.",
          "site": "arxiv.org",
          "rank": 26,
          "published": "2025-11-28T09:57:29Z",
          "authors": [
            "Zuolei Li",
            "Xingyu Gao",
            "Xiaofan Wang",
            "Jianlong Fu"
          ],
          "arxiv_id": "2511.23034",
          "abstract": "Learning transferable latent actions from large-scale object manipulation videos can significantly enhance generalization in downstream robotics tasks, as such representations are agnostic to different robot embodiments. Existing approaches primarily rely on visual reconstruction objectives while neglecting physical priors, leading to sub-optimal performance in learning universal representations. To address these challenges, we propose a Universal Latent Action Learning framework that takes task instructions and multiple frames as inputs, and optimizes both future frame reconstruction and action sequence prediction. Unlike prior works, incorporating action predictions (e.g., gripper or hand trajectories and orientations) allows the model to capture richer physical priors such as real-world distances and orientations, thereby enabling seamless transferability to downstream tasks. We further decompose the latent actions into learnable motion and scene tokens to distinguish the robot's active movements from environmental changes, thus filtering out irrelevant dynamics. By distilling the learned latent actions into the latest VLA models, we achieve strong performance across both simulated (SIMPLER and LIBERO) and real-world robot settings. Notably, with only 10 real-world trajectories per task collected on a Franka robot, our approach successfully completes all five challenging tasks, demonstrating strong few-shot transferability in robotic manipulation.",
          "abstract_zh": "从大规模对象操作视频中学习可转移的潜在动作可以显着增强下游机器人任务的泛化能力，因为这种表示对于不同的机器人实施例是不可知的。现有的方法主要依赖于视觉重建目标，而忽略了物理先验，导致在学习通用表示方面表现不佳。为了应对这些挑战，我们提出了一个通用潜在动作学习框架，该框架以任务指令和多个帧作为输入，并优化未来帧重建和动作序列预测。与之前的工作不同，结合动作预测（例如，抓手或手部轨迹和方向）使模型能够捕获更丰富的物理先验，例如现实世界的距离和方向，从而能够无缝转移到下游任务。我们进一步将潜在动作分解为可学习的运动和场景标记，以区分机器人的主动运动和环境变化，从而过滤掉不相关的动态。通过将学习到的潜在动作提炼到最新的 VLA 模型中，我们在模拟（SIMPLER 和 LIBERO）和现实世界的机器人设置中实现了强大的性能。值得注意的是，在 Franka 机器人上每个任务仅收集 10 个真实世界轨迹的情况下，我们的方法成功完成了所有五项具有挑战性的任务，展示了机器人操作中强大的几次镜头可转移性。"
        },
        {
          "title": "Reinforcing Action Policies by Prophesying",
          "url": "http://arxiv.org/abs/2511.20633v1",
          "snippet": "Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.",
          "site": "arxiv.org",
          "rank": 27,
          "published": "2025-11-25T18:52:56Z",
          "authors": [
            "Jiahui Zhang",
            "Ze Huang",
            "Chun Gu",
            "Zipei Ma",
            "Li Zhang"
          ],
          "arxiv_id": "2511.20633",
          "abstract": "Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.",
          "abstract_zh": "视觉-语言-行动 (VLA) 策略在协调语言、感知和机器人控制方面表现出色。然而，大多数 VLA 纯粹是通过模仿来训练的，这与演示过度拟合，并且在分布转移下很脆弱。强化学习（RL）直接优化任务奖励，从而解决这种失调问题，但真实的机器人交互成本高昂，而且传统模拟器难以设计和迁移。我们通过学习的世界模型和针对基于流程的动作头定制的 RL 程序来解决 VLA 训练后的数据效率和优化稳定性问题。具体来说，我们引入了 Prophet，这是一种统一的动作到视频机器人驱动，经过大规模、异构机器人数据的预训练，以学习可重复使用的动作结果动态。它能够通过几次镜头适应新的机器人、物体和环境，从而产生一个可立即部署的模拟器。在 Prophet 上，我们使用 Flow-action-GRPO (FA-GRPO) 和 FlowScale 强化了动作策略，Flow-action-GRPO 使 Flow-GRPO 适应 VLA 动作，而 FlowScale 是一种逐步重新加权，可重新调整流头中的每步梯度。Prophet、FA-GRPO 和 FlowScale 共同构成了 ProphRL，这是一种实用、数据和计算高效的 VLA 后期训练路径。实验表明，不同 VLA 变体在公共基准上的成功率提高了 5-17%，在真实机器人上的成功率提高了 24-30%。"
        },
        {
          "title": "Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories",
          "url": "http://arxiv.org/abs/2511.19528v1",
          "snippet": "Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.",
          "site": "arxiv.org",
          "rank": 28,
          "published": "2025-11-24T07:54:49Z",
          "authors": [
            "Rushuai Yang",
            "Zhiyuan Feng",
            "Tianxiang Zhang",
            "Kaixin Wang",
            "Chuheng Zhang",
            "Li Zhao",
            "Xiu Su",
            "Yi Chen",
            "Jiang Bian"
          ],
          "arxiv_id": "2511.19528",
          "abstract": "Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.",
          "abstract_zh": "扩展视觉-语言-动作 (VLA) 模型预训练需要大量多样化、高质量的操作轨迹。目前大多数数据都是通过人工远程操作获得的，这种方法成本高昂且难以扩展。强化学习 (RL) 方法通过自主探索学习有用的技能，使其成为生成数据的可行方法。然而，标准强化学习训练会陷入狭窄的执行模式，限制了其在大规模预训练中的实用性。我们提出了发现、学习和强化 (DLR)，这是一种信息理论模式发现框架，可为 VLA 预训练生成多种不同的、高成功的行为模式。根据经验，DLR 在 LIBERO 上生成了一个明显更加多样化的轨迹语料库。具体来说，它为同一任务学习多种不同的、高成功的策略，而标准强化学习只发现一种策略，因此它覆盖了状态-动作空间的更广泛的区域。当适应看不见的下游任务套件时，在我们不同的 RL 数据上预训练的 VLA 模型超过了在同等大小的标准 RL 数据集上训练的模型。此外，DLR 表现出单模式 RL 所缺乏的积极的数据缩放行为。这些结果将多模式强化学习定位为用于具体基础模型的实用、可扩展的数据引擎。"
        },
        {
          "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
          "url": "http://arxiv.org/abs/2512.00903v1",
          "snippet": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
          "site": "arxiv.org",
          "rank": 29,
          "published": "2025-11-30T14:10:28Z",
          "authors": [
            "Chaojun Ni",
            "Cheng Chen",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Wenzhao Zheng",
            "Boyuan Wang",
            "Tianrun Chen",
            "Guosheng Zhao",
            "Haoyun Li",
            "Zhehao Dong",
            "Qiang Zhang",
            "Yun Ye",
            "Yang Wang",
            "Guan Huang",
            "Wenjun Mei"
          ],
          "arxiv_id": "2512.00903",
          "abstract": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
          "abstract_zh": "基于预训练视觉语言模型 (VLM) 构建的视觉语言动作 (VLA) 模型显示出强大的潜力，但由于参数数量较多，实用性受到限制。为了缓解这个问题，人们已经探索使用轻量级 VLM，但它会损害时空推理。尽管一些方法表明合并额外的 3D 输入会有所帮助，但它们通常依赖大型 VLM 来融合 3D 和 2D 输入，并且仍然缺乏时间理解。因此，我们提出了 SwiftVLA，这种架构可以增强具有 4D 理解的紧凑模型，同时保持设计效率。具体来说，我们的方法具有预训练的 4D 视觉几何变换器和时间缓存，可从 2D 图像中提取 4D 特征。然后，为了增强 VLM 利用 2D 图像和 4D 特征的能力，我们引入了 Fusion Token，这是一组可学习的 token，经过未来预测目标的训练，可以生成用于生成动作的统一表示。最后，我们引入了一种屏蔽和重建策略，该策略屏蔽 VLM 的 4D 输入并训练 VLA 来重建它们，使 VLM 能够学习有效的 4D 表示，并允许在推理时丢弃 4D 分支，同时性能损失最小。真实和模拟环境中的实验表明，SwiftVLA 的性能优于轻量级基线，与 VLA 相比，其性能提高了 7 倍，在边缘设备上实现了可比的性能，同时速度提高了 18 倍，内存占用减少了 12 倍。"
        },
        {
          "title": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
          "url": "http://arxiv.org/abs/2511.19221v1",
          "snippet": "Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.",
          "site": "arxiv.org",
          "rank": 30,
          "published": "2025-11-24T15:28:25Z",
          "authors": [
            "Jianhua Han",
            "Meng Tian",
            "Jiangtong Zhu",
            "Fan He",
            "Huixin Zhang",
            "Sitong Guo",
            "Dechang Zhu",
            "Hao Tang",
            "Pei Xu",
            "Yuze Guo",
            "Minzhe Niu",
            "Haojie Zhu",
            "Qichao Dong",
            "Xuechao Yan",
            "Siyuan Dong",
            "Lu Hou",
            "Qingqiu Huang",
            "Xiaosong Jia",
            "Hang Xu"
          ],
          "arxiv_id": "2511.19221",
          "abstract": "Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.",
          "abstract_zh": "自动驾驶很大程度上依赖于准确而强大的空间感知。许多失败都是由于不准确和不稳定引起的，特别是在长尾场景和复杂的交互中。然而，当前的视觉语言模型在空间基础和理解方面较弱，因此基于其构建的VLA系统表现出有限的感知和定位能力。为了应对这些挑战，我们引入了 Percept-WAM，这是一种感知增强的世界意识行动模型，它是第一个将 2D/3D 场景理解能力隐式集成到单一视觉语言模型 (VLM) 中的模型。Percept-WAM 没有依赖 QA 式的空间推理，而是将 2D/3D 感知任务统一为 World-PV 和 World-BEV 令牌，这些令牌对空间坐标和置信度进行编码。我们提出了一种用于密集对象感知的网格条件预测机制，结合了 IoU 感知评分和并行自回归解码，提高了长尾、远距离和小对象场景的稳定性。此外，Percept-WAM利用预训练的VLM参数来保留通用智能（例如逻辑推理），并可以直接输出感知结果和轨迹控制输出。实验表明，Percept-WAM 在下游感知基准上匹配或超越了经典检测器和分段器，在 COCO 2D 检测和 nuScenes BEV 3D 检测上实现了 51.7/58.9 mAP。当与轨迹解码器集成时，它进一步提高了 nuScenes 和 NAVSIM 上的规划性能，例如，在 NAVSIM 上的 PMDS 中超过 DiffusionDrive 2.1。定性结果进一步凸显了其强大的开​​放词汇和长尾泛化能力。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 6 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 6）。",
      "items": [
        {
          "title": "TianxingChen/Embodied-AI-Guide",
          "url": "https://github.com/TianxingChen/Embodied-AI-Guide",
          "snippet": "[Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 6
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-11-24",
  "week_end": "2025-11-30",
  "last_updated": "2026-01-07"
}