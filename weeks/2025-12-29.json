{
  "generated_at": "2026-01-07T13:56:54.990167",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 12 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 12）。",
      "items": [
        {
          "title": "VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots",
          "url": "http://arxiv.org/abs/2512.24673v1",
          "snippet": "Vision-Language-Action (VLA) models have achieved remarkable breakthroughs in robotics, with the action chunk playing a dominant role in these advances. Given the real-time and continuous nature of robotic motion control, the strategies for fusing a queue of successive action chunks have a profound impact on the overall performance of VLA models. Existing methods suffer from jitter, stalling, or even pauses in robotic action execution, which not only limits the achievable execution speed but also reduces the overall success rate of task completion. This paper introduces VLA-RAIL (A Real-Time Asynchronous Inference Linker), a novel framework designed to address these issues by conducting model inference and robot motion control asynchronously and guaranteeing smooth, continuous, and high-speed action execution. The core contributions of the paper are two fold: a Trajectory Smoother that effectively filters out the noise and jitter in the trajectory of one action chunk using polynomial fitting and a Chunk Fuser that seamlessly align the current executing trajectory and the newly arrived chunk, ensuring position, velocity, and acceleration continuity between two successive action chunks. We validate the effectiveness of VLA-RAIL on a benchmark of dynamic simulation tasks and several real-world manipulation tasks. Experimental results demonstrate that VLA-RAIL significantly reduces motion jitter, enhances execution speed, and improves task success rates, which will become a key infrastructure for the large-scale deployment of VLA models.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-12-31T06:59:42Z",
          "authors": [
            "Yongsheng Zhao",
            "Lei Zhao",
            "Baoping Cheng",
            "Gongxin Yao",
            "Xuanzhang Wen",
            "Han Gao"
          ],
          "arxiv_id": "2512.24673",
          "abstract": "Vision-Language-Action (VLA) models have achieved remarkable breakthroughs in robotics, with the action chunk playing a dominant role in these advances. Given the real-time and continuous nature of robotic motion control, the strategies for fusing a queue of successive action chunks have a profound impact on the overall performance of VLA models. Existing methods suffer from jitter, stalling, or even pauses in robotic action execution, which not only limits the achievable execution speed but also reduces the overall success rate of task completion. This paper introduces VLA-RAIL (A Real-Time Asynchronous Inference Linker), a novel framework designed to address these issues by conducting model inference and robot motion control asynchronously and guaranteeing smooth, continuous, and high-speed action execution. The core contributions of the paper are two fold: a Trajectory Smoother that effectively filters out the noise and jitter in the trajectory of one action chunk using polynomial fitting and a Chunk Fuser that seamlessly align the current executing trajectory and the newly arrived chunk, ensuring position, velocity, and acceleration continuity between two successive action chunks. We validate the effectiveness of VLA-RAIL on a benchmark of dynamic simulation tasks and several real-world manipulation tasks. Experimental results demonstrate that VLA-RAIL significantly reduces motion jitter, enhances execution speed, and improves task success rates, which will become a key infrastructure for the large-scale deployment of VLA models.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在机器人技术领域取得了显着的突破，其中动作块在这些进步中发挥着主导作用。鉴于机器人运动控制的实时性和连续性，融合连续动作块队列的策略对 VLA 模型的整体性能具有深远的影响。现有方法在机器人动作执行中存在抖动、停顿甚至暂停的问题，这不仅限制了可实现的执行速度，而且降低了任务完成的整体成功率。本文介绍了 VLA-RAIL（实时异步推理链接器），这是一种新颖的框架，旨在通过异步进行模型推理和机器人运动控制并保证平滑、连续和高速的动作执行来解决这些问题。该论文的核心贡献有两个方面：轨迹平滑器（Trajectory Smoother）使用多项式拟合有效地滤除一个动作块轨迹中的噪声和抖动；块融合器（Chunk Fuser）无缝对齐当前执行轨迹和新到达的块，确保两个连续动作块之间的位置、速度和加速度连续性。我们在动态模拟任务和几个实际操作任务的基准上验证了 VLA-RAIL 的有效性。实验结果表明，VLA-RAIL显着降低了运动抖动，提高了执行速度，提高了任务成功率，这将成为VLA模型大规模部署的关键基础设施。"
        },
        {
          "title": "Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation",
          "url": "http://arxiv.org/abs/2601.01618v1",
          "snippet": "Long-horizon robotic manipulation is increasingly important for real-world deployment, requiring spatial disambiguation in complex layouts and temporal resilience under dynamic interaction. However, existing end-to-end and hierarchical Vision-Language-Action (VLA) policies often rely on text-only cues while keeping plan intent latent, which undermines referential grounding in cluttered or underspecified scenes, impedes effective task decomposition of long-horizon goals with close-loop interaction, and limits causal explanation by obscuring the rationale behind action choices. To address these issues, we first introduce Visual Sketch, an implausible visual intermediate that renders points, boxes, arrows, and typed relations in the robot's current views to externalize spatial intent, connect language to scene geometry. Building on Visual Sketch, we present Action-Sketcher, a VLA framework that operates in a cyclic See-Think-Sketch-Act workflow coordinated by adaptive token-gated strategy for reasoning triggers, sketch revision, and action issuance, thereby supporting reactive corrections and human interaction while preserving real-time action prediction. To enable scalable training and evaluation, we curate diverse corpus with interleaved images, text, Visual Sketch supervision, and action sequences, and train Action-Sketcher with a multi-stage curriculum recipe that combines interleaved sequence alignment for modality unification, language-to-sketch consistency for precise linguistic grounding, and imitation learning augmented with sketch-to-action reinforcement for robustness. Extensive experiments on cluttered scenes and multi-object tasks, in simulation and on real-world tasks, show improved long-horizon success, stronger robustness to dynamic scene changes, and enhanced interpretability via editable sketches and step-wise plans. Project website: https://action-sketcher.github.io",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2026-01-04T17:53:42Z",
          "authors": [
            "Huajie Tan",
            "Peterson Co",
            "Yijie Xu",
            "Shanyu Rong",
            "Yuheng Ji",
            "Cheng Chi",
            "Xiansheng Chen",
            "Qiongyu Zhang",
            "Zhongxia Zhao",
            "Pengwei Wang",
            "Zhongyuan Wang",
            "Shanghang Zhang"
          ],
          "arxiv_id": "2601.01618",
          "abstract": "Long-horizon robotic manipulation is increasingly important for real-world deployment, requiring spatial disambiguation in complex layouts and temporal resilience under dynamic interaction. However, existing end-to-end and hierarchical Vision-Language-Action (VLA) policies often rely on text-only cues while keeping plan intent latent, which undermines referential grounding in cluttered or underspecified scenes, impedes effective task decomposition of long-horizon goals with close-loop interaction, and limits causal explanation by obscuring the rationale behind action choices. To address these issues, we first introduce Visual Sketch, an implausible visual intermediate that renders points, boxes, arrows, and typed relations in the robot's current views to externalize spatial intent, connect language to scene geometry. Building on Visual Sketch, we present Action-Sketcher, a VLA framework that operates in a cyclic See-Think-Sketch-Act workflow coordinated by adaptive token-gated strategy for reasoning triggers, sketch revision, and action issuance, thereby supporting reactive corrections and human interaction while preserving real-time action prediction. To enable scalable training and evaluation, we curate diverse corpus with interleaved images, text, Visual Sketch supervision, and action sequences, and train Action-Sketcher with a multi-stage curriculum recipe that combines interleaved sequence alignment for modality unification, language-to-sketch consistency for precise linguistic grounding, and imitation learning augmented with sketch-to-action reinforcement for robustness. Extensive experiments on cluttered scenes and multi-object tasks, in simulation and on real-world tasks, show improved long-horizon success, stronger robustness to dynamic scene changes, and enhanced interpretability via editable sketches and step-wise plans. Project website: https://action-sketcher.github.io",
          "abstract_zh": "长视距机器人操作对于现实世界的部署越来越重要，需要复杂布局中的空间消歧和动态交互下的时间弹性。然而，现有的端到端和分层的视觉-语言-行动（VLA）策略通常依赖于纯文本线索，同时保持计划意图的潜在性，这破坏了杂乱或未指定场景中的参考基础，阻碍了通过闭环交互对长期目标进行有效的任务分解，并通过模糊行动选择背后的基本原理来限制因果解释。为了解决这些问题，我们首先引入 Visual Sketch，这是一种令人难以置信的视觉中间体，可以在机器人当前视图中渲染点、框、箭头和类型化关系，以外部化空间意图，将语言与场景几何连接起来。在 Visual Sketch 的基础上，我们提出了 Action-Sketcher，这是一个 VLA 框架，它在循环的 See-Think-Sketch-Act 工作流程中运行，并通过自适应令牌门控策略进行协调，用于推理触发器、草图修订和动作发布，从而支持反应性校正和人类交互，同时保留实时动作预测。为了实现可扩展的训练和评估，我们利用交错的图像、文本、视觉草图监督和动作序列来策划不同的语料库，并使用多阶段课程方案来训练 Action-Sketcher，该课程方案结合了用于模态统一的交错序列对齐、用于精确语言基础的语言到草图的一致性、以及通过草图到动作强化来增强鲁棒性的模仿学习。对杂乱场景和多对象任务、模拟和现实世界任务进行的大量实验表明，长期成功率有所提高，对动态场景变化的鲁棒性更强，并且通过可编辑草图和逐步计划增强了可解释性。项目网站：https://action-sketcher.github.io"
        },
        {
          "title": "Value Vision-Language-Action Planning & Search",
          "url": "http://arxiv.org/abs/2601.00969v1",
          "snippet": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2026-01-02T19:40:34Z",
          "authors": [
            "Ali Salamatian",
            "Ke",
            "Ren",
            "Kieran Pattison",
            "Cyrus Neary"
          ],
          "arxiv_id": "2601.00969",
          "abstract": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior.",
          "abstract_zh": "视觉-语言-动作（VLA）模型已经成为机器人操作的强大通用策略，但它们仍然从根本上受到对行为克隆的依赖的限制，导致分布转移下的脆弱性。虽然使用蒙特卡罗树搜索 (MCTS) 等测试时搜索算法增强预训练模型可以减轻这些失败，但现有的公式仅依赖于 VLA 先验作为指导，缺乏对预期未来回报的可靠估计。因此，当先验不准确时，规划者只能通过探索项来纠正动作选择，这需要大量的模拟才能生效。为了解决这个限制，我们引入了价值愿景-语言-行动规划和搜索（V-VLAPS），这是一个通过轻量级、可学习的价值函数来增强 MCTS 的框架。通过在固定 VLA 主干 (Octo) 的潜在表示上训练简单的多层感知器 (MLP)，我​​们为搜索提供了明确的成功信号，使动作选择偏向高价值区域。我们在 LIBERO 机器人操作套件上评估 V-VLAPS，证明与仅依赖于 VLA 先验的基线相比，我们的价值引导搜索将成功率提高了 5 个百分点以上，同时将 MCTS 模拟的平均数量减少了 5-15%。"
        },
        {
          "title": "Counterfactual VLA: Self-Reflective Vision-Language-Action Model with Adaptive Reasoning",
          "url": "http://arxiv.org/abs/2512.24426v1",
          "snippet": "Recent reasoning-augmented Vision-Language-Action (VLA) models have improved the interpretability of end-to-end autonomous driving by generating intermediate reasoning traces. Yet these models primarily describe what they perceive and intend to do, rarely questioning whether their planned actions are safe or appropriate. This work introduces Counterfactual VLA (CF-VLA), a self-reflective VLA framework that enables the model to reason about and revise its planned actions before execution. CF-VLA first generates time-segmented meta-actions that summarize driving intent, and then performs counterfactual reasoning conditioned on both the meta-actions and the visual context. This step simulates potential outcomes, identifies unsafe behaviors, and outputs corrected meta-actions that guide the final trajectory generation. To efficiently obtain such self-reflective capabilities, we propose a rollout-filter-label pipeline that mines high-value scenes from a base (non-counterfactual) VLA's rollouts and labels counterfactual reasoning traces for subsequent training rounds. Experiments on large-scale driving datasets show that CF-VLA improves trajectory accuracy by up to 17.6%, enhances safety metrics by 20.5%, and exhibits adaptive thinking: it only enables counterfactual reasoning in challenging scenarios. By transforming reasoning traces from one-shot descriptions to causal self-correction signals, CF-VLA takes a step toward self-reflective autonomous driving agents that learn to think before they act.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-12-30T19:04:17Z",
          "authors": [
            "Zhenghao \"Mark\" Peng",
            "Wenhao Ding",
            "Yurong You",
            "Yuxiao Chen",
            "Wenjie Luo",
            "Thomas Tian",
            "Yulong Cao",
            "Apoorva Sharma",
            "Danfei Xu",
            "Boris Ivanovic",
            "Boyi Li",
            "Bolei Zhou",
            "Yan Wang",
            "Marco Pavone"
          ],
          "arxiv_id": "2512.24426",
          "abstract": "Recent reasoning-augmented Vision-Language-Action (VLA) models have improved the interpretability of end-to-end autonomous driving by generating intermediate reasoning traces. Yet these models primarily describe what they perceive and intend to do, rarely questioning whether their planned actions are safe or appropriate. This work introduces Counterfactual VLA (CF-VLA), a self-reflective VLA framework that enables the model to reason about and revise its planned actions before execution. CF-VLA first generates time-segmented meta-actions that summarize driving intent, and then performs counterfactual reasoning conditioned on both the meta-actions and the visual context. This step simulates potential outcomes, identifies unsafe behaviors, and outputs corrected meta-actions that guide the final trajectory generation. To efficiently obtain such self-reflective capabilities, we propose a rollout-filter-label pipeline that mines high-value scenes from a base (non-counterfactual) VLA's rollouts and labels counterfactual reasoning traces for subsequent training rounds. Experiments on large-scale driving datasets show that CF-VLA improves trajectory accuracy by up to 17.6%, enhances safety metrics by 20.5%, and exhibits adaptive thinking: it only enables counterfactual reasoning in challenging scenarios. By transforming reasoning traces from one-shot descriptions to causal self-correction signals, CF-VLA takes a step toward self-reflective autonomous driving agents that learn to think before they act.",
          "abstract_zh": "最近的推理增强视觉-语言-动作（VLA）模型通过生成中间推理轨迹提高了端到端自动驾驶的可解释性。然而，这些模型主要描述他们的感知和打算做什么，很少质疑他们计划的行动是否安全或适当。这项工作引入了 Counterfactual VLA (CF-VLA)，这是一种自我反思的 VLA 框架，使模型能够在执行之前推理并修改其计划的操作。CF-VLA 首先生成总结驾驶意图的时间分段元动作，然后根据元动作和视觉上下文执行反事实推理。此步骤模拟潜在结果，识别不安全行为，并输出指导最终轨迹生成的纠正元操作。为了有效地获得这种自我反思能力，我们提出了一个 rollout-filter-label 管道，该管道从基础（非反事实）VLA 的 rollout 中挖掘高价值场景，并为后续训练轮次标记反事实推理痕迹。在大规模驾驶数据集上的实验表明，CF-VLA 将轨迹精度提高了 17.6%，将安全指标提高了 20.5%，并表现出自适应思维：它只在具有挑战性的场景中实现反事实推理。通过将推理痕迹从一次性描述转变为因果自我校正信号，CF-VLA 向自我反思的自动驾驶代理迈出了一步，这些代理学会在行动之前思考。"
        },
        {
          "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
          "url": "http://arxiv.org/abs/2512.23162v3",
          "snippet": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-12-29T03:03:00Z",
          "authors": [
            "Yufan He",
            "Pengfei Guo",
            "Mengya Xu",
            "Zhaoshuo Li",
            "Andriy Myronenko",
            "Dillan Imans",
            "Bingjie Liu",
            "Dongren Yang",
            "Mingxue Gu",
            "Yongnan Ji",
            "Yueming Jin",
            "Ren Zhao",
            "Baiyong Shen",
            "Daguang Xu"
          ],
          "arxiv_id": "2512.23162",
          "abstract": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.",
          "abstract_zh": "数据稀缺仍然是实现完全自主手术机器人的根本障碍。虽然大规模视觉语言动作 (VLA) 模型通过利用来自不同领域的配对视频动作数据，在家庭和工业操作中显示出令人印象深刻的通用性，但手术机器人却面临着缺乏包括视觉观察和精确机器人运动学的数据集的问题。相比之下，存在大量的手术视频语料库，但它们缺乏相应的动作标签，阻碍了模仿学习或 VLA 训练的直接应用。在这项工作中，我们的目标是通过学习 SurgWorld 的政策模型来缓解这个问题，SurgWorld 是一个专为外科物理人工智能设计的世界模型。我们专门为手术机器人策划了手术动作文本对齐（SATA）数据集，其中包含详细的动作描述。然后我们基于最先进的物理AI世界模型和SATA构建了SurgeWorld。它能够生成多样化、通用且逼真的手术视频。我们也是第一个使用逆动力学模型从合成手术视频推断伪运动学，生成合成配对视频动作数据的人。我们证明，使用这些增强数据训练的外科 VLA 策略显着优于仅在真实手术机器人平台上进行真实演示训练的模型。我们的方法通过利用大量未标记的手术视频和生成世界建模，为自主手术技能获取提供了一条可扩展的路径，从而为通用和数据高效的手术机器人策略打开了大门。"
        },
        {
          "title": "Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training",
          "url": "http://arxiv.org/abs/2512.24125v2",
          "snippet": "General-purpose robotic systems operating in open-world environments must achieve both broad generalization and high-precision action execution, a combination that remains challenging for existing Vision-Language-Action (VLA) models. While large Vision-Language Models (VLMs) improve semantic generalization, insufficient embodied reasoning leads to brittle behavior, and conversely, strong reasoning alone is inadequate without precise control. To provide a decoupled and quantitative assessment of this bottleneck, we introduce Embodied Reasoning Intelligence Quotient (ERIQ), a large-scale embodied reasoning benchmark in robotic manipulation, comprising 6K+ question-answer pairs across four reasoning dimensions. By decoupling reasoning from execution, ERIQ enables systematic evaluation and reveals a strong positive correlation between embodied reasoning capability and end-to-end VLA generalization. To bridge the gap from reasoning to precise execution, we propose FACT, a flow-matching-based action tokenizer that converts continuous control into discrete sequences while preserving high-fidelity trajectory reconstruction. The resulting GenieReasoner jointly optimizes reasoning and action in a unified space, outperforming both continuous-action and prior discrete-action baselines in real-world tasks. Together, ERIQ and FACT provide a principled framework for diagnosing and overcoming the reasoning-precision trade-off, advancing robust, general-purpose robotic manipulation. Project page: https://geniereasoner.github.io/GenieReasoner/",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-12-30T10:18:42Z",
          "authors": [
            "Yi Liu",
            "Sukai Wang",
            "Dafeng Wei",
            "Xiaowei Cai",
            "Linqing Zhong",
            "Jiange Yang",
            "Guanghui Ren",
            "Jinyu Zhang",
            "Maoqing Yao",
            "Chuankang Li",
            "Xindong He",
            "Liliang Chen",
            "Jianlan Luo"
          ],
          "arxiv_id": "2512.24125",
          "abstract": "General-purpose robotic systems operating in open-world environments must achieve both broad generalization and high-precision action execution, a combination that remains challenging for existing Vision-Language-Action (VLA) models. While large Vision-Language Models (VLMs) improve semantic generalization, insufficient embodied reasoning leads to brittle behavior, and conversely, strong reasoning alone is inadequate without precise control. To provide a decoupled and quantitative assessment of this bottleneck, we introduce Embodied Reasoning Intelligence Quotient (ERIQ), a large-scale embodied reasoning benchmark in robotic manipulation, comprising 6K+ question-answer pairs across four reasoning dimensions. By decoupling reasoning from execution, ERIQ enables systematic evaluation and reveals a strong positive correlation between embodied reasoning capability and end-to-end VLA generalization. To bridge the gap from reasoning to precise execution, we propose FACT, a flow-matching-based action tokenizer that converts continuous control into discrete sequences while preserving high-fidelity trajectory reconstruction. The resulting GenieReasoner jointly optimizes reasoning and action in a unified space, outperforming both continuous-action and prior discrete-action baselines in real-world tasks. Together, ERIQ and FACT provide a principled framework for diagnosing and overcoming the reasoning-precision trade-off, advancing robust, general-purpose robotic manipulation. Project page: https://geniereasoner.github.io/GenieReasoner/",
          "abstract_zh": "在开放世界环境中运行的通用机器人系统必须实现广泛的泛化和高精度的动作执行，这对于现有的视觉-语言-动作（VLA）模型来说仍然具有挑战性。虽然大型视觉语言模型（VLM）改善了语义泛化，但不充分的具体推理会导致脆弱的行为，相反，如果没有精确的控制，仅靠强大的推理是不够的。为了对这一瓶颈进行解耦和定量评估，我们引入了体现推理智商（ERIQ），这是机器人操作中的大规模体现推理基准，由跨越四个推理维度的 6K+ 问答对组成。通过将推理与执行解耦，ERIQ 实现了系统评估，并揭示了具体推理能力与端到端 VLA 泛化之间的强正相关性。为了弥合从推理到精确执行的差距，我们提出了 FACT，一种基于流匹配的动作分词器，它将连续控制转换为离散序列，同时保留高保真轨迹重建。由此产生的 GenieReasoner 在统一空间中联合优化推理和行动，在现实世界任务中优于连续行动和先前的离散行动基线。ERIQ 和 FACT 共同提供了一个原则框架，用于诊断和克服推理精度权衡，推进稳健的通用机器人操作。项目页面：https://geniereasoner.github.io/GenieReasoner/"
        },
        {
          "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2601.02456v1",
          "snippet": "Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\\% improvement in daily tasks and a 40\\%-73.3\\% boost in dynamic settings, such as conveyor belt sorting.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2026-01-05T18:54:29Z",
          "authors": [
            "Junhao Cai",
            "Zetao Cai",
            "Jiafei Cao",
            "Yilun Chen",
            "Zeyu He",
            "Lei Jiang",
            "Hang Li",
            "Hengjie Li",
            "Yang Li",
            "Yufei Liu",
            "Yanan Lu",
            "Qi Lv",
            "Haoxiang Ma",
            "Jiangmiao Pang",
            "Yu Qiao",
            "Zherui Qiu",
            "Yanqing Shen",
            "Xu Shi",
            "Yang Tian",
            "Bolun Wang",
            "Hanqing Wang",
            "Jiaheng Wang",
            "Tai Wang",
            "Xueyuan Wei",
            "Chao Wu",
            "Yiman Xie",
            "Boyang Xing",
            "Yuqiang Yang",
            "Yuyin Yang",
            "Qiaojun Yu",
            "Feng Yuan",
            "Jia Zeng",
            "Jingjing Zhang",
            "Shenghan Zhang",
            "Shi Zhang",
            "Zhuoma Zhaxi",
            "Bowen Zhou",
            "Yuanzhen Zhou",
            "Yunsong Zhou",
            "Hongrui Zhu",
            "Yangkun Zhu",
            "Yuchen Zhu"
          ],
          "arxiv_id": "2601.02456",
          "abstract": "Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\\% improvement in daily tasks and a 40\\%-73.3\\% boost in dynamic settings, such as conveyor belt sorting.",
          "abstract_zh": "流行的视觉-语言-动作 (VLA) 模型通常基于多模态大型语言模型 (MLLM) 构建，并在语义理解方面表现出卓越的熟练程度，但它们本质上缺乏推断物理世界动态的能力。因此，最近的方法已经转向世界模型，通常通过视频预测来制定；然而，这些方法常常缺乏语义基础，并且在处理预测错误时表现出脆弱性。为了协同语义理解与动态预测功能，我们提出了 InternVLA-A1。该模型采用统一的 Mixture-of-Transformers 架构，协调三位专家进行场景理解、视觉预见生成和动作执行。这些组件通过统一的屏蔽自注意力机制无缝交互。在 InternVL3 和 Qwen3-VL 的基础上，我们在 2B 和 3B 参数尺度上实例化 InternVLA-A1。我们在跨越 InternData-A1 和 Agibot-World 的混合合成真实数据集上预训练这些模型，覆盖超过 5.33 亿帧。这种混合训练策略有效地利用了合成模拟数据的多样性，同时最大限度地减少了模拟与真实的差距。我们通过 12 个现实世界的机器人任务和模拟基准评估了 InternVLA-A1。它的性能显着优于 pi0 和 GR00T N1.5 等领先模型，在日常任务方面实现了 14.5% 的提升，在动态设置（例如传送带分拣）方面实现了 40%-73.3% 的提升。"
        },
        {
          "title": "CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding",
          "url": "http://arxiv.org/abs/2601.02295v1",
          "snippet": "Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2026-01-05T17:31:01Z",
          "authors": [
            "Chenyang Ma",
            "Guangyu Yang",
            "Kai Lu",
            "Shitong Xu",
            "Bill Byrne",
            "Niki Trigoni",
            "Andrew Markham"
          ],
          "arxiv_id": "2601.02295",
          "abstract": "Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/",
          "abstract_zh": "目前机器人故障检测和纠正的工作通常以事后方式进行，仅在故障发生后分析错误并应用纠正。这项工作引入了 CycleVLA，这是一个为视觉-语言-动作模型 (VLA) 配备主动自我纠正功能的系统，能够预测初期故障并在执行过程中完全显现之前进行恢复。CycleVLA 通过集成一个进度感知 VLA（标记最常发生故障的关键子任务转换点）、一个基于 VLM 的故障预测器和规划器（在预测故障时触发子任务回溯）以及一个基于最小贝叶斯风险 (MBR) 解码的测试时间扩展策略来实现这一目标，以提高回溯后的重试成功率。大量实验表明，CycleVLA 可以提高训练有素和训练不足的 VLA 的性能，并且 MBR 可以作为 VLA 的有效零样本测试时间扩展策略。项目页面：https://dannymcy.github.io/cyclevla/"
        },
        {
          "title": "GR-Dexter Technical Report",
          "url": "http://arxiv.org/abs/2512.24210v1",
          "snippet": "Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-12-30T13:22:16Z",
          "authors": [
            "Ruoshi Wen",
            "Guangzeng Chen",
            "Zhongren Cui",
            "Min Du",
            "Yang Gou",
            "Zhigang Han",
            "Liqun Huang",
            "Mingyu Lei",
            "Yunfei Li",
            "Zhuohang Li",
            "Wenlei Liu",
            "Yuxiao Liu",
            "Xiao Ma",
            "Hao Niu",
            "Yutao Ouyang",
            "Zeyu Ren",
            "Haixin Shi",
            "Wei Xu",
            "Haoxiang Zhang",
            "Jiajun Zhang",
            "Xiao Zhang",
            "Liwei Zheng",
            "Weiheng Zhong",
            "Yifei Zhou",
            "Zhengming Zhu",
            "Hang Li"
          ],
          "arxiv_id": "2512.24210",
          "abstract": "Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.",
          "abstract_zh": "视觉-语言-动作（VLA）模型已经实现了语言条件下的长视野机器人操作，但大多数现有系统仅限于夹具。由于动作空间扩大、频繁的手部物体遮挡以及收集真实机器人数据的成本，将 VLA 策略扩展到具有高自由度 (DoF) 灵巧手的双手机器人仍然具有挑战性。我们提出了 GR-Dexter，这是一个整体硬件模型数据框架，用于在双手灵巧手机器人上进行基于 VLA 的通用操作。我们的方法结合了紧凑型 21 自由度机器人手的设计、用于真实机器人数据收集的直观双手遥控系统，以及利用遥控操作机器人轨迹以及大规模视觉语言和精心策划的跨实体数据集的训练方法。在涵盖长期日常操作和通用拾放的现实世界评估中，GR-Dexter 实现了强大的域内性能，并提高了对看不见的物体和看不见的指令的鲁棒性。我们希望 GR-Dexter 能够成为通向通用灵巧手机器人操作的实际一步。"
        },
        {
          "title": "RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence",
          "url": "http://arxiv.org/abs/2512.24653v2",
          "snippet": "While data-driven imitation learning has revolutionized robotic manipulation, current approaches remain constrained by the scarcity of large-scale, diverse real-world demonstrations. Consequently, the ability of existing models to generalize across long-horizon bimanual tasks and mobile manipulation in unstructured environments remains limited. To bridge this gap, we present RoboMIND 2.0, a comprehensive real-world dataset comprising over 310K dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks. Crucially, to support research in contact-rich and spatially extended tasks, the dataset incorporates 12K tactile-enhanced episodes and 20K mobile manipulation trajectories. Complementing this physical data, we construct high-fidelity digital twins of our real-world environments, releasing an additional 20K-trajectory simulated dataset to facilitate robust sim-to-real transfer. To fully exploit the potential of RoboMIND 2.0, we propose MIND-2 system, a hierarchical dual-system frame-work optimized via offline reinforcement learning. MIND-2 integrates a high-level semantic planner (MIND-2-VLM) to decompose abstract natural language instructions into grounded subgoals, coupled with a low-level Vision-Language-Action executor (MIND-2-VLA), which generates precise, proprioception-aware motor actions.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-12-31T05:59:40Z",
          "authors": [
            "Chengkai Hou",
            "Kun Wu",
            "Jiaming Liu",
            "Zhengping Che",
            "Di Wu",
            "Fei Liao",
            "Guangrun Li",
            "Jingyang He",
            "Qiuxuan Feng",
            "Zhao Jin",
            "Chenyang Gu",
            "Zhuoyang Liu",
            "Nuowei Han",
            "Xiangju Mi",
            "Yaoxu Lv",
            "Yankai Fu",
            "Gaole Dai",
            "Langzhe Gu",
            "Tao Li",
            "Yuheng Zhang",
            "Yixue Zhang",
            "Xinhua Wang",
            "Shichao Fan",
            "Meng Li",
            "Zhen Zhao",
            "Ning Liu",
            "Zhiyuan Xu",
            "Pei Ren",
            "Junjie Ji",
            "Haonan Liu",
            "Kuan Cheng",
            "Shanghang Zhang",
            "Jian Tang"
          ],
          "arxiv_id": "2512.24653",
          "abstract": "While data-driven imitation learning has revolutionized robotic manipulation, current approaches remain constrained by the scarcity of large-scale, diverse real-world demonstrations. Consequently, the ability of existing models to generalize across long-horizon bimanual tasks and mobile manipulation in unstructured environments remains limited. To bridge this gap, we present RoboMIND 2.0, a comprehensive real-world dataset comprising over 310K dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks. Crucially, to support research in contact-rich and spatially extended tasks, the dataset incorporates 12K tactile-enhanced episodes and 20K mobile manipulation trajectories. Complementing this physical data, we construct high-fidelity digital twins of our real-world environments, releasing an additional 20K-trajectory simulated dataset to facilitate robust sim-to-real transfer. To fully exploit the potential of RoboMIND 2.0, we propose MIND-2 system, a hierarchical dual-system frame-work optimized via offline reinforcement learning. MIND-2 integrates a high-level semantic planner (MIND-2-VLM) to decompose abstract natural language instructions into grounded subgoals, coupled with a low-level Vision-Language-Action executor (MIND-2-VLA), which generates precise, proprioception-aware motor actions.",
          "abstract_zh": "虽然数据驱动的模仿学习彻底改变了机器人操作，但目前的方法仍然受到缺乏大规模、多样化的现实世界演示的限制。因此，现有模型在非结构化环境中泛化长期双手任务和移动操作的能力仍然有限。为了弥补这一差距，我们推出了 RoboMIND 2.0，这是一个全面的现实世界数据集，包含在 6 个不同的机器人实施例和 739 个复杂任务中收集的超过 310K 个双臂操作轨迹。至关重要的是，为了支持接触丰富和空间扩展任务的研究，该数据集包含 12K 触觉增强片段和 20K 移动操作轨迹。为了补充这些物理数据，我们构建了现实世界环境的高保真数字孪生，并发布了额外的 20K 轨迹模拟数据集，以促进稳健的模拟到真实的传输。为了充分发挥 RoboMIND 2.0 的潜力，我们提出了 MIND-2 系统，这是一个通过离线强化学习优化的分层双系统框架。MIND-2 集成了一个高级语义规划器 (MIND-2-VLM)，可将抽象的自然语言指令分解为基础子目标，再加上一个低级视觉-语言-动作执行器 (MIND-2-VLA)，可生成精确的、本体感觉感知的运动动作。"
        },
        {
          "title": "Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation",
          "url": "http://arxiv.org/abs/2512.23864v1",
          "snippet": "Vision-Language-Action (VLA) models have shown remarkable generalization by mapping web-scale knowledge to robotic control, yet they remain blind to physical contact. Consequently, they struggle with contact-rich manipulation tasks that require reasoning about force, texture, and slip. While some approaches incorporate low-dimensional tactile signals, they fail to capture the high-resolution dynamics essential for such interactions. To address this limitation, we introduce DreamTacVLA, a framework that grounds VLA models in contact physics by learning to feel the future. Our model adopts a hierarchical perception scheme in which high-resolution tactile images serve as micro-vision inputs coupled with wrist-camera local vision and third-person macro vision. To reconcile these multi-scale sensory streams, we first train a unified policy with a Hierarchical Spatial Alignment (HSA) loss that aligns tactile tokens with their spatial counterparts in the wrist and third-person views. To further deepen the model's understanding of fine-grained contact dynamics, we finetune the system with a tactile world model that predicts future tactile signals. To mitigate tactile data scarcity and the wear-prone nature of tactile sensors, we construct a hybrid large-scale dataset sourced from both high-fidelity digital twin and real-world experiments. By anticipating upcoming tactile states, DreamTacVLA acquires a rich model of contact physics and conditions its actions on both real observations and imagined consequences. Across contact-rich manipulation tasks, it outperforms state-of-the-art VLA baselines, achieving up to 95% success, highlighting the importance of understanding physical contact for robust, touch-aware robotic agents.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-12-29T21:06:33Z",
          "authors": [
            "Guo Ye",
            "Zexi Zhang",
            "Xu Zhao",
            "Shang Wu",
            "Haoran Lu",
            "Shihan Lu",
            "Han Liu"
          ],
          "arxiv_id": "2512.23864",
          "abstract": "Vision-Language-Action (VLA) models have shown remarkable generalization by mapping web-scale knowledge to robotic control, yet they remain blind to physical contact. Consequently, they struggle with contact-rich manipulation tasks that require reasoning about force, texture, and slip. While some approaches incorporate low-dimensional tactile signals, they fail to capture the high-resolution dynamics essential for such interactions. To address this limitation, we introduce DreamTacVLA, a framework that grounds VLA models in contact physics by learning to feel the future. Our model adopts a hierarchical perception scheme in which high-resolution tactile images serve as micro-vision inputs coupled with wrist-camera local vision and third-person macro vision. To reconcile these multi-scale sensory streams, we first train a unified policy with a Hierarchical Spatial Alignment (HSA) loss that aligns tactile tokens with their spatial counterparts in the wrist and third-person views. To further deepen the model's understanding of fine-grained contact dynamics, we finetune the system with a tactile world model that predicts future tactile signals. To mitigate tactile data scarcity and the wear-prone nature of tactile sensors, we construct a hybrid large-scale dataset sourced from both high-fidelity digital twin and real-world experiments. By anticipating upcoming tactile states, DreamTacVLA acquires a rich model of contact physics and conditions its actions on both real observations and imagined consequences. Across contact-rich manipulation tasks, it outperforms state-of-the-art VLA baselines, achieving up to 95% success, highlighting the importance of understanding physical contact for robust, touch-aware robotic agents.",
          "abstract_zh": "视觉-语言-动作（VLA）模型通过将网络规模的知识映射到机器人控制而显示出显着的泛化性，但它们仍然对物理接触视而不见。因此，他们很难完成需要对力、纹理和滑动进行推理的接触丰富的操作任务。虽然一些方法结合了低维触觉信号，但它们无法捕获此类交互所必需的高分辨率动态。为了解决这一限制，我们引入了 DreamTacVLA，这是一个框架，通过学习感知未来，将 VLA 模型建立在接触物理基础上。我们的模型采用分层感知方案，其中高分辨率触觉图像作为微观视觉输入，加上手腕相机局部视觉和第三人称宏观视觉。为了协调这些多尺度的感觉流，我们首先训练一个具有分层空间对齐（HSA）损失的统一策略，该策略将触觉标记与其在手腕和第三人称视图中的空间对应物对齐。为了进一步加深模型对细粒度接触动力学的理解，我们使用预测未来触觉信号的触觉世界模型对系统进行微调。为了缓解触觉数据的稀缺性和触觉传感器的易磨损特性，我们构建了一个来自高保真数字孪生和现实世界实验的混合大规模数据集。通过预测即将到来的触觉状态，DreamTacVLA 获得了丰富的接触物理模型，并根据真实观察和想象结果来调整其行为。在接触丰富的操作任务中，它的表现优于最先进的 VLA 基线，成功率高达 95%，这凸显了理解物理接触对于强大的触摸感知机器人代理的重要性。"
        },
        {
          "title": "Dichotomous Diffusion Policy Optimization",
          "url": "http://arxiv.org/abs/2601.00898v1",
          "snippet": "Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-12-31T16:56:56Z",
          "authors": [
            "Ruiming Liang",
            "Yinan Zheng",
            "Kexin Zheng",
            "Tianyi Tan",
            "Jianxiong Li",
            "Liyuan Mao",
            "Zhihao Wang",
            "Guang Chen",
            "Hangjun Ye",
            "Jingjing Liu",
            "Jinqiao Wang",
            "Xianyuan Zhan"
          ],
          "arxiv_id": "2601.00898",
          "abstract": "Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.",
          "abstract_zh": "基于扩散的策略由于其卓越的表达能力和推理过程中的可控生成而在解决各种决策任务中越来越受欢迎。然而，使用强化学习（RL）有效地训练大规模扩散策略仍然具有挑战性。现有方法要么由于直接最大化价值目标而遭受不稳定的训练，要么由于依赖粗略的高斯似然近似而面临计算问题，这需要大量足够小的去噪步骤。在这项工作中，我们提出了 DIPOLE（二分扩散策略改进），这是一种专为稳定可控扩散策略优化而设计的新型强化学习算法。我们首先重新审视 RL 中的 KL 正则化目标，它为扩散策略提取提供了理想的加权回归目标，但常常难以平衡贪婪和稳定性。然后，我们制定了一个贪婪的策略正则化方案，它自然地能够将最优策略分解为一对稳定学习的二分策略：一个旨在奖励最大化，另一个专注于奖励最小化。在这样的设计下，可以通过在推理过程中线性组合二分策略的分数来生成优化的动作，从而能够灵活地控制贪婪程度。ExORL和OGBench上的离线和离线到在线RL设置的评估证明了我们方法的有效性。我们还使用 DIPOLE 训练用于端到端自动驾驶 (AD) 的大型视觉语言动作 (VLA) 模型，并在大规模现实世界 AD 基准 NAVSIM 上对其进行评估，突出其在复杂现实世界应用中的潜力。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 1 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 1）。",
      "items": [
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 1
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-12-29",
  "week_end": "2026-01-04",
  "last_updated": "2026-01-07"
}