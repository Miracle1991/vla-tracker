{
  "generated_at": "2026-01-07T13:33:28.103930",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 27 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 27）。",
      "items": [
        {
          "title": "Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer",
          "url": "http://arxiv.org/abs/2510.03342v3",
          "snippet": "General-purpose robots need a deep understanding of the physical world, advanced reasoning, and general and dexterous control. This report introduces the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5, a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER 1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together three major innovations. First, Gemini Robotics 1.5 features a novel architecture and a Motion Transfer (MT) mechanism, which enables it to learn from heterogeneous, multi-embodiment robot data and makes the VLA more general. Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal reasoning process in natural language. This enables the robot to \"think before acting\" and notably improves its ability to decompose and execute complex, multi-step tasks, and also makes the robot's behavior more interpretable to the user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for embodied reasoning, i.e., for reasoning capabilities that are critical for robots, such as visual and spatial understanding, task planning, and progress estimation. Together, this family of models takes us a step towards an era of physical agents-enabling robots to perceive, think and then act so they can solve complex multi-step tasks.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-10-02T14:32:45Z",
          "authors": [
            "Gemini Robotics Team",
            "Abbas Abdolmaleki",
            "Saminda Abeyruwan",
            "Joshua Ainslie",
            "Jean-Baptiste Alayrac",
            "Montserrat Gonzalez Arenas",
            "Ashwin Balakrishna",
            "Nathan Batchelor",
            "Alex Bewley",
            "Jeff Bingham",
            "Michael Bloesch",
            "Konstantinos Bousmalis",
            "Philemon Brakel",
            "Anthony Brohan",
            "Thomas Buschmann",
            "Arunkumar Byravan",
            "Serkan Cabi",
            "Ken Caluwaerts",
            "Federico Casarini",
            "Christine Chan",
            "Oscar Chang",
            "London Chappellet-Volpini",
            "Jose Enrique Chen",
            "Xi Chen",
            "Hao-Tien Lewis Chiang",
            "Krzysztof Choromanski",
            "Adrian Collister",
            "David B. D'Ambrosio",
            "Sudeep Dasari",
            "Todor Davchev",
            "Meet Kirankumar Dave",
            "Coline Devin",
            "Norman Di Palo",
            "Tianli Ding",
            "Carl Doersch",
            "Adil Dostmohamed",
            "Yilun Du",
            "Debidatta Dwibedi",
            "Sathish Thoppay Egambaram",
            "Michael Elabd",
            "Tom Erez",
            "Xiaolin Fang",
            "Claudio Fantacci",
            "Cody Fong",
            "Erik Frey",
            "Chuyuan Fu",
            "Ruiqi Gao",
            "Marissa Giustina",
            "Keerthana Gopalakrishnan",
            "Laura Graesser",
            "Oliver Groth",
            "Agrim Gupta",
            "Roland Hafner",
            "Steven Hansen",
            "Leonard Hasenclever",
            "Sam Haves",
            "Nicolas Heess",
            "Brandon Hernaez",
            "Alex Hofer",
            "Jasmine Hsu",
            "Lu Huang",
            "Sandy H. Huang",
            "Atil Iscen",
            "Mithun George Jacob",
            "Deepali Jain",
            "Sally Jesmonth",
            "Abhishek Jindal",
            "Ryan Julian",
            "Dmitry Kalashnikov",
            "M. Emre Karagozler",
            "Stefani Karp",
            "Matija Kecman",
            "J. Chase Kew",
            "Donnie Kim",
            "Frank Kim",
            "Junkyung Kim",
            "Thomas Kipf",
            "Sean Kirmani",
            "Ksenia Konyushkova",
            "Li Yang Ku",
            "Yuheng Kuang",
            "Thomas Lampe",
            "Antoine Laurens",
            "Tuan Anh Le",
            "Isabel Leal",
            "Alex X. Lee",
            "Tsang-Wei Edward Lee",
            "Guy Lever",
            "Jacky Liang",
            "Li-Heng Lin",
            "Fangchen Liu",
            "Shangbang Long",
            "Caden Lu",
            "Sharath Maddineni",
            "Anirudha Majumdar",
            "Kevis-Kokitsi Maninis",
            "Andrew Marmon",
            "Sergio Martinez",
            "Assaf Hurwitz Michaely",
            "Niko Milonopoulos",
            "Joss Moore",
            "Robert Moreno",
            "Michael Neunert",
            "Francesco Nori",
            "Joy Ortiz",
            "Kenneth Oslund",
            "Carolina Parada",
            "Emilio Parisotto",
            "Amaris Paryag",
            "Acorn Pooley",
            "Thomas Power",
            "Alessio Quaglino",
            "Haroon Qureshi",
            "Rajkumar Vasudeva Raju",
            "Helen Ran",
            "Dushyant Rao",
            "Kanishka Rao",
            "Isaac Reid",
            "David Rendleman",
            "Krista Reymann",
            "Miguel Rivas",
            "Francesco Romano",
            "Yulia Rubanova",
            "Peter Pastor Sampedro",
            "Pannag R Sanketi",
            "Dhruv Shah",
            "Mohit Sharma",
            "Kathryn Shea",
            "Mohit Shridhar",
            "Charles Shu",
            "Vikas Sindhwani",
            "Sumeet Singh",
            "Radu Soricut",
            "Rachel Sterneck",
            "Ian Storz",
            "Razvan Surdulescu",
            "Jie Tan",
            "Jonathan Tompson",
            "Saran Tunyasuvunakool",
            "Jake Varley",
            "Grace Vesom",
            "Giulia Vezzani",
            "Maria Bauza Villalonga",
            "Oriol Vinyals",
            "René Wagner",
            "Ayzaan Wahid",
            "Stefan Welker",
            "Paul Wohlhart",
            "Chengda Wu",
            "Markus Wulfmeier",
            "Fei Xia",
            "Ted Xiao",
            "Annie Xie",
            "Jinyu Xie",
            "Peng Xu",
            "Sichun Xu",
            "Ying Xu",
            "Zhuo Xu",
            "Jimmy Yan",
            "Sherry Yang",
            "Skye Yang",
            "Yuxiang Yang",
            "Hiu Hong Yu",
            "Wenhao Yu",
            "Wentao Yuan",
            "Yuan Yuan",
            "Jingwei Zhang",
            "Tingnan Zhang",
            "Zhiyuan Zhang",
            "Allan Zhou",
            "Guangyao Zhou",
            "Yuxiang Zhou"
          ],
          "arxiv_id": "2510.03342",
          "abstract": "General-purpose robots need a deep understanding of the physical world, advanced reasoning, and general and dexterous control. This report introduces the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5, a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER 1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together three major innovations. First, Gemini Robotics 1.5 features a novel architecture and a Motion Transfer (MT) mechanism, which enables it to learn from heterogeneous, multi-embodiment robot data and makes the VLA more general. Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal reasoning process in natural language. This enables the robot to \"think before acting\" and notably improves its ability to decompose and execute complex, multi-step tasks, and also makes the robot's behavior more interpretable to the user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for embodied reasoning, i.e., for reasoning capabilities that are critical for robots, such as visual and spatial understanding, task planning, and progress estimation. Together, this family of models takes us a step towards an era of physical agents-enabling robots to perceive, think and then act so they can solve complex multi-step tasks.",
          "abstract_zh": "通用机器人需要对物理世界的深刻理解、先进的推理能力以及通用而灵巧的控制能力。本报告介绍了最新一代 Gemini Robotics 模型系列：Gemini Robotics 1.5（一种多具体化视觉-语言-动作（VLA）模型）和 Gemini Robotics-ER 1.5（一种最先进的具体化推理（ER）模型）。我们正在汇集三项重大创新。首先，Gemini Robotics 1.5 采用新颖的架构和运动传输（MT）机制，使其能够从异构、多实施例的机器人数据中学习，并使 VLA 更加通用。其次，Gemini Robotics 1.5 将动作与自然语言的多级内部推理过程交织在一起。这使得机器人能够“先思考后行动”，显着提高其分解和执行复杂、多步骤任务的能力，也使机器人的行为更容易被用户解释。第三，Gemini Robotics-ER 1.5 为体现推理建立了新的最先进技术，即对机器人至关重要的推理能力，例如视觉和空间理解、任务规划和进度估计。总之，这一系列模型使我们向物理代理时代迈进了一步——使机器人能够感知、思考然后采取行动，以便它们能够解决复杂的多步骤任务。"
        },
        {
          "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.01623v1",
          "snippet": "Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-10-02T02:54:03Z",
          "authors": [
            "Angen Ye",
            "Zeyu Zhang",
            "Boyuan Wang",
            "Xiaofeng Wang",
            "Dapeng Zhang",
            "Zheng Zhu"
          ],
          "arxiv_id": "2510.01623",
          "abstract": "Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型旨在统一感知、语言理解和动作生成，提供强大的跨任务和跨场景泛化能力，对具体人工智能产生广泛影响。然而，当前的 VLA 模型通常缺乏明确的逐步推理，而是在不考虑可供性约束或几何关系的情况下发出最终动作。他们的训练后流程也很少强化推理质量，主要依赖于弱奖励设计的监督微调。为了应对这些挑战，我们提出了 VLA-R1，这是一种推理增强型 VLA，它将可验证奖励的强化学习 (RLVR) 与组相对策略优化 (GRPO) 相结合，以系统地优化推理和执行。具体来说，我们设计了一种基于 RLVR 的后训练策略，对区域对齐、轨迹一致性和输出格式提供可验证的奖励，从而增强推理的稳健性和执行的准确性。此外，我们开发了 VLA-CoT-13K，这是一个高质量的数据集，可提供与可供性和轨迹注释明确一致的思想链监督。此外，对域内、域外、仿真和真实机器人平台的广泛评估表明，与之前的 VLA 方法相比，VLA-R1 实现了卓越的泛化性和实际性能。我们计划在这项工作发表后发布模型、代码和数据集。代码：https://github.com/GigaAI-research/VLA-R1。网站：https://gigaai-research.github.io/VLA-R1。"
        },
        {
          "title": "IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks",
          "url": "http://arxiv.org/abs/2509.24768v1",
          "snippet": "Vision-language-action models (VLAs) have become an increasingly popular approach for addressing robot manipulation problems in recent years. However, such models need to output actions at a rate suitable for robot control, which limits the size of the language model they can be based on, and consequently, their language understanding capabilities. Manipulation tasks may require complex language instructions, such as identifying target objects by their relative positions, to specify human intention. Therefore, we introduce IA-VLA, a framework that utilizes the extensive language understanding of a large vision language model as a pre-processing stage to generate improved context to augment the input of a VLA. We evaluate the framework on a set of semantically complex tasks which have been underexplored in VLA literature, namely tasks involving visual duplicates, i.e., visually indistinguishable objects. A dataset of three types of scenes with duplicate objects is used to compare a baseline VLA against two augmented variants. The experiments show that the VLA benefits from the augmentation scheme, especially when faced with language instructions that require the VLA to extrapolate from concepts it has seen in the demonstrations. For the code, dataset, and videos, see https://sites.google.com/view/ia-vla.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-09-29T13:34:59Z",
          "authors": [
            "Eric Hannus",
            "Miika Malin",
            "Tran Nguyen Le",
            "Ville Kyrki"
          ],
          "arxiv_id": "2509.24768",
          "abstract": "Vision-language-action models (VLAs) have become an increasingly popular approach for addressing robot manipulation problems in recent years. However, such models need to output actions at a rate suitable for robot control, which limits the size of the language model they can be based on, and consequently, their language understanding capabilities. Manipulation tasks may require complex language instructions, such as identifying target objects by their relative positions, to specify human intention. Therefore, we introduce IA-VLA, a framework that utilizes the extensive language understanding of a large vision language model as a pre-processing stage to generate improved context to augment the input of a VLA. We evaluate the framework on a set of semantically complex tasks which have been underexplored in VLA literature, namely tasks involving visual duplicates, i.e., visually indistinguishable objects. A dataset of three types of scenes with duplicate objects is used to compare a baseline VLA against two augmented variants. The experiments show that the VLA benefits from the augmentation scheme, especially when faced with language instructions that require the VLA to extrapolate from concepts it has seen in the demonstrations. For the code, dataset, and videos, see https://sites.google.com/view/ia-vla.",
          "abstract_zh": "近年来，视觉-语言-动作模型（VLA）已成为解决机器人操作问题的越来越流行的方法。然而，此类模型需要以适合机器人控制的速率输出动作，这限制了它们所基于的语言模型的大小，从而限制了它们的语言理解能力。操纵任务可能需要复杂的语言指令，例如通过目标对象的相对位置来识别目标对象，以指定人类意图。因此，我们引入了 IA-VLA，该框架利用大型视觉语言模型的广泛语言理解作为预处理阶段来生成改进的上下文以增强 VLA 的输入。我们在一组语义复杂的任务上评估该框架，这些任务在 VLA 文献中尚未得到充分探索，即涉及视觉重复的任务，即视觉上无法区分的对象。具有重复对象的三种类型场景的数据集用于将基线 VLA 与两个增强变体进行比较。实验表明，VLA 受益于增强方案，尤其是在面对需要 VLA 从演示中看到的概念进行推断的语言指令时。有关代码、数据集和视频，请参阅 https://sites.google.com/view/ia-vla。"
        },
        {
          "title": "MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation",
          "url": "http://arxiv.org/abs/2509.26642v1",
          "snippet": "Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLA's understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: https://sites.google.com/view/open-mla",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-09-30T17:59:50Z",
          "authors": [
            "Zhuoyang Liu",
            "Jiaming Liu",
            "Jiadong Xu",
            "Nuowei Han",
            "Chenyang Gu",
            "Hao Chen",
            "Kaichen Zhou",
            "Renrui Zhang",
            "Kai Chin Hsieh",
            "Kun Wu",
            "Zhengping Che",
            "Jian Tang",
            "Shanghang Zhang"
          ],
          "arxiv_id": "2509.26642",
          "abstract": "Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLA's understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: https://sites.google.com/view/open-mla",
          "abstract_zh": "视觉语言动作模型（VLA）通过继承视觉语言模型（VLM）和学习动作生成，在机器人操作任务中表现出了泛化能力。大多数 VLA 模型专注于解释视觉和语言以生成动作，而机器人必须在空间物理世界中感知和交互。这一差距凸显了对机器人特定多感官信息的全面理解的必要性，这对于实现复杂和接触丰富的控制至关重要。为此，我们引入了一种多感官语言动作（MLA）模型，该模型可协作感知异构感官模式并预测未来的多感官目标，以促进物理世界建模。具体来说，为了增强感知表示，我们提出了一种无编码器的多模态对齐方案，该方案创新地将大语言模型本身重新调整为感知模块，通过位置对应对齐 2D 图像、3D 点云和触觉标记来直接解释多模态线索。为了进一步增强 MLA 对物理动力学的理解，我们设计了一种未来的多感官生成训练后策略，使 MLA 能够推理语义、几何和交互信息，为动作生成提供更稳健的条件。在评估方面，MLA 模型在复杂、接触丰富的现实世界任务中的性能分别比之前最先进的 2D 和 3D VLA 方法高出 12% 和 24%，同时还展示了对不可见配置的改进泛化能力。项目网站：https://sites.google.com/view/open-mla"
        },
        {
          "title": "FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.01642v2",
          "snippet": "Recent advances in robotic manipulation have integrated low-level robotic control into Vision-Language Models (VLMs), extending them into Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve strong performance in downstream robotic applications, supported by large-scale crowd-sourced robot training data, they still inevitably encounter failures during execution. Enabling robots to reason and recover from unpredictable and abrupt failures remains a critical challenge. Existing robotic manipulation datasets, collected in either simulation or the real world, primarily provide only ground-truth trajectories, leaving robots unable to recover once failures occur. Moreover, the few datasets that address failure detection typically offer only textual explanations, which are difficult to utilize directly in VLA models. To address this gap, we introduce FailSafe, a novel failure generation and recovery system that automatically produces diverse failure cases paired with executable recovery actions. FailSafe can be seamlessly applied to any manipulation task in any simulator, enabling scalable creation of failure action data. To demonstrate its effectiveness, we fine-tune LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results show that FailSafe-VLM successfully helps robotic arms detect and recover from potential failures, improving the performance of three state-of-the-art VLA models (pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different spatial configurations, camera viewpoints, object and robotic embodiments. We plan to release the FailSafe code to the community.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-10-02T03:48:07Z",
          "authors": [
            "Zijun Lin",
            "Jiafei Duan",
            "Haoquan Fang",
            "Dieter Fox",
            "Ranjay Krishna",
            "Cheston Tan",
            "Bihan Wen"
          ],
          "arxiv_id": "2510.01642",
          "abstract": "Recent advances in robotic manipulation have integrated low-level robotic control into Vision-Language Models (VLMs), extending them into Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve strong performance in downstream robotic applications, supported by large-scale crowd-sourced robot training data, they still inevitably encounter failures during execution. Enabling robots to reason and recover from unpredictable and abrupt failures remains a critical challenge. Existing robotic manipulation datasets, collected in either simulation or the real world, primarily provide only ground-truth trajectories, leaving robots unable to recover once failures occur. Moreover, the few datasets that address failure detection typically offer only textual explanations, which are difficult to utilize directly in VLA models. To address this gap, we introduce FailSafe, a novel failure generation and recovery system that automatically produces diverse failure cases paired with executable recovery actions. FailSafe can be seamlessly applied to any manipulation task in any simulator, enabling scalable creation of failure action data. To demonstrate its effectiveness, we fine-tune LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results show that FailSafe-VLM successfully helps robotic arms detect and recover from potential failures, improving the performance of three state-of-the-art VLA models (pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different spatial configurations, camera viewpoints, object and robotic embodiments. We plan to release the FailSafe code to the community.",
          "abstract_zh": "机器人操作的最新进展已将低级机器人控制集成到视觉语言模型（VLM）中，并将其扩展到视觉语言动作（VLA）模型。尽管最先进的VLA在大规模众包机器人训练数据的支持下在下游机器人应用中取得了强大的性能，但它们在执行过程中仍然不可避免地遇到故障。让机器人能够推理并从不可预测的突然故障中恢复仍然是一个严峻的挑战。现有的机器人操纵数据集（无论是在模拟还是在现实世界中收集）主要仅提供地面实况轨迹，一旦发生故障，机器人就无法恢复。此外，少数解决故障检测的数据集通常仅提供文本解释，很难在 VLA 模型中直接利用。为了解决这一差距，我们引入了 FailSafe，这是一种新颖的故障生成和恢复系统，可自动生成与可执行恢复操作配对的各种故障案例。FailSafe 可以无缝应用于任何模拟器中的任何操作任务，从而实现故障操作数据的可扩展创建。为了证明其有效性，我们对 LLaVa-OneVision-7B (LLaVa-OV-7B) 进行微调以构建 FailSafe-VLM。实验结果表明，FailSafe-VLM 成功帮助机械臂检测潜在故障并从中恢复，在 Maniskill 的多项任务中，三种最先进的 VLA 模型（pi0-FAST、OpenVLA、OpenVLA-OFT）的性能平均提高了 22.6%。此外，FailSafe-VLM 可以推广到不同的空间配置、相机视点、物体和机器人实施例。我们计划向社区发布 FailSafe 代码。"
        },
        {
          "title": "Contrastive Representation Regularization for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.01711v2",
          "snippet": "Vision-Language-Action (VLA) models have shown its capabilities in robot manipulation by leveraging rich representations from pre-trained Vision-Language Models (VLMs). However, their representations arguably remain suboptimal, lacking sensitivity to robotic signals such as control actions and proprioceptive states. To address the issue, we introduce Robot State-aware Contrastive Loss (RS-CL), a simple and effective representation regularization for VLA models, designed to bridge the gap between VLM representations and robotic signals. In particular, RS-CL aligns the representations more closely with the robot's proprioceptive states, by using relative distances between the states as soft supervision. Complementing the original action prediction objective, RS-CL effectively enhances control-relevant representation learning, while being lightweight and fully compatible with standard VLA training pipeline. Our empirical results demonstrate that RS-CL substantially improves the manipulation performance of state-of-the-art VLA models; it pushes the prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen, through more accurate positioning during grasping and placing, and boosts success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-10-02T06:41:22Z",
          "authors": [
            "Taeyoung Kim",
            "Jimin Lee",
            "Myungkyu Koo",
            "Dongyoung Kim",
            "Kyungmin Lee",
            "Changyeon Kim",
            "Younggyo Seo",
            "Jinwoo Shin"
          ],
          "arxiv_id": "2510.01711",
          "abstract": "Vision-Language-Action (VLA) models have shown its capabilities in robot manipulation by leveraging rich representations from pre-trained Vision-Language Models (VLMs). However, their representations arguably remain suboptimal, lacking sensitivity to robotic signals such as control actions and proprioceptive states. To address the issue, we introduce Robot State-aware Contrastive Loss (RS-CL), a simple and effective representation regularization for VLA models, designed to bridge the gap between VLM representations and robotic signals. In particular, RS-CL aligns the representations more closely with the robot's proprioceptive states, by using relative distances between the states as soft supervision. Complementing the original action prediction objective, RS-CL effectively enhances control-relevant representation learning, while being lightweight and fully compatible with standard VLA training pipeline. Our empirical results demonstrate that RS-CL substantially improves the manipulation performance of state-of-the-art VLA models; it pushes the prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen, through more accurate positioning during grasping and placing, and boosts success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.",
          "abstract_zh": "视觉语言动作 (VLA) 模型通过利用预训练视觉语言模型 (VLM) 的丰富表示，展示了其在机器人操作方面的能力。然而，它们的表征可以说仍然不是最理想的，缺乏对机器人信号（例如控制动作和本体感受状态）的敏感性。为了解决这个问题，我们引入了机器人状态感知对比损失（RS-CL），这是一种简单有效的 VLA 模型表示正则化，旨在弥合 VLM 表示和机器人信号之间的差距。特别是，RS-CL 通过使用状态之间的相对距离作为软监督，使表示与机器人的本体感受状态更紧密地对齐。RS-CL 是对原始动作预测目标的补充，有效增强了控制相关的表示学习，同时重量轻且与标准 VLA 训练流程完全兼容。我们的实证结果表明，RS-CL 极大地提高了最先进的 VLA 模型的操纵性能；通过在抓取和放置过程中更准确的定位，它将 RoboCasa-Kitchen 中拾放任务的现有技术从 30.8% 提高到 41.5%，并将具有挑战性的真实机器人操作任务的成功率从 45.0% 提高到 58.3%。"
        },
        {
          "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought",
          "url": "http://arxiv.org/abs/2509.25681v1",
          "snippet": "Vision-Language-Action (VLA) models are emerging as a next-generation paradigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages a multimodal chain-of-thought to unify visual perception, language reasoning, and robotic control in a single system. dVLA jointly optimizes perception, language understanding, and action under a single diffusion objective, enabling stronger cross-modal reasoning and better generalization to novel instructions and objects. For practical deployment, we mitigate inference latency by incorporating two acceleration strategies, a prefix attention mask and KV caching, yielding up to around times speedup at test-time inference. We evaluate dVLA in both simulation and the real world: on the LIBERO benchmark, it achieves state-of-the-art performance with a 96.4% average success rate, consistently surpassing both discrete and continuous action policies; on a real Franka robot, it succeeds across a diverse task suite, including a challenging bin-picking task that requires multi-step planning, demonstrating robust real-world performance. Together, these results underscore the promise of unified diffusion frameworks for practical, high-performance VLA robotics.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-09-30T02:36:11Z",
          "authors": [
            "Junjie Wen",
            "Minjie Zhu",
            "Jiaming Liu",
            "Zhiyuan Liu",
            "Yicun Yang",
            "Linfeng Zhang",
            "Shanghang Zhang",
            "Yichen Zhu",
            "Yi Xu"
          ],
          "arxiv_id": "2509.25681",
          "abstract": "Vision-Language-Action (VLA) models are emerging as a next-generation paradigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages a multimodal chain-of-thought to unify visual perception, language reasoning, and robotic control in a single system. dVLA jointly optimizes perception, language understanding, and action under a single diffusion objective, enabling stronger cross-modal reasoning and better generalization to novel instructions and objects. For practical deployment, we mitigate inference latency by incorporating two acceleration strategies, a prefix attention mask and KV caching, yielding up to around times speedup at test-time inference. We evaluate dVLA in both simulation and the real world: on the LIBERO benchmark, it achieves state-of-the-art performance with a 96.4% average success rate, consistently surpassing both discrete and continuous action policies; on a real Franka robot, it succeeds across a diverse task suite, including a challenging bin-picking task that requires multi-step planning, demonstrating robust real-world performance. Together, these results underscore the promise of unified diffusion frameworks for practical, high-performance VLA robotics.",
          "abstract_zh": "视觉-语言-动作（VLA）模型正在成为下一代机器人范例。我们引入了 dVLA，这是一种基于扩散的 VLA，它利用多模态思想链将视觉感知、语言推理和机器人控制统一在一个系统中。dVLA 在单一扩散目标下联合优化感知、语言理解和行动，从而实现更强大的跨模态推理并更好地泛化到新的指令和对象。对于实际部署，我们通过结合两种加速策略（前缀注意掩码和 KV 缓存）来减少推理延迟，从而在测试时推理中产生大约数倍的加速。我们在模拟和现实世界中评估 dVLA：在 LIBERO 基准上，它实现了最先进的性能，平均成功率为 96.4%，始终超越离散和连续行动策略；在真正的 Franka 机器人上，它成功地完成了各种任务套件，包括需要多步骤规划的具有挑战性的垃圾箱拣选任务，展示了强大的现实性能。总之，这些结果强调了统一扩散框架对于实用、高性能 VLA 机器人的前景。"
        },
        {
          "title": "Hybrid Training for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.00600v1",
          "snippet": "Using Large Language Models to produce intermediate thoughts, a.k.a. Chain-of-thought (CoT), before providing an answer has been a successful recipe for solving complex language tasks. In robotics, similar embodied CoT strategies, generating thoughts before actions, have also been shown to lead to improved performance when using Vision-Language-Action models (VLAs). As these techniques increase the length of the model's generated outputs to include the thoughts, the inference time is negatively affected. Delaying an agent's actions in real-world executions, as in robotic manipulation settings, strongly affects the usability of a method, as tasks require long sequences of actions. However, is the generation of long chains-of-thought a strong prerequisite for achieving performance improvements? In this work, we explore the idea of Hybrid Training (HyT), a framework that enables VLAs to learn from thoughts and benefit from the associated performance gains, while enabling the possibility to leave out CoT generation during inference. Furthermore, by learning to conditionally predict a diverse set of outputs, HyT supports flexibility at inference time, enabling the model to either predict actions directly, generate thoughts or follow instructions. We evaluate the proposed method in a series of simulated benchmarks and real-world experiments.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-10-01T07:27:15Z",
          "authors": [
            "Pietro Mazzaglia",
            "Cansu Sancaktar",
            "Markus Peschl",
            "Daniel Dijkman"
          ],
          "arxiv_id": "2510.00600",
          "abstract": "Using Large Language Models to produce intermediate thoughts, a.k.a. Chain-of-thought (CoT), before providing an answer has been a successful recipe for solving complex language tasks. In robotics, similar embodied CoT strategies, generating thoughts before actions, have also been shown to lead to improved performance when using Vision-Language-Action models (VLAs). As these techniques increase the length of the model's generated outputs to include the thoughts, the inference time is negatively affected. Delaying an agent's actions in real-world executions, as in robotic manipulation settings, strongly affects the usability of a method, as tasks require long sequences of actions. However, is the generation of long chains-of-thought a strong prerequisite for achieving performance improvements? In this work, we explore the idea of Hybrid Training (HyT), a framework that enables VLAs to learn from thoughts and benefit from the associated performance gains, while enabling the possibility to leave out CoT generation during inference. Furthermore, by learning to conditionally predict a diverse set of outputs, HyT supports flexibility at inference time, enabling the model to either predict actions directly, generate thoughts or follow instructions. We evaluate the proposed method in a series of simulated benchmarks and real-world experiments.",
          "abstract_zh": "在提供答案之前使用大型语言模型产生中间思想，即思想链 (CoT)，是解决复杂语言任务的成功秘诀。在机器人技术中，类似的体现 CoT 策略（在行动之前产生想法）也被证明可以在使用视觉-语言-行动模型 (VLA) 时提高性能。由于这些技术增加了模型生成的输出的长度以包含思想，因此推理时间会受到负面影响。在现实世界的执行中延迟代理的操作（如在机器人操作设置中）会严重影响方法的可用性，因为任务需要长的操作序列。然而，长思维链的产生是实现绩效改进的强有力的先决条件吗？在这项工作中，我们探索了混合训练 (HyT) 的概念，该框架使 VLA 能够从思想中学习并从相关的性能增益中受益，同时能够在推理过程中忽略 CoT 生成。此外，通过学习有条件地预测一组不同的输出，HyT 支持推理时间的灵活性，使模型能够直接预测动作、生成想法或遵循指令。我们在一系列模拟基准和真实实验中评估了所提出的方法。"
        },
        {
          "title": "HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy",
          "url": "http://arxiv.org/abs/2510.00695v2",
          "snippet": "Inherently, robotic manipulation tasks are history-dependent: leveraging past context could be beneficial. However, most existing Vision-Language-Action models (VLAs) have been designed without considering this aspect, i.e., they rely solely on the current observation, ignoring preceding context. In this paper, we propose HAMLET, a scalable framework to adapt VLAs to attend to the historical context during action prediction. Specifically, we introduce moment tokens that compactly encode perceptual information at each timestep. Their representations are initialized with time-contrastive learning, allowing them to better capture temporally distinctive aspects. Next, we employ a lightweight memory module that integrates the moment tokens across past timesteps into memory features, which are then leveraged for action prediction. Through empirical evaluation, we show that HAMLET successfully transforms a state-of-the-art VLA into a history-aware policy, especially demonstrating significant improvements on long-horizon tasks that require historical context. In particular, on top of GR00T N1.5, HAMLET achieves an average success rate of 76.4% on history-dependent real-world tasks, surpassing the baseline performance by 47.2%. Furthermore, HAMLET pushes prior art performance from 64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% on LIBERO, highlighting its effectiveness even under generic robot-manipulation benchmarks.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-10-01T09:15:52Z",
          "authors": [
            "Myungkyu Koo",
            "Daewon Choi",
            "Taeyoung Kim",
            "Kyungmin Lee",
            "Changyeon Kim",
            "Younggyo Seo",
            "Jinwoo Shin"
          ],
          "arxiv_id": "2510.00695",
          "abstract": "Inherently, robotic manipulation tasks are history-dependent: leveraging past context could be beneficial. However, most existing Vision-Language-Action models (VLAs) have been designed without considering this aspect, i.e., they rely solely on the current observation, ignoring preceding context. In this paper, we propose HAMLET, a scalable framework to adapt VLAs to attend to the historical context during action prediction. Specifically, we introduce moment tokens that compactly encode perceptual information at each timestep. Their representations are initialized with time-contrastive learning, allowing them to better capture temporally distinctive aspects. Next, we employ a lightweight memory module that integrates the moment tokens across past timesteps into memory features, which are then leveraged for action prediction. Through empirical evaluation, we show that HAMLET successfully transforms a state-of-the-art VLA into a history-aware policy, especially demonstrating significant improvements on long-horizon tasks that require historical context. In particular, on top of GR00T N1.5, HAMLET achieves an average success rate of 76.4% on history-dependent real-world tasks, surpassing the baseline performance by 47.2%. Furthermore, HAMLET pushes prior art performance from 64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% on LIBERO, highlighting its effectiveness even under generic robot-manipulation benchmarks.",
          "abstract_zh": "本质上，机器人操作任务是依赖于历史的：利用过去的背景可能是有益的。然而，大多数现有的视觉-语言-动作模型（VLA）的设计都没有考虑这一方面，即它们仅依赖于当前的观察，忽略了先前的上下文。在本文中，我们提出了 HAMLET，这是一个可扩展的框架，用于调整 VLA 以在动作预测期间关注历史背景。具体来说，我们引入了在每个时间步紧凑地编码感知信息的矩标记。他们的表示是通过时间对比学习来初始化的，使他们能够更好地捕捉时间上的独特方面。接下来，我们采用一个轻量级内存模块，将过去时间步长的时刻标记集成到内存特征中，然后将其用于动作预测。通过实证评估，我们表明 HAMLET 成功地将最先进的 VLA 转变为历史感知策略，特别是在需要历史背景的长期任务上展示了显着改进。特别是，在 GR00T N1.5 之上，HAMLET 在依赖历史的现实世界任务上实现了 76.4% 的平均成功率，超出了基准性能 47.2%。此外，HAMLET 将 RoboCasa Kitchen（100 个演示设置）上的现有技术性能从 64.1% 提升至 66.4%，将 LIBERO 上的现有技术性能从 95.6% 提升至 97.7%，凸显了其即使在通用机器人操作基准下的有效性。"
        },
        {
          "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators",
          "url": "http://arxiv.org/abs/2510.00406v1",
          "snippet": "Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-10-01T01:33:10Z",
          "authors": [
            "Hengtao Li",
            "Pengxiang Ding",
            "Runze Suo",
            "Yihao Wang",
            "Zirui Ge",
            "Dongyuan Zang",
            "Kexian Yu",
            "Mingyang Sun",
            "Hongyin Zhang",
            "Donglin Wang",
            "Weihua Su"
          ],
          "arxiv_id": "2510.00406",
          "abstract": "Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.",
          "abstract_zh": "视觉-语言-动作（VLA）模型可以实现具体决策，但严重依赖模仿学习，导致分布偏移下的复合错误和鲁棒性差。强化学习 (RL) 可以缓解这些问题，但通常需要昂贵的现实世界交互或存在模拟与现实之间的差距。我们引入了 VLA-RFT，这是一种强化微调框架，利用数据驱动的世界模型作为可控模拟器。根据真实的交互数据进行训练，模拟器可以预测以行动为条件的未来视觉观察，从而允许政策推出，并从实现目标的参考中获得密集的轨迹级奖励。这种设计提供了高效且与行动一致的学习信号，大大降低了样本要求。VLA-RFT 的微调步骤少于 400 个，超越了强监督基线，并比基于模拟器的 RL 实现了更高的效率。此外，它在扰动条件下表现出很强的鲁棒性，维持稳定的任务执行。我们的结果将基于世界模型的 RFT 确立为一种实用的训练后范例，以增强 VLA 模型的泛化性和鲁棒性。更多详情请参考https://vla-rft.github.io/。"
        },
        {
          "title": "NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation",
          "url": "http://arxiv.org/abs/2510.03895v1",
          "snippet": "Vision-Language-Action (VLA) models represent a pivotal advance in embodied intelligence, yet they confront critical barriers to real-world deployment, most notably catastrophic forgetting. This issue stems from their overreliance on continuous action sequences or action chunks, which inadvertently create isolated data silos that disrupt knowledge retention across tasks. To tackle these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA) framework: a novel approach that narrows its focus to sparse trajectories, thereby avoiding the catastrophic forgetting associated with dense trajectory fine-tuning. A key innovation of NoTVLA lies in its trajectory planning strategy: instead of centering on the target object's trajectory, it leverages temporal compression and spatial reasoning pruning specifically for the robot end effector's trajectory. Furthermore, training is conducted using these sparse trajectories rather than dense action trajectories, an optimization that delivers remarkable practical advantages with better performance in zero-shot. In multi-task evaluation scenarios, NoTVLA achieves superior performance and generalization compared to pi0 while operating under two critical constraints: it uses over an order of magnitude less computing power than pi0 and requires no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy closely approximates that of single-task expert models. Crucially, it also preserves the model's inherent language capabilities, enabling zero-shot generalization in specific scenarios, supporting unified model deployment across multiple robot platforms, and fostering a degree of generalization even when perceiving tasks from novel perspectives.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-10-04T18:26:55Z",
          "authors": [
            "Zheng Huang",
            "Mingyu Liu",
            "Xiaoyi Lin",
            "Muzhi Zhu",
            "Canyu Zhao",
            "Zongze Du",
            "Xiaoman Li",
            "Yiduo Jia",
            "Hao Zhong",
            "Hao Chen",
            "Chunhua Shen"
          ],
          "arxiv_id": "2510.03895",
          "abstract": "Vision-Language-Action (VLA) models represent a pivotal advance in embodied intelligence, yet they confront critical barriers to real-world deployment, most notably catastrophic forgetting. This issue stems from their overreliance on continuous action sequences or action chunks, which inadvertently create isolated data silos that disrupt knowledge retention across tasks. To tackle these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA) framework: a novel approach that narrows its focus to sparse trajectories, thereby avoiding the catastrophic forgetting associated with dense trajectory fine-tuning. A key innovation of NoTVLA lies in its trajectory planning strategy: instead of centering on the target object's trajectory, it leverages temporal compression and spatial reasoning pruning specifically for the robot end effector's trajectory. Furthermore, training is conducted using these sparse trajectories rather than dense action trajectories, an optimization that delivers remarkable practical advantages with better performance in zero-shot. In multi-task evaluation scenarios, NoTVLA achieves superior performance and generalization compared to pi0 while operating under two critical constraints: it uses over an order of magnitude less computing power than pi0 and requires no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy closely approximates that of single-task expert models. Crucially, it also preserves the model's inherent language capabilities, enabling zero-shot generalization in specific scenarios, supporting unified model deployment across multiple robot platforms, and fostering a degree of generalization even when perceiving tasks from novel perspectives.",
          "abstract_zh": "视觉-语言-动作（VLA）模型代表了具身智能的关键进步，但它们在现实世界的部署中面临着关键障碍，尤其是灾难性遗忘。这个问题源于他们过度依赖连续的操作序列或操作块，这无意中创建了孤立的数据孤岛，破坏了跨任务的知识保留。为了应对这些挑战，我们提出了缩小轨迹 VLA (NoTVLA) 框架：一种将焦点缩小到稀疏轨迹的新颖方法，从而避免与密集轨迹微调相关的灾难性遗忘。NoTVLA的一个关键创新在于其轨迹规划策略：它不是以目标物体的轨迹为中心，而是专门针对机器人末端执行器的轨迹利用时间压缩和空间推理剪枝。此外，训练是使用这些稀疏轨迹而不是密集动作轨迹进行的，这种优化可以在零样本中提供显着的实际优势和更好的性能。在多任务评估场景中，与 pi0 相比，NoTVLA 实现了卓越的性能和泛化能力，同时在两个关键限制下运行：它使用的计算能力比 pi0 低一个数量级，并且不需要腕戴式摄像头。这种设计确保了 NoTVLA 的操作精度非常接近单任务专家模型。至关重要的是，它还保留了模型固有的语言功能，能够在特定场景下实现零样本泛化，支持跨多个机器人平台的统一模型部署，甚至在从新颖的角度感知任务时也能促进一定程度的泛化。"
        },
        {
          "title": "SITCOM: Scaling Inference-Time COMpute for VLAs",
          "url": "http://arxiv.org/abs/2510.04041v1",
          "snippet": "Learning robust robotic control policies remains a major challenge due to the high cost of collecting labeled data, limited generalization to unseen environments, and difficulties in planning over long horizons. While Vision-Language-Action (VLA) models offer a promising solution by grounding natural language instructions into single-step control commands, they often lack mechanisms for lookahead and struggle with compounding errors in dynamic tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs (SITCOM), a framework that augments any pretrained VLA with model-based rollouts and reward-based trajectory selection, inspired by Model Predictive Control algorithm. SITCOM leverages a learned dynamics model to simulate multi-step action rollouts to select the best candidate plan for real-world execution, transforming one-shot VLAs into robust long-horizon planners. We develop an efficient transformer-based dynamics model trained on large-scale BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim gap, and score candidate rollouts using rewards from simulator. Through comprehensive evaluation across multiple tasks and settings in the SIMPLER environment, we demonstrate that SITCOM when combined with a good reward function can significantly improve task completion rate from 48% to 72% using trained dynamics model.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-10-05T05:24:08Z",
          "authors": [
            "Ayudh Saxena",
            "Harsh Shah",
            "Sandeep Routray",
            "Rishi Rajesh Shah",
            "Esha Pahwa"
          ],
          "arxiv_id": "2510.04041",
          "abstract": "Learning robust robotic control policies remains a major challenge due to the high cost of collecting labeled data, limited generalization to unseen environments, and difficulties in planning over long horizons. While Vision-Language-Action (VLA) models offer a promising solution by grounding natural language instructions into single-step control commands, they often lack mechanisms for lookahead and struggle with compounding errors in dynamic tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs (SITCOM), a framework that augments any pretrained VLA with model-based rollouts and reward-based trajectory selection, inspired by Model Predictive Control algorithm. SITCOM leverages a learned dynamics model to simulate multi-step action rollouts to select the best candidate plan for real-world execution, transforming one-shot VLAs into robust long-horizon planners. We develop an efficient transformer-based dynamics model trained on large-scale BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim gap, and score candidate rollouts using rewards from simulator. Through comprehensive evaluation across multiple tasks and settings in the SIMPLER environment, we demonstrate that SITCOM when combined with a good reward function can significantly improve task completion rate from 48% to 72% using trained dynamics model.",
          "abstract_zh": "由于收集标记数据的成本高昂、对未见环境的泛化能力有限以及长期规划的困难，学习强大的机器人控制策略仍然是一个重大挑战。虽然视觉-语言-动作 (VLA) 模型通过将自然语言指令融入单步控制命令中提供了一种有前途的解决方案，但它们通常缺乏前瞻机制，并且难以应对动态任务中的复合错误。在这个项目中，我们引入了 VLA 的扩展推理时间计算 (SITCOM)，这是一个框架，受模型预测控制算法的启发，通过基于模型的部署和基于奖励的轨迹选择来增强任何预训练的 VLA。SITCOM 利用学习的动态模型来模拟多步骤行动部署，以选择现实世界执行的最佳候选计划，将一次性 VLA 转变为强大的长期规划器。我们开发了一种基于 Transformer 的高效动态模型，该模型在大规模 BridgeV2 数据上进行训练，并在 SIMPLER 环境中进行微调，以弥补 Real2Sim 的差距，并使用模拟器的奖励对候选部署进行评分。通过在 SIMPLER 环境中对多个任务和设置进行综合评估，我们证明 SITCOM 与良好的奖励函数相结合可以使用经过训练的动态模型将任务完成率从 48% 显着提高到 72%。"
        },
        {
          "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
          "url": "http://arxiv.org/abs/2510.04898v1",
          "snippet": "Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\\times$, and accelerates inference speed by $120\\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-10-06T15:15:38Z",
          "authors": [
            "Zheng Xiong",
            "Kang Li",
            "Zilin Wang",
            "Matthew Jackson",
            "Jakob Foerster",
            "Shimon Whiteson"
          ],
          "arxiv_id": "2510.04898",
          "abstract": "Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\\times$, and accelerates inference speed by $120\\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA",
          "abstract_zh": "视觉-语言-动作（VLA）模型建立在具有强大泛化能力的语言和视觉基础模型之上，并经过大规模机器人数据的训练，最近已成为学习通用机器人策略的一种有前途的方法。然而，现有 VLA 的一个主要缺点是其推理成本极高。在本文中，我们提出 HyperVLA 来解决这个问题。与在训练和推理期间激活整个模型的现有整体 VLA 不同，HyperVLA 使用一种新颖的基于超网络 (HN) 的架构，该架构在推理期间仅激活小型特定于任务的策略，同时仍然保留在训练期间适应不同多任务行为所需的高模型容量。成功训练基于 HN 的 VLA 并非易事，因此 HyperVLA 包含几个可提高其性能的关键算法设计功能，包括正确利用现有视觉基础模型的先验知识、HN 标准化和动作生成策略。与单片 VLA 相比，HyperVLA 在零样本泛化和少样本自适应方面实现了相似甚至更高的成功率，同时显着降低了推理成本。与最先进的 VLA 模型 OpenVLA 相比，HyperVLA 将测试时激活的参数数量减少了 90 倍，推理速度提高了 120 倍。代码可在 https://github.com/MasterXiong/HyperVLA 公开获取"
        },
        {
          "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context",
          "url": "http://arxiv.org/abs/2510.04246v1",
          "snippet": "Leveraging temporal context is crucial for success in partially observable robotic tasks. However, prior work in behavior cloning has demonstrated inconsistent performance gains when using multi-frame observations. In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. Our approach is motivated by the key observation that Vision-Language-Action models (VLA), i.e., policy models built upon a Vision-Language Model (VLM), more effectively utilize multi-frame observations for action generation. This suggests that VLMs' inherent temporal understanding capability enables them to extract more meaningful context from multi-frame observations. However, the high dimensionality of video inputs introduces significant computational overhead, making VLA training and inference inefficient. To address this, ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation. Our experiments show that ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-10-05T15:29:57Z",
          "authors": [
            "Huiwon Jang",
            "Sihyun Yu",
            "Heeseung Kwon",
            "Hojin Jeon",
            "Younggyo Seo",
            "Jinwoo Shin"
          ],
          "arxiv_id": "2510.04246",
          "abstract": "Leveraging temporal context is crucial for success in partially observable robotic tasks. However, prior work in behavior cloning has demonstrated inconsistent performance gains when using multi-frame observations. In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. Our approach is motivated by the key observation that Vision-Language-Action models (VLA), i.e., policy models built upon a Vision-Language Model (VLM), more effectively utilize multi-frame observations for action generation. This suggests that VLMs' inherent temporal understanding capability enables them to extract more meaningful context from multi-frame observations. However, the high dimensionality of video inputs introduces significant computational overhead, making VLA training and inference inefficient. To address this, ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation. Our experiments show that ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.",
          "abstract_zh": "利用时间上下文对于部分可观察机器人任务的成功至关重要。然而，行为克隆的先前工作已经证明，在使用多帧观察时，性能增益不一致。在本文中，我们介绍了 ContextVLA，这是一种策略模型，可通过有效利用多帧观察来稳健地提高机器人任务性能。我们的方法是受到关键观察的启发，即视觉-语言-行动模型（VLA），即基于视觉-语言模型（VLM）构建的政策模型，可以更有效地利用多框架观察来生成行动。这表明 VLM 固有的时间理解能力使它们能够从多帧观察中提取更有意义的上下文。然而，视频输入的高维度引入了巨大的计算开销，使得 VLA 训练和推理效率低下。为了解决这个问题，ContextVLA 将过去的观察结果压缩为单个上下文标记，从而允许策略有效地利用时间上下文来生成操作。我们的实验表明，ContextVLA 持续改进了单帧 VLA，并实现了完整多帧训练的优势，但减少了训练和推理时间。"
        },
        {
          "title": "World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training",
          "url": "http://arxiv.org/abs/2509.24948v3",
          "snippet": "Vision-Language-Action (VLA) models trained via imitation learning suffer from significant performance degradation in data-scarce scenarios due to their reliance on large-scale demonstration datasets. Although reinforcement learning (RL)-based post-training has proven effective in addressing data scarcity, its application to VLA models is hindered by the non-resettable nature of real-world environments. This limitation is particularly critical in high-risk domains such as industrial automation, where interactions often induce state changes that are costly or infeasible to revert. Furthermore, existing VLA approaches lack a reliable mechanism for detecting task completion, leading to redundant actions that reduce overall task success rates. To address these challenges, we propose World-Env, an RL-based post-training framework that replaces physical interaction with a low-cost, world model-based virtual simulator. World-Env consists of two key components: (1) a video-based world simulator that generates temporally consistent future visual observations, and (2) a vision-language model (VLM)-guided instant reflector that provides continuous reward signals and predicts action termination. This simulated environment enables VLA models to safely explore and generalize beyond their initial imitation learning distribution. Our method achieves notable performance gains with as few as five expert demonstrations per task. Experiments on complex robotic manipulation tasks demonstrate that World-Env effectively overcomes the data inefficiency, safety constraints, and inefficient execution of conventional VLA models that rely on real-world interaction, offering a practical and scalable solution for post-training in resource-constrained settings. Our code is available at https://github.com/amap-cvlab/world-env.",
          "site": "arxiv.org",
          "rank": 15,
          "published": "2025-09-29T15:45:19Z",
          "authors": [
            "Junjin Xiao",
            "Yandan Yang",
            "Xinyuan Chang",
            "Ronghan Chen",
            "Feng Xiong",
            "Mu Xu",
            "Wei-Shi Zheng",
            "Qing Zhang"
          ],
          "arxiv_id": "2509.24948",
          "abstract": "Vision-Language-Action (VLA) models trained via imitation learning suffer from significant performance degradation in data-scarce scenarios due to their reliance on large-scale demonstration datasets. Although reinforcement learning (RL)-based post-training has proven effective in addressing data scarcity, its application to VLA models is hindered by the non-resettable nature of real-world environments. This limitation is particularly critical in high-risk domains such as industrial automation, where interactions often induce state changes that are costly or infeasible to revert. Furthermore, existing VLA approaches lack a reliable mechanism for detecting task completion, leading to redundant actions that reduce overall task success rates. To address these challenges, we propose World-Env, an RL-based post-training framework that replaces physical interaction with a low-cost, world model-based virtual simulator. World-Env consists of two key components: (1) a video-based world simulator that generates temporally consistent future visual observations, and (2) a vision-language model (VLM)-guided instant reflector that provides continuous reward signals and predicts action termination. This simulated environment enables VLA models to safely explore and generalize beyond their initial imitation learning distribution. Our method achieves notable performance gains with as few as five expert demonstrations per task. Experiments on complex robotic manipulation tasks demonstrate that World-Env effectively overcomes the data inefficiency, safety constraints, and inefficient execution of conventional VLA models that rely on real-world interaction, offering a practical and scalable solution for post-training in resource-constrained settings. Our code is available at https://github.com/amap-cvlab/world-env.",
          "abstract_zh": "通过模仿学习训练的视觉-语言-动作（VLA）模型由于依赖大规模演示数据集，因此在数据稀缺场景中性能会显着下降。尽管基于强化学习 (RL) 的后训练已被证明可以有效解决数据稀缺问题，但其在 VLA 模型中的应用受到现实环境的不可重置特性的阻碍。这种限制在工业自动化等高风险领域尤其重要，其中交互通常会导致状态变化，而状态变化的恢复成本高昂或不可行。此外，现有的 VLA 方法缺乏可靠的机制来检测任务完成情况，从而导致冗余操作，从而降低总体任务成功率。为了应对这些挑战，我们提出了 World-Env，一种基于强化学习的后训练框架，用低成本、基于世界模型的虚拟模拟器取代物理交互。World-Env 由两个关键组件组成：(1) 基于视频的世界模拟器，可生成时间一致的未来视觉观察；(2) 视觉语言模型 (VLM) 引导的即时反射器，可提供连续奖励信号并预测动作终止。这种模拟环境使 VLA 模型能够安全地探索和推广超出其初始模仿学习分布的内容。我们的方法只需对每个任务进行五次专家演示即可实现显着的性能提升。对复杂机器人操作任务的实验表明，World-Env有效克服了依赖现实世界交互的传统VLA模型的数据效率低下、安全限制和执行效率低下的问题，为资源受限环境下的后期训练提供了实用且可扩展的解决方案。我们的代码可在 https://github.com/amap-cvlab/world-env 获取。"
        },
        {
          "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition",
          "url": "http://arxiv.org/abs/2510.01068v1",
          "snippet": "Diffusion-based models for robotic control, including vision-language-action (VLA) and vision-action (VA) policies, have demonstrated significant capabilities. Yet their advancement is constrained by the high cost of acquiring large-scale interaction datasets. This work introduces an alternative paradigm for enhancing policy performance without additional model training. Perhaps surprisingly, we demonstrate that the composed policies can exceed the performance of either parent policy. Our contribution is threefold. First, we establish a theoretical foundation showing that the convex composition of distributional scores from multiple diffusion models can yield a superior one-step functional objective compared to any individual score. A Grönwall-type bound is then used to show that this single-step improvement propagates through entire generation trajectories, leading to systemic performance gains. Second, motivated by these results, we propose General Policy Composition (GPC), a training-free method that enhances performance by combining the distributional scores of multiple pre-trained policies via a convex combination and test-time search. GPC is versatile, allowing for the plug-and-play composition of heterogeneous policies, including VA and VLA models, as well as those based on diffusion or flow-matching, irrespective of their input visual modalities. Third, we provide extensive empirical validation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside real-world robotic evaluations, confirm that GPC consistently improves performance and adaptability across a diverse set of tasks. Further analysis of alternative composition operators and weighting strategies offers insights into the mechanisms underlying the success of GPC. These results establish GPC as a simple yet effective method for improving control performance by leveraging existing policies.",
          "site": "arxiv.org",
          "rank": 16,
          "published": "2025-10-01T16:05:53Z",
          "authors": [
            "Jiahang Cao",
            "Yize Huang",
            "Hanzhong Guo",
            "Rui Zhang",
            "Mu Nan",
            "Weijian Mai",
            "Jiaxu Wang",
            "Hao Cheng",
            "Jingkai Sun",
            "Gang Han",
            "Wen Zhao",
            "Qiang Zhang",
            "Yijie Guo",
            "Qihao Zheng",
            "Chunfeng Song",
            "Xiao Li",
            "Ping Luo",
            "Andrew F. Luo"
          ],
          "arxiv_id": "2510.01068",
          "abstract": "Diffusion-based models for robotic control, including vision-language-action (VLA) and vision-action (VA) policies, have demonstrated significant capabilities. Yet their advancement is constrained by the high cost of acquiring large-scale interaction datasets. This work introduces an alternative paradigm for enhancing policy performance without additional model training. Perhaps surprisingly, we demonstrate that the composed policies can exceed the performance of either parent policy. Our contribution is threefold. First, we establish a theoretical foundation showing that the convex composition of distributional scores from multiple diffusion models can yield a superior one-step functional objective compared to any individual score. A Grönwall-type bound is then used to show that this single-step improvement propagates through entire generation trajectories, leading to systemic performance gains. Second, motivated by these results, we propose General Policy Composition (GPC), a training-free method that enhances performance by combining the distributional scores of multiple pre-trained policies via a convex combination and test-time search. GPC is versatile, allowing for the plug-and-play composition of heterogeneous policies, including VA and VLA models, as well as those based on diffusion or flow-matching, irrespective of their input visual modalities. Third, we provide extensive empirical validation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside real-world robotic evaluations, confirm that GPC consistently improves performance and adaptability across a diverse set of tasks. Further analysis of alternative composition operators and weighting strategies offers insights into the mechanisms underlying the success of GPC. These results establish GPC as a simple yet effective method for improving control performance by leveraging existing policies.",
          "abstract_zh": "基于扩散的机器人控制模型，包括视觉-语言-动作（VLA）和视觉-动作（VA）策略，已经展示了重要的功能。然而，它们的进步受到获取大规模交互数据集的高成本的限制。这项工作引入了一种无需额外模型训练即可增强政策绩效的替代范例。也许令人惊讶的是，我们证明了组合策略可以超过任一父策略的性能。我们的贡献是三重的。首先，我们建立了一个理论基础，表明与任何单独的分数相比，多个扩散模型的分布分数的凸组合可以产生优越的一步函数目标。然后使用 Grönwall 型界限来​​表明这种单步改进会传播到整个生成轨迹，从而带来系统性能增益。其次，受这些结果的启发，我们提出了通用策略组合（GPC），这是一种免训练方法，通过凸组合和测试时搜索组合多个预训练策略的分布分数来提高性能。GPC 是多功能的，允许即插即用地组合异构策略，包括 VA 和 VLA 模型，以及基于扩散或流匹配的模型，无论其输入视觉模式如何。第三，我们提供了广泛的实证验证。Robomimic、PushT 和 RoboTwin 基准测试以及现实世界的机器人评估都证实，GPC 能够持续提高各种任务的性能和适应性。对替代组合算子和加权策略的进一步分析可以深入了解 GPC 成功的机制。这些结果表明 GPC 是一种利用现有策略来提高控制性能的简单而有效的方法。"
        },
        {
          "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation",
          "url": "http://arxiv.org/abs/2510.05057v1",
          "snippet": "A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video.",
          "site": "arxiv.org",
          "rank": 17,
          "published": "2025-10-06T17:37:24Z",
          "authors": [
            "Mingyu Liu",
            "Jiuhe Shu",
            "Hui Chen",
            "Zeju Li",
            "Canyu Zhao",
            "Jiange Yang",
            "Shenyuan Gao",
            "Hao Chen",
            "Chunhua Shen"
          ],
          "arxiv_id": "2510.05057",
          "abstract": "A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video.",
          "abstract_zh": "具身智能的一个基本挑战是开发富有表现力和紧凑的状态表示，以实现高效的世界建模和决策。然而，现有的方法通常无法实现这种平衡，产生的表示要么过于冗余，要么缺乏任务关键信息。我们提出了一种无监督方法，使用轻量级编码器和预训练的扩散变换器（DiT）解码器学习高度压缩的双令牌状态表示，利用其强大的生成先验。我们的表示高效、可解释，并无缝集成到现有的基于 VLA 的模型中，以最小的推理开销将 LIBERO 的性能提高了 14.3%，将现实世界的任务成功率提高了 30%。更重要的是，我们发现通过潜在插值获得的这些标记之间的差异自然可以作为高效的潜在动作，可以进一步解码为可执行的机器人动作。这种新兴的能力表明，我们的表示可以在没有明确监督的情况下捕获结构化动态。我们将我们的方法命名为 StaMo，因为它能够从紧凑的状态表示中学习通用的机器人运动，该表示是从静态图像编码的，挑战了对复杂架构和视频数据学习潜在动作的普遍依赖。由此产生的潜在行动还增强了政策协同训练，比之前的方法提高了 10.4%，并提高了可解释性。此外，我们的方法可以有效地跨不同的数据源进行扩展，包括现实世界的机器人数据、模拟和人类以自我为中心的视频。"
        },
        {
          "title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning",
          "url": "http://arxiv.org/abs/2510.03142v1",
          "snippet": "Visual navigation policy is widely regarded as a promising direction, as it mimics humans by using egocentric visual observations for navigation. However, optical information of visual observations is difficult to be explicitly modeled like LiDAR point clouds or depth maps, which subsequently requires intelligent models and large-scale data. To this end, we propose to leverage the intelligence of the Vision-Language-Action (VLA) model to learn diverse navigation capabilities from synthetic expert data in a teacher-student manner. Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360 observations) based on pretrained large language models and visual foundation models. For large-scale navigation data, we collect expert data from three reinforcement learning (RL) experts trained with privileged depth information in three challenging tailor-made environments for different navigation capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA model using data collected online from RL experts, where the training ratio is dynamically balanced based on performance on individual capabilities. Through extensive experiments in synthetic environments, we demonstrate that our model achieves strong generalization capability. Moreover, we find that our student VLA model outperforms the RL teachers, demonstrating the synergistic effect of integrating multiple capabilities. Extensive real-world experiments further confirm the effectiveness of our method.",
          "site": "arxiv.org",
          "rank": 18,
          "published": "2025-10-03T16:15:09Z",
          "authors": [
            "Tianyu Xu",
            "Jiawei Chen",
            "Jiazhao Zhang",
            "Wenyao Zhang",
            "Zekun Qi",
            "Minghan Li",
            "Zhizheng Zhang",
            "He Wang"
          ],
          "arxiv_id": "2510.03142",
          "abstract": "Visual navigation policy is widely regarded as a promising direction, as it mimics humans by using egocentric visual observations for navigation. However, optical information of visual observations is difficult to be explicitly modeled like LiDAR point clouds or depth maps, which subsequently requires intelligent models and large-scale data. To this end, we propose to leverage the intelligence of the Vision-Language-Action (VLA) model to learn diverse navigation capabilities from synthetic expert data in a teacher-student manner. Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360 observations) based on pretrained large language models and visual foundation models. For large-scale navigation data, we collect expert data from three reinforcement learning (RL) experts trained with privileged depth information in three challenging tailor-made environments for different navigation capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA model using data collected online from RL experts, where the training ratio is dynamically balanced based on performance on individual capabilities. Through extensive experiments in synthetic environments, we demonstrate that our model achieves strong generalization capability. Moreover, we find that our student VLA model outperforms the RL teachers, demonstrating the synergistic effect of integrating multiple capabilities. Extensive real-world experiments further confirm the effectiveness of our method.",
          "abstract_zh": "视觉导航政策被广泛认为是一个有前途的方向，因为它通过使用以自我为中心的视觉观察来模仿人类进行导航。然而，视觉观测的光学信息很难像激光雷达点云或深度图那样明确建模，这随后需要智能模型和大规模数据。为此，我们建议利用视觉-语言-动作（VLA）模型的智能，以师生的方式从合成专家数据中学习各种导航功能。具体来说，我们将 VLA 模型 MM-Nav 实现为基于预训练的大型语言模型和视觉基础模型的多视图 VLA（具有 360 个观测值）。对于大规模导航数据，我们从三位强化学习 (RL) 专家那里收集了专家数据，他们在三种具有挑战性的定制环境中接受了特权深度信息的培训，以实现不同的导航功能：到达、挤压和躲避。我们使用 RL 专家在线收集的数据迭代训练我们的 VLA 模型，其中训练比例根据个人能力的表现动态平衡。通过在合成环境中进行大量实验，我们证明了我们的模型具有很强的泛化能力。此外，我们发现我们的学生 VLA 模型优于 RL 教师，展示了整合多种能力的协同效应。大量的现实世界实验进一步证实了我们方法的有效性。"
        },
        {
          "title": "INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.01389v1",
          "snippet": "Recent Vision-Language-Action (VLA) models show strong generalization capabilities, yet they lack introspective mechanisms for anticipating failures and requesting help from a human supervisor. We present \\textbf{INSIGHT}, a learning framework for leveraging token-level uncertainty signals to predict when a VLA should request help. Using $π_0$-FAST as the underlying model, we extract per-token \\emph{entropy}, \\emph{log-probability}, and Dirichlet-based estimates of \\emph{aleatoric and epistemic uncertainty}, and train compact transformer classifiers to map these sequences to help triggers. We explore supervision regimes for strong or weak supervision, and extensively compare them across in-distribution and out-of-distribution tasks. Our results show a trade-off: strong labels enable models to capture fine-grained uncertainty dynamics for reliable help detection, while weak labels, though noisier, still support competitive introspection when training and evaluation are aligned, offering a scalable path when dense annotation is impractical. Crucially, we find that modeling the temporal evolution of token-level uncertainty signals with transformers provides far greater predictive power than static sequence-level scores. This study provides the first systematic evaluation of uncertainty-based introspection in VLAs, opening future avenues for active learning and for real-time error mitigation through selective human intervention.",
          "site": "arxiv.org",
          "rank": 19,
          "published": "2025-10-01T19:22:48Z",
          "authors": [
            "Ulas Berk Karli",
            "Ziyao Shangguan",
            "Tesca FItzgerald"
          ],
          "arxiv_id": "2510.01389",
          "abstract": "Recent Vision-Language-Action (VLA) models show strong generalization capabilities, yet they lack introspective mechanisms for anticipating failures and requesting help from a human supervisor. We present \\textbf{INSIGHT}, a learning framework for leveraging token-level uncertainty signals to predict when a VLA should request help. Using $π_0$-FAST as the underlying model, we extract per-token \\emph{entropy}, \\emph{log-probability}, and Dirichlet-based estimates of \\emph{aleatoric and epistemic uncertainty}, and train compact transformer classifiers to map these sequences to help triggers. We explore supervision regimes for strong or weak supervision, and extensively compare them across in-distribution and out-of-distribution tasks. Our results show a trade-off: strong labels enable models to capture fine-grained uncertainty dynamics for reliable help detection, while weak labels, though noisier, still support competitive introspection when training and evaluation are aligned, offering a scalable path when dense annotation is impractical. Crucially, we find that modeling the temporal evolution of token-level uncertainty signals with transformers provides far greater predictive power than static sequence-level scores. This study provides the first systematic evaluation of uncertainty-based introspection in VLAs, opening future avenues for active learning and for real-time error mitigation through selective human intervention.",
          "abstract_zh": "最近的视觉-语言-动作（VLA）模型显示出强大的泛化能力，但它们缺乏预测失败和请求人类主管帮助的内省机制。我们提出了 \\textbf{INSIGHT}，这是一个利用令牌级不确定性信号来预测 VLA 何时应该请求帮助的学习框架。使用 $π_0$-FAST 作为底层模型，我们提取每个标记的 \\emph{entropy}、\\emph{log-probability} 和基于 Dirichlet 的 \\emph{任意和认知不确定性} 估计，并训练紧凑的 Transformer 分类器来映射这些序列以帮助触发。我们探索强监督或弱监督的监督制度，并在分布内和分布外任务中广泛比较它们。我们的结果显示了一种权衡：强标签使模型能够捕获细粒度的不确定性动态，以实现可靠的帮助检测，而弱标签虽然噪音更大，但在训练和评估一致时仍然支持竞争性内省，在密集注释不切实际时提供可扩展的路径。至关重要的是，我们发现使用变压器对令牌级不确定性信号的时间演化进行建模比静态序列级分数提供了更大的预测能力。这项研究首次对 VLA 中基于不确定性的内省进行了系统评估，为未来主动学习和通过选择性人为干预减少实时错误开辟了途径。"
        },
        {
          "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization",
          "url": "http://arxiv.org/abs/2510.03827v1",
          "snippet": "LIBERO has emerged as a widely adopted benchmark for evaluating Vision-Language-Action (VLA) models; however, its current training and evaluation settings are problematic, often leading to inflated performance estimates and preventing fair model comparison. To address these issues, we introduce LIBERO-PRO, an extended LIBERO benchmark that systematically evaluates model performance under reasonable perturbations across four dimensions: manipulated objects, initial states, task instructions, and environments. Experimental results reveal that, although existing models achieve over 90% accuracy under the standard LIBERO evaluation, their performance collapses to 0.0% under our generalized setting. Crucially, this discrepancy exposes the models' reliance on rote memorization of action sequences and environment layouts from the training set, rather than genuine task understanding or environmental perception. For instance, models persist in executing grasping actions when the target object is replaced with irrelevant items, and their outputs remain unchanged even when given corrupted instructions or even messy tokens. These findings expose the severe flaws in current evaluation practices, and we call on the community to abandon misleading methodologies in favor of robust assessments of model generalization and comprehension. Our code is available at: https://github.com/Zxy-MLlab/LIBERO-PRO.",
          "site": "arxiv.org",
          "rank": 20,
          "published": "2025-10-04T14:56:40Z",
          "authors": [
            "Xueyang Zhou",
            "Yangming Xu",
            "Guiyao Tie",
            "Yongchao Chen",
            "Guowen Zhang",
            "Duanfeng Chu",
            "Pan Zhou",
            "Lichao Sun"
          ],
          "arxiv_id": "2510.03827",
          "abstract": "LIBERO has emerged as a widely adopted benchmark for evaluating Vision-Language-Action (VLA) models; however, its current training and evaluation settings are problematic, often leading to inflated performance estimates and preventing fair model comparison. To address these issues, we introduce LIBERO-PRO, an extended LIBERO benchmark that systematically evaluates model performance under reasonable perturbations across four dimensions: manipulated objects, initial states, task instructions, and environments. Experimental results reveal that, although existing models achieve over 90% accuracy under the standard LIBERO evaluation, their performance collapses to 0.0% under our generalized setting. Crucially, this discrepancy exposes the models' reliance on rote memorization of action sequences and environment layouts from the training set, rather than genuine task understanding or environmental perception. For instance, models persist in executing grasping actions when the target object is replaced with irrelevant items, and their outputs remain unchanged even when given corrupted instructions or even messy tokens. These findings expose the severe flaws in current evaluation practices, and we call on the community to abandon misleading methodologies in favor of robust assessments of model generalization and comprehension. Our code is available at: https://github.com/Zxy-MLlab/LIBERO-PRO.",
          "abstract_zh": "LIBERO 已成为评估视觉-语言-行动 (VLA) 模型的广泛采用的基准；然而，其当前的培训和评估设置存在问题，常常导致性能估计过高并妨碍公平的模型比较。为了解决这些问题，我们引入了 LIBERO-PRO，这是一个扩展的 LIBERO 基准，它可以在四个维度的合理扰动下系统地评估模型性能：操纵对象、初始状态、任务指令和环境。实验结果表明，尽管现有模型在标准 LIBERO 评估下达到了 90% 以上的准确率，但在我们的广义设置下，其性能却下降至 0.0%。至关重要的是，这种差异暴露了模型对训练集中动作序列和环境布局的死记硬背的依赖，而不是真正的任务理解或环境感知。例如，当目标对象被不相关的项目替换时，模型会坚持执行抓取动作，并且即使给出损坏的指令甚至混乱的标记，它们的输出也保持不变。这些发现暴露了当前评估实践中的严重缺陷，我们呼吁社区放弃误导性的方法，转而对模型泛化和理解进行稳健的评估。我们的代码位于：https://github.com/Zxy-MLlab/LIBERO-PRO。"
        },
        {
          "title": "VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning",
          "url": "http://arxiv.org/abs/2509.25718v1",
          "snippet": "Reinforcement learning (RL) is a promising avenue for post-training vision-language-action (VLA) models, but practical deployment is hindered by sparse rewards and unstable training. This work mitigates these challenges by introducing an action chunk based on proximal policy optimization (PPO) with behavior cloning using self-collected demonstrations. Aggregating consecutive actions into chunks improves the temporal consistency of the policy and the density of informative feedback. In addition, an auxiliary behavior cloning loss is applied with a dynamically updated demonstration buffer that continually collects high-quality task trials during training. The relative weight between the action-chunked PPO objective and the self behavior clone auxiliary loss is adapted online to stabilize the post-training process. Experiments on the MetaWorld benchmark indicate improved performance over supervised fine-tuning, achieving a high success rate (0.93) and few steps to success (42.17). These results demonstrate the viability of RL for VLA post-training and help lay the groundwork for downstream VLA applications.",
          "site": "arxiv.org",
          "rank": 21,
          "published": "2025-09-30T03:24:20Z",
          "authors": [
            "Si-Cheng Wang",
            "Tian-Yu Xiang",
            "Xiao-Hu Zhou",
            "Mei-Jiang Gui",
            "Xiao-Liang Xie",
            "Shi-Qi Liu",
            "Shuang-Yi Wang",
            "Ao-Qun Jin",
            "Zeng-Guang Hou"
          ],
          "arxiv_id": "2509.25718",
          "abstract": "Reinforcement learning (RL) is a promising avenue for post-training vision-language-action (VLA) models, but practical deployment is hindered by sparse rewards and unstable training. This work mitigates these challenges by introducing an action chunk based on proximal policy optimization (PPO) with behavior cloning using self-collected demonstrations. Aggregating consecutive actions into chunks improves the temporal consistency of the policy and the density of informative feedback. In addition, an auxiliary behavior cloning loss is applied with a dynamically updated demonstration buffer that continually collects high-quality task trials during training. The relative weight between the action-chunked PPO objective and the self behavior clone auxiliary loss is adapted online to stabilize the post-training process. Experiments on the MetaWorld benchmark indicate improved performance over supervised fine-tuning, achieving a high success rate (0.93) and few steps to success (42.17). These results demonstrate the viability of RL for VLA post-training and help lay the groundwork for downstream VLA applications.",
          "abstract_zh": "强化学习（RL）是训练后视觉-语言-动作（VLA）模型的一个有前途的途径，但实际部署受到稀疏奖励和不稳定训练的阻碍。这项工作通过引入基于近端策略优化（PPO）的动作块以及使用自我收集的演示进行行为克隆来缓解这些挑战。将连续的动作聚合成块可以提高策略的时间一致性和信息反馈的密度。此外，辅助行为克隆损失与动态更新的演示缓冲区一起应用，该缓冲区在训练期间不断收集高质量的任务试验。在线调整动作分块 PPO 目标和自我行为克隆辅助损失之间的相对权重，以稳定训练后过程。MetaWorld 基准测试表明，与监督微调相比，性能得到了提高，取得了很高的成功率 (0.93)，并且成功的步骤很少 (42.17)。这些结果证明了 RL 在 VLA 后训练中的可行性，并有助于为下游 VLA 应用奠定基础。"
        },
        {
          "title": "PhysiAgent: An Embodied Agent Framework in Physical World",
          "url": "http://arxiv.org/abs/2509.24524v1",
          "snippet": "Vision-Language-Action (VLA) models have achieved notable success but often struggle with limited generalizations. To address this, integrating generalized Vision-Language Models (VLMs) as assistants to VLAs has emerged as a popular solution. However, current approaches often combine these models in rigid, sequential structures: using VLMs primarily for high-level scene understanding and task planning, and VLAs merely as executors of lower-level actions, leading to ineffective collaboration and poor grounding challenges. In this paper, we propose an embodied agent framework, PhysiAgent, tailored to operate effectively in physical environments. By incorporating monitor, memory, self-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent offers an autonomous scaffolding framework to prompt VLMs to organize different components based on real-time proficiency feedback from VLAs to maximally exploit VLAs' capabilities. Experimental results demonstrate significant improvements in task-solving performance on complex real-world robotic tasks, showcasing effective self-regulation of VLMs, coherent tool collaboration, and adaptive evolution of the framework during execution. PhysiAgent makes practical and pioneering efforts to integrate VLMs and VLAs, effectively grounding embodied agent frameworks in real-world settings.",
          "site": "arxiv.org",
          "rank": 22,
          "published": "2025-09-29T09:39:32Z",
          "authors": [
            "Zhihao Wang",
            "Jianxiong Li",
            "Jinliang Zheng",
            "Wencong Zhang",
            "Dongxiu Liu",
            "Yinan Zheng",
            "Haoyi Niu",
            "Junzhi Yu",
            "Xianyuan Zhan"
          ],
          "arxiv_id": "2509.24524",
          "abstract": "Vision-Language-Action (VLA) models have achieved notable success but often struggle with limited generalizations. To address this, integrating generalized Vision-Language Models (VLMs) as assistants to VLAs has emerged as a popular solution. However, current approaches often combine these models in rigid, sequential structures: using VLMs primarily for high-level scene understanding and task planning, and VLAs merely as executors of lower-level actions, leading to ineffective collaboration and poor grounding challenges. In this paper, we propose an embodied agent framework, PhysiAgent, tailored to operate effectively in physical environments. By incorporating monitor, memory, self-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent offers an autonomous scaffolding framework to prompt VLMs to organize different components based on real-time proficiency feedback from VLAs to maximally exploit VLAs' capabilities. Experimental results demonstrate significant improvements in task-solving performance on complex real-world robotic tasks, showcasing effective self-regulation of VLMs, coherent tool collaboration, and adaptive evolution of the framework during execution. PhysiAgent makes practical and pioneering efforts to integrate VLMs and VLAs, effectively grounding embodied agent frameworks in real-world settings.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型取得了显着的成功，但往往难以概括。为了解决这个问题，集成通用视觉语言模型 (VLM) 作为 VLA 的助手已成为一种流行的解决方案。然而，当前的方法通常将这些模型以严格的顺序结构结合起来：主要使用 VLM 进行高级场景理解和任务规划，而 VLA 仅作为较低级别操作的执行者，从而导致无效的协作和基础较差的挑战。在本文中，我们提出了一个具体的代理框架 PhysiAgent，专为在物理环境中有效运行而定制。通过整合监控、内存、自我反思机制和轻量级现成工具箱，PhysiAgent 提供了一个自主的脚手架框架，促使 VLM 根据 VLA 的实时熟练程度反馈来组织不同的组件，以最大限度地利用 VLA 的功能。实验结果表明，复杂的现实世界机器人任务的任务解决性能显着提高，展示了 VLM 的有效自我调节、连贯的工具协作以及执行过程中框架的自适应进化。PhysiAgent 在集成 VLM 和 VLA 方面做出了务实且开创性的努力，有效地将具体代理框架扎根于现实环境中。"
        },
        {
          "title": "AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation",
          "url": "http://arxiv.org/abs/2509.25032v1",
          "snippet": "As robots transition from controlled settings to unstructured human environments, building generalist agents that can reliably follow natural language instructions remains a central challenge. Progress in robust mobile manipulation requires large-scale multimodal datasets that capture contact-rich and long-horizon tasks, yet existing resources lack synchronized force-torque sensing, hierarchical annotations, and explicit failure cases. We address this gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset for mobile manipulation. It includes synchronized RGB images, joint states, six-axis wrist force-torque signals, and internal robot states, together with a novel two-layer annotation schema of sub-goals and primitive actions for hierarchical learning and error analysis. The initial dataset comprises 25,469 episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa provides a critical benchmark for advancing the next generation of Vision-Language-Action models. The first version of our dataset is now available at https://huggingface.co/datasets/airoa-org/airoa-moma .",
          "site": "arxiv.org",
          "rank": 23,
          "published": "2025-09-29T16:51:47Z",
          "authors": [
            "Ryosuke Takanami",
            "Petr Khrapchenkov",
            "Shu Morikuni",
            "Jumpei Arima",
            "Yuta Takaba",
            "Shunsuke Maeda",
            "Takuya Okubo",
            "Genki Sano",
            "Satoshi Sekioka",
            "Aoi Kadoya",
            "Motonari Kambara",
            "Naoya Nishiura",
            "Haruto Suzuki",
            "Takanori Yoshimoto",
            "Koya Sakamoto",
            "Shinnosuke Ono",
            "Hu Yang",
            "Daichi Yashima",
            "Aoi Horo",
            "Tomohiro Motoda",
            "Kensuke Chiyoma",
            "Hiroshi Ito",
            "Koki Fukuda",
            "Akihito Goto",
            "Kazumi Morinaga",
            "Yuya Ikeda",
            "Riko Kawada",
            "Masaki Yoshikawa",
            "Norio Kosuge",
            "Yuki Noguchi",
            "Kei Ota",
            "Tatsuya Matsushima",
            "Yusuke Iwasawa",
            "Yutaka Matsuo",
            "Tetsuya Ogata"
          ],
          "arxiv_id": "2509.25032",
          "abstract": "As robots transition from controlled settings to unstructured human environments, building generalist agents that can reliably follow natural language instructions remains a central challenge. Progress in robust mobile manipulation requires large-scale multimodal datasets that capture contact-rich and long-horizon tasks, yet existing resources lack synchronized force-torque sensing, hierarchical annotations, and explicit failure cases. We address this gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset for mobile manipulation. It includes synchronized RGB images, joint states, six-axis wrist force-torque signals, and internal robot states, together with a novel two-layer annotation schema of sub-goals and primitive actions for hierarchical learning and error analysis. The initial dataset comprises 25,469 episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa provides a critical benchmark for advancing the next generation of Vision-Language-Action models. The first version of our dataset is now available at https://huggingface.co/datasets/airoa-org/airoa-moma .",
          "abstract_zh": "随着机器人从受控环境过渡到非结构化人类环境，构建能够可靠地遵循自然语言指令的多面手代理仍然是一个核心挑战。鲁棒移动操纵的进展需要大规模多模态数据集来捕获接触丰富和长视野的任务，但现有资源缺乏同步的力-扭矩传感、分层注释和明确的故障案例。我们通过 AIRoA MoMa 数据集弥补了这一差距，这是一个用于移动操作的大规模现实世界多模式数据集。它包括同步 RGB 图像、关节状态、六轴手腕力扭矩信号和内部机器人状态，以及用于分层学习和错误分析的子目标和原始动作的新颖两层注释模式。初始数据集包含由人类支持机器人 (HSR) 收集的 25,469 个片段（约 94 小时），并以 LeRobot v2.1 格式完全标准化。通过独特地集成移动操作、丰富的接触交互和长视野结构，AIRoA MoMa 为推进下一代视觉-语言-动作模型提供了关键基准。我们数据集的第一个版本现已在 https://huggingface.co/datasets/airoa-org/airoa-moma 提供。"
        },
        {
          "title": "Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4",
          "url": "http://arxiv.org/abs/2510.02728v2",
          "snippet": "Cross-modal drone navigation remains a challenging task in robotics, requiring efficient retrieval of relevant images from large-scale databases based on natural language descriptions. The RoboSense 2025 Track 4 challenge addresses this challenge, focusing on robust, natural language-guided cross-view image retrieval across multiple platforms (drones, satellites, and ground cameras). Current baseline methods, while effective for initial retrieval, often struggle to achieve fine-grained semantic matching between text queries and visual content, especially in complex aerial scenes. To address this challenge, we propose a two-stage retrieval refinement method: Caption-Guided Retrieval System (CGRS) that enhances the baseline coarse ranking through intelligent reranking. Our method first leverages a baseline model to obtain an initial coarse ranking of the top 20 most relevant images for each query. We then use Vision-Language-Model (VLM) to generate detailed captions for these candidate images, capturing rich semantic descriptions of their visual content. These generated captions are then used in a multimodal similarity computation framework to perform fine-grained reranking of the original text query, effectively building a semantic bridge between the visual content and natural language descriptions. Our approach significantly improves upon the baseline, achieving a consistent 5\\% improvement across all key metrics (Recall@1, Recall@5, and Recall@10). Our approach win TOP-2 in the challenge, demonstrating the practical value of our semantic refinement strategy in real-world robotic navigation scenarios.",
          "site": "arxiv.org",
          "rank": 24,
          "published": "2025-10-03T05:13:19Z",
          "authors": [
            "Lingfeng Zhang",
            "Erjia Xiao",
            "Yuchen Zhang",
            "Haoxiang Fu",
            "Ruibin Hu",
            "Yanbiao Ma",
            "Wenbo Ding",
            "Long Chen",
            "Hangjun Ye",
            "Xiaoshuai Hao"
          ],
          "arxiv_id": "2510.02728",
          "abstract": "Cross-modal drone navigation remains a challenging task in robotics, requiring efficient retrieval of relevant images from large-scale databases based on natural language descriptions. The RoboSense 2025 Track 4 challenge addresses this challenge, focusing on robust, natural language-guided cross-view image retrieval across multiple platforms (drones, satellites, and ground cameras). Current baseline methods, while effective for initial retrieval, often struggle to achieve fine-grained semantic matching between text queries and visual content, especially in complex aerial scenes. To address this challenge, we propose a two-stage retrieval refinement method: Caption-Guided Retrieval System (CGRS) that enhances the baseline coarse ranking through intelligent reranking. Our method first leverages a baseline model to obtain an initial coarse ranking of the top 20 most relevant images for each query. We then use Vision-Language-Model (VLM) to generate detailed captions for these candidate images, capturing rich semantic descriptions of their visual content. These generated captions are then used in a multimodal similarity computation framework to perform fine-grained reranking of the original text query, effectively building a semantic bridge between the visual content and natural language descriptions. Our approach significantly improves upon the baseline, achieving a consistent 5\\% improvement across all key metrics (Recall@1, Recall@5, and Recall@10). Our approach win TOP-2 in the challenge, demonstrating the practical value of our semantic refinement strategy in real-world robotic navigation scenarios.",
          "abstract_zh": "跨模式无人机导航仍然是机器人技术中的一项具有挑战性的任务，需要根据自然语言描述从大规模数据库中高效检索相关图像。RoboSense 2025 Track 4 挑战赛解决了这一挑战，重点关注跨多个平台（无人机、卫星和地面摄像机）的稳健、自然语言引导的跨视图图像检索。当前的基线方法虽然对于初始检索有效，但通常难以实现文本查询和视觉内容之间的细粒度语义匹配，尤其是在复杂的航空场景中。为了应对这一挑战，我们提出了一种两阶段检索细化方法：标题引导检索系统（CGRS），它通过智能重新排名来增强基线粗排名。我们的方法首先利用基线模型来获得每个查询的前 20 张最相关图像的初始粗略排名。然后，我们使用视觉语言模型 (VLM) 为这些候选图像生成详细的说明文字，捕获其视觉内容的丰富语义描述。然后，将这些生成的标题用于多模态相似性计算框架，对原始文本查询进行细粒度的重新排序，从而有效地在视觉内容和自然语言描述之间建立语义桥梁。我们的方法在基线的基础上显着改进，在所有关键指标（Recall@1、Recall@5 和 Recall@10）上实现了一致的 5\\% 改进。我们的方法在挑战中赢得了 TOP-2，展示了我们的语义细化策略在现实世界机器人导航场景中的实用价值。"
        },
        {
          "title": "Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert",
          "url": "http://arxiv.org/abs/2510.03896v1",
          "snippet": "Although Vision-Language Models (VLM) have demonstrated impressive planning and reasoning capabilities, translating these abilities into the physical world introduces significant challenges. Conventional Vision-Language-Action (VLA) models, which integrate reasoning and action into a monolithic architecture, generalize poorly because they are constrained by scarce, narrow-domain data. While recent dual-system approaches attempt to decouple \"thinking\" from \"acting\", they are often constrained by semantic ambiguities within the action module. This ambiguity makes large-scale, cross-task training infeasible. Consequently, these systems typically necessitate fine-tuning on newly collected data when deployed to novel environments, and the cooperation mechanism between the two systems remains ill-defined. To address these limitations, we introduce, for the first time, a framework centered around a generalizable action expert. Our approach utilizes sparse 3D trajectories as an intermediate representation, effectively bridging the high-level planning capabilities of the VLM with the low-level physical action module. During the planning phase, the VLM is only required to generate coarse 3D waypoints. These waypoints are then processed by our generalizable action expert, which refines them into dense, executable action sequences by sampling real-time point cloud observations of the environment. To promote training efficiency and robust generalization, we introduce a novel \"Action Pre-training, Pointcloud Fine-tuning\" paradigm. Our method combines the broad generalization capabilities of VLMs in visual understanding and planning with the fine-grained, action-level generalization of action expert.",
          "site": "arxiv.org",
          "rank": 25,
          "published": "2025-10-04T18:33:27Z",
          "authors": [
            "Mingyu Liu",
            "Zheng Huang",
            "Xiaoyi Lin",
            "Muzhi Zhu",
            "Canyu Zhao",
            "Zongze Du",
            "Yating Wang",
            "Haoyi Zhu",
            "Hao Chen",
            "Chunhua Shen"
          ],
          "arxiv_id": "2510.03896",
          "abstract": "Although Vision-Language Models (VLM) have demonstrated impressive planning and reasoning capabilities, translating these abilities into the physical world introduces significant challenges. Conventional Vision-Language-Action (VLA) models, which integrate reasoning and action into a monolithic architecture, generalize poorly because they are constrained by scarce, narrow-domain data. While recent dual-system approaches attempt to decouple \"thinking\" from \"acting\", they are often constrained by semantic ambiguities within the action module. This ambiguity makes large-scale, cross-task training infeasible. Consequently, these systems typically necessitate fine-tuning on newly collected data when deployed to novel environments, and the cooperation mechanism between the two systems remains ill-defined. To address these limitations, we introduce, for the first time, a framework centered around a generalizable action expert. Our approach utilizes sparse 3D trajectories as an intermediate representation, effectively bridging the high-level planning capabilities of the VLM with the low-level physical action module. During the planning phase, the VLM is only required to generate coarse 3D waypoints. These waypoints are then processed by our generalizable action expert, which refines them into dense, executable action sequences by sampling real-time point cloud observations of the environment. To promote training efficiency and robust generalization, we introduce a novel \"Action Pre-training, Pointcloud Fine-tuning\" paradigm. Our method combines the broad generalization capabilities of VLMs in visual understanding and planning with the fine-grained, action-level generalization of action expert.",
          "abstract_zh": "尽管视觉语言模型 (VLM) 已展现出令人印象深刻的规划和推理能力，但将这些能力转化为物理世界会带来重大挑战。传统的视觉-语言-动作（VLA）模型将推理和动作集成到一个整体架构中，但由于受到稀缺、窄域数据的限制，泛化能力很差。虽然最近的双系统方法试图将“思考”与“行动”分离，但它们常常受到行动模块内语义模糊性的限制。这种模糊性使得大规模的跨任务训练变得不可行。因此，这些系统在部署到新环境时通常需要对新收集的数据进行微调，并且两个系统之间的合作机制仍然不明确。为了解决这些限制，我们首次引入了一个以通用行动专家为中心的框架。我们的方法利用稀疏 3D 轨迹作为中间表示，有效地将 VLM 的高级规划功能与低级物理动作模块联系起来。在规划阶段，VLM 只需要生成粗略的 3D 航路点。然后，这些路径点由我们的通用动作专家处理，通过对环境的实时点云观测进行采样，将它们细化为密集的、可执行的动作序列。为了提高训练效率和鲁棒泛化能力，我们引入了一种新颖的“动作预训练，点云微调”范式。我们的方法将 VLM 在视觉理解和规划方面的广泛泛化能力与动作专家的细粒度、动作级泛化能力相结合。"
        },
        {
          "title": "TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses",
          "url": "http://arxiv.org/abs/2509.25746v1",
          "snippet": "Despite progress in both traditional dexterous grasping pipelines and recent Vision-Language-Action (VLA) approaches, the grasp execution stage remains prone to pose inaccuracies, especially in long-horizon tasks, which undermines overall performance. To address this \"last-mile\" challenge, we propose TacRefineNet, a tactile-only framework that achieves fine in-hand pose refinement of known objects in arbitrary target poses using multi-finger fingertip sensing. Our method iteratively adjusts the end-effector pose based on tactile feedback, aligning the object to the desired configuration. We design a multi-branch policy network that fuses tactile inputs from multiple fingers along with proprioception to predict precise control updates. To train this policy, we combine large-scale simulated data from a physics-based tactile model in MuJoCo with real-world data collected from a physical system. Comparative experiments show that pretraining on simulated data and fine-tuning with a small amount of real data significantly improves performance over simulation-only training. Extensive real-world experiments validate the effectiveness of the method, achieving millimeter-level grasp accuracy using only tactile input. To our knowledge, this is the first method to enable arbitrary in-hand pose refinement via multi-finger tactile sensing alone. Project website is available at https://sites.google.com/view/tacrefinenet",
          "site": "arxiv.org",
          "rank": 26,
          "published": "2025-09-30T04:05:03Z",
          "authors": [
            "Shuaijun Wang",
            "Haoran Zhou",
            "Diyun Xiang",
            "Yangwei You"
          ],
          "arxiv_id": "2509.25746",
          "abstract": "Despite progress in both traditional dexterous grasping pipelines and recent Vision-Language-Action (VLA) approaches, the grasp execution stage remains prone to pose inaccuracies, especially in long-horizon tasks, which undermines overall performance. To address this \"last-mile\" challenge, we propose TacRefineNet, a tactile-only framework that achieves fine in-hand pose refinement of known objects in arbitrary target poses using multi-finger fingertip sensing. Our method iteratively adjusts the end-effector pose based on tactile feedback, aligning the object to the desired configuration. We design a multi-branch policy network that fuses tactile inputs from multiple fingers along with proprioception to predict precise control updates. To train this policy, we combine large-scale simulated data from a physics-based tactile model in MuJoCo with real-world data collected from a physical system. Comparative experiments show that pretraining on simulated data and fine-tuning with a small amount of real data significantly improves performance over simulation-only training. Extensive real-world experiments validate the effectiveness of the method, achieving millimeter-level grasp accuracy using only tactile input. To our knowledge, this is the first method to enable arbitrary in-hand pose refinement via multi-finger tactile sensing alone. Project website is available at https://sites.google.com/view/tacrefinenet",
          "abstract_zh": "尽管传统的灵巧抓取管道和最近的视觉-语言-动作（VLA）方法都取得了进展，但抓取执行阶段仍然容易出现不准确的情况，特别是在长期任务中，这会损害整体性能。为了解决这一“最后一英里”的挑战，我们提出了 TacRefineNet，这是一种纯触觉框架，可使用多指指尖感应对任意目标姿势中的已知物体进行精细的手中姿势细化。我们的方法根据触觉反馈迭代调整末端执行器姿势，将物体对齐到所需的配置。我们设计了一个多分支策略网络，将多个手指的触觉输入与本体感觉融合在一起，以预测精确的控制更新。为了训练这一策略，我们将 MuJoCo 中基于物理的触觉模型的大规模模拟数据与从物理系统收集的真实世界数据相结合。对比实验表明，对模拟数据进行预训练并用少量真实数据进行微调，比仅模拟训练显着提高了性能。大量的现实世界实验验证了该方法的有效性，仅使用触觉输入即可实现毫米级的抓取精度。据我们所知，这是第一种仅通过多指触觉感知来实现任意手部姿势细化的方法。项目网站位于 https://sites.google.com/view/tacrefinenet"
        },
        {
          "title": "MUVLA: Learning to Explore Object Navigation via Map Understanding",
          "url": "http://arxiv.org/abs/2509.25966v1",
          "snippet": "In this paper, we present MUVLA, a Map Understanding Vision-Language-Action model tailored for object navigation. It leverages semantic map abstractions to unify and structure historical information, encoding spatial context in a compact and consistent form. MUVLA takes the current and history observations, as well as the semantic map, as inputs and predicts the action sequence based on the description of goal object. Furthermore, it amplifies supervision through reward-guided return modeling based on dense short-horizon progress signals, enabling the model to develop a detailed understanding of action value for reward maximization. MUVLA employs a three-stage training pipeline: learning map-level spatial understanding, imitating behaviors from mixed-quality demonstrations, and reward amplification. This strategy allows MUVLA to unify diverse demonstrations into a robust spatial representation and generate more rational exploration strategies. Experiments on HM3D and Gibson benchmarks demonstrate that MUVLA achieves great generalization and learns effective exploration behaviors even from low-quality or partially successful trajectories.",
          "site": "arxiv.org",
          "rank": 27,
          "published": "2025-09-30T09:02:58Z",
          "authors": [
            "Peilong Han",
            "Fan Jia",
            "Min Zhang",
            "Yutao Qiu",
            "Hongyao Tang",
            "Yan Zheng",
            "Tiancai Wang",
            "Jianye Hao"
          ],
          "arxiv_id": "2509.25966",
          "abstract": "In this paper, we present MUVLA, a Map Understanding Vision-Language-Action model tailored for object navigation. It leverages semantic map abstractions to unify and structure historical information, encoding spatial context in a compact and consistent form. MUVLA takes the current and history observations, as well as the semantic map, as inputs and predicts the action sequence based on the description of goal object. Furthermore, it amplifies supervision through reward-guided return modeling based on dense short-horizon progress signals, enabling the model to develop a detailed understanding of action value for reward maximization. MUVLA employs a three-stage training pipeline: learning map-level spatial understanding, imitating behaviors from mixed-quality demonstrations, and reward amplification. This strategy allows MUVLA to unify diverse demonstrations into a robust spatial representation and generate more rational exploration strategies. Experiments on HM3D and Gibson benchmarks demonstrate that MUVLA achieves great generalization and learns effective exploration behaviors even from low-quality or partially successful trajectories.",
          "abstract_zh": "在本文中，我们提出了 MUVLA，一种专为对象导航量身定制的地图理解视觉-语言-动作模型。它利用语义地图抽象来统一和构建历史信息，以紧凑且一致的形式编码空间上下文。MUVLA 将当前和历史观察以及语义图作为输入，并根据目标对象的描述来预测动作序列。此外，它通过基于密集的短期进度信号的奖励引导回报模型来放大监督，使模型能够详细了解奖励最大化的行动价值。MUVLA 采用三阶段训练流程：学习地图级空间理解、模仿混合质量演示的行为以及奖励放大。该策略使 MUVLA 能够将不同的演示统一为强大的空间表示，并生成更合理的探索策略。HM3D 和 Gibson 基准测试表明，MUVLA 实现了很好的泛化，即使从低质量或部分成功的轨迹中也能学习有效的探索行为。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 9 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 9）。",
      "items": [
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "ReinFlow/ReinFlow",
          "url": "https://github.com/ReinFlow/ReinFlow",
          "snippet": "[NeurIPS 2025] Flow x RL. \"ReinFlow: Fine-tuning Flow Policy with Online Reinforcement Learning\". Support VLAs e.g., pi0, pi0.5. Fully open-sourced. ",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "HCPLab-SYSU/Embodied_AI_Paper_List",
          "url": "https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List",
          "snippet": "[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "FlagOpen/RoboBrain2.0",
          "url": "https://github.com/FlagOpen/RoboBrain2.0",
          "snippet": "RoboBrain 2.0: Advanced version of RoboBrain. See Better. Think Harder. Do Smarter. 🎉🎉🎉",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "BridgeVLA/BridgeVLA",
          "url": "https://github.com/BridgeVLA/BridgeVLA",
          "snippet": "✨✨【NeurIPS 2025】Official implementation of BridgeVLA",
          "site": "github.com",
          "rank": 9
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-09-29",
  "week_end": "2025-10-05",
  "last_updated": "2026-01-07"
}