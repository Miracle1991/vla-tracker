{
  "generated_at": "2026-01-07T13:28:47.014462",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 22 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 22）。",
      "items": [
        {
          "title": "RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI",
          "url": "http://arxiv.org/abs/2509.14687v1",
          "snippet": "The emerging field of Vision-Language-Action (VLA) for humanoid robots faces several fundamental challenges, including the high cost of data acquisition, the lack of a standardized benchmark, and the significant gap between simulation and the real world. To overcome these obstacles, we propose RealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror builds an efficient, low-cost data collection, model training, and inference system that enables end-to-end VLA research without requiring a real robot. To facilitate model evolution and fair comparison, we also introduce a dedicated VLA benchmark for humanoid robots, featuring multiple scenarios, extensive trajectories, and various VLA models. Furthermore, by integrating generative models and 3D Gaussian Splatting to reconstruct realistic environments and robot models, we successfully demonstrate zero-shot Sim2Real transfer, where models trained exclusively on simulation data can perform tasks on a real robot seamlessly, without any fine-tuning. In conclusion, with the unification of these critical components, RealMirror provides a robust framework that significantly accelerates the development of VLA models for humanoid robots. Project page: https://terminators2025.github.io/RealMirror.github.io",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-09-18T07:27:34Z",
          "authors": [
            "Cong Tai",
            "Zhaoyu Zheng",
            "Haixu Long",
            "Hansheng Wu",
            "Haodong Xiang",
            "Zhengbin Long",
            "Jun Xiong",
            "Rong Shi",
            "Shizhuang Zhang",
            "Gang Qiu",
            "He Wang",
            "Ruifeng Li",
            "Jun Huang",
            "Bin Chang",
            "Shuai Feng",
            "Tao Shen"
          ],
          "arxiv_id": "2509.14687",
          "abstract": "The emerging field of Vision-Language-Action (VLA) for humanoid robots faces several fundamental challenges, including the high cost of data acquisition, the lack of a standardized benchmark, and the significant gap between simulation and the real world. To overcome these obstacles, we propose RealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror builds an efficient, low-cost data collection, model training, and inference system that enables end-to-end VLA research without requiring a real robot. To facilitate model evolution and fair comparison, we also introduce a dedicated VLA benchmark for humanoid robots, featuring multiple scenarios, extensive trajectories, and various VLA models. Furthermore, by integrating generative models and 3D Gaussian Splatting to reconstruct realistic environments and robot models, we successfully demonstrate zero-shot Sim2Real transfer, where models trained exclusively on simulation data can perform tasks on a real robot seamlessly, without any fine-tuning. In conclusion, with the unification of these critical components, RealMirror provides a robust framework that significantly accelerates the development of VLA models for humanoid robots. Project page: https://terminators2025.github.io/RealMirror.github.io",
          "abstract_zh": "人形机器人视觉-语言-动作（VLA）这一新兴领域面临着几个基本挑战，包括数据采集成本高、缺乏标准化基准以及模拟与现实世界之间的巨大差距。为了克服这些障碍，我们提出了 RealMirror，这是一个全面的、开源的 AI VLA 平台。RealMirror 构建了一个高效、低成本的数据收集、模型训练和推理系统，无需真正的机器人即可实现端到端 VLA 研究。为了促进模型进化和公平比较，我们还引入了专门用于人形机器人的VLA基准，具有多场景、广泛的轨迹和各种VLA模型。此外，通过集成生成模型和 3D Gaussian Splatting 来重建现实环境和机器人模型，我们成功演示了零样本 Sim2Real 迁移，其中专门基于仿真数据训练的模型可以在真实机器人上无缝执行任务，无需任何微调。总之，通过统一这些关键组件，RealMirror 提供了一个强大的框架，可显着加速人形机器人 VLA 模型的开发。项目页面：https://terminators2025.github.io/RealMirror.github.io"
        },
        {
          "title": "Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale",
          "url": "http://arxiv.org/abs/2509.14932v1",
          "snippet": "Vision-Language-Action models (VLAs) mark a major shift in robot learning. They replace specialized architectures and task-tailored components of expert policies with large-scale data collection and setup-specific fine-tuning. In this machine learning-focused workflow that is centered around models and scalable training, traditional robotics software frameworks become a bottleneck, while robot simulations offer only limited support for transitioning from and to real-world experiments. In this work, we close this gap by introducing Robot Control Stack (RCS), a lean ecosystem designed from the ground up to support research in robot learning with large-scale generalist policies. At its core, RCS features a modular and easily extensible layered architecture with a unified interface for simulated and physical robots, facilitating sim-to-real transfer. Despite its minimal footprint and dependencies, it offers a complete feature set, enabling both real-world experiments and large-scale training in simulation. Our contribution is twofold: First, we introduce the architecture of RCS and explain its design principles. Second, we evaluate its usability and performance along the development cycle of VLA and RL policies. Our experiments also provide an extensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed light on how simulation data can improve real-world policy performance. Our code, datasets, weights, and videos are available at: https://robotcontrolstack.github.io/",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-09-18T13:12:16Z",
          "authors": [
            "Tobias Jülg",
            "Pierre Krack",
            "Seongjin Bien",
            "Yannik Blei",
            "Khaled Gamal",
            "Ken Nakahara",
            "Johannes Hechtl",
            "Roberto Calandra",
            "Wolfram Burgard",
            "Florian Walter"
          ],
          "arxiv_id": "2509.14932",
          "abstract": "Vision-Language-Action models (VLAs) mark a major shift in robot learning. They replace specialized architectures and task-tailored components of expert policies with large-scale data collection and setup-specific fine-tuning. In this machine learning-focused workflow that is centered around models and scalable training, traditional robotics software frameworks become a bottleneck, while robot simulations offer only limited support for transitioning from and to real-world experiments. In this work, we close this gap by introducing Robot Control Stack (RCS), a lean ecosystem designed from the ground up to support research in robot learning with large-scale generalist policies. At its core, RCS features a modular and easily extensible layered architecture with a unified interface for simulated and physical robots, facilitating sim-to-real transfer. Despite its minimal footprint and dependencies, it offers a complete feature set, enabling both real-world experiments and large-scale training in simulation. Our contribution is twofold: First, we introduce the architecture of RCS and explain its design principles. Second, we evaluate its usability and performance along the development cycle of VLA and RL policies. Our experiments also provide an extensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed light on how simulation data can improve real-world policy performance. Our code, datasets, weights, and videos are available at: https://robotcontrolstack.github.io/",
          "abstract_zh": "视觉-语言-动作模型（VLA）标志着机器人学习的重大转变。它们用大规模数据收集和特定于设置的微调来取代专家策略的专门架构和任务定制组件。在这种以模型和可扩展训练为中心的以机器学习为中心的工作流程中，传统的机器人软件框架成为瓶颈，而机器人模拟仅为现实世界实验的过渡提供有限的支持。在这项工作中，我们通过引入机器人控制堆栈（RCS）来缩小这一差距，这是一个从头开始设计的精益生态系统，旨在支持具有大规模通用政策的机器人学习研究。RCS 的核心特点是模块化且易于扩展的分层架构，为模拟和物理机器人提供统一的接口，促进模拟到真实的转换。尽管其占用空间和依赖性最小，但它提供了完整的功能集，支持现实世界的实验和大规模的模拟训练。我们的贡献是双重的：首先，我们介绍了 RCS 的架构并解释了其设计原理。其次，我们沿着 VLA 和 RL 策略的开发周期评估其可用性和性能。我们的实验还对多个机器人上的 Octo、OpenVLA 和 Pi Zero 进行了广泛的评估，并揭示了模拟数据如何提高现实世界的策略性能。我们的代码、数据集、权重和视频可在以下位置获取：https://robotcontrolstack.github.io/"
        },
        {
          "title": "GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model",
          "url": "http://arxiv.org/abs/2509.14117v3",
          "snippet": "Vision-Language-Action (VLA) models often fail to generalize to novel camera viewpoints, a limitation stemming from their difficulty in inferring robust 3D geometry from 2D images. We introduce GeoAware-VLA, a simple yet effective approach that enhances viewpoint invariance by integrating strong geometric priors into the vision backbone. Instead of training a visual encoder or relying on explicit 3D data, we leverage a frozen, pretrained geometric vision model as a feature extractor. A trainable projection layer then adapts these geometrically-rich features for the policy decoder, relieving it of the burden of learning 3D consistency from scratch. Through extensive evaluations on LIBERO benchmark subsets, we show GeoAware-VLA achieves substantial improvements in zero-shot generalization to novel camera poses, boosting success rates by over 2x in simulation. Crucially, these benefits translate to the physical world; our model shows a significant performance gain on a real robot, especially when evaluated from unseen camera angles. Our approach proves effective across both continuous and discrete action spaces, highlighting that robust geometric grounding is a key component for creating more generalizable robotic agents.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-09-17T15:57:51Z",
          "authors": [
            "Ali Abouzeid",
            "Malak Mansour",
            "Zezhou Sun",
            "Dezhen Song"
          ],
          "arxiv_id": "2509.14117",
          "abstract": "Vision-Language-Action (VLA) models often fail to generalize to novel camera viewpoints, a limitation stemming from their difficulty in inferring robust 3D geometry from 2D images. We introduce GeoAware-VLA, a simple yet effective approach that enhances viewpoint invariance by integrating strong geometric priors into the vision backbone. Instead of training a visual encoder or relying on explicit 3D data, we leverage a frozen, pretrained geometric vision model as a feature extractor. A trainable projection layer then adapts these geometrically-rich features for the policy decoder, relieving it of the burden of learning 3D consistency from scratch. Through extensive evaluations on LIBERO benchmark subsets, we show GeoAware-VLA achieves substantial improvements in zero-shot generalization to novel camera poses, boosting success rates by over 2x in simulation. Crucially, these benefits translate to the physical world; our model shows a significant performance gain on a real robot, especially when evaluated from unseen camera angles. Our approach proves effective across both continuous and discrete action spaces, highlighting that robust geometric grounding is a key component for creating more generalizable robotic agents.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型通常无法推广到新颖的相机视点，这是由于它们难以从 2D 图像推断出稳健的 3D 几何形状而造成的限制。我们引入了 GeoAware-VLA，这是一种简单而有效的方法，通过将强大的几何先验集成到视觉主干中来增强视点不变性。我们没有训练视觉编码器或依赖显式 3D 数据，而是利用冻结的、预先训练的几何视觉模型作为特征提取器。然后，可训练的投影层会为策略解码器调整这些丰富的几何特征，从而减轻其从头开始学习 3D 一致性的负担。通过对 LIBERO 基准子集的广泛评估，我们表明 GeoAware-VLA 在零镜头泛化到新相机姿势方面取得了显着改进，将模拟成功率提高了 2 倍以上。至关重要的是，这些好处可以转化为现实世界。我们的模型在真实机器人上显示出显着的性能提升，特别是从看不见的摄像机角度进行评估时。事实证明，我们的方法在连续和离散动作空间中都是有效的，这强调了强大的几何基础是创建更通用的机器人代理的关键组成部分。"
        },
        {
          "title": "CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping",
          "url": "http://arxiv.org/abs/2509.14143v1",
          "snippet": "Vision-language-action (VLA) models have recently emerged as a promising paradigm for robotic control, enabling end-to-end policies that ground natural language instructions into visuomotor actions. However, current VLAs often struggle to satisfy precise task constraints, such as stopping based on numeric thresholds, since their observation-to-action mappings are implicitly shaped by training data and lack explicit mechanisms for condition monitoring. In this work, we propose CLAW (CLIP-Language-Action for Weight), a framework that decouples condition evaluation from action generation. CLAW leverages a fine-tuned CLIP model as a lightweight prompt generator, which continuously monitors the digital readout of a scale and produces discrete directives based on task-specific weight thresholds. These prompts are then consumed by $π_0$, a flow-based VLA policy, which integrates the prompts with multi-view camera observations to produce continuous robot actions. This design enables CLAW to combine symbolic weight reasoning with high-frequency visuomotor control. We validate CLAW on three experimental setups: single-object grasping and mixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW reliably executes weight-aware behaviors and outperforms both raw-$π_0$ and fine-tuned $π_0$ models. We have uploaded the videos as supplementary materials.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-09-17T16:22:25Z",
          "authors": [
            "Zijian An",
            "Ran Yang",
            "Yiming Feng",
            "Lifeng Zhou"
          ],
          "arxiv_id": "2509.14143",
          "abstract": "Vision-language-action (VLA) models have recently emerged as a promising paradigm for robotic control, enabling end-to-end policies that ground natural language instructions into visuomotor actions. However, current VLAs often struggle to satisfy precise task constraints, such as stopping based on numeric thresholds, since their observation-to-action mappings are implicitly shaped by training data and lack explicit mechanisms for condition monitoring. In this work, we propose CLAW (CLIP-Language-Action for Weight), a framework that decouples condition evaluation from action generation. CLAW leverages a fine-tuned CLIP model as a lightweight prompt generator, which continuously monitors the digital readout of a scale and produces discrete directives based on task-specific weight thresholds. These prompts are then consumed by $π_0$, a flow-based VLA policy, which integrates the prompts with multi-view camera observations to produce continuous robot actions. This design enables CLAW to combine symbolic weight reasoning with high-frequency visuomotor control. We validate CLAW on three experimental setups: single-object grasping and mixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW reliably executes weight-aware behaviors and outperforms both raw-$π_0$ and fine-tuned $π_0$ models. We have uploaded the videos as supplementary materials.",
          "abstract_zh": "视觉-语言-动作（VLA）模型最近已成为机器人控制的一种有前途的范例，它支持将自然语言指令转化为视觉运动动作的端到端策略。然而，当前的 VLA 通常难以满足精确的任务约束，例如基于数字阈值停止，因为它们的观察到动作映射是由训练数据隐式塑造的，并且缺乏明确的状态监测机制。在这项工作中，我们提出了 CLAW（CLIP-Language-Action for Weight），这是一个将条件评估与动作生成分离的框架。CLAW 利用微调的 CLIP 模型作为轻量级提示生成器，持续监控秤的数字读数，并根据特定任务的重量阈值生成离散指令。然后，这些提示会被 $π_0$ 使用，这是一种基于流的 VLA 策略，它将提示与多视图摄像机观察相集成，以产生连续的机器人动作。这种设计使 CLAW 能够将符号重量推理与高频视觉运动控制结合起来。我们在三个实验设置上验证了 CLAW：单物体抓取和需要双臂操作的混合物体任务。在所有条件下，CLAW 都能可靠地执行重量感知行为，并且性能优于原始 $π_0$ 和微调 $π_0$ 模型。我们已上传视频作为补充材料。"
        },
        {
          "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies",
          "url": "http://arxiv.org/abs/2509.18282v1",
          "snippet": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: 1. end-effector paths specifying what actions to take, and 2. task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need--where, what, and how. Website at https://peek-robot.github.io/.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-09-22T18:10:14Z",
          "authors": [
            "Jesse Zhang",
            "Marius Memmel",
            "Kevin Kim",
            "Dieter Fox",
            "Jesse Thomason",
            "Fabio Ramos",
            "Erdem Bıyık",
            "Abhishek Gupta",
            "Anqi Li"
          ],
          "arxiv_id": "2509.18282",
          "abstract": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: 1. end-effector paths specifying what actions to take, and 2. task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need--where, what, and how. Website at https://peek-robot.github.io/.",
          "abstract_zh": "机器人操纵策略通常无法概括，因为它们必须同时学习去哪里参加、采取什么行动以及如何执行这些行动。我们认为，关于哪里和什么的高级推理可以转移到视觉语言模型（VLM），让政策专门研究如何采取行动。我们提出了 PEEK（与策略无关的基本关键点提取），它可以微调 VLM 以预测统一的基于点的中间表示：1. 末端执行器路径指定要采取的操作，2. 与任务相关的掩码指示要关注的位置。这些注释直接覆盖到机器人观察上，使得表示与策略无关并且可以跨架构转移。为了实现可扩展的训练，我们引入了自动注释管道，跨 9 个实施例的 20 多个机器人数据集生成标记数据。在现实世界的评估中，PEEK 始终如一地提高了零样本泛化能力，包括仅在模拟中训练的 3D 策略在现实世界中的改进为 41.4 倍，以及大型 VLA 和小型操纵策略的 2-3.5 倍的增益。通过让 VLM 吸收语义和视觉的复杂性，PEEK 为操作策略配备了所需的最少线索——地点、内容和方式。网站 https://peek-robot.github.io/。"
        },
        {
          "title": "A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning",
          "url": "http://arxiv.org/abs/2509.15937v1",
          "snippet": "Robotic real-world reinforcement learning (RL) with vision-language-action (VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient exploration. We introduce VLAC, a general process reward model built upon InternVL and trained on large scale heterogeneous datasets. Given pairwise observations and a language goal, it outputs dense progress delta and done signal, eliminating task-specific reward engineering, and supports one-shot in-context transfer to unseen tasks and environments. VLAC is trained on vision-language datasets to strengthen perception, dialogic and reasoning capabilities, together with robot and human trajectories data that ground action generation and progress estimation, and additionally strengthened to reject irrelevant prompts as well as detect regression or stagnation by constructing large numbers of negative and semantically mismatched samples. With prompt control, a single VLAC model alternately generating reward and action tokens, unifying critic and policy. Deployed inside an asynchronous real-world RL loop, we layer a graded human-in-the-loop protocol (offline demonstration replay, return and explore, human guided explore) that accelerates exploration and stabilizes early learning. Across four distinct real-world manipulation tasks, VLAC lifts success rates from about 30\\% to about 90\\% within 200 real-world interaction episodes; incorporating human-in-the-loop interventions yields a further 50% improvement in sample efficiency and achieves up to 100% final success.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-09-19T12:44:29Z",
          "authors": [
            "Shaopeng Zhai",
            "Qi Zhang",
            "Tianyi Zhang",
            "Fuxian Huang",
            "Haoran Zhang",
            "Ming Zhou",
            "Shengzhe Zhang",
            "Litao Liu",
            "Sixu Lin",
            "Jiangmiao Pang"
          ],
          "arxiv_id": "2509.15937",
          "abstract": "Robotic real-world reinforcement learning (RL) with vision-language-action (VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient exploration. We introduce VLAC, a general process reward model built upon InternVL and trained on large scale heterogeneous datasets. Given pairwise observations and a language goal, it outputs dense progress delta and done signal, eliminating task-specific reward engineering, and supports one-shot in-context transfer to unseen tasks and environments. VLAC is trained on vision-language datasets to strengthen perception, dialogic and reasoning capabilities, together with robot and human trajectories data that ground action generation and progress estimation, and additionally strengthened to reject irrelevant prompts as well as detect regression or stagnation by constructing large numbers of negative and semantically mismatched samples. With prompt control, a single VLAC model alternately generating reward and action tokens, unifying critic and policy. Deployed inside an asynchronous real-world RL loop, we layer a graded human-in-the-loop protocol (offline demonstration replay, return and explore, human guided explore) that accelerates exploration and stabilizes early learning. Across four distinct real-world manipulation tasks, VLAC lifts success rates from about 30\\% to about 90\\% within 200 real-world interaction episodes; incorporating human-in-the-loop interventions yields a further 50% improvement in sample efficiency and achieves up to 100% final success.",
          "abstract_zh": "采用视觉-语言-动作 (VLA) 模型的机器人现实世界强化学习 (RL) 受到稀疏、手工设计的奖励和低效探索的瓶颈。我们引入了 VLAC，这是一种基于 InternVL 构建并在大规模异构数据集上进行训练的通用过程奖励模型。给定成对观察和语言目标，它输出密集的进度增量和完成信号，消除特定于任务的奖励工程，并支持一次性上下文内迁移到未见过的任务和环境。VLAC 经过视觉语言数据集的训练，以增强感知、对话和推理能力，以及机器人和人类轨迹数据，为动作生成和进度估计奠定基础，并进一步增强拒绝不相关提示的能力，并通过构建大量负面和语义不匹配的样本来检测回归或停滞。通过即时控制，单个 VLAC 模型交替生成奖励和行动代币，统一批评者和政策。我们部署在异步现实世界 RL 循环内，分层分级人机循环协议（离线演示重播、返回和探索、人类引导探索），可加速探索并稳定早期学习。在四个不同的现实世界操作任务中，VLAC 在 200 个现实世界交互事件中将成功率从大约 30% 提高到大约 90%；结合人机交互干预，样本效率进一步提高 50%，最终成功率高达 100%。"
        },
        {
          "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
          "url": "http://arxiv.org/abs/2509.15212v1",
          "snippet": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-09-18T17:58:02Z",
          "authors": [
            "Yuming Jiang",
            "Siteng Huang",
            "Shengke Xue",
            "Yaxi Zhao",
            "Jun Cen",
            "Sicong Leng",
            "Kehan Li",
            "Jiayan Guo",
            "Kexiang Wang",
            "Mingxiu Chen",
            "Fan Wang",
            "Deli Zhao",
            "Xin Li"
          ],
          "arxiv_id": "2509.15212",
          "abstract": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.",
          "abstract_zh": "本文提出了 RynnVLA-001，这是一种基于人类演示的大规模视频生成预训练构建的视觉-语言-动作 (VLA) 模型。我们提出了一种新颖的两阶段预训练方法。第一阶段是以自我为中心的视频生成预训练，在 1200 万个以自我为中心的操作视频上训练图像到视频模型，以预测以初始帧和语言指令为条件的未来帧。第二阶段，以人为中心的轨迹感知建模，通过联合预测未来关键点轨迹来扩展这一阶段，从而有效地将视觉帧预测与动作预测联系起来。此外，为了增强动作表示，我们提出了 ActionVAE，一种变分自动编码器，它将动作序列压缩为紧凑的潜在嵌入，从而降低了 VLA 输出空间的复杂性。当在相同的下游机器人数据集上进行微调时，RynnVLA-001 实现了优于最先进基线的性能，表明所提出的预训练策略为 VLA 模型提供了更有效的初始化。"
        },
        {
          "title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs",
          "url": "http://arxiv.org/abs/2509.11480v1",
          "snippet": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-09-15T00:00:37Z",
          "authors": [
            "Amir Taherin",
            "Juyi Lin",
            "Arash Akbari",
            "Arman Akbari",
            "Pu Zhao",
            "Weiwei Chen",
            "David Kaeli",
            "Yanzhi Wang"
          ],
          "arxiv_id": "2509.11480",
          "abstract": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.",
          "abstract_zh": "视觉-语言-动作（VLA）模型已经成为机器人控制的强大通用策略，但它们在模型架构和硬件平台上的性能扩展以及相关的功率预算仍然知之甚少。这项工作对五种代表性 VLA 模型进行了评估，涵盖最先进的基线和两种新提出的架构，针对边缘和数据中心 GPU 平台。使用 LIBERO 基准，我们在不同的边缘功率限制和高性能数据中心 GPU 配置下测量准确性以及系统级指标，包括延迟、吞吐量和峰值内存使用情况。我们的结果确定了不同的扩展趋势：（1）架构选择，例如动作标记化和模型骨干大小，强烈影响吞吐量和内存占用；(2) 功率受限的边缘设备表现出非线性性能下降，某些配置匹配或超过旧数据中心 GPU；(3) 可以实现高通量变体，而不会显着损失准确性。这些发现为跨一系列部署限制选择和优化 VLA 提供了可行的见解。我们的工作挑战了当前关于数据中心硬件在机器人推理方面的优越性的假设。"
        },
        {
          "title": "VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation",
          "url": "http://arxiv.org/abs/2509.18183v1",
          "snippet": "The Visual-Language-Action (VLA) models can follow text instructions according to visual observations of the surrounding environment. This ability to map multimodal inputs to actions is derived from the training of the VLA model on extensive standard demonstrations. These visual observations captured by third-personal global and in-wrist local cameras are inevitably varied in number and perspective across different environments, resulting in significant differences in the visual features. This perspective heterogeneity constrains the generality of VLA models. In light of this, we first propose the lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models using only 2D data. VLA-LPAF is finetuned using images from a single view and fuses other multiview observations in the latent space, which effectively and efficiently bridge the gap caused by perspective inconsistency. We instantiate our VLA-LPAF framework with the VLA model RoboFlamingo to construct RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a customized simulation benchmark. We also demonstrate the developed viewadaptive characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-09-18T05:24:39Z",
          "authors": [
            "Jinyue Bian",
            "Zhaoxing Zhang",
            "Zhengyu Liang",
            "Shiwei Zheng",
            "Shengtao Zhang",
            "Rong Shen",
            "Chen Yang",
            "Anzhou Hou"
          ],
          "arxiv_id": "2509.18183",
          "abstract": "The Visual-Language-Action (VLA) models can follow text instructions according to visual observations of the surrounding environment. This ability to map multimodal inputs to actions is derived from the training of the VLA model on extensive standard demonstrations. These visual observations captured by third-personal global and in-wrist local cameras are inevitably varied in number and perspective across different environments, resulting in significant differences in the visual features. This perspective heterogeneity constrains the generality of VLA models. In light of this, we first propose the lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models using only 2D data. VLA-LPAF is finetuned using images from a single view and fuses other multiview observations in the latent space, which effectively and efficiently bridge the gap caused by perspective inconsistency. We instantiate our VLA-LPAF framework with the VLA model RoboFlamingo to construct RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a customized simulation benchmark. We also demonstrate the developed viewadaptive characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.",
          "abstract_zh": "视觉语言动作（VLA）模型可以根据对周围环境的视觉观察遵循文本指令。这种将多模式输入映射到行动的能力源自 VLA 模型在广泛的标准演示上的训练。这些由第三人称全局相机和腕部局部相机捕获的视觉观察在不同环境中不可避免地在数量和视角上有所不同，导致视觉特征存在显着差异。这种视角的异质性限制了 VLA 模型的通用性。有鉴于此，我们首先提出轻量级模块 VLA-LPAF，以促进仅使用 2D 数据的 VLA 模型的透视适应性。VLA-LPAF 使用来自单个视图的图像进行微调，并融合潜在空间中的其他多视图观察，从而有效且高效地弥合了因透视不一致而造成的差距。我们用 VLA 模型 RoboFlamingo 实例化我们的 VLA-LPAF 框架来构建 RoboFlamingo-LPAF。实验表明，RoboFlamingo-LPAF 在 CALVIN 上平均提高了约 8% 的任务成功率，在 LIBERO 上提高了 15%，在定制模拟基准上提高了 30%。我们还通过现实世界的任务展示了所提出的 RoboFlamingo-LPAF 所开发的视图自适应特性。"
        },
        {
          "title": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving",
          "url": "http://arxiv.org/abs/2509.13769v1",
          "snippet": "While reasoning technology like Chain of Thought (CoT) has been widely adopted in Vision Language Action (VLA) models, it demonstrates promising capabilities in end to end autonomous driving. However, recent efforts to integrate CoT reasoning often fall short in simple scenarios, introducing unnecessary computational overhead without improving decision quality. To address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode reasoning mechanism inspired by fast and slow thinking. First, our framework is pretrained on large scale autonomous driving (AD) scenarios using both question answering (QA) and trajectory datasets to acquire world knowledge and driving commonsense. During supervised fine tuning (SFT), we introduce a two mode dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the model to distinguish between scenarios that require reasoning. Furthermore, an Adaptive Think Reward strategy is proposed in conjunction with the Group Relative Policy Optimization (GRPO), which rewards the model for selectively applying CoT by comparing trajectory quality across different reasoning modes. Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points. Moreover, ablations show that AdaThinkDrive surpasses both the never Think and always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also reduces inference time by 14% compared to the always Think baseline, demonstrating its ability to balance accuracy and efficiency through adaptive reasoning.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-09-17T07:35:39Z",
          "authors": [
            "Yuechen Luo",
            "Fang Li",
            "Shaoqing Xu",
            "Zhiyi Lai",
            "Lei Yang",
            "Qimao Chen",
            "Ziang Luo",
            "Zixun Xie",
            "Shengyin Jiang",
            "Jiaxin Liu",
            "Long Chen",
            "Bing Wang",
            "Zhi-xin Yang"
          ],
          "arxiv_id": "2509.13769",
          "abstract": "While reasoning technology like Chain of Thought (CoT) has been widely adopted in Vision Language Action (VLA) models, it demonstrates promising capabilities in end to end autonomous driving. However, recent efforts to integrate CoT reasoning often fall short in simple scenarios, introducing unnecessary computational overhead without improving decision quality. To address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode reasoning mechanism inspired by fast and slow thinking. First, our framework is pretrained on large scale autonomous driving (AD) scenarios using both question answering (QA) and trajectory datasets to acquire world knowledge and driving commonsense. During supervised fine tuning (SFT), we introduce a two mode dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the model to distinguish between scenarios that require reasoning. Furthermore, an Adaptive Think Reward strategy is proposed in conjunction with the Group Relative Policy Optimization (GRPO), which rewards the model for selectively applying CoT by comparing trajectory quality across different reasoning modes. Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points. Moreover, ablations show that AdaThinkDrive surpasses both the never Think and always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also reduces inference time by 14% compared to the always Think baseline, demonstrating its ability to balance accuracy and efficiency through adaptive reasoning.",
          "abstract_zh": "虽然像 Chain of Thought (CoT) 这样的推理技术已在视觉语言动作 (VLA) 模型中广泛采用，但它在端到端自动驾驶方面展示了有前景的能力。然而，最近集成 CoT 推理的努力在简单的场景中常常达不到要求，引入了不必要的计算开销，而没有提高决策质量。为了解决这个问题，我们提出了 AdaThinkDrive，这是一种新颖的 VLA 框架，具有受快速和慢速思维启发的双模式推理机制。首先，我们的框架使用问答（QA）和轨迹数据集在大规模自动驾驶（AD）场景上进行预训练，以获取世界知识和驾驶常识。在监督微调（SFT）过程中，我们引入了两种模式的数据集：快速回答（无 CoT）和慢速思考（有 CoT），使模型能够区分需要推理的场景。此外，结合组相对策略优化（GRPO）提出了自适应思考奖励策略，该策略通过比较不同推理模式的轨迹质量来奖励选择性应用 CoT 的模型。Navsim 基准测试的大量实验表明，AdaThinkDrive 的 PDMS 达到 90.3，比最佳视觉基线高出 1.7 个百分点。此外，消融显示 AdaThinkDrive 超越了从不 Think 和始终 Think 基线，分别将 PDMS 提高了 2.0 和 1.4。与always Think 基线相比，它还减少了 14% 的推理时间，展示了其通过自适应推理平衡准确性和效率的能力。"
        },
        {
          "title": "Toward Embodiment Equivariant Vision-Language-Action Policy",
          "url": "http://arxiv.org/abs/2509.14630v1",
          "snippet": "Vision-language-action policies learn manipulation skills across tasks, environments and embodiments through large-scale pre-training. However, their ability to generalize to novel robot configurations remains limited. Most approaches emphasize model size, dataset scale and diversity while paying less attention to the design of action spaces. This leads to the configuration generalization problem, which requires costly adaptation. We address this challenge by formulating cross-embodiment pre-training as designing policies equivariant to embodiment configuration transformations. Building on this principle, we propose a framework that (i) establishes a embodiment equivariance theory for action space and policy design, (ii) introduces an action decoder that enforces configuration equivariance, and (iii) incorporates a geometry-aware network architecture to enhance embodiment-agnostic spatial reasoning. Extensive experiments in both simulation and real-world settings demonstrate that our approach improves pre-training effectiveness and enables efficient fine-tuning on novel robot embodiments. Our code is available at https://github.com/hhcaz/e2vla",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-09-18T05:20:22Z",
          "authors": [
            "Anzhe Chen",
            "Yifei Yang",
            "Zhenjie Zhu",
            "Kechun Xu",
            "Zhongxiang Zhou",
            "Rong Xiong",
            "Yue Wang"
          ],
          "arxiv_id": "2509.14630",
          "abstract": "Vision-language-action policies learn manipulation skills across tasks, environments and embodiments through large-scale pre-training. However, their ability to generalize to novel robot configurations remains limited. Most approaches emphasize model size, dataset scale and diversity while paying less attention to the design of action spaces. This leads to the configuration generalization problem, which requires costly adaptation. We address this challenge by formulating cross-embodiment pre-training as designing policies equivariant to embodiment configuration transformations. Building on this principle, we propose a framework that (i) establishes a embodiment equivariance theory for action space and policy design, (ii) introduces an action decoder that enforces configuration equivariance, and (iii) incorporates a geometry-aware network architecture to enhance embodiment-agnostic spatial reasoning. Extensive experiments in both simulation and real-world settings demonstrate that our approach improves pre-training effectiveness and enables efficient fine-tuning on novel robot embodiments. Our code is available at https://github.com/hhcaz/e2vla",
          "abstract_zh": "视觉-语言-动作策略通过大规模预训练学习跨任务、环境和实施例的操作技能。然而，它们推广到新型机器人配置的能力仍然有限。大多数方法强调模型大小、数据集规模和多样性，而较少关注动作空间的设计。这导致了配置泛化问题，需要昂贵的适应成本。我们通过制定跨实施例预训练作为设计与实施例配置转换等价的策略来应对这一挑战。基于这一原则，我们提出了一个框架，该框架（i）为动作空间和策略设计建立了实施例等变理论，（ii）引入了强制配置等变性的动作解码器，以及（iii）合并了几何感知网络架构以增强与实施例无关的空间推理。模拟和现实环境中的大量实验表明，我们的方法提高了预训练的有效性，并能够对新颖的机器人实施例进行有效的微调。我们的代码位于 https://github.com/hhcaz/e2vla"
        },
        {
          "title": "Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach",
          "url": "http://arxiv.org/abs/2509.13774v1",
          "snippet": "Vision-language-action (VLA) models demonstrate strong generalization in robotic manipulation but face challenges in complex, real-world tasks. While supervised fine-tuning with demonstrations is constrained by data quality, reinforcement learning (RL) offers a promising alternative. We propose a human-in-the-loop dual-actor fine-tuning framework grounded in RL. The framework integrates a primary actor for robust multi-task performance with a refinement actor for latent-space adaptation. Beyond standard physical interventions, we introduce a lightweight talk-and-tweak scheme that converts human corrections into semantically grounded language commands, thereby generating a new dataset for policy learning. In real-world multi-task experiments, our approach achieves 100% success across three tasks within 101 minutes of online fine-tuning. For long-horizon tasks, it sustains a 50% success rate over 12 consecutive operations. Furthermore, the framework scales effectively to multi-robot training, achieving up to a 2 times improvement in efficiency when using dual robots. The experiment videos are available at https://sites.google.com/view/hil-daft/.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-09-17T07:44:59Z",
          "authors": [
            "Piaopiao Jin",
            "Qi Wang",
            "Guokang Sun",
            "Ziwen Cai",
            "Pinjia He",
            "Yangwei You"
          ],
          "arxiv_id": "2509.13774",
          "abstract": "Vision-language-action (VLA) models demonstrate strong generalization in robotic manipulation but face challenges in complex, real-world tasks. While supervised fine-tuning with demonstrations is constrained by data quality, reinforcement learning (RL) offers a promising alternative. We propose a human-in-the-loop dual-actor fine-tuning framework grounded in RL. The framework integrates a primary actor for robust multi-task performance with a refinement actor for latent-space adaptation. Beyond standard physical interventions, we introduce a lightweight talk-and-tweak scheme that converts human corrections into semantically grounded language commands, thereby generating a new dataset for policy learning. In real-world multi-task experiments, our approach achieves 100% success across three tasks within 101 minutes of online fine-tuning. For long-horizon tasks, it sustains a 50% success rate over 12 consecutive operations. Furthermore, the framework scales effectively to multi-robot training, achieving up to a 2 times improvement in efficiency when using dual robots. The experiment videos are available at https://sites.google.com/view/hil-daft/.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在机器人操作方面表现出很强的通用性，但在复杂的现实任务中面临挑战。虽然通过演示进行监督微调受到数据质量的限制，但强化学习 (RL) 提供了一种有前途的替代方案。我们提出了一种基于强化学习的人机循环双参与者微调框架。该框架集成了用于稳健多任务性能的主要参与者和用于潜在空间适应的细化参与者。除了标准的物理干预之外，我们还引入了一种轻量级的谈话和调整方案，该方案将人类的纠正转换为基于语义的语言命令，从而生成用于政策学习的新数据集。在现实世界的多任务实验中，我们的方法在 101 分钟的在线微调内实现了三个任务 100% 的成功。对于长期任务，它在连续 12 次操作中保持 50% 的成功率。此外，该框架可以有效地扩展到多机器人训练，在使用双机器人时实现高达 2 倍的效率提升。实验视频可在 https://sites.google.com/view/hil-daft/ 上获取。"
        },
        {
          "title": "SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model",
          "url": "http://arxiv.org/abs/2509.14138v1",
          "snippet": "Long-horizon robotic manipulation tasks require executing multiple interdependent subtasks in strict sequence, where errors in detecting subtask completion can cascade into downstream failures. Existing Vision-Language-Action (VLA) models such as $π_0$ excel at continuous low-level control but lack an internal signal for identifying when a subtask has finished, making them brittle in sequential settings. We propose SeqVLA, a completion-aware extension of $π_0$ that augments the base architecture with a lightweight detection head perceiving whether the current subtask is complete. This dual-head design enables SeqVLA not only to generate manipulation actions but also to autonomously trigger transitions between subtasks. We investigate four finetuning strategies that vary in how the action and detection heads are optimized (joint vs. sequential finetuning) and how pretrained knowledge is preserved (full finetuning vs. frozen backbone). Experiments are performed on two multi-stage tasks: salad packing with seven distinct subtasks and candy packing with four distinct subtasks. Results show that SeqVLA significantly outperforms the baseline $π_0$ and other strong baselines in overall success rate. In particular, joint finetuning with an unfrozen backbone yields the most decisive and statistically reliable completion predictions, eliminating sequence-related failures and enabling robust long-horizon execution. Our results highlight the importance of coupling action generation with subtask-aware detection for scalable sequential manipulation.",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-09-17T16:17:46Z",
          "authors": [
            "Ran Yang",
            "Zijian An",
            "Lifeng ZHou",
            "Yiming Feng"
          ],
          "arxiv_id": "2509.14138",
          "abstract": "Long-horizon robotic manipulation tasks require executing multiple interdependent subtasks in strict sequence, where errors in detecting subtask completion can cascade into downstream failures. Existing Vision-Language-Action (VLA) models such as $π_0$ excel at continuous low-level control but lack an internal signal for identifying when a subtask has finished, making them brittle in sequential settings. We propose SeqVLA, a completion-aware extension of $π_0$ that augments the base architecture with a lightweight detection head perceiving whether the current subtask is complete. This dual-head design enables SeqVLA not only to generate manipulation actions but also to autonomously trigger transitions between subtasks. We investigate four finetuning strategies that vary in how the action and detection heads are optimized (joint vs. sequential finetuning) and how pretrained knowledge is preserved (full finetuning vs. frozen backbone). Experiments are performed on two multi-stage tasks: salad packing with seven distinct subtasks and candy packing with four distinct subtasks. Results show that SeqVLA significantly outperforms the baseline $π_0$ and other strong baselines in overall success rate. In particular, joint finetuning with an unfrozen backbone yields the most decisive and statistically reliable completion predictions, eliminating sequence-related failures and enabling robust long-horizon execution. Our results highlight the importance of coupling action generation with subtask-aware detection for scalable sequential manipulation.",
          "abstract_zh": "长视野机器人操作任务需要严格顺序执行多个相互依赖的子任务，其中检测子任务完成的错误可能会级联成下游故障。现有的视觉-语言-动作 (VLA) 模型（例如 $π_0$）擅长连续低级控制，但缺乏用于识别子任务何时完成的内部信号，这使得它们在顺序设置中很脆弱。我们提出了 SeqVLA，它是 $π_0$ 的完成感知扩展，它通过轻量级检测头来增强基础架构，以感知当前子任务是否完成。这种双头设计使 SeqVLA 不仅能够生成操作动作，还能自动触发子任务之间的转换。我们研究了四种微调策略，这些策略在如何优化动作和检测头（联合微调与顺序微调）以及如何保留预训练知识（完全微调与冻结主干）方面有所不同。实验在两个多阶段任务上进行：具有七个不同子任务的沙拉包装和具有四个不同子任务的糖果包装。结果表明，SeqVLA 在整体成功率方面显着优于基线 $π_0$ 和其他强基线。特别是，与未冻结主干的联合微调可以产生最具决定性和统计上可靠的完成预测，消除与序列相关的故障并实现强大的长期执行。我们的结果强调了将动作生成与子任务感知检测相结合以实现可扩展的顺序操作的重要性。"
        },
        {
          "title": "The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning",
          "url": "http://arxiv.org/abs/2509.12594v2",
          "snippet": "We present LightVLA, a simple yet effective differentiable token pruning framework for vision-language-action (VLA) models. While VLA models have shown impressive capability in executing real-world robotic tasks, their deployment on resource-constrained platforms is often bottlenecked by the heavy attention-based computation over large sets of visual tokens. LightVLA addresses this challenge through adaptive, performance-driven pruning of visual tokens: It generates dynamic queries to evaluate visual token importance, and adopts Gumbel softmax to enable differentiable token selection. Through fine-tuning, LightVLA learns to preserve the most informative visual tokens while pruning tokens which do not contribute to task execution, thereby improving efficiency and performance simultaneously. Notably, LightVLA requires no heuristic magic numbers and introduces no additional trainable parameters, making it compatible with modern inference frameworks. Experimental results demonstrate that LightVLA outperforms different VLA models and existing token pruning methods across diverse tasks on the LIBERO benchmark, achieving higher success rates with substantially reduced computational overhead. Specifically, LightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.6% improvement in task success rate. Meanwhile, we also investigate the learnable query-based token pruning method LightVLA* with additional trainable parameters, which also achieves satisfactory performance. Our work reveals that as VLA pursues optimal performance, LightVLA spontaneously learns to prune tokens from a performance-driven perspective. To the best of our knowledge, LightVLA is the first work to apply adaptive visual token pruning to VLA tasks with the collateral goals of efficiency and performance, marking a significant step toward more efficient, powerful and practical real-time robotic systems.",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-09-16T02:43:46Z",
          "authors": [
            "Titong Jiang",
            "Xuefeng Jiang",
            "Yuan Ma",
            "Xin Wen",
            "Bailin Li",
            "Kun Zhan",
            "Peng Jia",
            "Yahui Liu",
            "Sheng Sun",
            "Xianpeng Lang"
          ],
          "arxiv_id": "2509.12594",
          "abstract": "We present LightVLA, a simple yet effective differentiable token pruning framework for vision-language-action (VLA) models. While VLA models have shown impressive capability in executing real-world robotic tasks, their deployment on resource-constrained platforms is often bottlenecked by the heavy attention-based computation over large sets of visual tokens. LightVLA addresses this challenge through adaptive, performance-driven pruning of visual tokens: It generates dynamic queries to evaluate visual token importance, and adopts Gumbel softmax to enable differentiable token selection. Through fine-tuning, LightVLA learns to preserve the most informative visual tokens while pruning tokens which do not contribute to task execution, thereby improving efficiency and performance simultaneously. Notably, LightVLA requires no heuristic magic numbers and introduces no additional trainable parameters, making it compatible with modern inference frameworks. Experimental results demonstrate that LightVLA outperforms different VLA models and existing token pruning methods across diverse tasks on the LIBERO benchmark, achieving higher success rates with substantially reduced computational overhead. Specifically, LightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.6% improvement in task success rate. Meanwhile, we also investigate the learnable query-based token pruning method LightVLA* with additional trainable parameters, which also achieves satisfactory performance. Our work reveals that as VLA pursues optimal performance, LightVLA spontaneously learns to prune tokens from a performance-driven perspective. To the best of our knowledge, LightVLA is the first work to apply adaptive visual token pruning to VLA tasks with the collateral goals of efficiency and performance, marking a significant step toward more efficient, powerful and practical real-time robotic systems.",
          "abstract_zh": "我们提出了 LightVLA，这是一种用于视觉-语言-动作（VLA）模型的简单而有效的可微标记修剪框架。虽然 VLA 模型在执行现实世界的机器人任务方面表现出了令人印象深刻的能力，但它们在资源受限的平台上的部署往往受到大量视觉标记上基于注意力的大量计算的瓶颈。LightVLA 通过自适应、性能驱动的视觉标记修剪来解决这一挑战：它生成动态查询来评估视觉标记的重要性，并采用 Gumbel softmax 来实现可区分的标记选择。通过微调，LightVLA 学会保留信息最丰富的视觉标记，同时修剪对任务执行没有贡献的标记，从而同时提高效率和性能。值得注意的是，LightVLA 不需要启发式幻数，也不需要引入额外的可训练参数，使其与现代推理框架兼容。实验结果表明，LightVLA 在 LIBERO 基准上的各种任务中优于不同的 VLA 模型和现有的令牌修剪方法，在大幅降低计算开销的情况下实现了更高的成功率。具体来说，LightVLA 将 FLOP 和延迟分别降低了 59.1% 和 38.2%，任务成功率提高了 2.6%。同时，我们还研究了带有额外可训练参数的可学习的基于查询的标记修剪方法 LightVLA*，该方法也取得了令人满意的性能。我们的工作表明，当 VLA 追求最佳性能时，LightVLA 会自发地学会从性能驱动的角度修剪代币。据我们所知，LightVLA 是第一个将自适应视觉令牌修剪应用于 VLA 任务的工作，其附带目标是效率和性能，标志着向更高效、更强大和实用的实时机器人系统迈出了重要一步。"
        },
        {
          "title": "CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine",
          "url": "http://arxiv.org/abs/2509.15968v1",
          "snippet": "Autonomous Driving (AD) systems have made notable progress, but their performance in long-tail, safety-critical scenarios remains limited. These rare cases contribute a disproportionate number of accidents. Vision-Language Action (VLA) models have strong reasoning abilities and offer a potential solution, but their effectiveness is limited by the lack of high-quality data and inefficient learning in such conditions. To address these challenges, we propose CoReVLA, a continual learning end-to-end autonomous driving framework that improves the performance in long-tail scenarios through a dual-stage process of data Collection and behavior Refinement. First, the model is jointly fine-tuned on a mixture of open-source driving QA datasets, allowing it to acquire a foundational understanding of driving scenarios. Next, CoReVLA is deployed within the Cave Automatic Virtual Environment (CAVE) simulation platform, where driver takeover data is collected from real-time interactions. Each takeover indicates a long-tail scenario that CoReVLA fails to handle reliably. Finally, the model is refined via Direct Preference Optimization (DPO), allowing it to learn directly from human preferences and thereby avoid reward hacking caused by manually designed rewards. Extensive open-loop and closed-loop experiments demonstrate that the proposed CoReVLA model can accurately perceive driving scenarios and make appropriate decisions. On the Bench2Drive benchmark, CoReVLA achieves a Driving Score (DS) of 72.18 and a Success Rate (SR) of 50%, outperforming state-of-the-art methods by 7.96 DS and 15% SR under long-tail, safety-critical scenarios. Furthermore, case studies demonstrate the model's ability to continually improve its performance in similar failure-prone scenarios by leveraging past takeover experiences. All codea and preprocessed datasets are available at: https://github.com/FanGShiYuu/CoReVLA",
          "site": "arxiv.org",
          "rank": 15,
          "published": "2025-09-19T13:25:56Z",
          "authors": [
            "Shiyu Fang",
            "Yiming Cui",
            "Haoyang Liang",
            "Chen Lv",
            "Peng Hang",
            "Jian Sun"
          ],
          "arxiv_id": "2509.15968",
          "abstract": "Autonomous Driving (AD) systems have made notable progress, but their performance in long-tail, safety-critical scenarios remains limited. These rare cases contribute a disproportionate number of accidents. Vision-Language Action (VLA) models have strong reasoning abilities and offer a potential solution, but their effectiveness is limited by the lack of high-quality data and inefficient learning in such conditions. To address these challenges, we propose CoReVLA, a continual learning end-to-end autonomous driving framework that improves the performance in long-tail scenarios through a dual-stage process of data Collection and behavior Refinement. First, the model is jointly fine-tuned on a mixture of open-source driving QA datasets, allowing it to acquire a foundational understanding of driving scenarios. Next, CoReVLA is deployed within the Cave Automatic Virtual Environment (CAVE) simulation platform, where driver takeover data is collected from real-time interactions. Each takeover indicates a long-tail scenario that CoReVLA fails to handle reliably. Finally, the model is refined via Direct Preference Optimization (DPO), allowing it to learn directly from human preferences and thereby avoid reward hacking caused by manually designed rewards. Extensive open-loop and closed-loop experiments demonstrate that the proposed CoReVLA model can accurately perceive driving scenarios and make appropriate decisions. On the Bench2Drive benchmark, CoReVLA achieves a Driving Score (DS) of 72.18 and a Success Rate (SR) of 50%, outperforming state-of-the-art methods by 7.96 DS and 15% SR under long-tail, safety-critical scenarios. Furthermore, case studies demonstrate the model's ability to continually improve its performance in similar failure-prone scenarios by leveraging past takeover experiences. All codea and preprocessed datasets are available at: https://github.com/FanGShiYuu/CoReVLA",
          "abstract_zh": "自动驾驶（AD）系统取得了显着进展，但其在长尾、安全关键场景中的性能仍然有限。这些罕见的情况造成了不成比例的事故。视觉语言动作（VLA）模型具有很强的推理能力并提供了潜在的解决方案，但由于缺乏高质量数据和在这种情况下学习效率低下，其有效性受到限制。为了应对这些挑战，我们提出了CoReVLA，这是一种持续学习的端到端自动驾驶框架，通过数据收集和行为细化的双阶段过程来提高长尾场景中的性能。首先，该模型在开源驾驶 QA 数据集的混合上进行联合微调，使其能够获得对驾驶场景的基本了解。接下来，CoReVLA 部署在 Cave 自动虚拟环境 (CAVE) 模拟平台中，从实时交互中收集驾驶员接管数据。每次接管都表明 CoReVLA 无法可靠处理的长尾场景。最后，该模型通过直接偏好优化（DPO）进行完善，使其能够直接从人类偏好中学习，从而避免手动设计的奖励造成的奖励黑客行为。大量的开环和闭环实验表明，所提出的CoReVLA模型可以准确感知驾驶场景并做出适当的决策。在 Bench2Drive 基准测试中，CoReVLA 的驾驶分数 (DS) 为 72.18，成功率 (SR) 为 50%，在长尾安全关键场景下，比最先进的方法高出 7.96 DS 和 15% SR。此外，案例研究表明，该模型能够利用过去的接管经验，在类似的容易出现故障的场景中不断提高其性能。所有 codea 和预处理数据集均可在以下位置获取：https://github.com/FanGShiYuu/CoReVLA"
        },
        {
          "title": "CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human",
          "url": "http://arxiv.org/abs/2509.14889v1",
          "snippet": "In this work, we present CollabVLA, a self-reflective vision-language-action framework that transforms a standard visuomotor policy into a collaborative assistant. CollabVLA tackles key limitations of prior VLAs, including domain overfitting, non-interpretable reasoning, and the high latency of auxiliary generative models, by integrating VLM-based reflective reasoning with diffusion-based action generation under a mixture-of-experts design. Through a two-stage training recipe of action grounding and reflection tuning, it supports explicit self-reflection and proactively solicits human guidance when confronted with uncertainty or repeated failure. It cuts normalized Time by ~2x and Dream counts by ~4x vs. generative agents, achieving higher success rates, improved interpretability, and balanced low latency compared with existing methods. This work takes a pioneering step toward shifting VLAs from opaque controllers to genuinely assistive agents capable of reasoning, acting, and collaborating with humans.",
          "site": "arxiv.org",
          "rank": 16,
          "published": "2025-09-18T12:10:41Z",
          "authors": [
            "Nan Sun",
            "Yongchang Li",
            "Chenxu Wang",
            "Huiying Li",
            "Huaping Liu"
          ],
          "arxiv_id": "2509.14889",
          "abstract": "In this work, we present CollabVLA, a self-reflective vision-language-action framework that transforms a standard visuomotor policy into a collaborative assistant. CollabVLA tackles key limitations of prior VLAs, including domain overfitting, non-interpretable reasoning, and the high latency of auxiliary generative models, by integrating VLM-based reflective reasoning with diffusion-based action generation under a mixture-of-experts design. Through a two-stage training recipe of action grounding and reflection tuning, it supports explicit self-reflection and proactively solicits human guidance when confronted with uncertainty or repeated failure. It cuts normalized Time by ~2x and Dream counts by ~4x vs. generative agents, achieving higher success rates, improved interpretability, and balanced low latency compared with existing methods. This work takes a pioneering step toward shifting VLAs from opaque controllers to genuinely assistive agents capable of reasoning, acting, and collaborating with humans.",
          "abstract_zh": "在这项工作中，我们提出了 CollabVLA，这是一种自我反思的视觉-语言-动作框架，可将标准的视觉运动策略转变为协作助手。CollabVLA 通过在专家混合设计下将基于 VLM 的反射推理与基于扩散的动作生成相结合，解决了先前 VLA 的关键局限性，包括域过度拟合、不可解释的推理以及辅助生成模型的高延迟。通过行动基础和反思调整的两阶段训练方法，它支持明确的自我反思，并在面临不确定性或重复失败时主动寻求人类指导。与生成代理相比，它将归一化时间缩短约 2 倍，将梦想计数缩短约 4 倍，与现有方法相比，实现了更高的成功率、改进的可解释性和平衡的低延迟。这项工作朝着将 VLA 从不透明的控制器转变为能够推理、行动和与人类协作的真正辅助代理迈出了开创性的一步。"
        },
        {
          "title": "KV-Efficient VLA: A Method to Speed up Vision Language Models with RNN-Gated Chunked KV Cache",
          "url": "http://arxiv.org/abs/2509.21354v2",
          "snippet": "Vision-Language-Action (VLA) models offer a unified framework for robotic perception and control, but their ability to scale to real-world, long-horizon tasks is limited by the high computational cost of attention and the large memory required for storing key-value (KV) pairs during inference, particularly when retaining historical image tokens as context. Recent methods have focused on scaling backbone architectures to improve generalization, with less emphasis on addressing inference inefficiencies essential for real-time use. In this work, we present KV-Efficient VLA, a model-agnostic memory compression approach designed to address these limitations by introducing a lightweight mechanism to selectively retain high-utility context. Our method partitions the KV cache into fixed-size chunks and employs a recurrent gating module to summarize and filter the historical context according to learned utility scores. This design aims to preserve recent fine-grained detail while aggressively pruning stale, low-relevance memory. Based on experiments, our approach can yield an average of 24.6% FLOPs savings, 1.34x inference speedup, and 1.87x reduction in KV memory. Our method integrates seamlessly into recent VLA stacks, enabling scalable inference without modifying downstream control logic.",
          "site": "arxiv.org",
          "rank": 17,
          "published": "2025-09-20T02:04:24Z",
          "authors": [
            "Wanshun Xu",
            "Long Zhuang",
            "Lianlei Shan"
          ],
          "arxiv_id": "2509.21354",
          "abstract": "Vision-Language-Action (VLA) models offer a unified framework for robotic perception and control, but their ability to scale to real-world, long-horizon tasks is limited by the high computational cost of attention and the large memory required for storing key-value (KV) pairs during inference, particularly when retaining historical image tokens as context. Recent methods have focused on scaling backbone architectures to improve generalization, with less emphasis on addressing inference inefficiencies essential for real-time use. In this work, we present KV-Efficient VLA, a model-agnostic memory compression approach designed to address these limitations by introducing a lightweight mechanism to selectively retain high-utility context. Our method partitions the KV cache into fixed-size chunks and employs a recurrent gating module to summarize and filter the historical context according to learned utility scores. This design aims to preserve recent fine-grained detail while aggressively pruning stale, low-relevance memory. Based on experiments, our approach can yield an average of 24.6% FLOPs savings, 1.34x inference speedup, and 1.87x reduction in KV memory. Our method integrates seamlessly into recent VLA stacks, enabling scalable inference without modifying downstream control logic.",
          "abstract_zh": "视觉-语言-动作（VLA）模型为机器人感知和控制提供了统一的框架，但它们扩展到现实世界、长期任务的能力受到注意力的高计算成本和推理过程中存储键值（KV）对所需的大内存的限制，特别是在保留历史图像标记作为上下文时。最近的方法侧重于扩展主干架构以提高泛化能力，而不太重视解决实时使用所必需的推理低效问题。在这项工作中，我们提出了 KV-Efficient VLA，这是一种与模型无关的内存压缩方法，旨在通过引入轻量级机制来选择性地保留高实用性上下文来解决这些限制。我们的方法将 KV 缓存划分为固定大小的块，并采用循环门控模块根据学习到的效用分数来总结和过滤历史上下文。这种设计旨在保留最近的细粒度细节，同时积极修剪陈旧的、低相关性的内存。根据实验，我们的方法平均可以节省 24.6% 的 FLOPs，提高 1.34 倍的推理速度，并减少 1.87 倍的 KV 内存。我们的方法无缝集成到最新的 VLA 堆栈中，无需修改下游控制逻辑即可实现可扩展的推理。"
        },
        {
          "title": "Latent Action Pretraining Through World Modeling",
          "url": "http://arxiv.org/abs/2509.18428v1",
          "snippet": "Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and $π_{0}$, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging. In this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.",
          "site": "arxiv.org",
          "rank": 18,
          "published": "2025-09-22T21:19:10Z",
          "authors": [
            "Bahey Tharwat",
            "Yara Nasser",
            "Ali Abouzeid",
            "Ian Reid"
          ],
          "arxiv_id": "2509.18428",
          "abstract": "Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and $π_{0}$, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging. In this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.",
          "abstract_zh": "视觉-语言-动作（VLA）模型因学习遵循语言指令的机器人操作任务而受到欢迎。最先进的 VLA，例如 OpenVLA 和 $π_{0}$，是在通过远程操作收集的大规模、手动标记的动作数据集上进行训练的。最近的方法，包括 LAPA 和 Villa-X，引入了潜在动作表示，通过对帧之间的抽象视觉变化进行建模，可以对未标记的数据集进行无监督的预训练。尽管这些方法已经显示出强大的结果，但它们的模型规模较大，使得在现实环境中的部署具有挑战性。在这项工作中，我们提出了 LAWM，这是一种与模型无关的框架，通过世界建模从未标记的视频数据中学习潜在动作表示，以自我监督的方式预训练模仿学习模型。这些视频可以来自机器人录制的视频或人类对日常物体执行动作的视频。我们的框架旨在有效地跨任务、环境和实施例进行迁移。它的性能优于在 LIBERO 基准和现实世界设置上使用地面实况机器人动作和类似预训练方法训练的模型，同时对于现实世界设置而言更加高效和实用。"
        },
        {
          "title": "TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning",
          "url": "http://arxiv.org/abs/2509.11839v2",
          "snippet": "Recent Vision-Language-Action models show potential to generalize across embodiments but struggle to quickly align with a new robot's action space when high-quality demonstrations are scarce, especially for bipedal humanoids. We present TrajBooster, a cross-embodiment framework that leverages abundant wheeled-humanoid data to boost bipedal VLA. Our key idea is to use end-effector trajectories as a morphology-agnostic interface. TrajBooster (i) extracts 6D dual-arm end-effector trajectories from real-world wheeled humanoids, (ii) retargets them in simulation to Unitree G1 with a whole-body controller trained via a heuristic-enhanced harmonized online DAgger to lift low-dimensional trajectory references into feasible high-dimensional whole-body actions, and (iii) forms heterogeneous triplets that couple source vision/language with target humanoid-compatible actions to post-pre-train a VLA, followed by only 10 minutes of teleoperation data collection on the target humanoid domain. Deployed on Unitree G1, our policy achieves beyond-tabletop household tasks, enabling squatting, cross-height manipulation, and coordinated whole-body motion with markedly improved robustness and generalization. Results show that TrajBooster allows existing wheeled-humanoid data to efficiently strengthen bipedal humanoid VLA performance, reducing reliance on costly same-embodiment data while enhancing action space understanding and zero-shot skill transfer capabilities. For more details, For more details, please refer to our \\href{https://jiachengliu3.github.io/TrajBooster/}.",
          "site": "arxiv.org",
          "rank": 19,
          "published": "2025-09-15T12:25:39Z",
          "authors": [
            "Jiacheng Liu",
            "Pengxiang Ding",
            "Qihang Zhou",
            "Yuxuan Wu",
            "Da Huang",
            "Zimian Peng",
            "Wei Xiao",
            "Weinan Zhang",
            "Lixin Yang",
            "Cewu Lu",
            "Donglin Wang"
          ],
          "arxiv_id": "2509.11839",
          "abstract": "Recent Vision-Language-Action models show potential to generalize across embodiments but struggle to quickly align with a new robot's action space when high-quality demonstrations are scarce, especially for bipedal humanoids. We present TrajBooster, a cross-embodiment framework that leverages abundant wheeled-humanoid data to boost bipedal VLA. Our key idea is to use end-effector trajectories as a morphology-agnostic interface. TrajBooster (i) extracts 6D dual-arm end-effector trajectories from real-world wheeled humanoids, (ii) retargets them in simulation to Unitree G1 with a whole-body controller trained via a heuristic-enhanced harmonized online DAgger to lift low-dimensional trajectory references into feasible high-dimensional whole-body actions, and (iii) forms heterogeneous triplets that couple source vision/language with target humanoid-compatible actions to post-pre-train a VLA, followed by only 10 minutes of teleoperation data collection on the target humanoid domain. Deployed on Unitree G1, our policy achieves beyond-tabletop household tasks, enabling squatting, cross-height manipulation, and coordinated whole-body motion with markedly improved robustness and generalization. Results show that TrajBooster allows existing wheeled-humanoid data to efficiently strengthen bipedal humanoid VLA performance, reducing reliance on costly same-embodiment data while enhancing action space understanding and zero-shot skill transfer capabilities. For more details, For more details, please refer to our \\href{https://jiachengliu3.github.io/TrajBooster/}.",
          "abstract_zh": "最近的视觉-语言-动作模型显示出跨实施例泛化的潜力，但在高质量演示稀缺的情况下，很难快速与新机器人的动作空间保持一致，尤其是对于双足类人机器人。我们提出了 TrajBooster，一个跨实体框架，利用丰富的轮式人形数据来增强双足 VLA。我们的关键想法是使用末端执行器轨迹作为形态不可知的接口。TrajBooster (i) 从现实世界的轮式人形机器人中提取 6D 双臂末端执行器轨迹，(ii) 在模拟中将它们重新定位到 Unitree G1，并使用通过启发式增强的协调在线 DAgger 进行训练的全身控制器，将低维轨迹参考提升为可行的高维全身动作，以及 (iii) 形成异构三元组，将源视觉/语言与目标人形兼容动作结合起来，以进行预训练VLA，随后仅用 10 分钟就对目标人形领域进行了远程操作数据收集。部署在 Unitree G1 上，我们的策略实现了超越桌面的家庭任务，实现了蹲下、跨高度操纵和协调全身运动，并且鲁棒性和泛化性显着提高。结果表明，TrajBooster 允许现有的轮式人形数据有效增强双足人形 VLA 性能，减少对昂贵的相同实施例数据的依赖，同时增强动作空间理解和零镜头技能转移能力。更多详情，请参考我们的\\href{https://jia Chengliu3.github.io/TrajBooster/}。"
        },
        {
          "title": "Prepare Before You Act: Learning From Humans to Rearrange Initial States",
          "url": "http://arxiv.org/abs/2509.18043v1",
          "snippet": "Imitation learning (IL) has proven effective across a wide range of manipulation tasks. However, IL policies often struggle when faced with out-of-distribution observations; for instance, when the target object is in a previously unseen position or occluded by other objects. In these cases, extensive demonstrations are needed for current IL methods to reach robust and generalizable behaviors. But when humans are faced with these sorts of atypical initial states, we often rearrange the environment for more favorable task execution. For example, a person might rotate a coffee cup so that it is easier to grasp the handle, or push a box out of the way so they can directly grasp their target object. In this work we seek to equip robot learners with the same capability: enabling robots to prepare the environment before executing their given policy. We propose ReSET, an algorithm that takes initial states -- which are outside the policy's distribution -- and autonomously modifies object poses so that the restructured scene is similar to training data. Theoretically, we show that this two step process (rearranging the environment before rolling out the given policy) reduces the generalization gap. Practically, our ReSET algorithm combines action-agnostic human videos with task-agnostic teleoperation data to i) decide when to modify the scene, ii) predict what simplifying actions a human would take, and iii) map those predictions into robot action primitives. Comparisons with diffusion policies, VLAs, and other baselines show that using ReSET to prepare the environment enables more robust task execution with equal amounts of total training data. See videos at our project website: https://reset2025paper.github.io/",
          "site": "arxiv.org",
          "rank": 20,
          "published": "2025-09-22T17:18:52Z",
          "authors": [
            "Yinlong Dai",
            "Andre Keyser",
            "Dylan P. Losey"
          ],
          "arxiv_id": "2509.18043",
          "abstract": "Imitation learning (IL) has proven effective across a wide range of manipulation tasks. However, IL policies often struggle when faced with out-of-distribution observations; for instance, when the target object is in a previously unseen position or occluded by other objects. In these cases, extensive demonstrations are needed for current IL methods to reach robust and generalizable behaviors. But when humans are faced with these sorts of atypical initial states, we often rearrange the environment for more favorable task execution. For example, a person might rotate a coffee cup so that it is easier to grasp the handle, or push a box out of the way so they can directly grasp their target object. In this work we seek to equip robot learners with the same capability: enabling robots to prepare the environment before executing their given policy. We propose ReSET, an algorithm that takes initial states -- which are outside the policy's distribution -- and autonomously modifies object poses so that the restructured scene is similar to training data. Theoretically, we show that this two step process (rearranging the environment before rolling out the given policy) reduces the generalization gap. Practically, our ReSET algorithm combines action-agnostic human videos with task-agnostic teleoperation data to i) decide when to modify the scene, ii) predict what simplifying actions a human would take, and iii) map those predictions into robot action primitives. Comparisons with diffusion policies, VLAs, and other baselines show that using ReSET to prepare the environment enables more robust task execution with equal amounts of total training data. See videos at our project website: https://reset2025paper.github.io/",
          "abstract_zh": "模仿学习 (IL) 已被证明在各种操作任务中都有效。然而，IL 政策在面对分布外的观察结果时常常会陷入困境。例如，当目标对象处于先前未见过的位置或被其他对象遮挡时。在这些情况下，当前的 IL 方法需要进行广泛的演示才能实现稳健且可推广的行为。但是，当人类面临这些非典型的初始状态时，我们经常会重新安排环境以实现更有利的任务执行。例如，人们可能会旋转咖啡杯，以便更容易抓住把手，或者将盒子推开，以便他们可以直接抓住目标物体。在这项工作中，我们寻求为机器人学习者配备相同的能力：使机器人能够在执行给定策略之前准备好环境。我们提出了 ReSET，这是一种采用初始状态（在策略分布之外）的算法，并自动修改对象姿势，以便重构的场景与训练数据相似。从理论上讲，我们表明这两个步骤的过程（在推出给定策略之前重新安排环境）减少了泛化差距。实际上，我们的 ReSET 算法将与动作无关的人类视频与与任务无关的遥操作数据相结合，以 i) 决定何时修改场景，ii) 预测人类将采取哪些简化动作，以及 iii) 将这些预测映射到机器人动作原语中。与扩散策略、VLA 和其他基线的比较表明，使用 ReSET 准备环境可以使用等量的总训练数据实现更稳健的任务执行。在我们的项目网站上观看视频：https://reset2025paper.github.io/"
        },
        {
          "title": "Randomized Smoothing Meets Vision-Language Models",
          "url": "http://arxiv.org/abs/2509.16088v1",
          "snippet": "Randomized smoothing (RS) is one of the prominent techniques to ensure the correctness of machine learning models, where point-wise robustness certificates can be derived analytically. While RS is well understood for classification, its application to generative models is unclear, since their outputs are sequences rather than labels. We resolve this by connecting generative outputs to an oracle classification task and showing that RS can still be enabled: the final response can be classified as a discrete action (e.g., service-robot commands in VLAs), as harmful vs. harmless (content moderation or toxicity detection in VLMs), or even applying oracles to cluster answers into semantically equivalent ones. Provided that the error rate for the oracle classifier comparison is bounded, we develop the theory that associates the number of samples with the corresponding robustness radius. We further derive improved scaling laws analytically relating the certified radius and accuracy to the number of samples, showing that the earlier result of 2 to 3 orders of magnitude fewer samples sufficing with minimal loss remains valid even under weaker assumptions. Together, these advances make robustness certification both well-defined and computationally feasible for state-of-the-art VLMs, as validated against recent jailbreak-style adversarial attacks.",
          "site": "arxiv.org",
          "rank": 21,
          "published": "2025-09-19T15:33:22Z",
          "authors": [
            "Emmanouil Seferis",
            "Changshun Wu",
            "Stefanos Kollias",
            "Saddek Bensalem",
            "Chih-Hong Cheng"
          ],
          "arxiv_id": "2509.16088",
          "abstract": "Randomized smoothing (RS) is one of the prominent techniques to ensure the correctness of machine learning models, where point-wise robustness certificates can be derived analytically. While RS is well understood for classification, its application to generative models is unclear, since their outputs are sequences rather than labels. We resolve this by connecting generative outputs to an oracle classification task and showing that RS can still be enabled: the final response can be classified as a discrete action (e.g., service-robot commands in VLAs), as harmful vs. harmless (content moderation or toxicity detection in VLMs), or even applying oracles to cluster answers into semantically equivalent ones. Provided that the error rate for the oracle classifier comparison is bounded, we develop the theory that associates the number of samples with the corresponding robustness radius. We further derive improved scaling laws analytically relating the certified radius and accuracy to the number of samples, showing that the earlier result of 2 to 3 orders of magnitude fewer samples sufficing with minimal loss remains valid even under weaker assumptions. Together, these advances make robustness certification both well-defined and computationally feasible for state-of-the-art VLMs, as validated against recent jailbreak-style adversarial attacks.",
          "abstract_zh": "随机平滑（RS）是确保机器学习模型正确性的重要技术之一，可以通过分析得出逐点鲁棒性证书。虽然 RS 在分类方面得到了很好的理解，但它在生成模型中的应用尚不清楚，因为它们的输出是序列而不是标签。我们通过将生成输出连接到预言机分类任务并表明 RS 仍然可以启用来解决这个问题：最终响应可以分类为离散操作（例如，VLA 中的服务机器人命令）、有害与无害（VLM 中的内容审核或毒性检测），甚至应用预言机将答案聚类为语义等效的答案。假设预言分类器比较的错误率是有限的，我们开发了将样本数量与相应的鲁棒性半径相关联的理论。我们进一步推导出改进的缩放定律，通过分析将认证半径和精度与样本数量联系起来，表明即使在较弱的假设下，早期结果（减少 2 到 3 个数量级的样本即可满足最小损失）仍然有效。总之，这些进步使得鲁棒性认证对于最先进的 VLM 来说既定义明确又在计算上可行，并针对最近的越狱式对抗攻击进行了验证。"
        },
        {
          "title": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue",
          "url": "http://arxiv.org/abs/2509.15061v2",
          "snippet": "The ultimate goal of embodied agents is to create collaborators that can interact with humans, not mere executors that passively follow instructions. This requires agents to communicate, coordinate, and adapt their actions based on human feedback. Recently, advances in VLAs have offered a path toward this goal. However, most current VLA-based embodied agents operate in a one-way mode: they receive an instruction and execute it without feedback. This approach fails in real-world scenarios where instructions are often ambiguous. In this paper, we address this problem with the Ask-to-Clarify framework. Our framework first resolves ambiguous instructions by asking questions in a multi-turn dialogue. Then it generates low-level actions end-to-end. Specifically, the Ask-to-Clarify framework consists of two components, one VLM for collaboration and one diffusion for action. We also introduce a connection module that generates conditions for the diffusion based on the output of the VLM. This module adjusts the observation by instructions to create reliable conditions. We train our framework with a two-stage knowledge-insulation strategy. First, we fine-tune the collaboration component using ambiguity-solving dialogue data to handle ambiguity. Then, we integrate the action component while freezing the collaboration one. This preserves the interaction abilities while fine-tuning the diffusion to generate actions. The training strategy guarantees our framework can first ask questions, then generate actions. During inference, a signal detector functions as a router that helps our framework switch between asking questions and taking actions. We evaluate the Ask-to-Clarify framework in 8 real-world tasks, where it outperforms existing state-of-the-art VLAs. The results suggest that our proposed framework, along with the training strategy, provides a path toward collaborative embodied agents.",
          "site": "arxiv.org",
          "rank": 22,
          "published": "2025-09-18T15:25:31Z",
          "authors": [
            "Xingyao Lin",
            "Xinghao Zhu",
            "Tianyi Lu",
            "Sicheng Xie",
            "Hui Zhang",
            "Xipeng Qiu",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
          ],
          "arxiv_id": "2509.15061",
          "abstract": "The ultimate goal of embodied agents is to create collaborators that can interact with humans, not mere executors that passively follow instructions. This requires agents to communicate, coordinate, and adapt their actions based on human feedback. Recently, advances in VLAs have offered a path toward this goal. However, most current VLA-based embodied agents operate in a one-way mode: they receive an instruction and execute it without feedback. This approach fails in real-world scenarios where instructions are often ambiguous. In this paper, we address this problem with the Ask-to-Clarify framework. Our framework first resolves ambiguous instructions by asking questions in a multi-turn dialogue. Then it generates low-level actions end-to-end. Specifically, the Ask-to-Clarify framework consists of two components, one VLM for collaboration and one diffusion for action. We also introduce a connection module that generates conditions for the diffusion based on the output of the VLM. This module adjusts the observation by instructions to create reliable conditions. We train our framework with a two-stage knowledge-insulation strategy. First, we fine-tune the collaboration component using ambiguity-solving dialogue data to handle ambiguity. Then, we integrate the action component while freezing the collaboration one. This preserves the interaction abilities while fine-tuning the diffusion to generate actions. The training strategy guarantees our framework can first ask questions, then generate actions. During inference, a signal detector functions as a router that helps our framework switch between asking questions and taking actions. We evaluate the Ask-to-Clarify framework in 8 real-world tasks, where it outperforms existing state-of-the-art VLAs. The results suggest that our proposed framework, along with the training strategy, provides a path toward collaborative embodied agents.",
          "abstract_zh": "具身代理的最终目标是创建可以与人类交互的协作者，而不仅仅是被动遵循指令的执行者。这需要代理根据人类反馈进行沟通、协调和调整他们的行动。最近，VLA 的进展为实现这一目标提供了一条途径。然而，当前大多数基于 VLA 的实体代理以单向模式运行：它们接收指令并在没有反馈的情况下执行它。这种方法在指令通常不明确的现实场景中会失败。在本文中，我们通过“要求澄清”框架解决了这个问题。我们的框架首先通过在多轮对话中提出问题来解决不明确的指令。然后它会端到端地生成低级操作。具体来说，“要求澄清”框架由两个部分组成，一个用于协作的 VLM，一个用于行动的扩散。我们还引入了一个连接模块，它根据 VLM 的输出生成扩散条件。该模块通过指令调整观察以创造可靠的条件。我们使用两阶段知识隔离策略来训练我们的框架。首先，我们使用消除歧义的对话数据来微调协作组件来处理歧义。然后，我们集成操作组件，同时冻结协作组件。这保留了交互能力，同时微调扩散以生成动作。培训策略保证我们的框架可以首先提出问题，然后生成行动。在推理过程中，信号检测器充当路由器，帮助我们的框架在提出问题和采取行动之间切换。我们在 8 个实际任务中评估了 Ask-to-Clarify 框架，它的性能优于现有的最先进的 VLA。结果表明，我们提出的框架以及培训策略提供了一条通向协作具体代理的道路。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 10 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 10）。",
      "items": [
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "terminators2025/RealMirror",
          "url": "https://github.com/terminators2025/RealMirror",
          "snippet": "RealMirror, a comprehensive, open-source embodied AI VLA platform.  ",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "Vector-Wangel/XLeRobot",
          "url": "https://github.com/Vector-Wangel/XLeRobot",
          "snippet": "XLeRobot: Practical Dual-Arm Mobile Home Robot for $660",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "HCPLab-SYSU/Embodied_AI_Paper_List",
          "url": "https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List",
          "snippet": "[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 9
        },
        {
          "title": "yang-zj1026/NaVILA-Bench",
          "url": "https://github.com/yang-zj1026/NaVILA-Bench",
          "snippet": "Vision-Language Navigation Benchmark in Isaac Lab",
          "site": "github.com",
          "rank": 10
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 上共发现 1 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 1）。",
      "items": [
        {
          "title": "InternRobotics/VLAC",
          "url": "https://huggingface.co/InternRobotics/VLAC",
          "snippet": "InternRobotics/VLAC",
          "site": "huggingface.co",
          "rank": 3,
          "published": "2025-09-15T13:13:33.000Z"
        }
      ]
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-09-15",
  "week_end": "2025-09-21",
  "last_updated": "2026-01-07"
}