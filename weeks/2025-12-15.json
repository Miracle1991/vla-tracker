{
  "generated_at": "2026-01-07T13:54:51.932039",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 18 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 18）。",
      "items": [
        {
          "title": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion",
          "url": "http://arxiv.org/abs/2512.18007v1",
          "snippet": "Vision-Language-Action (VLA) models have achieved remarkable progress in robotic manipulation by mapping multimodal observations and instructions directly to actions. However, they typically mimic expert trajectories without predictive motion reasoning, which limits their ability to reason about what actions to take. To address this limitation, we propose joint learning with motion image diffusion, a novel strategy that enhances VLA models with motion reasoning capabilities. Our method extends the VLA architecture with a dual-head design: while the action head predicts action chunks as in vanilla VLAs, an additional motion head, implemented as a Diffusion Transformer (DiT), predicts optical-flow-based motion images that capture future dynamics. The two heads are trained jointly, enabling the shared VLM backbone to learn representations that couple robot control with motion knowledge. This joint learning builds temporally coherent and physically grounded representations without modifying the inference pathway of standard VLAs, thereby maintaining test-time latency. Experiments in both simulation and real-world environments demonstrate that joint learning with motion image diffusion improves the success rate of pi-series VLAs to 97.5% on the LIBERO benchmark and 58.0% on the RoboTwin benchmark, yielding a 23% improvement in real-world performance and validating its effectiveness in enhancing the motion reasoning capability of large-scale VLAs.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-12-19T19:07:53Z",
          "authors": [
            "Yu Fang",
            "Kanchana Ranasinghe",
            "Le Xue",
            "Honglu Zhou",
            "Juntao Tan",
            "Ran Xu",
            "Shelby Heinecke",
            "Caiming Xiong",
            "Silvio Savarese",
            "Daniel Szafir",
            "Mingyu Ding",
            "Michael S. Ryoo",
            "Juan Carlos Niebles"
          ],
          "arxiv_id": "2512.18007",
          "abstract": "Vision-Language-Action (VLA) models have achieved remarkable progress in robotic manipulation by mapping multimodal observations and instructions directly to actions. However, they typically mimic expert trajectories without predictive motion reasoning, which limits their ability to reason about what actions to take. To address this limitation, we propose joint learning with motion image diffusion, a novel strategy that enhances VLA models with motion reasoning capabilities. Our method extends the VLA architecture with a dual-head design: while the action head predicts action chunks as in vanilla VLAs, an additional motion head, implemented as a Diffusion Transformer (DiT), predicts optical-flow-based motion images that capture future dynamics. The two heads are trained jointly, enabling the shared VLM backbone to learn representations that couple robot control with motion knowledge. This joint learning builds temporally coherent and physically grounded representations without modifying the inference pathway of standard VLAs, thereby maintaining test-time latency. Experiments in both simulation and real-world environments demonstrate that joint learning with motion image diffusion improves the success rate of pi-series VLAs to 97.5% on the LIBERO benchmark and 58.0% on the RoboTwin benchmark, yielding a 23% improvement in real-world performance and validating its effectiveness in enhancing the motion reasoning capability of large-scale VLAs.",
          "abstract_zh": "视觉-语言-动作（VLA）模型通过将多模态观察和指令直接映射到动作，在机器人操作方面取得了显着的进展。然而，它们通常模仿专家轨迹，而没有预测运动推理，这限制了它们推理要采取什么动作的能力。为了解决这个限制，我们提出了运动图像扩散联合学习，这是一种增强 VLA 模型运动推理能力的新策略。我们的方法通过双头设计扩展了 VLA 架构：虽然动作头像普通 VLA 一样预测动作块，但作为扩散变压器 (DiT) 实现的附加运动头可以预测基于光流的运动图像，以捕获未来的动态。两个头部进行联合训练，使共享的 VLM 主干能够学习将机器人控制与运动知识相结合的表示。这种联合学习构建了时间连贯且物理基础的表示，而无需修改标准 VLA 的推理路径，从而保持测试时间延迟。仿真和现实环境中的实验表明，运动图像扩散联合学习将 pi 系列 VLA 的成功率在 LIBERO 基准上提高到 97.5%，在 RoboTwin 基准上提高到 58.0%，使实际性能提高 23%，验证了其在增强大规模 VLA 运动推理能力方面的有效性。"
        },
        {
          "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2512.18477v1",
          "snippet": "We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-12-20T19:40:25Z",
          "authors": [
            "Wenjun Lin",
            "Jensen Zhang",
            "Kaitong Cai",
            "Keze Wang"
          ],
          "arxiv_id": "2512.18477",
          "abstract": "We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation.",
          "abstract_zh": "我们提出了 STORM（搜索引导生成世界模型），这是一种用于机器人操作中时空推理的新颖框架，它统一了基于扩散的动作生成、条件视频预测和基于搜索的规划。与之前依赖抽象潜在动态或将推理委托给语言组件的视觉-语言-动作 (VLA) 模型不同，STORM 将规划建立在显式视觉展示的基础上，从而实现可解释和前瞻性驱动的决策。基于扩散的 VLA 策略提出了不同的候选动作，生成视频世界模型模拟了它们的视觉和奖励结果，蒙特卡洛树搜索 (MCTS) 通过前瞻评估有选择地完善计划。SimplerEnv 操纵基准的实验表明，STORM 达到了 51.0% 的新的最先进的平均成功率，优于 CogACT 等强大的基准。奖励增强视频预测显着提高了时空保真度和任务相关性，将 Frechet 视频距离缩短了 75% 以上。此外，STORM 表现出强大的重新规划和故障恢复行为，突出了搜索引导的生成世界模型在长视野机器人操作方面的优势。"
        },
        {
          "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future",
          "url": "http://arxiv.org/abs/2512.16760v2",
          "snippet": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-12-18T16:57:44Z",
          "authors": [
            "Tianshuai Hu",
            "Xiaolu Liu",
            "Song Wang",
            "Yiyao Zhu",
            "Ao Liang",
            "Lingdong Kong",
            "Guoyang Zhao",
            "Zeying Gong",
            "Jun Cen",
            "Zhiyu Huang",
            "Xiaoshuai Hao",
            "Linfeng Li",
            "Hang Song",
            "Xiangtai Li",
            "Jun Ma",
            "Shaojie Shen",
            "Jianke Zhu",
            "Dacheng Tao",
            "Ziwei Liu",
            "Junwei Liang"
          ],
          "arxiv_id": "2512.16760",
          "abstract": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.",
          "abstract_zh": "自动驾驶长期以来一直依赖于模块化的“感知-决策-行动”管道，其中手工制作的界面和基于规则的组件经常在复杂或长尾场景中崩溃。它们的级联设计进一步传播感知错误，降低下游规划和控制能力。视觉-动作（VA）模型通过学习从视觉输入到动作的直接映射来解决一些局限性，但它们仍然不透明，对分布变化敏感，并且缺乏结构化推理或指令跟踪能力。大语言模型（LLM）和多模态学习的最新进展推动了视觉-语言-行动（VLA）框架的出现，该框架将感知与基于语言的决策相结合。通过统一视觉理解、语言推理和可操作的输出，VLA 提供了一条通向更可解释、更通用和更人性化的驾驶政策的途径。这项工作提供了自动驾驶新兴 VLA 景观的结构化特征。我们追溯了从早期 VA 方法到现代 VLA 框架的演变，并将现有方法组织成两个主要范式：端到端 VLA（将感知、推理和规划集成在单个模型中）和双系统 VLA（将缓慢的审议（通过 VLM）与快速、安全关键的执行（通过规划器）分开。在这些范式中，我们进一步区分了子类，例如文本与数字动作生成器以及显式与隐式指导机制。我们还总结了用于评估基于 VLA 的驾驶系统的代表性数据集和基准，并强调了关键挑战和开放方向，包括鲁棒性、可解释性和指令保真度。总体而言，这项工作旨在为推进与人类兼容的自动驾驶系统奠定坚实的基础。"
        },
        {
          "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
          "url": "http://arxiv.org/abs/2512.15692v2",
          "snippet": "Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce mimic-video, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-12-17T18:47:31Z",
          "authors": [
            "Jonas Pai",
            "Liam Achenbach",
            "Victoriano Montesinos",
            "Benedek Forrai",
            "Oier Mees",
            "Elvis Nava"
          ],
          "arxiv_id": "2512.15692",
          "abstract": "Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce mimic-video, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.",
          "abstract_zh": "用于机器人操作的流行视觉语言动作模型（VLA）建立在视觉语言主干上，该主干在大规模但断开连接的静态网络数据上进行了预训练。因此，尽管语义泛化得到了改进，但该策略必须仅从机器人轨迹中隐式推断出复杂的物理动力学和时间依赖性。这种依赖造成了不可持续的数据负担，需要持续、大规模的专家数据收集来弥补天生物理理解的缺乏。我们认为，虽然视觉语言预训练有效地捕获了语义先验，但它仍然对物理因果关系视而不见。更有效的范式利用视频在预训练期间联合捕获语义和视觉动态，从而隔离低级控制的剩余任务。为此，我们引入了imit-video，一种新颖的视频动作模型（VAM），它将预训练的互联网规模视频模型与基于流匹配的动作解码器（以其潜在表示为条件）配对。解码器充当逆动力学模型（IDM），从视频空间动作计划的潜在表示生成低级机器人动作。我们的广泛评估表明，我们的方法在模拟和现实世界的机器人操作任务中实现了最先进的性能，与传统的 VLA 架构相比，样本效率提高了 10 倍，收敛速度提高了 2 倍。"
        },
        {
          "title": "MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training",
          "url": "http://arxiv.org/abs/2512.15411v2",
          "snippet": "While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\\boldsymbolπ_{0}$, $\\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-12-17T12:59:41Z",
          "authors": [
            "Zhenhan Yin",
            "Xuanhan Wang",
            "Jiahao Jiang",
            "Kaiyuan Deng",
            "Pengqi Chen",
            "Shuangle Li",
            "Chong Liu",
            "Xing Xu",
            "Jingkuan Song",
            "Lianli Gao",
            "Heng Tao Shen"
          ],
          "arxiv_id": "2512.15411",
          "abstract": "While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\\boldsymbolπ_{0}$, $\\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.",
          "abstract_zh": "虽然利用丰富的人类视频和模拟机器人数据为现实世界机器人数据的稀缺性提供了可扩展的解决方案，但现有视觉语言动作模型（VLA）的泛化能力仍然受到摄像机视图、视觉外观和实施例形态不匹配的限制。为了克服这一限制，我们提出了 MiVLA，这是一种由人机相互模仿预训练支持的通用 VLA，它利用人手和机器人手臂之间固有的行为相似性，为人类行为和机器人控制建立强大的行为先验基础。具体来说，我们的方法利用左/右手坐标系的运动学规则来实现人类和机器人动作空间之间的双向对齐。在给定人类或模拟机器人演示的情况下，MiVLA 经过训练可以预测一个实施例的行为轨迹，并模仿演示中未见过的另一个实施例的行为。基于这种相互模仿，它将现实世界人类数据的行为保真度与模拟机器人数据的操控多样性整合成一个统一的模型，从而增强下游任务的泛化能力。在模拟和现实世界平台上使用三个机器人（ARX、PiPer 和 LocoMan）进行的大量实验表明，MiVLA 实现了强大的泛化能力改进，在模拟中比最先进的 VLA（例如 $\\boldsymbolπ_{0}$、$\\boldsymbolπ_{0.5}$ 和 H-RDT）高出 25%，在现实世界机器人控制任务中高出 14%。"
        },
        {
          "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model",
          "url": "http://arxiv.org/abs/2512.14031v1",
          "snippet": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-12-16T02:56:13Z",
          "authors": [
            "Zhaofeng Hu",
            "Hongrui Yu",
            "Vaidhyanathan Chandramouli",
            "Ci-Jyun Liang"
          ],
          "arxiv_id": "2512.14031",
          "abstract": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.",
          "abstract_zh": "本研究评估了两种教授建筑机器人新技能的主要方法，以了解其在建筑自动化中的适用性：视觉-语言-动作（VLA）模型和强化学习（RL）方法。目标是了解任务绩效以及在实际工作中部署每种方法所需的实际工作。作者开发了两个远程操作界面来控制机器人并收集所需的演示，这两种界面都被证明对于训练机器人执行长视距和灵巧任务是有效的。此外，作者还进行了三阶段评估。首先，作者将多层感知器 (MLP) 策略与深度 Q 网络 (DQN) 模仿模型进行比较，以确定更强的 RL 基线，重点关注模型性能、泛化和拾取实验。其次，在两种不同的场景中训练三种不同的 VLA 模型并进行比较。第三，作者使用计算和样本效率措施，对选定的 RL 基线与 VLA 模型进行基准测试，然后对包括运输和安装在内的多阶段面板安装任务进行机器人实验。VLA模型表现出很强的泛化能力和少样本能力，在拾取阶段取得了60%和100%的成功率。相比之下，DQN 可以变得鲁棒，但在调整过程中需要额外的噪声，这增加了工作量。总体而言，研究结果表明，VLA 通过减少编程工作量并以最少的数据实现有用的性能，为更改任务提供了实际优势，而 DQN 在可以接受足够的调优工作时提供了可行的基线。"
        },
        {
          "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning",
          "url": "http://arxiv.org/abs/2512.13636v2",
          "snippet": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. Using the lightweight Qwen-0.5B LLM, MindDrive achieves Driving Score (DS) of 78.04 and Success Rate (SR) of 55.09% on the challenging Bench2Drive benchmark. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-12-15T18:31:32Z",
          "authors": [
            "Haoyu Fu",
            "Diankun Zhang",
            "Zongchuang Zhao",
            "Jianfeng Cui",
            "Hongwei Xie",
            "Bing Wang",
            "Guang Chen",
            "Dingkang Liang",
            "Xiang Bai"
          ],
          "arxiv_id": "2512.13636",
          "abstract": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. Using the lightweight Qwen-0.5B LLM, MindDrive achieves Driving Score (DS) of 78.04 and Success Rate (SR) of 55.09% on the challenging Bench2Drive benchmark. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.",
          "abstract_zh": "当前自动驾驶中的视觉-语言-动作（VLA）范式主要依赖于模仿学习（IL），这引入了分布偏移和因果混乱等固有挑战。在线强化学习提供了一条通过试错学习解决这些问题的有前途的途径。然而，将在线强化学习应用于自动驾驶中的 VLA 模型却因连续动作空间中的低效探索而受到阻碍。为了克服这一限制，我们提出了 MindDrive，这是一个 VLA 框架，包含一个具有两组不同 LoRA 参数的大型语言模型 (LLM)。一名法学硕士充当场景推理和驱动决策的决策专家，而另一名法学硕士则充当行动专家，将语言决策动态映射到可行的轨迹。通过将轨迹级奖励反馈回推理空间，MindDrive 可以对一组有限的离散语言驾驶决策进行试错学习，而不是直接在连续的动作空间中操作。该方法有效地平衡了复杂场景下的最优决策、类人驾驶行为以及在线强化学习的高效探索。使用轻量级 Qwen-0.5B LLM，MindDrive 在具有挑战性的 Bench2Drive 基准测试中获得了 78.04 的驾驶分数 (DS) 和 55.09% 的成功率 (SR)。据我们所知，这是第一个展示自动驾驶中 VLA 模型在线强化学习有效性的工作。"
        },
        {
          "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
          "url": "http://arxiv.org/abs/2512.19562v1",
          "snippet": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the π_{0}, π_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-12-22T16:44:23Z",
          "authors": [
            "Martin Sedlacek",
            "Pavlo Yefanov",
            "Georgy Ponimatkin",
            "Jai Bardhan",
            "Simon Pilc",
            "Mederic Fourmy",
            "Evangelos Kazakos",
            "Cees G. M. Snoek",
            "Josef Sivic",
            "Vladimir Petrik"
          ],
          "arxiv_id": "2512.19562",
          "abstract": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the π_{0}, π_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm",
          "abstract_zh": "视觉-语言-动作（VLA）模型使机器人能够理解和执行自然​​语言指令描述的任务。然而，一个关键的挑战在于它们的泛化能力超出了他们所接受训练的特定环境和条件，而目前在现实世界中评估这一点既困难又昂贵。为了解决这一差距，我们推出了 REALM，这是一种新的模拟环境和基准，旨在评估 VLA 模型的泛化能力，特别强调通过高保真视觉效果和对齐的机器人控制在模拟和现实世界性能之间建立强大的相关性。我们的环境提供了一套 15 个扰动因素、7 种操作技能和超过 3,500 个对象。最后，我们建立了两个任务集来构成我们的基准并评估 π_{0}、π_{0}-FAST 和 GR00T N1.5 VLA 模型，这表明泛化性和鲁棒性仍然是一个开放的挑战。更广泛地说，我们还表明，模拟为我们提供了现实世界的宝贵代理，使我们能够系统地探索和量化 VLA 的弱点和故障模式。项目页面：https://martin-sedlacek.com/realm"
        },
        {
          "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2512.14666v1",
          "snippet": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-12-16T18:26:38Z",
          "authors": [
            "Zechen Bai",
            "Chen Gao",
            "Mike Zheng Shou"
          ],
          "arxiv_id": "2512.14666",
          "abstract": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.",
          "abstract_zh": "要实现真正的自适应体现智能，智能体不仅要通过模仿静态演示来学习，还要通过环境交互不断改进，这类似于人类通过练习掌握技能的方式。视觉-语言-动作（VLA）模型通过利用大型语言模型实现了先进的机器人操作，但仍然受到监督微调（SFT）的根本限制：每个任务需要数百次演示，严格记住轨迹，并且在部署条件偏离训练时无法适应。我们推出了 EVOLVE-VLA，这是一个测试时训练框架，使 VLA 能够通过环境交互不断适应，并具有最少或零的特定任务演示。关键的技术挑战是用自主反馈取代预言机奖励信号（在测试时不可用）。我们通过提供密集反馈的学习进度估计器来解决这个问题，更重要的是，我们设计了我们的框架，通过两种机制来“驯服”这种固有的噪声信号：(1) 累积进度估计机制，平滑噪声逐点估计，(2) 渐进的视野扩展策略，支持渐进的政策演化。EVOLVE-VLA 取得了巨大的进步：在长视野任务上 +8.6\\%，在 1-shot 学习中 +22.0\\%，并实现了跨任务泛化——在没有特定任务演示训练的情况下，在未见过的任务上取得了 20.8\\% 的成功（而纯 SFT 为 0\\%）。定性分析揭示了演示中缺少的新兴功能，包括错误恢复和新颖的策略。这项工作代表了 VLA 迈出了真正学习和适应的关键一步，超越静态模仿，走向持续的自我完善。"
        },
        {
          "title": "Large Video Planner Enables Generalizable Robot Control",
          "url": "http://arxiv.org/abs/2512.15840v1",
          "snippet": "General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-12-17T18:35:54Z",
          "authors": [
            "Boyuan Chen",
            "Tianyuan Zhang",
            "Haoran Geng",
            "Kiwhan Song",
            "Caiyi Zhang",
            "Peihao Li",
            "William T. Freeman",
            "Jitendra Malik",
            "Pieter Abbeel",
            "Russ Tedrake",
            "Vincent Sitzmann",
            "Yilun Du"
          ],
          "arxiv_id": "2512.15840",
          "abstract": "General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.",
          "abstract_zh": "通用机器人需要能够泛化不同任务和环境的决策模型。最近的工作通过使用动作输出扩展多模态大语言模型（MLLM）来构建机器人基础模型，创建视觉-语言-动作（VLA）系统。这些努力的动机是MLLM 的大规模语言和图像预训练可以有效地转移到动作输出模态。在这项工作中，我们探索了一种使用大规模视频预训练作为构建机器人基础模型的主要方式的替代范例。与静态图像和语言不同，视频捕捉物理世界中与机器人行为自然一致的状态和动作的时空序列。我们策划了人类活动和任务演示的互联网规模视频数据集，并首次在基础模型规模上训练用于生成机器人规划的开放视频模型。该模型为新颖的场景和任务生成零镜头视频计划，我们对其进行后处理以提取可执行的机器人动作。我们通过第三方在野外和真实机器人实验中选择的任务来评估任务级泛化，展示成功的物理执行。总之，这些结果显示了强大的指令遵循性、强大的泛化性和现实世界的可行性。我们发布了模型和数据集，以支持开放、可重复的基于视频的机器人学习。我们的网站位于 https://www.boyuan.space/large-video-planner/。"
        },
        {
          "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
          "url": "http://arxiv.org/abs/2512.16811v1",
          "snippet": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-12-18T17:51:42Z",
          "authors": [
            "Jingjing Qian",
            "Boyao Han",
            "Chen Shi",
            "Lei Xiao",
            "Long Yang",
            "Shaoshuai Shi",
            "Li Jiang"
          ],
          "arxiv_id": "2512.16811",
          "abstract": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型在机器人操作方面实现了很强的泛化，但在很大程度上仍然是反应性的和以 2D 为中心的，这使得它们在需要精确 3D 推理的任务中不可靠。我们提出了 GeoPredict，这是一个几何感知的 VLA 框架，它通过预测运动学和几何先验增强了连续动作策略。GeoPredict 引入了一个轨迹级模块，用于对运动历史进行编码并预测机器人手臂的多步 3D 关键点轨迹，以及一个预测性 3D 高斯几何模块，用于通过沿着未来关键点轨迹的轨迹引导细化来预测工作空间几何形状。这些预测模块专门通过基于深度的渲染充当训练时监督，而推理仅需要轻量级的附加查询标记，而无需调用任何 3D 解码。RoboCasa Human-50、LIBERO 和现实世界操作任务的实验表明，GeoPredict 始终优于强大的 VLA 基线，特别是在几何密集型和空间要求较高的场景中。"
        },
        {
          "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments",
          "url": "http://arxiv.org/abs/2512.15258v2",
          "snippet": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-12-17T10:02:55Z",
          "authors": [
            "Yuze Wu",
            "Mo Zhu",
            "Xingxing Li",
            "Yuheng Du",
            "Yuxin Fan",
            "Wenjun Li",
            "Zhichao Han",
            "Xin Zhou",
            "Fei Gao"
          ],
          "arxiv_id": "2512.15258",
          "abstract": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.",
          "abstract_zh": "本文提出了 VLA-AN，这是一种高效的机载视觉-语言-动作（VLA）框架，专用于复杂环境中的自主无人机导航。VLA-AN 解决了​​现有大型空中导航模型的四个主要限制：数据域差距、推理时间导航不足、生成行动策略的安全问题以及机载部署限制。首先，我们利用 3D 高斯分布 (3D-GS) 构建高保真数据集，以有效弥合域差距。其次，我们引入了一个渐进的三阶段训练框架，依次加强场景理解、核心飞行技能和复杂的导航能力。第三，我们设计了一个带有几何安全校正的轻量级实时动作模块。该模块确保快速、无碰撞且稳定的命令生成，减轻随机生成策略固有的安全风险。最后，通过对机载部署流程的深度优化，VLA-AN 在资源受限的无人机上实现了 8.3 倍的实时推理吞吐量的稳健提升。大量实验表明，VLA-AN显着提高了空间接地、场景推理和长视距导航能力，单任务成功率最高达到98.1%，为轻型空中机器人实现全链闭环自主提供了高效、实用的解决方案。"
        },
        {
          "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
          "url": "http://arxiv.org/abs/2512.13080v1",
          "snippet": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-12-15T08:31:47Z",
          "authors": [
            "Yicheng Feng",
            "Wanpeng Zhang",
            "Ye Wang",
            "Hao Luo",
            "Haoqi Yuan",
            "Sipeng Zheng",
            "Zongqing Lu"
          ],
          "arxiv_id": "2512.13080",
          "abstract": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
          "abstract_zh": "视觉-语言-动作（VLA）模型通过将视觉感知与语言引导的策略学习相结合，为机器人学习提供了一种有前途的范例。然而，大多数现有方法依赖 2D 视觉输入在 3D 物理环境中执行动作，从而在感知和动作基础之间造成了巨大差距。为了弥补这一差距，我们提出了一种空间感知 VLA 预训练范例，该范例在预训练期间执行视觉空间和物理空间之间的显式对齐，使模型能够在机器人策略学习之前获得 3D 空间理解。从预训练的视觉语言模型开始，我们利用大规模人类演示视频来提取 3D 视觉和 3D 动作注释，形成将 2D 视觉观察与 3D 空间推理结合起来的新监督来源。我们使用 VIPA-VLA 实例化了这一范例，VIPA-VLA 是一种双编码器架构，它结合了 3D 视觉编码器，通过 3D 感知功能增强语义视觉表示。当适应下游机器人任务时，VIPA-VLA 显着改善了 2D 视觉和 3D 动作之间的基础，从而产生更稳健和通用的机器人策略。"
        },
        {
          "title": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation",
          "url": "http://arxiv.org/abs/2512.18396v1",
          "snippet": "Recent advances in Vision-Language-Action (VLA) and world-model methods have improved generalization in tasks such as robotic manipulation and object interaction. However, Successful execution of such tasks depends on large, costly collections of real demonstrations, especially for fine-grained manipulation of articulated objects. To address this, we present AOMGen, a scalable data generation framework for articulated manipulation which is instantiated from a single real scan, demonstration and a library of readily available digital assets, yielding photoreal training data with verified physical states. The framework synthesizes synchronized multi-view RGB temporally aligned with action commands and state annotations for joints and contacts, and systematically varies camera viewpoints, object styles, and object poses to expand a single execution into a diverse corpus. Experimental results demonstrate that fine-tuning VLA policies on AOMGen data increases the success rate from 0% to 88.7%, and the policies are tested on unseen objects and layouts.",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-12-20T15:21:25Z",
          "authors": [
            "Yulu Wu",
            "Jiujun Cheng",
            "Haowen Wang",
            "Dengyang Suo",
            "Pei Ren",
            "Qichao Mao",
            "Shangce Gao",
            "Yakun Huang"
          ],
          "arxiv_id": "2512.18396",
          "abstract": "Recent advances in Vision-Language-Action (VLA) and world-model methods have improved generalization in tasks such as robotic manipulation and object interaction. However, Successful execution of such tasks depends on large, costly collections of real demonstrations, especially for fine-grained manipulation of articulated objects. To address this, we present AOMGen, a scalable data generation framework for articulated manipulation which is instantiated from a single real scan, demonstration and a library of readily available digital assets, yielding photoreal training data with verified physical states. The framework synthesizes synchronized multi-view RGB temporally aligned with action commands and state annotations for joints and contacts, and systematically varies camera viewpoints, object styles, and object poses to expand a single execution into a diverse corpus. Experimental results demonstrate that fine-tuning VLA policies on AOMGen data increases the success rate from 0% to 88.7%, and the policies are tested on unseen objects and layouts.",
          "abstract_zh": "视觉-语言-动作（VLA）和世界模型方法的最新进展提高了机器人操作和对象交互等任务的泛化能力。然而，此类任务的成功执行取决于大量、昂贵的真实演示集合，特别是对于铰接对象的细粒度操作。为了解决这个问题，我们提出了 AOMGen，这是一种用于铰接式操作的可扩展数据生成框架，它通过单个真实扫描、演示和现成的数字资产库进行实例化，生成具有经过验证的物理状态的照片级真实训练数据。该框架合成同步多视图 RGB，与动作命令和关节和接触的状态注释在时间上对齐，并系统地改变相机视点、对象样式和对象姿势，以将单个执行扩展为多样化的语料库。实验结果表明，在 AOMGen 数据上微调 VLA 策略可将成功率从 0% 提高到 88.7%，并且这些策略在看不见的对象和布局上进行了测试。"
        },
        {
          "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
          "url": "http://arxiv.org/abs/2512.16793v1",
          "snippet": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
          "site": "arxiv.org",
          "rank": 15,
          "published": "2025-12-18T17:27:03Z",
          "authors": [
            "Xiaopeng Lin",
            "Shijie Lian",
            "Bin Yu",
            "Ruoqi Yang",
            "Changti Wu",
            "Yuzhuo Miao",
            "Yurun Jin",
            "Yukun Shi",
            "Cong Huang",
            "Bojun Cheng",
            "Kai Chen"
          ],
          "arxiv_id": "2512.16793",
          "abstract": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
          "abstract_zh": "机器人的泛化依赖于物理智能：在以自我为中心的感知和行动下推理状态变化、丰富的接触交互以及长期规划的能力。然而，大多数 VLM 主要是根据第三人称数据进行训练的，这为人形机器人造成了基本的视点不匹配。由于成本高昂和多样性有限，扩展机器人以自我为中心的数据收集仍然不切实际，而大规模人类以自我为中心的视频提供了一种可扩展的替代方案，可以自然地捕获丰富的交互上下文和因果结构。关键的挑战是将原始的以自我为中心的视频转换为结构化且可靠的体现培训监督。因此，我们提出了一种 Egocentric2Embodiment 翻译管道，将第一人称视频转换为多层次、模式驱动的 VQA 监督，具有强制证据基础和时间一致性，从而能够大规模构建 Egocentric2Embodiment 数据集 (E2E-3M)。通过在 E2E-3M 数据集上进行训练获得了一个具有自我中心意识的实体大脑，称为 PhysBrain。PhysBrain 表现出显着改善的以自我为中心的理解，特别是对于 EgoThink 的规划。它提供了以自我为中心的感知初始化，可以实现更高效的 VLA 微调和更高的 SimplerEnv 成功率 (53.9\\%)，证明了从人类以自我为中心的监督到下游机器人控制的有效转移。"
        },
        {
          "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
          "url": "http://arxiv.org/abs/2512.19024v1",
          "snippet": "Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \\textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \\textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \\textbf{IndoorUAV-VLA} subset. Finally, we introduce \\textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.",
          "site": "arxiv.org",
          "rank": 16,
          "published": "2025-12-22T04:42:35Z",
          "authors": [
            "Xu Liu",
            "Yu Liu",
            "Hanshuo Qiu",
            "Yang Qirong",
            "Zhouhui Lian"
          ],
          "arxiv_id": "2512.19024",
          "abstract": "Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \\textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \\textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \\textbf{IndoorUAV-VLA} subset. Finally, we introduce \\textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.",
          "abstract_zh": "视觉语言导航（VLN）使智能体能够通过遵循基于视觉观察的自然语言指令在复杂的环境中进行导航。尽管大多数现有工作都集中在地面机器人或室外无人机 (UAV) 上，但基于室内无人机的 VLN 仍然未被充分开发，尽管它与有限空间中的检查、交付和搜索救援等实际应用相关。为了弥补这一差距，我们引入了 \\textbf{IndoorUAV}，这是一种专为室内无人机 VLN 量身定制的新颖基准和方法。我们首先从 Habitat 模拟器中策划 1,000 多个多样化且结构丰富的 3D 室内场景。在这些环境中，我们模拟真实的无人机飞行动力学，以手动收集不同的 3D 导航轨迹，并通过数据增强技术进一步丰富。此外，我们设计了一个自动注释管道来为每个轨迹生成不同粒度的自然语言指令。该过程产生超过 16,000 个高质量轨迹，包括 \\textbf{IndoorUAV-VLN} 子集，该子集专注于长视野 VLN。为了支持短视野规划，我们通过选择语义上显着的关键帧并重新生成简洁的指令，将长轨迹分割成子轨迹，形成 \\textbf{IndoorUAV-VLA} 子集。最后，我们介绍 \\textbf{IndoorUAV-Agent}，这是一种专为我们的基准测试而设计的新颖导航模型，利用任务分解和多模态推理。我们希望 IndoorUAV 成为推进室内空中导航领域视觉语言人工智能研究的宝贵资源。"
        },
        {
          "title": "Motus: A Unified Latent Action World Model",
          "url": "http://arxiv.org/abs/2512.13030v2",
          "snippet": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
          "site": "arxiv.org",
          "rank": 17,
          "published": "2025-12-15T06:58:40Z",
          "authors": [
            "Hongzhe Bi",
            "Hengkai Tan",
            "Shenghao Xie",
            "Zeyuan Wang",
            "Shuhe Huang",
            "Haitian Liu",
            "Ruowen Zhao",
            "Yao Feng",
            "Chendong Xiang",
            "Yinze Rong",
            "Hongyan Zhao",
            "Hanyu Liu",
            "Zhizhong Su",
            "Lei Ma",
            "Hang Su",
            "Jun Zhu"
          ],
          "arxiv_id": "2512.13030",
          "abstract": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
          "abstract_zh": "虽然通用的实体代理必须作为一个统一的系统发挥作用，但当前的方法是建立在用于理解、世界建模和控制的孤立模型之上的。这种碎片化阻碍了多模式生成能力的统一，并阻碍了从大规模异构数据中进行学习。在本文中，我们提出了 Motus，这是一种统一的潜在动作世界模型，它利用现有的通用预训练模型和丰富的、可共享的运动信息。Motus引入了Mixture-of-Transformer（MoT）架构来集成三个专家（即理解、视频生成和动作），并采用UniDiffuser式调度器来实现不同建模模式（即世界模型、视觉-语言-动作模型、逆动力学模型、视频生成模型和视频-动作联合预测模型）之间的灵活切换。Motus进一步利用光流来学习潜在动作，并采用三相训练管道和六层数据金字塔的配方，从而提取像素级的“增量动作”并实现大规模动作预训练。实验表明，Motus 在模拟（比 X-VLA 提高了 +15%，比 Pi0.5 提高了 +45%）和现实场景（提高了 +11~48%）方面均实现了优于最先进方法的性能，证明所有功能和先验的统一建模显着有利于下游机器人任务。"
        },
        {
          "title": "Point What You Mean: Visually Grounded Instruction Policy",
          "url": "http://arxiv.org/abs/2512.18933v1",
          "snippet": "Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control.",
          "site": "arxiv.org",
          "rank": 18,
          "published": "2025-12-22T00:44:19Z",
          "authors": [
            "Hang Yu",
            "Juntu Zhao",
            "Yufeng Liu",
            "Kaiyu Li",
            "Cheng Ma",
            "Di Zhang",
            "Yingdong Hu",
            "Guang Chen",
            "Junyuan Xie",
            "Junliang Guo",
            "Junqiao Zhao",
            "Yang Gao"
          ],
          "arxiv_id": "2512.18933",
          "abstract": "Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control.",
          "abstract_zh": "视觉-语言-动作（VLA）模型将视觉和语言与具体控制结合起来，但是当仅依赖文本提示时，它们的对象引用能力仍然有限，特别是在杂乱或分布外（OOD）场景中。在本研究中，我们引入了 Point-VLA，这是一种即插即用策略，可通过明确的视觉提示（例如边界框）增强语言指令，以解决引用歧义并实现精确的对象级接地。为了有效地扩展基于视觉的数据集，我们进一步开发了一种需要最少人力的自动数据注释管道。我们在不同的现实世界引用任务上评估 Point-VLA，并观察到比纯文本指令 VLA 始终具有更强的性能，特别是在杂乱或看不见的对象场景中，具有强大的泛化能力。这些结果表明，Point-VLA 通过像素级视觉基础有效解决了对象指代模糊性，实现了更通用的体现控制。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 18 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 18）。",
      "items": [
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "terminators2025/RealMirror",
          "url": "https://github.com/terminators2025/RealMirror",
          "snippet": "RealMirror, a comprehensive, open-source embodied AI VLA platform.  ",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "thu-ml/Motus",
          "url": "https://github.com/thu-ml/Motus",
          "snippet": "Official code of Motus: A Unified Latent Action World Model",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "Vector-Wangel/XLeRobot",
          "url": "https://github.com/Vector-Wangel/XLeRobot",
          "snippet": "XLeRobot: Practical Dual-Arm Mobile Home Robot for $660",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "Dexmal/dexbotic",
          "url": "https://github.com/Dexmal/dexbotic",
          "snippet": "Dexbotic: Open-Source Vision-Language-Action Toolbox",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "IliaLarchenko/behavior-1k-solution",
          "url": "https://github.com/IliaLarchenko/behavior-1k-solution",
          "snippet": "1st place solution of 2025 BEHAVIOR Challenge",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "microsoft/VITRA",
          "url": "https://github.com/microsoft/VITRA",
          "snippet": "VITRA: Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos",
          "site": "github.com",
          "rank": 9
        },
        {
          "title": "HCPLab-SYSU/Embodied_AI_Paper_List",
          "url": "https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List",
          "snippet": "[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI",
          "site": "github.com",
          "rank": 10
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 11
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 12
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 13
        },
        {
          "title": "SalvatoreRa/ML-news-of-the-week",
          "url": "https://github.com/SalvatoreRa/ML-news-of-the-week",
          "snippet": "A collection of the the best ML and AI news every week (research, news, resources)",
          "site": "github.com",
          "rank": 14
        },
        {
          "title": "52CV/CVPR-2025-Papers",
          "url": "https://github.com/52CV/CVPR-2025-Papers",
          "snippet": "CVPR-2025-Papers",
          "site": "github.com",
          "rank": 15
        },
        {
          "title": "WayneMao/RoboMatrix",
          "url": "https://github.com/WayneMao/RoboMatrix",
          "snippet": "The Official Implementation of RoboMatrix",
          "site": "github.com",
          "rank": 16
        },
        {
          "title": "Hub-Tian/UAVs_Meet_LLMs",
          "url": "https://github.com/Hub-Tian/UAVs_Meet_LLMs",
          "snippet": "UAVs_Meet_LLMs",
          "site": "github.com",
          "rank": 17
        },
        {
          "title": "52CV/ECCV-2024-Papers",
          "url": "https://github.com/52CV/ECCV-2024-Papers",
          "snippet": "ECCV-2024-Papers",
          "site": "github.com",
          "rank": 18
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-12-15",
  "week_end": "2025-12-21",
  "last_updated": "2026-01-07"
}