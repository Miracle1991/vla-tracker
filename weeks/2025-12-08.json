{
  "generated_at": "2026-01-07T13:53:24.634804",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 24 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 24）。",
      "items": [
        {
          "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation",
          "url": "http://arxiv.org/abs/2512.07472v1",
          "snippet": "Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the \"Memory Trap\". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($π_{0}$ and $π_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-12-08T11:57:13Z",
          "authors": [
            "Siyu Xu",
            "Zijian Wang",
            "Yunke Wang",
            "Chenghao Xia",
            "Tao Huang",
            "Chang Xu"
          ],
          "arxiv_id": "2512.07472",
          "abstract": "Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the \"Memory Trap\". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($π_{0}$ and $π_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.",
          "abstract_zh": "视觉-语言-动作（VLA）模型通过将视觉观察和语言指令直接映射到动作，在机器人操作方面表现出了出色的性能。然而，它们在分布变化下仍然很脆弱：当测试场景发生变化时，VLA 通常会重现记忆的轨迹，而不是适应更新的场景，这是一种我们称为“内存陷阱”的故障模式。这种限制源于端到端设计，缺乏明确的 3D 空间推理，无法在不熟悉的环境中可靠地识别可操作区域。为了弥补这种空间理解的缺失，3D 空间功能域 (SAF) 可以提供几何表示，突出显示交互在物理上可行的位置，并提供有关机器人应接近或避开的区域的明确提示。因此，我们引入了 Affordance Field Intervention (AFI)，这是一种轻量级混合框架，它使用 SAF 作为按需插件来指导 VLA 行为。我们的系统通过本体感觉检测记忆陷阱，将机器人重新定位到最近的高可供性区域，并提出可供性驱动的路径点来锚定 VLA 生成的动作。然后，基于 SAF 的评分器会选择具有最高累积可供性的轨迹。大量实验表明，我们的方法在现实机器人平台上的分布外场景下，在不同的 VLA 主干（$π_{0}$ 和 $π_{0.5}$）上实现了 23.5% 的平均改进，在 LIBERO-Pro 基准上实现了 20.2%，验证了其在增强 VLA 对分布变化的鲁棒性方面的有效性。"
        },
        {
          "title": "Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control",
          "url": "http://arxiv.org/abs/2512.11921v1",
          "snippet": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-12-11T16:25:30Z",
          "authors": [
            "Abdullah Yahya Abdullah Omaisan",
            "Ibrahim Sheikh Mohamed"
          ],
          "arxiv_id": "2512.11921",
          "abstract": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型在机器人操作方面表现出了卓越的能力，使机器人能够通过视觉观察的端到端学习来执行自然语言命令。然而，由于计算限制以及有效适应新机器人实施例的需要，在经济实惠的机器人平台上部署大规模 VLA 模型仍然具有挑战性。本文提出了一种有效的微调方法和实际部署分析，使 VLA 模型适应低成本机器人操纵系统。我们提出了一种使用低秩适应 (LoRA) 和量化技术的资源高效微调策略，使数十亿参数的 VLA 模型（3.1B 参数）能够在具有 8GB VRAM 的消费级 GPU 上运行。我们的方法解决了将预先训练的 VLA 模型适应具有有限演示数据的新机器人实施例的关键挑战，重点关注冻结和未冻结视觉编码器之间的权衡。通过在 SO101 机械臂上实际部署按钮按下操作任务，我们证明了我们的方法在保持计算效率的同时实现了有效的操作性能。我们提供了部署挑战、故障模式以及训练数据量与实际性能之间关系的详细分析，并经过 200 个演示集的训练。我们的结果表明，通过适当的微调方法，VLA 模型可以成功部署在经济实惠的机器人平台上，从而使先进的操作能力超越昂贵的研究机器人。"
        },
        {
          "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2512.09928v1",
          "snippet": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-12-10T18:59:32Z",
          "authors": [
            "Minghui Lin",
            "Pengxiang Ding",
            "Shu Wang",
            "Zifeng Zhuang",
            "Yang Liu",
            "Xinyang Tong",
            "Wenxuan Song",
            "Shangke Lyu",
            "Siteng Huang",
            "Donglin Wang"
          ],
          "arxiv_id": "2512.09928",
          "abstract": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.",
          "abstract_zh": "视觉-语言-动作（VLA）模型最近通过将视觉和语言线索融入动作中，实现了机器人操作。然而，大多数 VLA 假定马尔可夫特性，仅依赖于当前的观察，因此患有时间近视，从而降低了长视界相干性。在这项工作中，我们将运动视为时间上下文和世界动态的更紧凑和信息丰富的表示，捕获状态间变化，同时过滤静态像素级噪声。基于这个想法，我们提出了 HiF-VLA（VLA 的 Hindsight、Insight 和 Foresight），这是一个利用运动进行双向时间推理的统一框架。HiF-VLA 通过后见之明先验对过去的动态进行编码，通过前瞻推理预测未来的运动，并通过后见之明调制的联合专家将两者集成起来，以实现长视野操纵的“边思考边行动”范式。因此，HiF-VLA 超越了 LIBERO-Long 和 CALVIN ABC-D 基准的强大基线，同时产生的额外推理延迟可以忽略不计。此外，HiF-VLA 在现实世界的长视距操作任务中实现了实质性改进，展示了其在实际机器人环境中的广泛有效性。"
        },
        {
          "title": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2512.11769v1",
          "snippet": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-12-12T18:30:45Z",
          "authors": [
            "Xiaoyu Ma",
            "Zhengqing Yuan",
            "Zheyuan Zhang",
            "Kaiwen Shi",
            "Lichao Sun",
            "Yanfang Ye"
          ],
          "arxiv_id": "2512.11769",
          "abstract": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型可实现令人印象深刻的零射击操作，但其推理堆栈对于响应式 Web 演示或商用 GPU 上的高频机器人控制来说通常太重。我们推出了 BLURR，这是一种轻量级推理包装器，可以插入现有的 VLA 控制器，而无需重新训练或更改模型检查点。BLURR 在 pi-0 VLA 控制器上实例化，保留了原始观察接口，并通过结合指令前缀键值缓存、混合精度执行和减少每步计算的单步推出计划来加速控制。在我们基于 SimplerEnv 的评估中，BLURR 保持了与原始控制器相当的任务成功率，同时显着降低了有效 FLOP 和挂钟延迟。我们还构建了一个交互式网络演示，允许用户在观看操作片段时实时切换控制器并切换推理选项。这凸显了 BLURR 作为在紧张的计算预算下部署现代 VLA 策略的实用方法。"
        },
        {
          "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning",
          "url": "http://arxiv.org/abs/2512.13636v2",
          "snippet": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. Using the lightweight Qwen-0.5B LLM, MindDrive achieves Driving Score (DS) of 78.04 and Success Rate (SR) of 55.09% on the challenging Bench2Drive benchmark. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-12-15T18:31:32Z",
          "authors": [
            "Haoyu Fu",
            "Diankun Zhang",
            "Zongchuang Zhao",
            "Jianfeng Cui",
            "Hongwei Xie",
            "Bing Wang",
            "Guang Chen",
            "Dingkang Liang",
            "Xiang Bai"
          ],
          "arxiv_id": "2512.13636",
          "abstract": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. Using the lightweight Qwen-0.5B LLM, MindDrive achieves Driving Score (DS) of 78.04 and Success Rate (SR) of 55.09% on the challenging Bench2Drive benchmark. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.",
          "abstract_zh": "当前自动驾驶中的视觉-语言-动作（VLA）范式主要依赖于模仿学习（IL），这引入了分布偏移和因果混乱等固有挑战。在线强化学习提供了一条通过试错学习解决这些问题的有前途的途径。然而，将在线强化学习应用于自动驾驶中的 VLA 模型却因连续动作空间中的低效探索而受到阻碍。为了克服这一限制，我们提出了 MindDrive，这是一个 VLA 框架，包含一个具有两组不同 LoRA 参数的大型语言模型 (LLM)。一名法学硕士充当场景推理和驱动决策的决策专家，而另一名法学硕士则充当行动专家，将语言决策动态映射到可行的轨迹。通过将轨迹级奖励反馈回推理空间，MindDrive 可以对一组有限的离散语言驾驶决策进行试错学习，而不是直接在连续的动作空间中操作。该方法有效地平衡了复杂场景下的最优决策、类人驾驶行为以及在线强化学习的高效探索。使用轻量级 Qwen-0.5B LLM，MindDrive 在具有挑战性的 Bench2Drive 基准测试中获得了 78.04 的驾驶分数 (DS) 和 55.09% 的成功率 (SR)。据我们所知，这是第一个展示自动驾驶中 VLA 模型在线强化学习有效性的工作。"
        },
        {
          "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning",
          "url": "http://arxiv.org/abs/2512.08580v2",
          "snippet": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-12-09T13:19:37Z",
          "authors": [
            "Peijun Tang",
            "Shangjin Xie",
            "Binyan Sun",
            "Baifu Huang",
            "Kuncheng Luo",
            "Haotian Yang",
            "Weiqi Jin",
            "Jianan Wang"
          ],
          "arxiv_id": "2512.08580",
          "abstract": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.",
          "abstract_zh": "人类根据情境和意图行事，推理起着核心作用。虽然互联网规模的数据使人工智能系统具有广泛的推理能力，但将这些能力扎根于实际行动仍然是一个重大挑战。我们介绍了 Lumo-1，这是一种通用视觉-语言-动作 (VLA) 模型，它将机器人推理（“思维”）与机器人动作（“手”）统一起来。我们的方法建立在预训练视觉语言模型（VLM）的通用多模态推理能力的基础上，逐步将其扩展到具体推理和动作预测，并最终实现结构化推理和推理-动作对齐。这导致了一个三阶段的预训练流程：（1）持续对精选视觉语言数据进行 VLM 预训练，以增强具体推理技能，例如规划、空间理解和轨迹预测；(2) 跨实体机器人数据与视觉语言数据的协同训练；（3）对 Astribot S1 上收集的轨迹进行推理过程的动作训练，Astribot S1 是一款具有类人灵巧性和敏捷性的双手移动机械臂。最后，我们整合强化学习以进一步完善推理-动作一致性并闭合语义推理和运动控制之间的循环。大量实验表明，Lumo-1 在具体视觉语言推理（通用机器人控制的关键组成部分）方面实现了显着的性能改进。现实世界的评估进一步表明，Lumo-1 在各种具有挑战性的机器人任务中都超越了强大的基线，对新颖的物体和环境具有很强的泛化能力，尤其在长视野任务和响应需要对策略、概念和空间进行推理的人类自然指令方面表现出色。"
        },
        {
          "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
          "url": "http://arxiv.org/abs/2512.11362v3",
          "snippet": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \\href{https://suyuz1.github.io/VLA-Survey-Anatomy/}{project page}.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-12-12T08:22:03Z",
          "authors": [
            "Chao Xu",
            "Suyu Zhang",
            "Yang Liu",
            "Baigui Sun",
            "Weihong Chen",
            "Bo Xu",
            "Qi Liu",
            "Juncheng Wang",
            "Shujun Wang",
            "Shan Luo",
            "Jan Peters",
            "Athanasios V. Vasilakos",
            "Stefanos Zafeiriou",
            "Jiankang Deng"
          ],
          "arxiv_id": "2512.11362",
          "abstract": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \\href{https://suyuz1.github.io/VLA-Survey-Anatomy/}{project page}.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型正在推动机器人技术的一场革命，使机器能够理解指令并与物理世界交互。这个领域正在爆炸性地出现新的模型和数据集，使得跟上步伐既令人兴奋又充满挑战。这项调查为 VLA 景观提供了清晰、结构化的指南。我们将其设计为遵循研究人员的自然学习路径：我们从任何 VLA 模型的基本模块开始，通过关键里程碑追溯历史，然后深入研究定义近期研究前沿的核心挑战。我们的主要贡献是对五个最大挑战的详细分析：(1) 表示、(2) 执行、(3) 泛化、(4) 安全性和 (5) 数据集和评估。这种结构反映了多面手代理的发展路线图：建立基本的感知-行动循环，跨不同实施例和环境扩展能力，并最终确保值得信赖的部署——所有这些都由基本数据基础设施支持。对于每一个，我们都会回顾现有的方法并强调未来的机会。我们将本文定位为新手的基础指南和经验丰富的研究人员的战略路线图，其双重目标是加速学习和激发具身智能的新想法。我们的\\href{https://suyuz1.github.io/VLA-Survey-Anatomy/}{项目页面}上维护着该调查的实时版本，并不断更新。"
        },
        {
          "title": "RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI",
          "url": "http://arxiv.org/abs/2512.10394v1",
          "snippet": "Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-12-11T07:58:19Z",
          "authors": [
            "Weifan Guan",
            "Huasen Xi",
            "Chenxiao Zhang",
            "Aosheng Li",
            "Qinghao Hu",
            "Jian Cheng"
          ],
          "arxiv_id": "2512.10394",
          "abstract": "Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.",
          "abstract_zh": "目前的实体人工智能系统面临着严重的工程障碍，主要特点是跨场景适应性差、模块间耦合僵化、推理加速碎片化。为了克服这些限制，我们提出了 RoboNeuron，一种用于体现智能的通用部署框架。RoboNeuron 是第一个将大型语言模型 (LLM) 和视觉语言动作 (VLA) 模型的认知能力与机器人操作系统 (ROS) 的实时执行主干深度集成的框架。我们利用模型上下文协议（MCP）作为语义桥梁，使法学硕士能够动态编排底层机器人工具。该框架建立了高度模块化的架构，利用ROS的统一通信接口，严格解耦感知、推理和控制。至关重要的是，我们引入了一个自动化工具，将 ROS 消息转换为可调用的 MCP 函数，从而显着简化了开发。RoboNeuron显着增强了跨场景适应性和组件灵活性，同时建立了横向性能基准测试的系统平台，为可扩展的现实世界应用奠定了坚实的基础。"
        },
        {
          "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations",
          "url": "http://arxiv.org/abs/2512.07582v1",
          "snippet": "Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-12-08T14:25:30Z",
          "authors": [
            "Guangyan Chen",
            "Meiling Wang",
            "Qi Shao",
            "Zichen Zhou",
            "Weixin Mao",
            "Te Cui",
            "Minzhao Zhu",
            "Yinan Deng",
            "Luojie Yang",
            "Zhanqi Zhang",
            "Yi Yang",
            "Hua Chen",
            "Yufeng Yue"
          ],
          "arxiv_id": "2512.07582",
          "abstract": "Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.",
          "abstract_zh": "开发强大且通用的操纵策略是机器人研究的一个基本目标。虽然视觉-语言-动作（VLA）模型已经展示了端到端机器人控制的有前景的能力，但现有方法对于超出其训练分布的任务的泛化能力仍然有限。相比之下，人类通过简单地观察别人执行一次新技能就拥有惊人的熟练程度。受此功能的启发，我们提出了 ViVLA，这是一种通用机器人操作策略，可在测试时从单个专家演示视频中实现高效的任务学习。我们的方法联合处理专家演示视频和机器人的视觉观察，以预测演示的动作序列和后续的机器人动作，有效地从专家行为中提取细粒度的操作知识并将其无缝传输给代理。为了提高 ViVLA 的性能，我们开发了一个可扩展的专家代理对数据生成管道，能够从易于访问的人类视频中合成配对轨迹，并通过来自公开数据集的精选对进一步增强。该管道总共生成 892,911 个专家代理样本用于训练 ViVLA。实验结果表明，我们的 ViVLA 在测试时仅通过单个专家演示视频即可获得新颖的操作技能。我们的方法在未见过的 LIBERO 任务上实现了超过 30% 的改进，并在跨实体视频上保持了 35% 以上的增益。现实世界的实验证明了从人类视频中进行的有效学习，在未见过的任务上取得了超过 38% 的改进。"
        },
        {
          "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer",
          "url": "http://arxiv.org/abs/2512.11891v1",
          "snippet": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-12-09T16:53:44Z",
          "authors": [
            "Songqiao Hu",
            "Zeyi Liu",
            "Shuang Liu",
            "Jun Cen",
            "Zihan Meng",
            "Xiao He"
          ],
          "arxiv_id": "2512.11891",
          "abstract": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在泛化各种机器人操作任务方面表现出了卓越的能力。然而，由于对同时任务合规性和安全保证的迫切需求，特别是在防止物理交互期间潜在的碰撞方面，在非结构化环境中部署这些模型仍然具有挑战性。在这项工作中，我们引入了一种名为 AEGIS 的视觉语言安全操作（VLSA）架构，其中包含通过控制屏障函数制定的即插即用安全约束（SC）层。AEGIS直接与现有的VLA模型集成，在理论上保证提高安全性，同时保持其原有的指令跟踪性能。为了评估我们架构的有效性，我们构建了一个全面的安全关键基准 SafeLIBERO，涵盖以不同程度的空间复杂性和障碍物干预为特征的不同操作场景。大量的实验证明了我们的方法相对于最先进的基线的优越性。值得注意的是，AEGIS在避障率方面实现了59.16%的提升，同时任务执行成功率大幅提升了17.25%。为了促进可重复性和未来的研究，我们在 https://vlsa-aegis.github.io/ 上公开提供我们的代码、模型和基准数据集。"
        },
        {
          "title": "Benchmarking the Generality of Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2512.11315v1",
          "snippet": "Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-12-12T06:31:52Z",
          "authors": [
            "Pranav Guruprasad",
            "Sudipta Chowdhury",
            "Harsh Sikka",
            "Mridul Sharma",
            "Helen Lu",
            "Sean Rivera",
            "Aryan Khurana",
            "Hangliang Ren",
            "Yangyue Wang"
          ],
          "arxiv_id": "2512.11315",
          "abstract": "Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.",
          "abstract_zh": "多才多艺的多模态智能体有望统一感知、语言和控制——在不同的现实世界领域中稳健运行。然而，当前的评估实践仍然分散在孤立的基准中，因此很难评估当今的基础模型是否真正能够超越其训练分布。我们推出了 MultiNet v1.0，这是一个统一的基准，用于测量跨六个基础能力体系的视觉语言模型 (VLM) 和视觉语言动作模型 (VLA) 的跨域通用性。视觉基础、空间推理、工具使用、物理常识、多智能体协调和连续机器人控制。评估 GPT 5、Pi0 和 Magma，我们发现没有模型表现出一致的通用性。尽管在训练分布中表现强劲，但所有这些都在未见过的领域、不熟悉的模态或跨域任务转移上表现出严重的退化。这些失败表现为模态错位、输出格式不稳定和领域转移下灾难性的知识退化。我们的研究结果揭示了通才智能的愿望与当前基础模型的实际能力之间存在持续的差距。MultiNet v1.0 提供了一个标准化的评估基础，用于诊断这些差距并指导未来通才代理的开发。代码、数据和排行榜是公开的。"
        },
        {
          "title": "Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models",
          "url": "http://arxiv.org/abs/2512.11908v1",
          "snippet": "Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \\href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-12-10T21:01:02Z",
          "authors": [
            "Heng Zhang",
            "Rui Dai",
            "Gokhan Solak",
            "Pokuang Zhou",
            "Yu She",
            "Arash Ajoudani"
          ],
          "arxiv_id": "2512.11908",
          "abstract": "Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \\href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.",
          "abstract_zh": "由于固有的不确定性、复杂的动力学以及交互过程中损坏的高风险，接触丰富的任务给机器人系统带来了重大挑战。基于学习的控制的最新进展显示出机器人在此类环境中获得和概括复杂操作技能的巨大潜力，但确保探索和执行过程中的安全仍然是可靠的现实部署的关键瓶颈。这项调查全面概述了用于机器人接触丰富的任务的基于安全学习的方法。我们将现有方法分为两个主要领域：安全探索和安全执行。我们回顾了关键技术，包括约束强化学习、风险敏感优化、不确定性感知建模、控制屏障函数和模型预测安全防护罩，并强调这些方法如何结合先验知识、任务结构和在线适应来平衡安全性和效率。这项调查的特别重点是这些安全学习原则如何扩展到新兴的机器人基础模型并与之交互，特别是视觉语言模型（VLM）和视觉语言动作模型（VLA），它们统一了感知、语言和控制以实现丰富的接触操作。我们讨论了基于 VLM/VLA 的方法带来的新的安全机会，例如约束的语言级规范和安全信号的多模态接地，以及它们带来的放大的风险和评估挑战。最后，我们概述了在复杂的接触丰富的环境中部署可靠、安全且支持基础模型的机器人的当前局限性和有前途的未来方向。更多详细信息和材料请访问我们的 \\href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}。"
        },
        {
          "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
          "url": "http://arxiv.org/abs/2512.13080v1",
          "snippet": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-12-15T08:31:47Z",
          "authors": [
            "Yicheng Feng",
            "Wanpeng Zhang",
            "Ye Wang",
            "Hao Luo",
            "Haoqi Yuan",
            "Sipeng Zheng",
            "Zongqing Lu"
          ],
          "arxiv_id": "2512.13080",
          "abstract": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
          "abstract_zh": "视觉-语言-动作（VLA）模型通过将视觉感知与语言引导的策略学习相结合，为机器人学习提供了一种有前途的范例。然而，大多数现有方法依赖 2D 视觉输入在 3D 物理环境中执行动作，从而在感知和动作基础之间造成了巨大差距。为了弥补这一差距，我们提出了一种空间感知 VLA 预训练范例，该范例在预训练期间执行视觉空间和物理空间之间的显式对齐，使模型能够在机器人策略学习之前获得 3D 空间理解。从预训练的视觉语言模型开始，我们利用大规模人类演示视频来提取 3D 视觉和 3D 动作注释，形成将 2D 视觉观察与 3D 空间推理结合起来的新监督来源。我们使用 VIPA-VLA 实例化了这一范例，VIPA-VLA 是一种双编码器架构，它结合了 3D 视觉编码器，通过 3D 感知功能增强语义视觉表示。当适应下游机器人任务时，VIPA-VLA 显着改善了 2D 视觉和 3D 动作之间的基础，从而产生更稳健和通用的机器人策略。"
        },
        {
          "title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2512.09927v1",
          "snippet": "Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-12-10T18:59:24Z",
          "authors": [
            "Yifan Ye",
            "Jiaqi Ma",
            "Jun Cen",
            "Zhihe Lu"
          ],
          "arxiv_id": "2512.09927",
          "abstract": "Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}",
          "abstract_zh": "在大规模多模态数据集上预训练的视觉-语言-动作（VLA）模型已成为机器人感知和控制的强大基础。然而，它们的大规模（通常有数十亿个参数）给实时部署带来了重大挑战，因为在动态环境中推理变得计算成本高昂且对延迟敏感。为了解决这个问题，我们提出了 Token Expand-and-Merge-VLA (TEAM-VLA)，这是一种免训练的令牌压缩框架，可以加速 VLA 推理，同时保持任务性能。TEAM-VLA 引入了一种动态令牌扩展机制，该机制可以识别和采样关注突出显示区域的空间附近的附加信息令牌，从而增强上下文完整性。然后，这些扩展的标记在动作感知的指导下有选择地合并到更深的层中，有效减少冗余，同时保持语义一致性。通过在单个前馈通道中耦合扩展和合并，TEAM-VLA 实现了效率和有效性之间的平衡权衡，无需任何重新训练或参数更新。LIBERO 基准上的大量实验表明，TEAM-VLA 持续提高推理速度，同时保持甚至超越完整 VLA 模型的任务成功率。该代码可在 \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA} 上公开获取"
        },
        {
          "title": "Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy",
          "url": "http://arxiv.org/abs/2512.11218v1",
          "snippet": "The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA.",
          "site": "arxiv.org",
          "rank": 15,
          "published": "2025-12-12T01:59:23Z",
          "authors": [
            "Kechun Xu",
            "Zhenjie Zhu",
            "Anzhe Chen",
            "Shuqi Zhao",
            "Qing Huang",
            "Yifei Yang",
            "Haojian Lu",
            "Rong Xiong",
            "Masayoshi Tomizuka",
            "Yue Wang"
          ],
          "arxiv_id": "2512.11218",
          "abstract": "The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA.",
          "abstract_zh": "在视觉-语言-动作（VLA）模型中追求分布外泛化常常受到微调过程中视觉-语言模型（VLM）主干的灾难性遗忘的阻碍。虽然与外部推理数据的协同训练有所帮助，但它需要经验丰富的调优和数据相关的开销。除了这种外部依赖性之外，我们还确定了 VLA 数据集中的一个内在原因：模态不平衡，其中语言多样性远低于视觉和动作多样性。这种不平衡使模型偏向于视觉捷径和语言遗忘。为了解决这个问题，我们引入了 BayesVLA，这是一种贝叶斯分解，它将策略分解为视觉动作先验（支持“看到行动”）和语言条件可能性（支持提示指定）。这本质上保留了概括性并促进了指令的遵循。我们进一步合并接触前和接触后阶段，以更好地利用预先训练的基础模型。信息论分析正式验证了我们在减少捷径学习方面的有效性。大量实验表明，与现有方法相比，对未见过的指令、对象和环境具有更好的泛化能力。项目页面位于：https://xukechun.github.io/papers/BayesVLA。"
        },
        {
          "title": "Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging",
          "url": "http://arxiv.org/abs/2512.08333v2",
          "snippet": "Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging performance scales with the amount of pretraining data, and enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.",
          "site": "arxiv.org",
          "rank": 16,
          "published": "2025-12-09T08:02:11Z",
          "authors": [
            "Yajat Yadav",
            "Zhiyuan Zhou",
            "Andrew Wagenmaker",
            "Karl Pertsch",
            "Sergey Levine"
          ],
          "arxiv_id": "2512.08333",
          "abstract": "Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging performance scales with the amount of pretraining data, and enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.",
          "abstract_zh": "在大型且多样化的数据集上进行训练的通才机器人策略已经证明了泛化广泛行为的能力，使单个策略能够在不同的现实世界环境中发挥作用。然而，它们仍然无法完成训练数据中未涵盖的新任务。当对新任务的有限演示进行微调时，这些策略通常会过度适应特定的演示，不仅失去了解决各种通用任务的先前能力，而且也无法在新任务本身中进行概括。在这项工作中，我们的目标是开发一种方法，在微调过程中保留通才策略的泛化能力，从而允许单个策略将新技能强有力地纳入其库中。我们的目标是制定一个单一的策略，既能学习泛化到新任务的变化，又能保留从预训练中获得的广泛能力。我们证明，这可以通过一种简单而有效的策略来实现：将微调模型的权重与预训练模型的权重进行插值。我们通过广泛的模拟和现实实验证明，这种模型合并产生了一个单一模型，该模型继承了基础模型的通用能力，并学习稳健地解决新任务，在新任务的分布外变化方面优于预训练和微调模型。此外，我们还表明，模型合并性能随着预训练数据量的增加而变化，并且能够在终身学习环境中持续获取新技能，而不会牺牲以前学习的通才能力。"
        },
        {
          "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
          "url": "http://arxiv.org/abs/2512.12799v1",
          "snippet": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI",
          "site": "arxiv.org",
          "rank": 17,
          "published": "2025-12-14T18:45:54Z",
          "authors": [
            "Zhe Liu",
            "Runhui Huang",
            "Rui Yang",
            "Siming Yan",
            "Zining Wang",
            "Lu Hou",
            "Di Lin",
            "Xiang Bai",
            "Hengshuang Zhao"
          ],
          "arxiv_id": "2512.12799",
          "abstract": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI",
          "abstract_zh": "尽管多模态大语言模型 (MLLM) 在不同领域表现出了强大的能力，但它们在自动驾驶中生成细粒度 3D 感知和预测输出的应用仍未得到充分探索。在本文中，我们提出了 DrivePI，一种新颖的空间感知 4D MLLM，它作为统一的视觉-语言-动作 (VLA) 框架，也与视觉-动作 (VA) 模型兼容。我们的方法通过端到端优化并行地联合执行空间理解、3D 感知（即 3D 占用）、预测（即占用流）和规划（即动作输出）。为了获得精确的几何信息和丰富的视觉外观，我们的方法将点云、多视图图像和语言指令集成在统一的 MLLM 架构中。我们进一步开发了一个数据引擎来生成文本占用和文本流 QA 对，以实现 4D 空间理解。值得注意的是，仅使用 0.5B Qwen2.5 模型作为 MLLM 主干，DrivePI 作为单个统一模型即可匹配或超过现有的 VLA 模型和专用 VA 模型。具体来说，与 VLA 模型相比，DrivePI 在 nuScenes-QA 上的平均准确度比 OpenDriveVLA-7B 高出 2.5%，并且在 nuScenes 上比 ORION 的碰撞率降低了 70%（从 0.37% 到 0.11%）。相对于专门的 VA 模型，DrivePI 在 OpenOcc 上的 3D 占用率超过 FB-OCC 10.3 RayIoU，将 OpenOcc 上的占用流的 mAVE 从 0.591 降低到 0.509，并且在 nuScenes 规划中实现比 VAD（从 0.72m 到 0.49m）低 32% 的 L2 误差。代码将在 https://github.com/happinesslz/DrivePI 提供"
        },
        {
          "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
          "url": "http://arxiv.org/abs/2512.09864v1",
          "snippet": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.",
          "site": "arxiv.org",
          "rank": 18,
          "published": "2025-12-10T17:50:29Z",
          "authors": [
            "Hao Lu",
            "Ziyang Liu",
            "Guangfeng Jiang",
            "Yuanfei Luo",
            "Sheng Chen",
            "Yangang Zhang",
            "Ying-Cong Chen"
          ],
          "arxiv_id": "2512.09864",
          "abstract": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.",
          "abstract_zh": "由于有限的世界知识和薄弱的视觉动态建模，自动驾驶（AD）系统在长尾场景中举步维艰。现有的基于视觉-语言-动作（VLA）的方法无法利用未标记的视频进行视觉因果学习，而基于世界模型的方法缺乏大型语言模型的推理能力。在本文中，我们构建了多个专门的数据集，为复杂场景提供推理和规划注释。然后，提出了一个名为 UniUGP 的统一理解-生成-规划框架，通过混合专家架构来协同场景推理、未来视频生成和轨迹规划。通过集成预先训练的 VLM 和视频生成模型，UniUGP 利用视觉动态和语义推理来提高规划性能。以多帧观察和语言指令作为输入，它产生可解释的思维链推理、物理上一致的轨迹和连贯的未来视频。我们引入了一个四阶段训练策略，可以在多个现有 AD 数据集以及建议的专用数据集上逐步构建这些功能。实验证明了其在感知、推理和决策方面的最先进的性能，并对具有挑战性的长尾情况具有卓越的泛化能力。"
        },
        {
          "title": "GLaD: Geometric Latent Distillation for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2512.09619v1",
          "snippet": "Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.",
          "site": "arxiv.org",
          "rank": 19,
          "published": "2025-12-10T13:07:27Z",
          "authors": [
            "Minghao Guo",
            "Meng Cao",
            "Jiachen Tao",
            "Rongtao Xu",
            "Yan Yan",
            "Xiaodan Liang",
            "Ivan Laptev",
            "Xiaojun Chang"
          ],
          "arxiv_id": "2512.09619",
          "abstract": "Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.",
          "abstract_zh": "大多数现有的视觉-语言-动作 (VLA) 模型主要依赖于 RGB 信息，而忽略了对于空间推理和操作至关重要的几何线索。在这项工作中，我们介绍了 GLaD，一种几何感知的 VLA 框架，它在预训练过程中通过知识蒸馏结合了 3D 几何先验。我们不是将几何特征仅仅提取到视觉编码器中，而是将与视觉标记相对应的 LLM 隐藏状态与来自冻结几何感知视觉变换器 (VGGT) 的特征进行对齐，确保几何理解深度集成到驱动动作预测的多模态表示中。使用这种几何蒸馏机制在 Bridge 数据集上进行预训练，GLaD 在四个 LIBERO 任务套件中实现了 94.1% 的平均成功率，优于使用相同预训练数据的 UniVLA (92.5%)。这些结果验证了几何感知预训练可以增强空间推理和策略泛化，而无需显式深度传感器或 3D 注释。"
        },
        {
          "title": "WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control",
          "url": "http://arxiv.org/abs/2512.11047v2",
          "snippet": "Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To execute the desired locomotion commands more precisely, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.",
          "site": "arxiv.org",
          "rank": 20,
          "published": "2025-12-11T19:07:31Z",
          "authors": [
            "Haoran Jiang",
            "Jin Chen",
            "Qingwen Bu",
            "Li Chen",
            "Modi Shi",
            "Yanjie Zhang",
            "Delong Li",
            "Chuanzhe Suo",
            "Chuang Wang",
            "Zhihui Peng",
            "Hongyang Li"
          ],
          "arxiv_id": "2512.11047",
          "abstract": "Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To execute the desired locomotion commands more precisely, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.",
          "abstract_zh": "人形机器人需要精确的运动和灵巧的操纵来执行具有挑战性的运动操纵任务。然而，现有的模块化或端到端的方法在操纵感知运动方面存在缺陷。这将机器人限制在有限的工作空间内，使其无法执行大空间的局部操纵。我们将其归因于：（1）由于人形遥操作数据的稀缺，获取运动操纵知识的挑战；（2）由于现有强化学习控制器的精度和稳定性有限，忠实可靠地执行运动命令的困难。为了获得更丰富的局部操作知识，我们提出了一个统一的潜在学习框架，使视觉-语言-动作（VLA）系统能够从低成本的无动作的自我中心视频中学习。此外，还设计了高效的人类数据收集管道来扩充数据集并扩大效益。为了更精确地执行所需的运动命令，我们提出了一种面向运动操纵（LMO）的强化学习策略，专门针对准确且稳定的核心运动操纵运动（例如前进、转身和蹲下）而定制。在这些组件的基础上，我们引入了 WholeBodyVLA，这是一个用于人形机器人操作的统一框架。据我们所知，WholeBodyVLA 是同类产品之一，能够实现大空间人形机器人控制。它通过 AgiBot X2 人形机器人的综合实验得到验证，比之前的基线提高了 21.3%。它还在广泛的任务中表现出强大的通用性和高可扩展性。"
        },
        {
          "title": "Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents",
          "url": "http://arxiv.org/abs/2512.11584v1",
          "snippet": "Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)",
          "site": "arxiv.org",
          "rank": 21,
          "published": "2025-12-12T14:14:27Z",
          "authors": [
            "Stefan Tabakov",
            "Asen Popov",
            "Dimitar Dimitrov",
            "S. Ensiye Kiyamousavi",
            "Vladimir Hristov",
            "Boris Kraychev"
          ],
          "arxiv_id": "2512.11584",
          "abstract": "Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)",
          "abstract_zh": "当前的视觉-语言-动作（VLA）模型泛化能力较差，特别是当任务需要新的技能或物体组合时。我们引入了原子操作切片（AAS），这是一种与规划者保持一致的方法，它将长期演示分解为简短的、类型化的原子操作，这些操作更易于规划者使用和策略学习。使用 LIBERO 演示，AAS 生成了包含 2,124 个原子片段的经过验证的数据集，并标有动作类型、时间跨度和置信度。更强大的分段器（Gemini 2.5 Pro）与规划器定义的计划紧密匹配，并且在关键帧抖动下保持稳健，而较小的模型在多对象任务上表现较差。在我们的原子数据集上微调 CLIP-RT+ 将 LIBERO-Goal 上的任务成功率从 94.2% 提高到 95.3%，将 LIBERO-Long 上的任务成功率从 83.8% 提高到 88.8%。我们在HuggingFace上公开发布GATE-VLAP数据集（https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets）"
        },
        {
          "title": "Embodied Image Compression",
          "url": "http://arxiv.org/abs/2512.11612v1",
          "snippet": "Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.",
          "site": "arxiv.org",
          "rank": 22,
          "published": "2025-12-12T14:49:34Z",
          "authors": [
            "Chunyi Li",
            "Rui Qing",
            "Jianbo Zhang",
            "Yuan Tian",
            "Xiangyang Zhu",
            "Zicheng Zhang",
            "Xiaohong Liu",
            "Weisi Lin",
            "Guangtao Zhai"
          ],
          "arxiv_id": "2512.11612",
          "abstract": "Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.",
          "abstract_zh": "机器图像压缩（ICM）已成为视觉数据压缩领域的一个关键研究方向。然而，随着机器智能的快速发展，压缩的目标已经从特定于任务的虚拟模型转移到在现实环境中运行的实体代理。为了解决多智能体系统中Embodied AI的通信限制并保证任务的实时执行，本文首次引入了Embodied图像压缩的科学问题。我们建立了标准化基准 EmbodiedComp，以促进闭环设置中超低比特率条件下的系统评估。通过在模拟和现实环境中进行广泛的实证研究，我们证明现有的视觉-语言-动作模型（VLA）在压缩到低于嵌入比特率阈值时甚至无法可靠地执行简单的操作任务。我们预计 EmbodiedComp 将促进为 Embodied 代理量身定制的特定领域压缩的开发，从而加速 Embodied AI 在现实世界中的部署。"
        },
        {
          "title": "Motus: A Unified Latent Action World Model",
          "url": "http://arxiv.org/abs/2512.13030v2",
          "snippet": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
          "site": "arxiv.org",
          "rank": 23,
          "published": "2025-12-15T06:58:40Z",
          "authors": [
            "Hongzhe Bi",
            "Hengkai Tan",
            "Shenghao Xie",
            "Zeyuan Wang",
            "Shuhe Huang",
            "Haitian Liu",
            "Ruowen Zhao",
            "Yao Feng",
            "Chendong Xiang",
            "Yinze Rong",
            "Hongyan Zhao",
            "Hanyu Liu",
            "Zhizhong Su",
            "Lei Ma",
            "Hang Su",
            "Jun Zhu"
          ],
          "arxiv_id": "2512.13030",
          "abstract": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
          "abstract_zh": "虽然通用的实体代理必须作为一个统一的系统发挥作用，但当前的方法是建立在用于理解、世界建模和控制的孤立模型之上的。这种碎片化阻碍了多模式生成能力的统一，并阻碍了从大规模异构数据中进行学习。在本文中，我们提出了 Motus，这是一种统一的潜在动作世界模型，它利用现有的通用预训练模型和丰富的、可共享的运动信息。Motus引入了Mixture-of-Transformer（MoT）架构来集成三个专家（即理解、视频生成和动作），并采用UniDiffuser式调度器来实现不同建模模式（即世界模型、视觉-语言-动作模型、逆动力学模型、视频生成模型和视频-动作联合预测模型）之间的灵活切换。Motus进一步利用光流来学习潜在动作，并采用三相训练管道和六层数据金字塔的配方，从而提取像素级的“增量动作”并实现大规模动作预训练。实验表明，Motus 在模拟（比 X-VLA 提高了 +15%，比 Pi0.5 提高了 +45%）和现实场景（提高了 +11~48%）方面均实现了优于最先进方法的性能，证明所有功能和先验的统一建模显着有利于下游机器人任务。"
        },
        {
          "title": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
          "url": "http://arxiv.org/abs/2512.10226v1",
          "snippet": "Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.",
          "site": "arxiv.org",
          "rank": 24,
          "published": "2025-12-11T02:22:07Z",
          "authors": [
            "Shuhan Tan",
            "Kashyap Chitta",
            "Yuxiao Chen",
            "Ran Tian",
            "Yurong You",
            "Yan Wang",
            "Wenjie Luo",
            "Yulong Cao",
            "Philipp Krahenbuhl",
            "Marco Pavone",
            "Boris Ivanovic"
          ],
          "arxiv_id": "2512.10226",
          "abstract": "Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.",
          "abstract_zh": "最近的自动驾驶视觉-语言-动作（VLA）模型探索推理时间推理，作为在具有挑战性的场景中提高驾驶性能和安全性的一种方法。大多数先前的工作在产生驾驶动作之前使用自然语言来表达思想链（CoT）推理。然而，文本可能不是最有效的推理表示。在这项工作中，我们提出了 Latent-CoT-Drive (LCDrive)：一种用潜在语言表达 CoT 的模型，该语言捕获正在考虑的驾驶行为的可能结果。我们的方法通过在与动作一致的潜在空间中表示来统一 CoT 推理和决策。该模型不是通过自然语言进行推理，而是通过交错 (1) 动作建议标记，这些标记使用与模型的输出动作相同的词汇；(2) 世界模型代币，它基于学习的潜在世界模型并表达这些行为的未来结果。我们根据未来场景的真实情况，通过监督模型的行动建议和世界模型令牌来冷启动潜在的 CoT。然后，我们通过闭环强化学习进行后期训练，以增强推理能力。在大规模端到端驾驶基准上，与非推理和文本推理基准相比，LCDrive 实现了更快的推理、更好的轨迹质量以及交互式强化学习的更大改进。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 10 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 10）。",
      "items": [
        {
          "title": "patrick-llgc/Learning-Deep-Learning",
          "url": "https://github.com/patrick-llgc/Learning-Deep-Learning",
          "snippet": "Paper reading notes on Deep Learning and Machine Learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "Vector-Wangel/XLeRobot",
          "url": "https://github.com/Vector-Wangel/XLeRobot",
          "snippet": "XLeRobot: Practical Dual-Arm Mobile Home Robot for $660",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "HCPLab-SYSU/Embodied_AI_Paper_List",
          "url": "https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List",
          "snippet": "[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 9
        },
        {
          "title": "WayneMao/RoboMatrix",
          "url": "https://github.com/WayneMao/RoboMatrix",
          "snippet": "The Official Implementation of RoboMatrix",
          "site": "github.com",
          "rank": 10
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 上共发现 1 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 1）。",
      "items": [
        {
          "title": "Robot-Learning-Collective/VLA-0-Smol",
          "url": "https://huggingface.co/Robot-Learning-Collective/VLA-0-Smol",
          "snippet": "Robot-Learning-Collective/VLA-0-Smol",
          "site": "huggingface.co",
          "rank": 4,
          "published": "2025-12-10T12:23:02.000Z"
        }
      ]
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-12-08",
  "week_end": "2025-12-14",
  "last_updated": "2026-01-07"
}