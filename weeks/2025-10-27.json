{
  "generated_at": "2026-01-07T13:42:09.590331",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 28 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 28）。",
      "items": [
        {
          "title": "OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2511.01210v2",
          "snippet": "Vision-language-action (VLA) models have shown strong generalization for robotic action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception guides the robotic manipulation. OmniVLA achieves an average task success rate of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59% and 28% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-11-03T04:10:44Z",
          "authors": [
            "Heyu Guo",
            "Shanmu Wang",
            "Ruichun Ma",
            "Shiqi Jiang",
            "Yasaman Ghasempour",
            "Omid Abari",
            "Baining Guo",
            "Lili Qiu"
          ],
          "arxiv_id": "2511.01210",
          "abstract": "Vision-language-action (VLA) models have shown strong generalization for robotic action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception guides the robotic manipulation. OmniVLA achieves an average task success rate of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59% and 28% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.",
          "abstract_zh": "视觉语言动作（VLA）模型通过大规模视觉语言预训练显示出对机器人动作预测的强大泛化性。然而，大多数现有模型仅依赖 RGB 摄像头，限制了它们的感知能力，从而限制了操纵能力。我们推出了 OmniVLA，这是一种全模态 VLA 模型，它集成了新颖的传感模态，可实现超越 RGB 感知的基于物理的空间智能。我们方法的核心是传感器遮罩图像，这是一种统一的表示形式，将空间接地和物理上有意义的遮罩叠加到 RGB 图像上，这些图像源自包括红外摄像机、毫米波雷达和麦克风阵列在内的传感器。这种图像原生统一使传感器输入接近 RGB 统计数据以促进训练，提供跨传感器硬件的统一接口，并通过轻量级每个传感器投影仪实现数据高效学习。在此基础上，我们提出了一种多感官视觉-语言-动作模型架构，并基于 RGB 预训练的 VLA 主干来训练该模型。我们在具有挑战性的现实世界任务中评估 OmniVLA，其中传感器模态感知指导机器人操作。OmniVLA 的平均任务成功率为 84%，显着优于仅 RGB 和原始传感器输入基线模型，分别提高了 59% 和 28%，同时表现出更高的学习效率和更强的泛化能力。"
        },
        {
          "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
          "url": "http://arxiv.org/abs/2510.23763v3",
          "snippet": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-10-27T18:49:03Z",
          "authors": [
            "Siyin Wang",
            "Jinlan Fu",
            "Feihong Liu",
            "Xinzhe He",
            "Huangxuan Wu",
            "Junhao Shi",
            "Kexin Huang",
            "Zhaoye Fei",
            "Jingjing Gong",
            "Zuxuan Wu",
            "Yu-Gang Jiang",
            "See-Kiong Ng",
            "Tat-Seng Chua",
            "Xipeng Qiu"
          ],
          "arxiv_id": "2510.23763",
          "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.",
          "abstract_zh": "多模态大语言模型 (MLLM) 的最新进展推动了机器人操作的视觉-语言-动作 (VLA) 模型的快速进步。尽管在许多场景中有效，但当前的方法很大程度上依赖于显式指令，而在现实世界的交互中，人类很少直接发出指令。有效的协作需要机器人主动推断用户意图。在这项工作中，我们引入了跨模式上下文指令，这是一种新的设置，其中意图源自口头对话、环境声音和视觉提示，而不是明确的命令。为了应对这一新环境，我们推出了 RoboOmni，这是一个基于端到端全模态法学硕士的感知器-思考者-说话者-执行器框架，它统一了意图识别、交互确认和动作执行。RoboOmni 融合听觉和视觉信号，实现强大的意图识别，同时支持直接语音交互。为了解决机器人操作中主动意图识别训练数据的缺乏问题，我们构建了 OmniAction，其中包括 140k 个片段、5k+ 个扬声器、2.4k 个事件声音、640 个背景和六种上下文指令类型。模拟和现实环境中的实验表明，RoboOmni 在成功率、推理速度、意图识别和主动协助方面超越了基于文本和 ASR 的基线。"
        },
        {
          "title": "Running VLAs at Real-time Speed",
          "url": "http://arxiv.org/abs/2510.26742v1",
          "snippet": "In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate and at most 480Hz trajectory frequency using a single consumer GPU. This enables dynamic and real-time tasks that were previously believed to be unattainable by large VLA models. To achieve it, we introduce a bag of strategies to eliminate the overheads in model inference. The real-world experiment shows that the pi0 policy with our strategy achieves a 100% success rate in grasping a falling pen task. Based on the results, we further propose a full streaming inference framework for real-time robot control of VLA. Code is available at https://github.com/Dexmal/realtime-vla.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-10-30T17:38:14Z",
          "authors": [
            "Yunchao Ma",
            "Yizhuang Zhou",
            "Yunhuan Yang",
            "Tiancai Wang",
            "Haoqiang Fan"
          ],
          "arxiv_id": "2510.26742",
          "abstract": "In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate and at most 480Hz trajectory frequency using a single consumer GPU. This enables dynamic and real-time tasks that were previously believed to be unattainable by large VLA models. To achieve it, we introduce a bag of strategies to eliminate the overheads in model inference. The real-world experiment shows that the pi0 policy with our strategy achieves a 100% success rate in grasping a falling pen task. Based on the results, we further propose a full streaming inference framework for real-time robot control of VLA. Code is available at https://github.com/Dexmal/realtime-vla.",
          "abstract_zh": "在本文中，我们展示了如何使用单个消费级 GPU 以 30Hz 帧速率和最多 480Hz 轨迹频率运行 pi0 级多视图 VLA。这使得以前认为大型 VLA 模型无法完成的动态和实时任务成为可能。为了实现这一目标，我们引入了一系列策略来消除模型推理中的开销。现实世界的实验表明，pi0 策略与我们的策略在抓取落笔任务方面实现了 100% 的成功率。基于结果，我们进一步提出了用于 VLA 实时机器人控制的全流式推理框架。代码可在 https://github.com/Dexmal/realtime-vla 获取。"
        },
        {
          "title": "Human-in-the-loop Online Rejection Sampling for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2510.26406v1",
          "snippet": "Reinforcement learning (RL) is widely used to produce robust robotic manipulation policies, but fine-tuning vision-language-action (VLA) models with RL can be unstable due to inaccurate value estimates and sparse supervision at intermediate steps. In contrast, imitation learning (IL) is easy to train but often underperforms due to its offline nature. In this paper, we propose Hi-ORS, a simple yet effective post-training method that utilizes rejection sampling to achieve both training stability and high robustness. Hi-ORS stabilizes value estimation by filtering out negatively rewarded samples during online fine-tuning, and adopts a reward-weighted supervised training objective to provide dense intermediate-step supervision. For systematic study, we develop an asynchronous inference-training framework that supports flexible online human-in-the-loop corrections, which serve as explicit guidance for learning error-recovery behaviors. Across three real-world tasks and two embodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich manipulation in just 1.5 hours of real-world training, outperforming RL and IL baselines by a substantial margin in both effectiveness and efficiency. Notably, the fine-tuned policy exhibits strong test-time scalability by reliably executing complex error-recovery behaviors to achieve better performance.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-10-30T11:53:08Z",
          "authors": [
            "Guanxing Lu",
            "Rui Zhao",
            "Haitao Lin",
            "He Zhang",
            "Yansong Tang"
          ],
          "arxiv_id": "2510.26406",
          "abstract": "Reinforcement learning (RL) is widely used to produce robust robotic manipulation policies, but fine-tuning vision-language-action (VLA) models with RL can be unstable due to inaccurate value estimates and sparse supervision at intermediate steps. In contrast, imitation learning (IL) is easy to train but often underperforms due to its offline nature. In this paper, we propose Hi-ORS, a simple yet effective post-training method that utilizes rejection sampling to achieve both training stability and high robustness. Hi-ORS stabilizes value estimation by filtering out negatively rewarded samples during online fine-tuning, and adopts a reward-weighted supervised training objective to provide dense intermediate-step supervision. For systematic study, we develop an asynchronous inference-training framework that supports flexible online human-in-the-loop corrections, which serve as explicit guidance for learning error-recovery behaviors. Across three real-world tasks and two embodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich manipulation in just 1.5 hours of real-world training, outperforming RL and IL baselines by a substantial margin in both effectiveness and efficiency. Notably, the fine-tuned policy exhibits strong test-time scalability by reliably executing complex error-recovery behaviors to achieve better performance.",
          "abstract_zh": "强化学习 (RL) 广泛用于生成稳健的机器人操作策略，但由于中间步骤的值估计不准确和稀疏监督，使用 RL 微调视觉语言动作 (VLA) 模型可能会不稳定。相比之下，模仿学习（IL）很容易训练，但由于其离线性质而常常表现不佳。在本文中，我们提出了 Hi-ORS，这是一种简单而有效的训练后方法，利用拒绝采样来实现训练稳定性和高鲁棒性。Hi-ORS通过在线微调时过滤掉负奖励样本来稳定价值估计，并采用奖励加权监督训练目标来提供密集的中间步骤监督。为了进行系统研究，我们开发了一个异步推理训练框架，支持灵活的在线人机循环校正，为学习错误恢复行为提供明确的指导。在三个现实世界任务和两个实施例中，Hi-ORS 微调了 pi-base 策略，在短短 1.5 小时的现实世界训练中掌握了丰富的接触操作，在有效性和效率方面都远远优于 RL 和 IL 基线。值得注意的是，微调策略通过可靠地执行复杂的错误恢复行为来实现更好的性能，从而表现出强大的测试时可扩展性。"
        },
        {
          "title": "Robotic Assistant: Completing Collaborative Tasks with Dexterous Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.25713v1",
          "snippet": "We adapt a pre-trained Vision-Language-Action (VLA) model (Open-VLA) for dexterous human-robot collaboration with minimal language prompting. Our approach adds (i) FiLM conditioning to visual backbones for task-aware perception, (ii) an auxiliary intent head that predicts collaborator hand pose and target cues, and (iii) action-space post-processing that predicts compact deltas (position/rotation) and PCA-reduced finger joints before mapping to full commands. Using a multi-view, teleoperated Franka and Mimic-hand dataset augmented with MediaPipe hand poses, we demonstrate that delta actions are well-behaved and that four principal components explain ~96% of hand-joint variance. Ablations identify action post-processing as the primary performance driver; auxiliary intent helps, FiLM is mixed, and a directional motion loss is detrimental. A real-time stack (~0.3 s latency on one RTX 4090) composes \"pick-up\" and \"pass\" into a long-horizon behavior. We surface \"trainer overfitting\" to specific demonstrators as the key limitation.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-10-29T17:22:59Z",
          "authors": [
            "Boshi An",
            "Chenyu Yang",
            "Robert Katzschmann"
          ],
          "arxiv_id": "2510.25713",
          "abstract": "We adapt a pre-trained Vision-Language-Action (VLA) model (Open-VLA) for dexterous human-robot collaboration with minimal language prompting. Our approach adds (i) FiLM conditioning to visual backbones for task-aware perception, (ii) an auxiliary intent head that predicts collaborator hand pose and target cues, and (iii) action-space post-processing that predicts compact deltas (position/rotation) and PCA-reduced finger joints before mapping to full commands. Using a multi-view, teleoperated Franka and Mimic-hand dataset augmented with MediaPipe hand poses, we demonstrate that delta actions are well-behaved and that four principal components explain ~96% of hand-joint variance. Ablations identify action post-processing as the primary performance driver; auxiliary intent helps, FiLM is mixed, and a directional motion loss is detrimental. A real-time stack (~0.3 s latency on one RTX 4090) composes \"pick-up\" and \"pass\" into a long-horizon behavior. We surface \"trainer overfitting\" to specific demonstrators as the key limitation.",
          "abstract_zh": "我们采用预先训练的视觉-语言-动作（VLA）模型（Open-VLA），以最少的语言提示实现灵巧的人机协作。我们的方法添加了（i）FiLM调节到视觉主干以实现任务感知感知，（ii）预测协作者手势和目标线索的辅助意图头，以及（iii）动作空间后处理，在映射到完整命令之前预测紧凑增量（位置/旋转）和PCA减少的手指关节。使用通过 MediaPipe 手势增强的多视图远程操作 Franka 和模仿手数据集，我们证明了 delta 动作表现良好，并且四个主要成分解释了约 96% 的手关节方差。消融将动作后处理视为主要的性能驱动因素；辅助意图有帮助，FiLM 是混合的，方向运动损失是有害的。实时堆栈（一台 RTX 4090 上的延迟约为 0.3 秒）将“拾取”和“通过”组合成长范围行为。我们将“训练器过度拟合”对特定演示者视为关键限制。"
        },
        {
          "title": "PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model",
          "url": "http://arxiv.org/abs/2511.01571v1",
          "snippet": "Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA, while requiring only 1.5% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-11-03T13:39:37Z",
          "authors": [
            "Wenqi Liang",
            "Gan Sun",
            "Yao He",
            "Jiahua Dong",
            "Suyan Dai",
            "Ivan Laptev",
            "Salman Khan",
            "Yang Cong"
          ],
          "arxiv_id": "2511.01571",
          "abstract": "Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA, while requiring only 1.5% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.",
          "abstract_zh": "视觉-语言-动作模型（VLA）正在成为学习通用视觉运动控制策略的强大工具。然而，当前的 VLA 主要是在大规模图像文本动作数据上进行训练，并且在两个关键方面仍然受到限制：（i）它们在像素级场景理解方面遇到困难，（ii）它们严重依赖文本提示，这降低了它们在现实世界设置中的灵活性。为了应对这些挑战，我们推出了 PixelVLA，这是第一个 VLA 模型，旨在支持像素级推理以及文本和视觉输入的多模式提示。我们的方法建立在一个新的视觉运动指令调整框架之上，该框架集成了多尺度像素感知编码器和视觉提示编码器。为了有效地训练 PixelVLA，我们进一步提出了一个两阶段自动注释管道，可生成 Pixel-160K，这是一个具有从现有机器人数据派生的像素级注释的大型数据集。对三个标准 VLA 基准测试和两个 VLA 模型变体的实验表明，PixelVLA 比 OpenVLA 提高了 10.1%-17.8% 的操作成功率，而预训练成本仅为其 1.5%。这些结果表明，PixelVLA 可以集成到现有的 VLA 中，从而在复杂环境中实现更准确、高效和多功能的机器人控制。数据集和代码将作为开源发布。"
        },
        {
          "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies",
          "url": "http://arxiv.org/abs/2510.25122v1",
          "snippet": "Vision-language-action (VLA) models have significantly advanced robotic manipulation by integrating vision-language models (VLMs), and action decoders into a unified architecture. However, their deployment on resource-constrained edge devices, such as mobile robots or embedded systems (e.g., Jetson Orin Nano), remains challenging due to high computational demands, especially in real-world scenarios where power, latency, and computational resources are critical. To close this gap, we introduce Nano-scale Vision-Language Action (NanoVLA), a family of lightweight VLA architectures that achieve high performance with minimal resources. Our core innovations include: (1) vision-language decoupling that moves conventional early vision and language inputs fusion in VLM to late stage, achieving better performance while enabling caching and reduce inference overhead and latency; (2) long-short action chunking to ensure smooth, coherent multi-step planning without sacrificing real-time responsiveness; (3) dynamic routing that adaptively assigns lightweight or heavy backbones based on task complexity, further optimizing inference efficiency. Experimental results on several benchmarks, as well as real-world deployments, demonstrate that NanoVLA achieves up to 52x faster inference on edge devices compared to previous state-of-the-art VLA models, with 98% less parameters while maintaining or surpassing their task accuracy and generalization. Ablation studies confirm that our decoupling strategy preserves cross-task transferability, and the routing module enhances cost-performance trade-offs, enabling practical, high-precision robotic manipulation on resource-constrained hardware.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-10-29T03:00:36Z",
          "authors": [
            "Jiahong Chen",
            "Jing Wang",
            "Long Chen",
            "Chuwei Cai",
            "Jinghui Lu"
          ],
          "arxiv_id": "2510.25122",
          "abstract": "Vision-language-action (VLA) models have significantly advanced robotic manipulation by integrating vision-language models (VLMs), and action decoders into a unified architecture. However, their deployment on resource-constrained edge devices, such as mobile robots or embedded systems (e.g., Jetson Orin Nano), remains challenging due to high computational demands, especially in real-world scenarios where power, latency, and computational resources are critical. To close this gap, we introduce Nano-scale Vision-Language Action (NanoVLA), a family of lightweight VLA architectures that achieve high performance with minimal resources. Our core innovations include: (1) vision-language decoupling that moves conventional early vision and language inputs fusion in VLM to late stage, achieving better performance while enabling caching and reduce inference overhead and latency; (2) long-short action chunking to ensure smooth, coherent multi-step planning without sacrificing real-time responsiveness; (3) dynamic routing that adaptively assigns lightweight or heavy backbones based on task complexity, further optimizing inference efficiency. Experimental results on several benchmarks, as well as real-world deployments, demonstrate that NanoVLA achieves up to 52x faster inference on edge devices compared to previous state-of-the-art VLA models, with 98% less parameters while maintaining or surpassing their task accuracy and generalization. Ablation studies confirm that our decoupling strategy preserves cross-task transferability, and the routing module enhances cost-performance trade-offs, enabling practical, high-precision robotic manipulation on resource-constrained hardware.",
          "abstract_zh": "视觉语言动作 (VLA) 模型通过将视觉语言模型 (VLM) 和动作解码器集成到统一架构中，显着提高了机器人操作的性能。然而，由于高计算需求，它们在资源受限的边缘设备（例如移动机器人或嵌入式系统（例如 Jetson Orin Nano））上的部署仍然具有挑战性，特别是在功耗、延迟和计算资源至关重要的现实场景中。为了缩小这一差距，我们引入了纳米级视觉语言动作 (NanoVLA)，这是一系列轻量级 VLA 架构，可以用最少的资源实现高性能。我们的核心创新包括：（1）视觉语言解耦，将VLM中传统的早期视觉和语言输入融合移至后期，在实现更好的性能的同时启用缓存并减少推理开销和延迟；(2) 长短动作分块，以确保平滑、连贯的多步骤规划，而不牺牲实时响应能力；（3）动态路由，根据任务复杂度自适应分配轻量级或重度主干网，进一步优化推理效率。多个基准测试以及实际部署的实验结果表明，与之前最先进的 VLA 模型相比，NanoVLA 在边缘设备上的推理速度提高了 52 倍，参数减少了 98%，同时保持或超越了任务准确性和泛化能力。消融研究证实，我们的解耦策略保留了跨任务可转移性，并且路由模块增强了成本性能权衡，从而在资源受限的硬件上实现了实用的高精度机器人操作。"
        },
        {
          "title": "RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2511.01331v2",
          "snippet": "Vision-Language-Action (VLA) models have recently emerged as powerful general-purpose policies for robotic manipulation, benefiting from large-scale multi-modal pre-training. However, they often fail to generalize reliably in out-of-distribution deployments, where unavoidable disturbances such as observation noise, sensor errors, or actuation perturbations become prevalent. While recent Reinforcement Learning (RL)-based post-training provides a practical means to adapt pre-trained VLA models, existing methods mainly emphasize reward maximization and overlook robustness to environmental uncertainty. In this work, we introduce RobustVLA, a lightweight online RL post-training method designed to explicitly enhance the resilience of VLA models. Through a systematic robustness analysis, we identify two key regularizations: Jacobian regularization, which mitigates sensitivity to observation noise, and smoothness regularization, which stabilizes policies under action perturbations. Extensive experiments across diverse robotic environments demonstrate that RobustVLA significantly outperforms prior state-of-the-art methods in robustness and reliability. Our results highlight the importance of principled robustness-aware RL post-training as a key step toward improving the reliability and robustness of VLA models.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-11-03T08:30:48Z",
          "authors": [
            "Hongyin Zhang",
            "Shuo Zhang",
            "Junxi Jin",
            "Qixin Zeng",
            "Runze Li",
            "Donglin Wang"
          ],
          "arxiv_id": "2511.01331",
          "abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful general-purpose policies for robotic manipulation, benefiting from large-scale multi-modal pre-training. However, they often fail to generalize reliably in out-of-distribution deployments, where unavoidable disturbances such as observation noise, sensor errors, or actuation perturbations become prevalent. While recent Reinforcement Learning (RL)-based post-training provides a practical means to adapt pre-trained VLA models, existing methods mainly emphasize reward maximization and overlook robustness to environmental uncertainty. In this work, we introduce RobustVLA, a lightweight online RL post-training method designed to explicitly enhance the resilience of VLA models. Through a systematic robustness analysis, we identify two key regularizations: Jacobian regularization, which mitigates sensitivity to observation noise, and smoothness regularization, which stabilizes policies under action perturbations. Extensive experiments across diverse robotic environments demonstrate that RobustVLA significantly outperforms prior state-of-the-art methods in robustness and reliability. Our results highlight the importance of principled robustness-aware RL post-training as a key step toward improving the reliability and robustness of VLA models.",
          "abstract_zh": "受益于大规模多模态预训练，视觉-语言-动作（VLA）模型最近已成为强大的机器人操作通用策略。然而，它们通常无法在分布外部署中可靠地进行泛化，在分布外部署中，观察噪声、传感器错误或驱动扰动等不可避免的干扰变得普遍。虽然最近基于强化学习 (RL) 的后训练提供了一种适应预训练 VLA 模型的实用方法，但现有方法主要强调奖励最大化，而忽视了对环境不确定性的鲁棒性。在这项工作中，我们引入了 RobustVLA，这是一种轻量级在线 RL 后训练方法，旨在显着增强 VLA 模型的弹性。通过系统的稳健性分析，我们确定了两个关键的正则化：雅可比正则化，它可以减轻对观测噪声的敏感性；平滑正则化，可以在行动扰动下稳定策略。在不同机器人环境中进行的大量实验表明，RobustVLA 在稳健性和可靠性方面显着优于先前最先进的方法。我们的结果强调了有原则的鲁棒性感知 RL 后训练的重要性，这是提高 VLA 模型可靠性和鲁棒性的关键一步。"
        },
        {
          "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation",
          "url": "http://arxiv.org/abs/2510.23571v1",
          "snippet": "The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining \"success\" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-10-27T17:41:38Z",
          "authors": [
            "Yash Jangir",
            "Yidi Zhang",
            "Kashu Yamazaki",
            "Chenyu Zhang",
            "Kuan-Hsun Tu",
            "Tsung-Wei Ke",
            "Lei Ke",
            "Yonatan Bisk",
            "Katerina Fragkiadaki"
          ],
          "arxiv_id": "2510.23571",
          "abstract": "The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining \"success\" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.",
          "abstract_zh": "追求机器人通才——能够在不同环境中执行不同任务的可指导代理——需要严格且可扩展的评估。然而，机器人政策的现实测试仍然受到根本限制：它是劳动密集型的、缓慢的、大规模不安全的，并且难以复制。现有的模拟基准同样受到限制，因为它们在同一合成领域内训练和测试策略，并且无法评估从现实世界演示或替代模拟环境中训练的模型。随着政策范围和复杂性的扩大，这些障碍只会加剧，因为机器人技术“成功”的定义往往取决于人类对执行质量的细致判断。在本文中，我们介绍了一种新的基准测试框架，该框架通过将 VLA 评估转移到通过在线人类反馈增强的大规模模拟环境中来克服这些挑战。利用视觉语言模型、2D 到 3D 生成建模和可微分渲染方面的进步，我们的方法自动将广泛使用的机器人数据集的视频演示转换为模拟的对应数据。在这些数字孪生中，我们使用自动 VLM 引导评分和从众包工作者收集的可扩展人类偏好判断来评估 VLA 策略，将人类参与从繁琐的场景设置、重置和安全监督转变为轻量级偏好比较。为了衡量鲁棒性，我们沿着多个轴系统地扰动模拟环境，例如纹理和对象放置、受控变化下的压力测试策略泛化。其结果是为现实世界中训练有素的机器人操作策略提供了一个不断发展、可重复和可扩展的基准，解决了当今机器人领域中关键的缺失能力。"
        },
        {
          "title": "A Survey on Efficient Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.24795v1",
          "snippet": "Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-10-27T17:57:33Z",
          "authors": [
            "Zhaoshu Yu",
            "Bo Wang",
            "Pengpeng Zeng",
            "Haonan Zhang",
            "Ji Zhang",
            "Lianli Gao",
            "Jingkuan Song",
            "Nicu Sebe",
            "Heng Tao Shen"
          ],
          "arxiv_id": "2510.24795",
          "abstract": "Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/",
          "abstract_zh": "视觉-语言-动作模型（VLA）代表了体现智能的重要前沿，旨在架起数字知识与物理世界交互的桥梁。虽然这些模型表现出了卓越的通才能力，但其部署却受到其底层大规模基础模型固有的大量计算和数据要求的严重阻碍。出于应对这些挑战的迫切需要，本次调查首次对整个数据模型训练过程中的高效视觉-语言-行动模型（高效 VLA）进行了全面审查。具体来说，我们引入了一个统一的分类法来系统地组织该领域的不同工作，将当前技术分为三个核心支柱：（1）高效模型设计，重点关注高效架构和模型压缩；（2）高效训练，减少模型学习过程中的计算负担；(3)高效数据采集，解决机器人数据获取和利用的瓶颈。通过在此框架内对最先进的方法进行批判性审查，本次调查不仅为社区建立了基础参考，还总结了代表性应用，描绘了关键挑战，并为未来的研究制定了路线图。我们维护一个不断更新的项目页面来跟踪我们的最新进展：https://evla-survey.github.io/"
        },
        {
          "title": "Embodiment Transfer Learning for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2511.01224v1",
          "snippet": "Vision-language-action (VLA) models have significantly advanced robotic learning, enabling training on large-scale, cross-embodiment data and fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs struggle with multi-robot collaboration. We introduce embodiment transfer learning, denoted as ET-VLA, a novel framework for efficient and effective transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic Continued Pretraining (SCP), which uses synthetically generated data to warm up the model for the new embodiment, bypassing the need for real human demonstrations and reducing data collection costs. SCP enables the model to learn correct actions and precise action token numbers. Following SCP, the model is fine-tuned on target embodiment data. To further enhance the model performance on multi-embodiment, we present the Embodied Graph-of-Thought technique, a novel approach that formulates each sub-task as a node, that allows the VLA model to distinguish the functionalities and roles of each embodiment during task execution. Our work considers bimanual robots, a simple version of multi-robot to verify our approaches. We validate the effectiveness of our method on both simulation benchmarks and real robots covering three different bimanual embodiments. In particular, our proposed ET-VLA \\space can outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all codes to support the community in advancing VLA models for robot learning.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-11-03T04:50:27Z",
          "authors": [
            "Chengmeng Li",
            "Yaxin Peng"
          ],
          "arxiv_id": "2511.01224",
          "abstract": "Vision-language-action (VLA) models have significantly advanced robotic learning, enabling training on large-scale, cross-embodiment data and fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs struggle with multi-robot collaboration. We introduce embodiment transfer learning, denoted as ET-VLA, a novel framework for efficient and effective transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic Continued Pretraining (SCP), which uses synthetically generated data to warm up the model for the new embodiment, bypassing the need for real human demonstrations and reducing data collection costs. SCP enables the model to learn correct actions and precise action token numbers. Following SCP, the model is fine-tuned on target embodiment data. To further enhance the model performance on multi-embodiment, we present the Embodied Graph-of-Thought technique, a novel approach that formulates each sub-task as a node, that allows the VLA model to distinguish the functionalities and roles of each embodiment during task execution. Our work considers bimanual robots, a simple version of multi-robot to verify our approaches. We validate the effectiveness of our method on both simulation benchmarks and real robots covering three different bimanual embodiments. In particular, our proposed ET-VLA \\space can outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all codes to support the community in advancing VLA models for robot learning.",
          "abstract_zh": "视觉-语言-动作（VLA）模型具有显着先进的机器人学习能力，可以对大规模、跨实体数据进行训练，并对特定机器人进行微调。然而，最先进的自回归 VLA 在多机器人协作方面遇到了困难。我们引入了体现迁移学习，表示为 ET-VLA，这是一种新颖的框架，用于将预先训练的 VLA 高效且有效地迁移到多机器人。ET-VLA 的核心是综合持续预训练（SCP），它使用综合生成的数据来预热新实施例的模型，绕过真实人类演示的需要并降低数据收集成本。SCP 使模型能够学习正确的动作和精确的动作标记数量。SCP 之后，模型根据目标实施例数据进行微调。为了进一步提高多实施例的模型性能，我们提出了具体化思维图技术，这是一种将每个子任务表示为节点的新颖方法，允许 VLA 模型在任务执行期间区分每个实施例的功能和角色。我们的工作考虑了双手机器人，这是多机器人的简单版本，以验证我们的方法。我们在模拟基准和涵盖三种不同双手实施例的真实机器人上验证了我们的方法的有效性。特别是，我们提出的 ET-VLA \\space 在六项实际任务上的性能优于 OpenVLA 超过 53.2%。我们将开源所有代码，以支持社区推进机器人学习的 VLA 模型。"
        },
        {
          "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process",
          "url": "http://arxiv.org/abs/2511.01718v1",
          "snippet": "Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act -- reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4$\\times$ faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at https://irpn-eai.github.io/UD-VLA.github.io/.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-11-03T16:26:54Z",
          "authors": [
            "Jiayi Chen",
            "Wenxuan Song",
            "Pengxiang Ding",
            "Ziyang Zhou",
            "Han Zhao",
            "Feilong Tang",
            "Donglin Wang",
            "Haoang Li"
          ],
          "arxiv_id": "2511.01718",
          "abstract": "Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act -- reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4$\\times$ faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at https://irpn-eai.github.io/UD-VLA.github.io/.",
          "abstract_zh": "视觉-语言-动作（VLA）模型旨在理解自然语言指令和视觉观察，并作为具体代理执行相应的动作。最近的工作将未来图像整合到理解-行动循环中，产生统一的 VLA，共同理解、生成和行动——读取文本和图像并生成未来图像和行动。然而，这些模型要么依赖外部专家进行模态统一，要么将图像生成和动作预测视为单独的过程，限制了这些任务之间直接协同的好处。我们的核心理念是通过同步去噪过程共同优化生成和动作，其中迭代细化使动作能够在持续且充分的视觉引导下从初始化演变。我们将这一理念植根于我们提出的统一扩散VLA和联合离散去噪扩散过程（JD3P）中，这是一种联合扩散过程，将多种模态集成到单个去噪轨迹中，作为关键机制，使理解、生成和行动具有本质上的协同作用。我们的模型和理论建立在所有模式的统一标记化空间和混合注意力机制的基础上。我们进一步提出了一个两阶段训练管道和几种优化性能和效率的推理时间技术。我们的方法在 CALVIN、LIBERO 和 SimplerEnv 等基准测试中实现了最先进的性能，推理速度比自回归方法快 4 倍，并且我们通过深入分析和实际评估证明了其有效性。我们的项目页面位于 https://irpn-eai.github.io/UD-VLA.github.io/。"
        },
        {
          "title": "iFlyBot-VLA Technical Report",
          "url": "http://arxiv.org/abs/2511.01914v1",
          "snippet": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-11-01T06:24:56Z",
          "authors": [
            "Yuan Zhang",
            "Chenyu Xue",
            "Wenjie Xu",
            "Chao Ji",
            "Jiajia wu",
            "Jia Pan"
          ],
          "arxiv_id": "2511.01914",
          "abstract": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community",
          "abstract_zh": "我们介绍了 iFlyBot-VLA，这是一种在新颖框架下训练的大规模视觉-语言-动作（VLA）模型。主要贡献如下：（1）在大规模人类和机器人操作视频上彻底训练的潜在动作模型；（2）双层动作表示框架，在训练过程中共同监督视觉语言模型（VLM）和动作专家；（3）混合训练策略，将机器人轨迹数据与通用QA和空间QA数据集相结合，有效增强VLM主干的3D感知和推理能力。具体来说，VLM 被训练来预测两种互补形式的动作：潜在动作，源自我们在跨实施例操作数据上预训练的潜在动作模型，它捕获隐含的高级意图；以及通过连续控制信号的频域变换获得的结构化离散动作令牌，这些令牌编码显式的低级动态。这种双重监督调整了语言、视觉和动作的表示空间，使 VLM 能够直接促进动作的生成。LIBERO Franka 基准测试的实验结果证明了我们框架的优越性，而现实世界的评估进一步表明 iFlyBot-VLA 在各种具有挑战性的操作任务中实现了具有竞争力的成功率。此外，我们计划开源部分自建数据集，以支持社区未来的研究"
        },
        {
          "title": "Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots",
          "url": "http://arxiv.org/abs/2511.00917v2",
          "snippet": "Today's best-explored routes towards generalist robots center on collecting ever larger \"observations-in actions-out\" robotics datasets to train large end-to-end models, copying a recipe that has worked for vision-language models (VLMs). We pursue a road less traveled: building generalist policies directly around VLMs by augmenting their general capabilities with specific robot capabilities encapsulated in a carefully curated set of perception, planning, and control modules. In Maestro, a VLM coding agent dynamically composes these modules into a programmatic policy for the current task and scenario. Maestro's architecture benefits from a streamlined closed-loop interface without many manually imposed structural constraints, and a comprehensive and diverse tool repertoire. As a result, it largely surpasses today's VLA models for zero-shot performance on challenging manipulation skills. Further, Maestro is easily extensible to incorporate new modules, easily editable to suit new embodiments such as a quadruped-mounted arm, and even easily adapts from minimal real-world experiences through local code edits.",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-11-02T12:34:37Z",
          "authors": [
            "Junyao Shi",
            "Rujia Yang",
            "Kaitian Chao",
            "Selina Bingqing Wan",
            "Yifei Shao",
            "Jiahui Lei",
            "Jianing Qian",
            "Long Le",
            "Pratik Chaudhari",
            "Kostas Daniilidis",
            "Chuan Wen",
            "Dinesh Jayaraman"
          ],
          "arxiv_id": "2511.00917",
          "abstract": "Today's best-explored routes towards generalist robots center on collecting ever larger \"observations-in actions-out\" robotics datasets to train large end-to-end models, copying a recipe that has worked for vision-language models (VLMs). We pursue a road less traveled: building generalist policies directly around VLMs by augmenting their general capabilities with specific robot capabilities encapsulated in a carefully curated set of perception, planning, and control modules. In Maestro, a VLM coding agent dynamically composes these modules into a programmatic policy for the current task and scenario. Maestro's architecture benefits from a streamlined closed-loop interface without many manually imposed structural constraints, and a comprehensive and diverse tool repertoire. As a result, it largely surpasses today's VLA models for zero-shot performance on challenging manipulation skills. Further, Maestro is easily extensible to incorporate new modules, easily editable to suit new embodiments such as a quadruped-mounted arm, and even easily adapts from minimal real-world experiences through local code edits.",
          "abstract_zh": "当今通才机器人的最佳探索路线集中在收集更大的“观察-行动-输出”机器人数据集来训练大型端到端模型，复制适用于视觉语言模型（VLM）的秘诀。我们走的是一条少有人走的路：通过封装在一组精心策划的感知、规划和控制模块中的特定机器人功能来增强 VLM 的一般功能，直接围绕 VLM 制定通用政策。在 Maestro 中，VLM 编码代理动态地将这些模块组合成当前任务和场景的编程策略。Maestro 的架构受益于精简的闭环界面，没有许多手动施加的结构约束，以及全面且多样化的工具库。因此，它在具有挑战性的操作技能方面的零射击性能大大超越了当今的 VLA 模型。此外，Maestro 可以轻松扩展以合并新模块，轻松编辑以适应新的实施例（例如四足安装手臂），甚至可以通过本地代码编辑轻松适应最小的现实世界体验。"
        },
        {
          "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model",
          "url": "http://arxiv.org/abs/2510.27607v2",
          "snippet": "Recently, augmenting vision-language-action models (VLAs) with world-models has shown promise in robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while enabling cross-modal knowledge sharing. In addition, we propose training techniques such as independent noise perturbations for each modality and a decoupled flow matching loss, which enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Furthermore, based on the decoupled training framework, we introduce a sampling method where we sample action and vision tokens asynchronously at different rates, which shows improvement through inference-time scaling. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over a standard VLA baseline and implicit world-modeling methods, with our inference-time scaling approach providing an additional 2-5% gain on success rate. On real-world tasks with the Franka Research 3, DUST outperforms baselines in success rate by 13%, confirming its effectiveness beyond simulation. Lastly, we demonstrate the effectiveness of DUST in large-scale pretraining with action-free videos from BridgeV2, where DUST leads to significant gain when transferred to the RoboCasa benchmark.",
          "site": "arxiv.org",
          "rank": 15,
          "published": "2025-10-31T16:32:12Z",
          "authors": [
            "John Won",
            "Kyungmin Lee",
            "Huiwon Jang",
            "Dongyoung Kim",
            "Jinwoo Shin"
          ],
          "arxiv_id": "2510.27607",
          "abstract": "Recently, augmenting vision-language-action models (VLAs) with world-models has shown promise in robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while enabling cross-modal knowledge sharing. In addition, we propose training techniques such as independent noise perturbations for each modality and a decoupled flow matching loss, which enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Furthermore, based on the decoupled training framework, we introduce a sampling method where we sample action and vision tokens asynchronously at different rates, which shows improvement through inference-time scaling. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over a standard VLA baseline and implicit world-modeling methods, with our inference-time scaling approach providing an additional 2-5% gain on success rate. On real-world tasks with the Franka Research 3, DUST outperforms baselines in success rate by 13%, confirming its effectiveness beyond simulation. Lastly, we demonstrate the effectiveness of DUST in large-scale pretraining with action-free videos from BridgeV2, where DUST leads to significant gain when transferred to the RoboCasa benchmark.",
          "abstract_zh": "最近，用世界模型增强视觉语言动作模型（VLA）在机器人策略学习中显示出了前景。然而，由于两种模式之间存在固有的差异，联合预测下一状态观察和行动序列仍然具有挑战性。为了解决这个问题，我们提出了 DUal-STream 扩散（DUST），这是一种世界模型增强的 VLA 框架，可以处理模态冲突并增强 VLA 在不同任务中的性能。具体来说，我们提出了一种多模态扩散变压器架构，该架构显式地维护单独的模态流，同时实现跨模态知识共享。此外，我们提出了训练技术，例如每种模态的独立噪声扰动和解耦流匹配损失，这使得模型能够以双向方式学习联合分布，同时避免需要统一的潜在空间。此外，基于解耦训练框架，我们引入了一种采样方法，以不同的速率异步采样动作和视觉标记，这通过推理时间缩放显示了改进。通过在 RoboCasa 和 GR-1 等模拟基准上进行的实验，DUST 比标准 VLA 基准和隐式世界建模方法实现了高达 6% 的增益，而我们的推理时间缩放方法可将成功率额外提高 2-5%。在使用 Franka Research 3 执行实际任务时，DUST 的成功率比基准高出 13%，证实了其超越模拟的有效性。最后，我们通过 BridgeV2 的无动作视频展示了 DUST 在大规模预训练中的有效性，其中 DUST 在转移到 RoboCasa 基准测试时会带来显着的收益。"
        },
        {
          "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
          "url": "http://arxiv.org/abs/2510.23576v1",
          "snippet": "Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties.",
          "site": "arxiv.org",
          "rank": 16,
          "published": "2025-10-27T17:46:43Z",
          "authors": [
            "Anqi Li",
            "Zhiyong Wang",
            "Jiazhao Zhang",
            "Minghan Li",
            "Yunpeng Qi",
            "Zhibo Chen",
            "Zhizheng Zhang",
            "He Wang"
          ],
          "arxiv_id": "2510.23576",
          "abstract": "Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties.",
          "abstract_zh": "城市微移动应用（例如送货机器人）需要在大规模城市环境中进行可靠导航，同时遵循长视距路线指令。由于现实城市地区的动态和非结构化性质，这项任务特别具有挑战性，但大多数现有的导航方法仍然是针对小规模和可控场景的。有效的城市微交通需要两个互补级别的导航技能：低级能力（例如点目标到达和避障）和高级能力（例如路线视觉对齐）。为此，我们提出了 UrbanVLA，这是一个专为可扩展的城市导航而设计的路线条件视觉-语言-行动（VLA）框架。我们的方法在执行过程中明确地将噪声路径点与视觉观察对齐，然后规划驱动机器人的轨迹。为了使 UrbanVLA 能够掌握两个级别的导航，我们采用了两阶段训练流程。该过程首先使用模拟环境和从网络视频解析的轨迹进行监督微调 (SFT)。接下来是对模拟和现实世界数据的混合进行强化微调（RFT），这增强了模型在现实世界环境中的安全性和适应性。实验表明，UrbanVLA 在 MetaUrban 上的 SocialNav 任务中超出了强基线 55% 以上。此外，UrbanVLA 实现了可靠的现实世界导航，展示了大规模城市环境的可扩展性和针对现实世界不确定性的鲁棒性。"
        },
        {
          "title": "DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2511.15669v1",
          "snippet": "Enabling Vision-Language-Action (VLA) models to \"think before acting\" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.",
          "site": "arxiv.org",
          "rank": 17,
          "published": "2025-10-31T05:26:16Z",
          "authors": [
            "Cheng Yin",
            "Yankai Lin",
            "Wang Xu",
            "Sikyuen Tam",
            "Xiangrui Zeng",
            "Zhiyuan Liu",
            "Zhouping Yin"
          ],
          "arxiv_id": "2511.15669",
          "abstract": "Enabling Vision-Language-Action (VLA) models to \"think before acting\" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.",
          "abstract_zh": "通过思想链 (CoT) 使视觉-语言-行动 (VLA) 模型能够“先思考后行动”，这是克服端到端机器人策略的数据匮乏性质的一条有希望的途径。然而，由于一个根本性的冲突，进展陷入停滞：现有模型使用单个自回归解码器来进行顺序 CoT 推理和高维、可并行的机器人动作。这种结构上的不匹配会降低运动控制能力，并且无法在思想和行动之间建立牢固的因果联系。我们引入了 DeepThinkVLA，它通过紧密集成的架构和训练策略解决了这一冲突。在架构上，我们的混合注意力解码器生成具有因果注意力的顺序 CoT，然后切换到双向注意力以快速、并行地解码动作向量。该设计辅以两阶段训练流程：我们首先使用监督微调（SFT）来教授模型基础推理，然后应用带有任务成功奖励的强化学习（RL），将完整的推理-行动序列与期望的结果进行因果对齐。这种协同作用带来了最先进的性能，在 LIBERO 基准测试中实现了 97.0% 的成功率。我们的消融证实了设计的有效性：仅混合架构就比标准解码器性能高出 15.5%，最后的 RL 阶段提供了关键的 2% 提升，以确保最佳性能。"
        },
        {
          "title": "Dexbotic: Open-Source Vision-Language-Action Toolbox",
          "url": "http://arxiv.org/abs/2510.23511v1",
          "snippet": "In this paper, we present Dexbotic, an open-source Vision-Language-Action (VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLA research service for professionals in the field of embodied intelligence. It offers a codebase that supports multiple mainstream VLA policies simultaneously, allowing users to reproduce various VLA methods with just a single environment setup. The toolbox is experiment-centric, where the users can quickly develop new VLA experiments by simply modifying the Exp script. Moreover, we provide much stronger pretrained models to achieve great performance improvements for state-of-the-art VLA policies. Dexbotic will continuously update to include more of the latest pre-trained foundation models and cutting-edge VLA models in the industry.",
          "site": "arxiv.org",
          "rank": 18,
          "published": "2025-10-27T16:47:24Z",
          "authors": [
            "Bin Xie",
            "Erjin Zhou",
            "Fan Jia",
            "Hao Shi",
            "Haoqiang Fan",
            "Haowei Zhang",
            "Hebei Li",
            "Jianjian Sun",
            "Jie Bin",
            "Junwen Huang",
            "Kai Liu",
            "Kaixin Liu",
            "Kefan Gu",
            "Lin Sun",
            "Meng Zhang",
            "Peilong Han",
            "Ruitao Hao",
            "Ruitao Zhang",
            "Saike Huang",
            "Songhan Xie",
            "Tiancai Wang",
            "Tianle Liu",
            "Wenbin Tang",
            "Wenqi Zhu",
            "Yang Chen",
            "Yingfei Liu",
            "Yizhuang Zhou",
            "Yu Liu",
            "Yucheng Zhao",
            "Yunchao Ma",
            "Yunfei Wei",
            "Yuxiang Chen",
            "Ze Chen",
            "Zeming Li",
            "Zhao Wu",
            "Ziheng Zhang",
            "Ziming Liu",
            "Ziwei Yan",
            "Ziyu Zhang"
          ],
          "arxiv_id": "2510.23511",
          "abstract": "In this paper, we present Dexbotic, an open-source Vision-Language-Action (VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLA research service for professionals in the field of embodied intelligence. It offers a codebase that supports multiple mainstream VLA policies simultaneously, allowing users to reproduce various VLA methods with just a single environment setup. The toolbox is experiment-centric, where the users can quickly develop new VLA experiments by simply modifying the Exp script. Moreover, we provide much stronger pretrained models to achieve great performance improvements for state-of-the-art VLA policies. Dexbotic will continuously update to include more of the latest pre-trained foundation models and cutting-edge VLA models in the industry.",
          "abstract_zh": "在本文中，我们提出了 Dexbotic，一个基于 PyTorch 的开源视觉-语言-动作（VLA）模型工具箱。旨在为具身智能领域的专业人士提供一站式VLA研究服务。它提供了同时支持多种主流VLA策略的代码库，允许用户仅通过单个环境设置即可重现各种VLA方法。该工具箱以实验为中心，用户只需修改Exp脚本即可快速开发新的VLA实验。此外，我们提供了更强大的预训练模型，以实现最先进的 VLA 策略的巨大性能改进。Dexbotic 将不断更新，纳入更多最新的预训练基础模型和业界最前沿的 VLA 模型。"
        },
        {
          "title": "RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration",
          "url": "http://arxiv.org/abs/2510.26536v1",
          "snippet": "The proliferation of collaborative robots across diverse tasks and embodiments presents a central challenge: achieving lifelong adaptability, scalable coordination, and robust scheduling in multi-agent systems. Existing approaches, from vision-language-action (VLA) models to hierarchical frameworks, fall short due to their reliance on limited or dividual-agent memory. This fundamentally constrains their ability to learn over long horizons, scale to heterogeneous teams, or recover from failures, highlighting the need for a unified memory representation. To address these limitations, we introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable, and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene geometry, temporal event history, and embodiment profiles into a shared representation. This memory-centric design is integrated into a brain-cerebellum framework, where a high-level brain model performs global planning by retrieving and updating STEM, while low-level controllers execute actions locally. This closed loop between cognition, memory, and execution enables dynamic task allocation, fault-tolerant collaboration, and consistent state synchronization. We conduct extensive experiments spanning complex coordination tasks in restaurants, supermarkets, and households. Our results demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous embodiments, validating its effectiveness in enabling lifelong, scalable, and robust multi-robot collaboration. Project website: https://flagopen.github.io/RoboOS/",
          "site": "arxiv.org",
          "rank": 19,
          "published": "2025-10-30T14:26:40Z",
          "authors": [
            "Huajie Tan",
            "Cheng Chi",
            "Xiansheng Chen",
            "Yuheng Ji",
            "Zhongxia Zhao",
            "Xiaoshuai Hao",
            "Yaoxu Lyu",
            "Mingyu Cao",
            "Junkai Zhao",
            "Huaihai Lyu",
            "Enshen Zhou",
            "Ning Chen",
            "Yankai Fu",
            "Cheng Peng",
            "Wei Guo",
            "Dong Liang",
            "Zhuo Chen",
            "Mengsi Lyu",
            "Chenrui He",
            "Yulong Ao",
            "Yonghua Lin",
            "Pengwei Wang",
            "Zhongyuan Wang",
            "Shanghang Zhang"
          ],
          "arxiv_id": "2510.26536",
          "abstract": "The proliferation of collaborative robots across diverse tasks and embodiments presents a central challenge: achieving lifelong adaptability, scalable coordination, and robust scheduling in multi-agent systems. Existing approaches, from vision-language-action (VLA) models to hierarchical frameworks, fall short due to their reliance on limited or dividual-agent memory. This fundamentally constrains their ability to learn over long horizons, scale to heterogeneous teams, or recover from failures, highlighting the need for a unified memory representation. To address these limitations, we introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable, and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene geometry, temporal event history, and embodiment profiles into a shared representation. This memory-centric design is integrated into a brain-cerebellum framework, where a high-level brain model performs global planning by retrieving and updating STEM, while low-level controllers execute actions locally. This closed loop between cognition, memory, and execution enables dynamic task allocation, fault-tolerant collaboration, and consistent state synchronization. We conduct extensive experiments spanning complex coordination tasks in restaurants, supermarkets, and households. Our results demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous embodiments, validating its effectiveness in enabling lifelong, scalable, and robust multi-robot collaboration. Project website: https://flagopen.github.io/RoboOS/",
          "abstract_zh": "协作机器人在不同任务和实施例中的激增提出了一个核心挑战：在多智能体系统中实现终身适应性、可扩展的协调和稳健的调度。现有的方法，从视觉-语言-动作（VLA）模型到分层框架，由于依赖有限的或个体代理的记忆而存在缺陷。这从根本上限制了他们长期学习、扩展到异构团队或从故障中恢复的能力，这凸显了对统一内存表示的需求。为了解决这些限制，我们引入了 RoboOS-NeXT，这是一个基于内存的统一框架，用于实现终身、可扩展且强大的多机器人协作。RoboOS-NeXT 的核心是新颖的时空实体记忆 (STEM)，它将空间场景几何、时间事件历史和实体轮廓集成到共享表示中。这种以记忆为中心的设计被集成到大脑-小脑框架中，其中高级大脑模型通过检索和更新 STEM 来执行全局规划，而低级控制器则在本地执行操作。这种认知、记忆和执行之间的闭环可以实现动态任务分配、容错协作和一致的状态同步。我们在餐馆、超市和家庭中进行了涉及复杂协调任务的广泛实验。我们的结果表明，RoboOS-NeXT 在异构实施例中实现了卓越的性能，验证了其在实现终身、可扩展和稳健的多机器人协作方面的有效性。项目网站：https://flagopen.github.io/RoboOS/"
        },
        {
          "title": "Self-Improving Vision-Language-Action Models with Data Generation via Residual RL",
          "url": "http://arxiv.org/abs/2511.00091v1",
          "snippet": "Supervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist's deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.",
          "site": "arxiv.org",
          "rank": 20,
          "published": "2025-10-30T06:24:04Z",
          "authors": [
            "Wenli Xiao",
            "Haotian Lin",
            "Andy Peng",
            "Haoru Xue",
            "Tairan He",
            "Yuqi Xie",
            "Fengyuan Hu",
            "Jimmy Wu",
            "Zhengyi Luo",
            "Linxi \"Jim\" Fan",
            "Guanya Shi",
            "Yuke Zhu"
          ],
          "arxiv_id": "2511.00091",
          "abstract": "Supervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist's deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.",
          "abstract_zh": "有监督微调（SFT）已成为大型视觉语言动作（VLA）模型事实上的后训练策略，但其对昂贵的人类演示的依赖限制了可扩展性和泛化性。我们提出了 Probe、Learn、Distill (PLD)，这是一个三阶段即插即用框架，可通过残差强化学习 (RL) 和分布感知数据收集来改进 VLA。在第一阶段，我们训练轻量级剩余参与者来探测 VLA 通才的失败区域。在第二阶段，我们使用混合推出方案，将收集的轨迹与通才的部署分布保持一致，同时捕获恢复行为。在第 3 阶段，我们使用标准 SFT 将策划的轨迹提炼回通才。PLD 在 LIBERO 上实现了近乎饱和的 99% 任务成功率，在 SimplerEnv 上实现了超过 50% 的增益，在现实世界的 Franka 和 YAM 手臂操作任务上实现了 100% 的成功。消融表明，残余探测和分布感知重放是收集与部署一致的数据的关键，这些数据可以改进可见和不可见的任务，从而为自我改进 VLA 模型提供可扩展的路径。"
        },
        {
          "title": "End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection",
          "url": "http://arxiv.org/abs/2511.00139v2",
          "snippet": "Achieving human-like dexterous manipulation remains a major challenge for general-purpose robots. While Vision-Language-Action (VLA) models show potential in learning skills from demonstrations, their scalability is limited by scarce high-quality training data. Existing data collection methods face inherent constraints: manual teleoperation overloads human operators, while automated planning often produces unnatural motions. We propose a Shared Autonomy framework that divides control between macro and micro motions. A human operator guides the robot's arm pose through intuitive VR teleoperation, while an autonomous DexGrasp-VLA policy handles fine-grained hand control using real-time tactile and visual feedback. This division significantly reduces cognitive load and enables efficient collection of high-quality coordinated arm-hand demonstrations. Using this data, we train an end-to-end VLA policy enhanced with our novel Arm-Hand Feature Enhancement module, which captures both distinct and shared representations of macro and micro movements for more natural coordination. Our Corrective Teleoperation system enables continuous policy improvement through human-in-the-loop failure recovery. Experiments demonstrate that our framework generates high-quality data with minimal manpower and achieves a 90% success rate across diverse objects, including unseen instances. Comprehensive evaluations validate the system's effectiveness in developing dexterous manipulation capabilities.",
          "site": "arxiv.org",
          "rank": 21,
          "published": "2025-10-31T16:12:02Z",
          "authors": [
            "Yu Cui",
            "Yujian Zhang",
            "Lina Tao",
            "Yang Li",
            "Xinyu Yi",
            "Zhibin Li"
          ],
          "arxiv_id": "2511.00139",
          "abstract": "Achieving human-like dexterous manipulation remains a major challenge for general-purpose robots. While Vision-Language-Action (VLA) models show potential in learning skills from demonstrations, their scalability is limited by scarce high-quality training data. Existing data collection methods face inherent constraints: manual teleoperation overloads human operators, while automated planning often produces unnatural motions. We propose a Shared Autonomy framework that divides control between macro and micro motions. A human operator guides the robot's arm pose through intuitive VR teleoperation, while an autonomous DexGrasp-VLA policy handles fine-grained hand control using real-time tactile and visual feedback. This division significantly reduces cognitive load and enables efficient collection of high-quality coordinated arm-hand demonstrations. Using this data, we train an end-to-end VLA policy enhanced with our novel Arm-Hand Feature Enhancement module, which captures both distinct and shared representations of macro and micro movements for more natural coordination. Our Corrective Teleoperation system enables continuous policy improvement through human-in-the-loop failure recovery. Experiments demonstrate that our framework generates high-quality data with minimal manpower and achieves a 90% success rate across diverse objects, including unseen instances. Comprehensive evaluations validate the system's effectiveness in developing dexterous manipulation capabilities.",
          "abstract_zh": "实现像人类一样的灵巧操作仍然是通用机器人的主要挑战。虽然视觉-语言-动作 (VLA) 模型显示出通过演示学习技能的潜力，但其可扩展性受到稀缺的高质量训练数据的限制。现有的数据收集方法面临固有的限制：手动远程操作使操作员负担过重，而自动规划往往会产生不自然的运动。我们提出了一个共享自治框架，将控制分为宏观运动和微观运动。人类操作员通过直观的 VR 远程操作引导机器人的手臂姿势，而自主的 DexGrasp-VLA 策略则使用实时触觉和视觉反馈来处理细粒度的手部控制。这种划分显着减少了认知负荷，并能够有效收集高质量的协调臂手演示。利用这些数据，我们训练了一个端到端的 VLA 策略，该策略通过我们新颖的手臂-手特征增强模块得到增强，该模块捕获宏观和微观运动的独特和共享表示，以实现更自然的协调。我们的纠正性远程操作系统通过人在环故障恢复来实现持续的策略改进。实验表明，我们的框架可以用最少的人力生成高质量的数据，并在不同的对象（包括未见过的实例）上实现 90% 的成功率。综合评估验证了系统在发展灵巧操控能力方面的有效性。"
        },
        {
          "title": "Foundation Models for Trajectory Planning in Autonomous Driving: A Review of Progress and Open Challenges",
          "url": "http://arxiv.org/abs/2512.00021v1",
          "snippet": "The emergence of multi-modal foundation models has markedly transformed the technology for autonomous driving, shifting away from conventional and mostly hand-crafted design choices towards unified, foundation-model-based approaches, capable of directly inferring motion trajectories from raw sensory inputs. This new class of methods can also incorporate natural language as an additional modality, with Vision-Language-Action (VLA) models serving as a representative example. In this review, we provide a comprehensive examination of such methods through a unifying taxonomy to critically evaluate their architectural design choices, methodological strengths, and their inherent capabilities and limitations. Our survey covers 37 recently proposed approaches that span the landscape of trajectory planning with foundation models. Furthermore, we assess these approaches with respect to the openness of their source code and datasets, offering valuable information to practitioners and researchers. We provide an accompanying webpage that catalogs the methods based on our taxonomy, available at: https://github.com/fiveai/FMs-for-driving-trajectories",
          "site": "arxiv.org",
          "rank": 22,
          "published": "2025-10-31T18:05:02Z",
          "authors": [
            "Kemal Oksuz",
            "Alexandru Buburuzan",
            "Anthony Knittel",
            "Yuhan Yao",
            "Puneet K. Dokania"
          ],
          "arxiv_id": "2512.00021",
          "abstract": "The emergence of multi-modal foundation models has markedly transformed the technology for autonomous driving, shifting away from conventional and mostly hand-crafted design choices towards unified, foundation-model-based approaches, capable of directly inferring motion trajectories from raw sensory inputs. This new class of methods can also incorporate natural language as an additional modality, with Vision-Language-Action (VLA) models serving as a representative example. In this review, we provide a comprehensive examination of such methods through a unifying taxonomy to critically evaluate their architectural design choices, methodological strengths, and their inherent capabilities and limitations. Our survey covers 37 recently proposed approaches that span the landscape of trajectory planning with foundation models. Furthermore, we assess these approaches with respect to the openness of their source code and datasets, offering valuable information to practitioners and researchers. We provide an accompanying webpage that catalogs the methods based on our taxonomy, available at: https://github.com/fiveai/FMs-for-driving-trajectories",
          "abstract_zh": "多模态基础模型的出现显着改变了自动驾驶技术，从传统的、大部分手工制作的设计选择转向统一的、基于基础模型的方法，能够直接从原始感官输入推断运动轨迹。这类新方法还可以将自然语言作为附加模态，以视觉-语言-动作（VLA）模型作为代表性示例。在这篇综述中，我们通过统一的分类法对这些方法进行了全面的检查，以批判性地评估它们的架构设计选择、方法论优势及其固有的能力和局限性。我们的调查涵盖了最近提出的 37 种方法，这些方法涵盖了基础模型的轨迹规划领域。此外，我们还评估这些方法的源代码和数据集的开放性，为从业者和研究人员提供有价值的信息。我们提供了一个随附的网页，根据我们的分类法对方法进行了分类，网址为：https://github.com/ Fiveai/FMs-for-driven-trajectories"
        },
        {
          "title": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail",
          "url": "http://arxiv.org/abs/2511.00088v1",
          "snippet": "End-to-end architectures trained via imitation learning have advanced autonomous driving by scaling model size and data, yet performance remains brittle in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. To address this, we introduce Alpamayo-R1 (AR1), a vision-language-action model (VLA) that integrates Chain of Causation reasoning with trajectory planning to enhance decision-making in complex driving scenarios. Our approach features three key innovations: (1) the Chain of Causation (CoC) dataset, built through a hybrid auto-labeling and human-in-the-loop pipeline producing decision-grounded, causally linked reasoning traces aligned with driving behaviors; (2) a modular VLA architecture combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI applications, with a diffusion-based trajectory decoder that generates dynamically feasible plans in real time; (3) a multi-stage training strategy using supervised fine-tuning to elicit reasoning and reinforcement learning (RL) to optimize reasoning quality via large reasoning model feedback and enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12% improvement in planning accuracy on challenging cases compared to a trajectory-only baseline, with a 35% reduction in off-road rate and 25% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% as measured by a large reasoning model critic and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters shows consistent improvements. On-vehicle road tests confirm real-time performance (99 ms latency) and successful urban deployment. By bridging interpretable reasoning with precise control, AR1 demonstrates a practical path towards Level 4 autonomous driving. We plan to release AR1 models and a subset of the CoC in a future update.",
          "site": "arxiv.org",
          "rank": 23,
          "published": "2025-10-30T01:25:34Z",
          "authors": [
            "NVIDIA",
            ":",
            "Yan Wang",
            "Wenjie Luo",
            "Junjie Bai",
            "Yulong Cao",
            "Tong Che",
            "Ke Chen",
            "Yuxiao Chen",
            "Jenna Diamond",
            "Yifan Ding",
            "Wenhao Ding",
            "Liang Feng",
            "Greg Heinrich",
            "Jack Huang",
            "Peter Karkus",
            "Boyi Li",
            "Pinyi Li",
            "Tsung-Yi Lin",
            "Dongran Liu",
            "Ming-Yu Liu",
            "Langechuan Liu",
            "Zhijian Liu",
            "Jason Lu",
            "Yunxiang Mao",
            "Pavlo Molchanov",
            "Lindsey Pavao",
            "Zhenghao Peng",
            "Mike Ranzinger",
            "Ed Schmerling",
            "Shida Shen",
            "Yunfei Shi",
            "Sarah Tariq",
            "Ran Tian",
            "Tilman Wekel",
            "Xinshuo Weng",
            "Tianjun Xiao",
            "Eric Yang",
            "Xiaodong Yang",
            "Yurong You",
            "Xiaohui Zeng",
            "Wenyuan Zhang",
            "Boris Ivanovic",
            "Marco Pavone"
          ],
          "arxiv_id": "2511.00088",
          "abstract": "End-to-end architectures trained via imitation learning have advanced autonomous driving by scaling model size and data, yet performance remains brittle in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. To address this, we introduce Alpamayo-R1 (AR1), a vision-language-action model (VLA) that integrates Chain of Causation reasoning with trajectory planning to enhance decision-making in complex driving scenarios. Our approach features three key innovations: (1) the Chain of Causation (CoC) dataset, built through a hybrid auto-labeling and human-in-the-loop pipeline producing decision-grounded, causally linked reasoning traces aligned with driving behaviors; (2) a modular VLA architecture combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI applications, with a diffusion-based trajectory decoder that generates dynamically feasible plans in real time; (3) a multi-stage training strategy using supervised fine-tuning to elicit reasoning and reinforcement learning (RL) to optimize reasoning quality via large reasoning model feedback and enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12% improvement in planning accuracy on challenging cases compared to a trajectory-only baseline, with a 35% reduction in off-road rate and 25% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% as measured by a large reasoning model critic and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters shows consistent improvements. On-vehicle road tests confirm real-time performance (99 ms latency) and successful urban deployment. By bridging interpretable reasoning with precise control, AR1 demonstrates a practical path towards Level 4 autonomous driving. We plan to release AR1 models and a subset of the CoC in a future update.",
          "abstract_zh": "通过模仿学习训练的端到端架构通过扩展模型大小和数据来实现先进的自动驾驶，但在监督稀疏且因果理解有限的安全关键长尾场景中，性能仍然很脆弱。为了解决这个问题，我们引入了 Alpamayo-R1 (AR1)，这是一种视觉-语言-动作模型 (VLA)，它将因果链推理与轨迹规划相结合，以增强复杂驾驶场景中的决策。我们的方法具有三个关键创新：（1）因果链（CoC）数据集，通过混合自动标记和人机循环管道构建，产生与驾驶行为一致的基于决策、因果关联的推理轨迹；(2) 模块化 VLA 架构，结合了 Cosmos-Reason（一种针对物理 AI 应用预先训练的视觉语言模型）和基于扩散的轨迹解码器，可实时生成动态可行的计划；（3）多阶段训练策略，使用监督微调来引发推理和强化学习（RL），通过大型推理模型反馈来优化推理质量并强制推理-动作一致性。评估显示，与仅使用轨迹的基线相比，AR1 在挑战性情况下的规划精度提高了 12%，在闭环模拟中越野率降低了 35%，近距离遭遇率降低了 25%。根据大型推理模型批评家的测量，强化学习后训练将推理质量提高了 45%，推理-动作一致性提高了 37%。模型从 0.5B 参数缩放到 7B 参数显示出一致的改进。车载道路测试证实了实时性能（99 毫秒延迟）和成功的城市部署。通过将可解释推理与精确控制结合起来，AR1 展示了通往 4 级自动驾驶的实用道路。我们计划在未来的更新中发布 AR1 模型和 CoC 的子集。"
        },
        {
          "title": "$π_\\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.25889v2",
          "snippet": "Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (\\eg, $π_0$, $π_{0.5}$) remains challenging due to intractable action log-likelihoods from iterative denoising. We address this challenge with $π_{\\texttt{RL}}$, an open-source framework for training flow-based VLAs in parallel simulation. $π_{\\texttt{RL}}$ implements two RL algorithms: (1) \\textbf{Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) \\textbf{Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration. We evaluate $π_{\\texttt{RL}}$ on LIBERO, ManiSkill, and MetaWorld benchmarks. On LIBERO, $π_{\\texttt{RL}}$ boosts few-shot SFT models $π_0$ and $π_{0.5}$ from 57.6\\% to 97.6\\% and from 77.1\\% to 98.3\\%, respectively. On ManiSkill, we train $π_{\\texttt{RL}}$ in 320 parallel environments, improving $π_0$ from 38.4\\% to 78.8\\% and $π_{0.5}$ from 40.1\\% to 90.8\\% across 4352 variations of pick-and-place task. On MetaWorld, RL is conducted over 50 different manipulation tasks and yields performance gains of 35.0\\% and 26.9\\% for $π_0$ and $π_{0.5}$ models, respectively. Overall, $π_{\\texttt{RL}}$ achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs.",
          "site": "arxiv.org",
          "rank": 24,
          "published": "2025-10-29T18:37:39Z",
          "authors": [
            "Kang Chen",
            "Zhihao Liu",
            "Tonghe Zhang",
            "Zhen Guo",
            "Si Xu",
            "Hao Lin",
            "Hongzhi Zang",
            "Xiang Li",
            "Quanlu Zhang",
            "Zhaofei Yu",
            "Guoliang Fan",
            "Tiejun Huang",
            "Yu Wang",
            "Chao Yu"
          ],
          "arxiv_id": "2510.25889",
          "abstract": "Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (\\eg, $π_0$, $π_{0.5}$) remains challenging due to intractable action log-likelihoods from iterative denoising. We address this challenge with $π_{\\texttt{RL}}$, an open-source framework for training flow-based VLAs in parallel simulation. $π_{\\texttt{RL}}$ implements two RL algorithms: (1) \\textbf{Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) \\textbf{Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration. We evaluate $π_{\\texttt{RL}}$ on LIBERO, ManiSkill, and MetaWorld benchmarks. On LIBERO, $π_{\\texttt{RL}}$ boosts few-shot SFT models $π_0$ and $π_{0.5}$ from 57.6\\% to 97.6\\% and from 77.1\\% to 98.3\\%, respectively. On ManiSkill, we train $π_{\\texttt{RL}}$ in 320 parallel environments, improving $π_0$ from 38.4\\% to 78.8\\% and $π_{0.5}$ from 40.1\\% to 90.8\\% across 4352 variations of pick-and-place task. On MetaWorld, RL is conducted over 50 different manipulation tasks and yields performance gains of 35.0\\% and 26.9\\% for $π_0$ and $π_{0.5}$ models, respectively. Overall, $π_{\\texttt{RL}}$ achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型使机器人能够理解并执行来自多模式输入的复杂任务。尽管最近的工作探索使用强化学习 (RL) 来自动化扩展监督微调 (SFT) 中繁琐的数据收集过程，但由于迭代去噪带来的棘手的动作对数似然，将大规模 RL 应用于基于流的 VLA（例如 $π_0$、$π_{0.5}$）仍然具有挑战性。我们使用 $π_{\\texttt{RL}}$ 来应对这一挑战，这是一个开源框架，用于在并行模拟中训练基于流的 VLA。$π_{\\texttt{RL}}$ 实现两种强化学习算法： (1) \\textbf{Flow-Noise} 将去噪过程建模为离散时间 MDP，并使用可学习噪声网络进行精确的对数似然计算。(2) \\textbf{Flow-SDE} 将去噪与智能体-环境交互相结合，制定了一个两层 MDP，利用 ODE 到 SDE 的转换来实现高效的 RL 探索。我们在 LIBERO、ManiSkill 和 MetaWorld 基准上评估 $π_{\\texttt{RL}}$。在 LIBERO 上，$π_{\\texttt{RL}}$ 将小样本 SFT 模型 $π_0$ 和 $π_{0.5}$ 分别从 57.6\\% 提升到 97.6\\% 和从 77.1\\% 提升到 98.3\\%。在 ManiSkill 上，我们在 320 个并行环境中训练 $π_{\\texttt{RL}}$，在 4352 个拾放任务变体中，将 $π_0$ 从 38.4% 提高到 78.8%，将 $π_{0.5}$ 从 40.1% 提高到 90.8%。在 MetaWorld 上，强化学习执行了 50 多种不同的操作任务，$π_0$ 和 $π_{0.5}$ 模型的性能分别提高了 35.0\\% 和 26.9\\%。总体而言，$π_{\\texttt{RL}}$ 比 SFT 模型实现了显着的性能提升和更强的泛化能力，验证了基于流的 VLA 的在线 RL 的有效性。"
        },
        {
          "title": "Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization",
          "url": "http://arxiv.org/abs/2510.25616v1",
          "snippet": "The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying a foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct a systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLA's hidden representations and analyze attention maps, further, we design a set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate a range of strategies for aligning visual representations and introduce a simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: https://blind-vla-paper.github.io",
          "site": "arxiv.org",
          "rank": 25,
          "published": "2025-10-29T15:20:10Z",
          "authors": [
            "Nikita Kachaev",
            "Mikhail Kolosov",
            "Daniil Zelezetsky",
            "Alexey K. Kovalev",
            "Aleksandr I. Panov"
          ],
          "arxiv_id": "2510.25616",
          "abstract": "The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying a foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct a systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLA's hidden representations and analyze attention maps, further, we design a set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate a range of strategies for aligning visual representations and introduce a simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: https://blind-vla-paper.github.io",
          "abstract_zh": "视觉语言动作（VLA）模型的日益成功源于这样的承诺：预训练的视觉语言模型（VLM）可以赋予智能体可转移的世界知识和视觉语言（VL）基础，为具有更广泛泛化能力的动作模型奠定基础。然而，当这些 VLM 适应行动模式时，仍不清楚它们原始的 VL 表示和知识在多大程度上得以保留。在这项工作中，我们对 VLA 微调过程中的表示保留进行了系统研究，表明朴素动作微调会导致视觉表示的退化。为了表征和测量这些效果，我们探究了 VLA 的隐藏表示并分析了注意力图，此外，我们设计了一组有针对性的任务和方法，将 VLA 模型与其对应的 VLM 进行对比，隔离由动作微调引起的 VL 能力的变化。我们进一步评估了一系列对齐视觉表示的策略，并引入了一种简单而有效的方法，可以减轻退化并提高对分布外（OOD）场景的泛化能力。总而言之，我们的分析阐明了动作微调和 VL 表示退化之间的权衡，并强调了恢复继承的 VL 功能的实用方法。代码公开：https://blind-vla-paper.github.io"
        },
        {
          "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities",
          "url": "http://arxiv.org/abs/2510.27545v1",
          "snippet": "Implicit policies parameterized by generative models, such as Diffusion Policy, have become the standard for policy learning and Vision-Language-Action (VLA) models in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) address these issues by learning energy landscapes end-to-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce a new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, a 50x reduction compared to Diffusion Policy's 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers a promising path toward robust, generalizable robot behavior under distribution shifts.",
          "site": "arxiv.org",
          "rank": 26,
          "published": "2025-10-31T15:21:05Z",
          "authors": [
            "Travis Davies",
            "Yiqi Huang",
            "Alexi Gladstone",
            "Yunxin Liu",
            "Xiang Chen",
            "Heng Ji",
            "Huxian Liu",
            "Luhui Hu"
          ],
          "arxiv_id": "2510.27545",
          "abstract": "Implicit policies parameterized by generative models, such as Diffusion Policy, have become the standard for policy learning and Vision-Language-Action (VLA) models in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) address these issues by learning energy landscapes end-to-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce a new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, a 50x reduction compared to Diffusion Policy's 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers a promising path toward robust, generalizable robot behavior under distribution shifts.",
          "abstract_zh": "由生成模型参数化的隐式策略（例如扩散策略）已成为机器人技术中策略学习和视觉-语言-动作（VLA）模型的标准。然而，这些方法通常面临计算成本高、暴露偏差和推理动态不稳定的问题，从而导致分布变化下的发散。基于能源的模型 (EBM) 通过端到端学习能源景观和平衡动态建模来解决这些问题，从而提高稳健性并减少暴露偏差。然而，由 EBM 参数化的政策历来难以有效扩展。最近关于基于能量的变压器（EBT）的工作证明了 EBM 在高维空间中的可扩展性，但它们解决物理实体模型中核心挑战的潜力仍未得到充分开发。我们引入了一种新的基于能源的架构，EBT-Policy，它解决了机器人和现实世界环境中的核心问题。在模拟和现实世界的任务中，EBT-Policy 始终优于基于扩散的策略，同时需要较少的训练和推理计算。值得注意的是，在某些任务上，它只需两个推理步骤即可收敛，与 Diffusion Policy 的 100 步相比减少了 50 倍。此外，EBT-Policy 展现了先前模型中未见的新兴功能，例如仅使用行为克隆且无需显式重试训练即可从失败的动作序列中进行零样本恢复。通过利用其标量能量进行不确定性感知推理和动态计算分配，EBT-Policy 为在分布变化下实现稳健、可概括的机器人行为提供了一条有前途的道路。"
        },
        {
          "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning",
          "url": "http://arxiv.org/abs/2510.24161v1",
          "snippet": "Multimodal large language models (MLLMs) have advanced vision-language reasoning and are increasingly deployed in embodied agents. However, significant limitations remain: MLLMs generalize poorly across digital-physical spaces and embodiments; vision-language-action models (VLAs) produce low-level actions yet lack robust high-level embodied reasoning; and most embodied large language models (ELLMs) are constrained to digital-space with poor generalization to the physical world. Thus, unified models that operate seamlessly across digital and physical spaces while generalizing across embodiments and tasks remain absent. We introduce the \\textbf{Boundless Large Model (BLM$_1$)}, a multimodal spatial foundation model that preserves instruction following and reasoning, incorporates embodied knowledge, and supports robust cross-embodiment control. BLM$_1$ integrates three key capabilities -- \\textit{cross-space transfer, cross-task learning, and cross-embodiment generalization} -- via a two-stage training paradigm. Stage I injects embodied knowledge into the MLLM through curated digital corpora while maintaining language competence. Stage II trains a policy module through an intent-bridging interface that extracts high-level semantics from the MLLM to guide control, without fine-tuning the MLLM backbone. This process is supported by a self-collected cross-embodiment demonstration suite spanning four robot embodiments and six progressively challenging tasks. Evaluations across digital and physical benchmarks show that a single BLM$_1$ instance outperforms four model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving $\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical tasks.",
          "site": "arxiv.org",
          "rank": 27,
          "published": "2025-10-28T07:58:39Z",
          "authors": [
            "Wentao Tan",
            "Bowen Wang",
            "Heng Zhi",
            "Chenyu Liu",
            "Zhe Li",
            "Jian Liu",
            "Zengrong Lin",
            "Yukun Dai",
            "Yipeng Chen",
            "Wenjie Yang",
            "Enci Xie",
            "Hao Xue",
            "Baixu Ji",
            "Chen Xu",
            "Zhibin Wang",
            "Tianshi Wang",
            "Lei Zhu",
            "Heng Tao Shen"
          ],
          "arxiv_id": "2510.24161",
          "abstract": "Multimodal large language models (MLLMs) have advanced vision-language reasoning and are increasingly deployed in embodied agents. However, significant limitations remain: MLLMs generalize poorly across digital-physical spaces and embodiments; vision-language-action models (VLAs) produce low-level actions yet lack robust high-level embodied reasoning; and most embodied large language models (ELLMs) are constrained to digital-space with poor generalization to the physical world. Thus, unified models that operate seamlessly across digital and physical spaces while generalizing across embodiments and tasks remain absent. We introduce the \\textbf{Boundless Large Model (BLM$_1$)}, a multimodal spatial foundation model that preserves instruction following and reasoning, incorporates embodied knowledge, and supports robust cross-embodiment control. BLM$_1$ integrates three key capabilities -- \\textit{cross-space transfer, cross-task learning, and cross-embodiment generalization} -- via a two-stage training paradigm. Stage I injects embodied knowledge into the MLLM through curated digital corpora while maintaining language competence. Stage II trains a policy module through an intent-bridging interface that extracts high-level semantics from the MLLM to guide control, without fine-tuning the MLLM backbone. This process is supported by a self-collected cross-embodiment demonstration suite spanning four robot embodiments and six progressively challenging tasks. Evaluations across digital and physical benchmarks show that a single BLM$_1$ instance outperforms four model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving $\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical tasks.",
          "abstract_zh": "多模态大语言模型（MLLM）具有先进的视觉语言推理能力，并且越来越多地部署在实体代理中。然而，仍然存在重大局限性：MLLM 在数字物理空间和实施例中的泛化性很差；视觉-语言-动作模型（VLA）产生低级动作，但缺乏强大的高级具体推理；大多数实体化大型语言模型（ELLM）都局限于数字空间，对物理世界的泛化能力很差。因此，跨数字和物理空间无缝操作同时跨实施例和任务进行概括的统一模型仍然不存在。我们引入了 \\textbf{Boundless Large Model (BLM$_1$)}，这是一种多模态空间基础模型，可以保留指令跟踪和推理，结合具体知识，并支持鲁棒的跨具体控制。BLM$_1$ 通过两阶段训练范例集成了三个关键功能——\\textit{跨空间转移、跨任务学习和跨实施例泛化}。第一阶段通过精心策划的数字语料库将具体知识注入 MLLM，同时保持语言能力。第二阶段通过意图桥接接口训练策略模块，该接口从 MLLM 中提取高级语义来指导控制，而无需微调 MLLM 主干。这个过程由一个自我收集的跨实施例演示套件支持，涵盖四个机器人实施例和六个逐渐具有挑战性的任务。跨数字和物理基准的评估表明，单个 BLM$_1$ 实例的性能优于四个模型系列（MLLM、ELLM、VLA 和 GMLM），在数字任务中实现 $\\sim\\!\\textbf{6%}$ 收益，在物理任务中实现 $\\sim\\!\\textbf{3%}$ 收益。"
        },
        {
          "title": "Token Is All You Need: Cognitive Planning through Belief-Intent Co-Evolution",
          "url": "http://arxiv.org/abs/2511.05540v2",
          "snippet": "We challenge the long-standing assumption that exhaustive scene modeling is required for high-performance end-to-end autonomous driving (E2EAD). Inspired by cognitive science, we propose that effective planning arises not from reconstructing the world, but from the co-evolution of belief and intent within a minimal set of semantically rich tokens. Experiments on the nuPlan benchmark (720 scenarios, 11k+ samples) reveal three principles: (1) sparse intent tokens alone achieve 0.487 m ADE, demonstrating strong performance without future prediction; (2) conditioning trajectory decoding on predicted future tokens reduces ADE to 0.382 m, a 21.6% improvement, showing that performance emerges from cognitive planning; and (3) explicit reconstruction loss degrades performance, confirming that task-driven belief-intent co-evolution suffices under reliable perception inputs. Crucially, we observe the emergence of cognitive consistency: through prolonged training, the model spontaneously develops stable token dynamics that balance current perception (belief) and future goals (intent). This process, accompanied by \"temporal fuzziness,\" enables robustness under uncertainty and continuous self-optimization. Our work establishes a new paradigm: intelligence lies not in pixel fidelity, but in the tokenized duality of belief and intent. By reframing planning as understanding rather than reaction, TIWM bridges the gap between world models and VLA systems, paving the way for foresightful agents that plan through imagination. Note: Numerical comparisons with methods reporting results on nuScenes are indicative only, as nuPlan presents a more challenging planning-focused evaluation.",
          "site": "arxiv.org",
          "rank": 28,
          "published": "2025-10-30T12:16:45Z",
          "authors": [
            "Shiyao Sang"
          ],
          "arxiv_id": "2511.05540",
          "abstract": "We challenge the long-standing assumption that exhaustive scene modeling is required for high-performance end-to-end autonomous driving (E2EAD). Inspired by cognitive science, we propose that effective planning arises not from reconstructing the world, but from the co-evolution of belief and intent within a minimal set of semantically rich tokens. Experiments on the nuPlan benchmark (720 scenarios, 11k+ samples) reveal three principles: (1) sparse intent tokens alone achieve 0.487 m ADE, demonstrating strong performance without future prediction; (2) conditioning trajectory decoding on predicted future tokens reduces ADE to 0.382 m, a 21.6% improvement, showing that performance emerges from cognitive planning; and (3) explicit reconstruction loss degrades performance, confirming that task-driven belief-intent co-evolution suffices under reliable perception inputs. Crucially, we observe the emergence of cognitive consistency: through prolonged training, the model spontaneously develops stable token dynamics that balance current perception (belief) and future goals (intent). This process, accompanied by \"temporal fuzziness,\" enables robustness under uncertainty and continuous self-optimization. Our work establishes a new paradigm: intelligence lies not in pixel fidelity, but in the tokenized duality of belief and intent. By reframing planning as understanding rather than reaction, TIWM bridges the gap between world models and VLA systems, paving the way for foresightful agents that plan through imagination. Note: Numerical comparisons with methods reporting results on nuScenes are indicative only, as nuPlan presents a more challenging planning-focused evaluation.",
          "abstract_zh": "我们挑战了长期以来的假设，即高性能端到端自动驾驶（E2EAD）需要详尽的场景建模。受认知科学的启发，我们提出有效的规划不是源于重建世界，而是源于信念和意图在最小的一组语义丰富的标记内的共同进化。nuPlan 基准测试（720 个场景，11k+ 样本）的实验揭示了三个原理：（1）仅稀疏意图标记就达到了 0.487 m ADE，展示了无需未来预测的强大性能；(2) 对预测的未来标记进行条件轨迹解码，将 ADE 降低至 0.382 m，提高了 21.6%，表明性能来自于认知规划；（3）显式重建损失会降低性能，证实任务驱动的信念意图共同进化在可靠的感知输入下就足够了。至关重要的是，我们观察到认知一致性的出现：通过长时间的训练，模型自发地发展出稳定的令牌动态，以平衡当前的感知（信念）和未来的目标（意图）。这个过程伴随着“时间模糊性”，使得不确定性下的鲁棒性和持续的自我优化成为可能。我们的工作建立了一个新的范式：智能不在于像素保真度，而在于信念和意图的标记化二元性。通过将规划重新定义为理解而不是反应，TIWM 弥合了世界模型和 VLA 系统之间的差距，为有远见的智能体通过想象力进行规划铺平了道路。注意：与报告 nuScenes 结果的方法进行的数值比较仅供参考，因为 nuPlan 提出了更具挑战性的以规划为重点的评估。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 7 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 7）。",
      "items": [
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "FlagOpen/RoboBrain2.0",
          "url": "https://github.com/FlagOpen/RoboBrain2.0",
          "snippet": "RoboBrain 2.0: Advanced version of RoboBrain. See Better. Think Harder. Do Smarter. 🎉🎉🎉",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "SalvatoreRa/ML-news-of-the-week",
          "url": "https://github.com/SalvatoreRa/ML-news-of-the-week",
          "snippet": "A collection of the the best ML and AI news every week (research, news, resources)",
          "site": "github.com",
          "rank": 7
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-10-27",
  "week_end": "2025-11-02",
  "last_updated": "2026-01-07"
}