{
  "generated_at": "2026-01-07T13:35:45.133726",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 28 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 28）。",
      "items": [
        {
          "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
          "url": "http://arxiv.org/abs/2510.07077v1",
          "snippet": "Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io .",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-10-08T14:38:25Z",
          "authors": [
            "Kento Kawaharazuka",
            "Jihoon Oh",
            "Jun Yamada",
            "Ingmar Posner",
            "Yuke Zhu"
          ],
          "arxiv_id": "2510.07077",
          "abstract": "Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io .",
          "abstract_zh": "在利用机器人大语言模型 (LLM) 和视觉语言模型 (VLM) 的进步不断努力的过程中，视觉语言动作 (VLA) 模型最近受到了极大的关注。通过大规模统一传统上单独研究的视觉、语言和动作数据，VLA 模型旨在学习跨不同任务、对象、实施例和环境的泛化策略。这种泛化能力预计将使机器人能够用最少的或不需要额外的特定任务数据来解决新的下游任务，从而促进更灵活和可扩展的实际部署。与之前狭隘地关注动作表示或高级模型架构的调查不同，这项工作提供了全面的全堆栈审查，集成了 VLA 系统的软件和硬件组件。特别是，本文对 VLA 进行了系统回顾，涵盖其策略和架构过渡、架构和构建块、特定模态的处理技术和学习范式。此外，为了支持 VLA 在现实世界机器人应用中的部署，我们还回顾了常用的机器人平台、数据收集策略、公开可用的数据集、数据增强方法和评估基准。通过这项全面的调查，本文旨在为机器人社区将 VLA 应用到现实世界的机器人系统提供实用指导。按训练方法、评估方法、模式和数据集分类的所有参考文献均可在我们项目网站的表格中找到：https://vla-survey.github.io。"
        },
        {
          "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation",
          "url": "http://arxiv.org/abs/2510.11660v2",
          "snippet": "While Vision-Language-Action (VLA) models have demonstrated impressive capabilities in robotic manipulation, their performance in complex reasoning and long-horizon task planning is limited by data scarcity and model capacity. To address this, we introduce ManiAgent, an agentic architecture for general manipulation tasks that achieves end-to-end output from task descriptions and environmental inputs to robotic manipulation actions. In this framework, multiple agents involve inter-agent communication to perform environmental perception, sub-task decomposition and action generation, enabling efficient handling of complex manipulation scenarios. Evaluations show ManiAgent achieves an 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world pick-and-place tasks, enabling efficient data collection that yields VLA models with performance comparable to those trained on human-annotated datasets. The project webpage is available at https://yi-yang929.github.io/ManiAgent/.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-10-13T17:34:48Z",
          "authors": [
            "Yi Yang",
            "Kefan Gu",
            "Yuqing Wen",
            "Hebei Li",
            "Yucheng Zhao",
            "Tiancai Wang",
            "Xudong Liu"
          ],
          "arxiv_id": "2510.11660",
          "abstract": "While Vision-Language-Action (VLA) models have demonstrated impressive capabilities in robotic manipulation, their performance in complex reasoning and long-horizon task planning is limited by data scarcity and model capacity. To address this, we introduce ManiAgent, an agentic architecture for general manipulation tasks that achieves end-to-end output from task descriptions and environmental inputs to robotic manipulation actions. In this framework, multiple agents involve inter-agent communication to perform environmental perception, sub-task decomposition and action generation, enabling efficient handling of complex manipulation scenarios. Evaluations show ManiAgent achieves an 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world pick-and-place tasks, enabling efficient data collection that yields VLA models with performance comparable to those trained on human-annotated datasets. The project webpage is available at https://yi-yang929.github.io/ManiAgent/.",
          "abstract_zh": "虽然视觉-语言-动作（VLA）模型在机器人操作方面表现出了令人印象深刻的能力，但它们在复杂推理和长期任务规划方面的表现受到数据稀缺和模型容量的限制。为了解决这个问题，我们引入了 ManiAgent，这是一种用于一般操作任务的代理架构，可实现从任务描述和环境输入到机器人操作动作的端到端输出。在此框架中，多个智能体涉及智能体间通信来执行环境感知、子任务分解和动作生成，从而能够有效处理复杂的操作场景。评估显示，ManiAgent 在 SimplerEnv 基准测试中实现了 86.8% 的成功率，在现实世界的拾放任务中实现了 95.8% 的成功率，从而实现了高效的数据收集，生成的 VLA 模型的性能可与在人工注释数据集上训练的模型相媲美。该项目网页位于 https://yi-yang929.github.io/ManiAgent/。"
        },
        {
          "title": "Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models",
          "url": "http://arxiv.org/abs/2510.07067v1",
          "snippet": "Vision Language Action (VLA) models are widely used in Embodied AI, enabling robots to interpret and execute language instructions. However, their robustness to natural language variability in real-world scenarios has not been thoroughly investigated. In this work, we present a novel systematic study of the robustness of state-of-the-art VLA models under linguistic perturbations. Specifically, we evaluate model performance under two types of instruction noise: (1) human-generated paraphrasing and (2) the addition of irrelevant context. We further categorize irrelevant contexts into two groups according to their length and their semantic and lexical proximity to robot commands. In this study, we observe consistent performance degradation as context size expands. We also demonstrate that the model can exhibit relative robustness to random context, with a performance drop within 10%, while semantically and lexically similar context of the same length can trigger a quality decline of around 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To mitigate this, we propose an LLM-based filtering framework that extracts core commands from noisy inputs. Incorporating our filtering step allows models to recover up to 98.5% of their original performance under noisy conditions.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-10-08T14:31:35Z",
          "authors": [
            "Daria Pugacheva",
            "Andrey Moskalenko",
            "Denis Shepelev",
            "Andrey Kuznetsov",
            "Vlad Shakhuro",
            "Elena Tutubalina"
          ],
          "arxiv_id": "2510.07067",
          "abstract": "Vision Language Action (VLA) models are widely used in Embodied AI, enabling robots to interpret and execute language instructions. However, their robustness to natural language variability in real-world scenarios has not been thoroughly investigated. In this work, we present a novel systematic study of the robustness of state-of-the-art VLA models under linguistic perturbations. Specifically, we evaluate model performance under two types of instruction noise: (1) human-generated paraphrasing and (2) the addition of irrelevant context. We further categorize irrelevant contexts into two groups according to their length and their semantic and lexical proximity to robot commands. In this study, we observe consistent performance degradation as context size expands. We also demonstrate that the model can exhibit relative robustness to random context, with a performance drop within 10%, while semantically and lexically similar context of the same length can trigger a quality decline of around 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To mitigate this, we propose an LLM-based filtering framework that extracts core commands from noisy inputs. Incorporating our filtering step allows models to recover up to 98.5% of their original performance under noisy conditions.",
          "abstract_zh": "视觉语言动作（VLA）模型广泛应用于嵌入式人工智能中，使机器人能够解释和执行语言指令。然而，它们对现实世界场景中自然语言变异的鲁棒性尚未得到彻底研究。在这项工作中，我们对最先进的 VLA 模型在语言扰动下的鲁棒性进行了一项新颖的系统研究。具体来说，我们在两种类型的指令噪声下评估模型性能：（1）人类生成的释义和（2）添加不相关的上下文。我们根据不相关的上下文的长度以及它们与机器人命令的语义和词汇的接近程度，进一步将不相关的上下文分为两组。在这项研究中，我们观察到随着上下文大小的扩大，性能会持续下降。我们还证明，该模型可以对随机上下文表现出相对鲁棒性，性能下降在 10% 以内，而语义和词汇上相似的相同长度的上下文可能会导致质量下降约 50%。指令的人工释义导致下降近 20%。为了缓解这个问题，我们提出了一个基于 LLM 的过滤框架，可以从嘈杂的输入中提取核心命令。结合我们的过滤步骤，模型可以在噪声条件下恢复高达 98.5% 的原始性能。"
        },
        {
          "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model",
          "url": "http://arxiv.org/abs/2510.10274v1",
          "snippet": "Successful generalist Vision-Language-Action (VLA) models rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders, enjoying both scalability and simplicity. Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep of benchmarks, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks. Website: https://thu-air-dream.github.io/X-VLA/",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-10-11T16:20:17Z",
          "authors": [
            "Jinliang Zheng",
            "Jianxiong Li",
            "Zhihao Wang",
            "Dongxiu Liu",
            "Xirui Kang",
            "Yuchun Feng",
            "Yinan Zheng",
            "Jiayin Zou",
            "Yilun Chen",
            "Jia Zeng",
            "Ya-Qin Zhang",
            "Jiangmiao Pang",
            "Jingjing Liu",
            "Tai Wang",
            "Xianyuan Zhan"
          ],
          "arxiv_id": "2510.10274",
          "abstract": "Successful generalist Vision-Language-Action (VLA) models rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders, enjoying both scalability and simplicity. Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep of benchmarks, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks. Website: https://thu-air-dream.github.io/X-VLA/",
          "abstract_zh": "成功的多面手视觉-语言-动作（VLA）模型依赖于跨不同机器人平台的有效训练，这些平台具有大规模、跨实体、异构数据集。为了促进和利用丰富多样的机器人数据源中的异构性，我们提出了一种新的软提示方法，其添加的参数最少，将提示学习概念融入到跨实施例的机器人学习中，并为每个不同的数据源引入单独的可学习嵌入集。这些嵌入充当特定于实施例的提示，它们统一使 VLA 模型能够有效利用不同的跨实施例特征。我们的新 X-VLA 是一种基于流匹配的简洁 VLA 架构，完全依赖于软提示的标准 Transformer 编码器，具有可扩展性和简单性。经过 6 个模拟和 3 个真实机器人的评估，我们的 0.9B 实例化-X-VLA-0.9B 在一系列基准测试中同时实现了 SOTA 性能，在广泛的功能轴上展示了卓越的结果，从灵活的灵活性到跨实施例、环境和任务的快速适应。网站：https://thu-air-dream.github.io/X-VLA/"
        },
        {
          "title": "VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation",
          "url": "http://arxiv.org/abs/2510.09607v2",
          "snippet": "Vision-Language Action (VLA) models significantly advance robotic manipulation by leveraging the strong perception capabilities of pretrained vision-language models (VLMs). By integrating action modules into these pretrained models, VLA methods exhibit improved generalization. However, training them from scratch is costly. In this work, we propose a simple yet effective distillation-based framework that equips VLMs with action-execution capability by transferring knowledge from pretrained small action models. Our architecture retains the original VLM structure, adding only an action token and a state encoder to incorporate physical inputs. To distill action knowledge, we adopt a two-stage training strategy. First, we perform lightweight alignment by mapping VLM hidden states into the action space of the small action model, enabling effective reuse of its pretrained action decoder and avoiding expensive pretraining. Second, we selectively fine-tune the language model, state encoder, and action modules, enabling the system to integrate multimodal inputs with precise action generation. Specifically, the action token provides the VLM with a direct handle for predicting future actions, while the state encoder allows the model to incorporate robot dynamics not captured by vision alone. This design yields substantial efficiency gains over training large VLA models from scratch. Compared with previous state-of-the-art methods, our method achieves 97.3% average success rate on LIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In real-world experiments across five manipulation tasks, our method consistently outperforms the teacher model, achieving 82.0% success rate (17% improvement), which demonstrate that action distillation effectively enables VLMs to generate precise actions while substantially reducing training costs.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-10-10T17:59:56Z",
          "authors": [
            "Shaoqi Dong",
            "Chaoyou Fu",
            "Haihan Gao",
            "Yi-Fan Zhang",
            "Chi Yan",
            "Chu Wu",
            "Xiaoyu Liu",
            "Yunhang Shen",
            "Jing Huo",
            "Deqiang Jiang",
            "Haoyu Cao",
            "Yang Gao",
            "Xing Sun",
            "Ran He",
            "Caifeng Shan"
          ],
          "arxiv_id": "2510.09607",
          "abstract": "Vision-Language Action (VLA) models significantly advance robotic manipulation by leveraging the strong perception capabilities of pretrained vision-language models (VLMs). By integrating action modules into these pretrained models, VLA methods exhibit improved generalization. However, training them from scratch is costly. In this work, we propose a simple yet effective distillation-based framework that equips VLMs with action-execution capability by transferring knowledge from pretrained small action models. Our architecture retains the original VLM structure, adding only an action token and a state encoder to incorporate physical inputs. To distill action knowledge, we adopt a two-stage training strategy. First, we perform lightweight alignment by mapping VLM hidden states into the action space of the small action model, enabling effective reuse of its pretrained action decoder and avoiding expensive pretraining. Second, we selectively fine-tune the language model, state encoder, and action modules, enabling the system to integrate multimodal inputs with precise action generation. Specifically, the action token provides the VLM with a direct handle for predicting future actions, while the state encoder allows the model to incorporate robot dynamics not captured by vision alone. This design yields substantial efficiency gains over training large VLA models from scratch. Compared with previous state-of-the-art methods, our method achieves 97.3% average success rate on LIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In real-world experiments across five manipulation tasks, our method consistently outperforms the teacher model, achieving 82.0% success rate (17% improvement), which demonstrate that action distillation effectively enables VLMs to generate precise actions while substantially reducing training costs.",
          "abstract_zh": "视觉语言动作 (VLA) 模型利用预训练视觉语言模型 (VLM) 的强大感知能力，显着推进机器人操作。通过将动作模块集成到这些预训练模型中，VLA 方法表现出改进的泛化能力。然而，从头开始培训他们的成本很高。在这项工作中，我们提出了一个简单而有效的基于蒸馏的框架，通过从预先训练的小动作模型中转移知识，为 VLM 配备动作执行能力。我们的架构保留了原始的 VLM 结构，仅添加了一个动作令牌和一个状态编码器来合并物理输入。为了提炼动作知识，我们采用两阶段训练策略。首先，我们通过将 VLM 隐藏状态映射到小动作模型的动作空间来执行轻量级对齐，从而能够有效地重用其预训练的动作解码器并避免昂贵的预训练。其次，我们有选择地微调语言模型、状态编码器和动作模块，使系统能够将多模态输入与精确的动作生成相结合。具体来说，动作令牌为 VLM 提供了预测未来动作的直接句柄，而状态编码器允许模型结合仅由视觉捕获的机器人动力学。与从头开始训练大型 VLA 模型相比，这种设计可显着提高效率。与之前最先进的方法相比，我们的方法在 LIBERO 上实现了 97.3% 的平均成功率（提高了 11.8%），在 LIBERO-LONG 上实现了 93.5% 的平均成功率（提高了 24.5%）。在五个操作任务的真实实验中，我们的方法始终优于教师模型，成功率达到 82.0%（提高了 17%），这表明动作蒸馏有效地使 VLM 能够生成精确的动作，同时大幅降低训练成本。"
        },
        {
          "title": "USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots",
          "url": "http://arxiv.org/abs/2510.07869v3",
          "snippet": "Underwater environments present unique challenges for robotic operation, including complex hydrodynamics, limited visibility, and constrained communication. Although data-driven approaches have advanced embodied intelligence in terrestrial robots and enabled task-specific autonomous underwater robots, developing underwater intelligence capable of autonomously performing multiple tasks remains highly challenging, as large-scale, high-quality underwater datasets are still scarce. To address these limitations, we introduce USIM, a simulation-based multi-task Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over 561K frames from 1,852 trajectories, totaling approximately 15.6 hours of BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from visual navigation to mobile manipulation. Building upon this dataset, we propose U0, a VLA model for general underwater robots, which integrates binocular vision and other sensor modalities through multimodal fusion, and further incorporates a convolution-attention-based perception focus enhancement module (CAP) to improve spatial understanding and mobile manipulation. Across tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking, the framework achieves a success rate of 80%, while in challenging mobile manipulation tasks, it reduces the distance to the target by 21.2% compared with baseline methods, demonstrating its effectiveness. USIM and U0 show that VLA models can be effectively applied to underwater robotic applications, providing a foundation for scalable dataset construction, improved task autonomy, and the practical realization of intelligent general underwater robots.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-10-09T07:19:29Z",
          "authors": [
            "Junwen Gu",
            "Zhiheng Wu",
            "Pengxuan Si",
            "Shuang Qiu",
            "Yukai Feng",
            "Luoyang Sun",
            "Laien Luo",
            "Lianyi Yu",
            "Jian Wang",
            "Zhengxing Wu"
          ],
          "arxiv_id": "2510.07869",
          "abstract": "Underwater environments present unique challenges for robotic operation, including complex hydrodynamics, limited visibility, and constrained communication. Although data-driven approaches have advanced embodied intelligence in terrestrial robots and enabled task-specific autonomous underwater robots, developing underwater intelligence capable of autonomously performing multiple tasks remains highly challenging, as large-scale, high-quality underwater datasets are still scarce. To address these limitations, we introduce USIM, a simulation-based multi-task Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over 561K frames from 1,852 trajectories, totaling approximately 15.6 hours of BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from visual navigation to mobile manipulation. Building upon this dataset, we propose U0, a VLA model for general underwater robots, which integrates binocular vision and other sensor modalities through multimodal fusion, and further incorporates a convolution-attention-based perception focus enhancement module (CAP) to improve spatial understanding and mobile manipulation. Across tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking, the framework achieves a success rate of 80%, while in challenging mobile manipulation tasks, it reduces the distance to the target by 21.2% compared with baseline methods, demonstrating its effectiveness. USIM and U0 show that VLA models can be effectively applied to underwater robotic applications, providing a foundation for scalable dataset construction, improved task autonomy, and the practical realization of intelligent general underwater robots.",
          "abstract_zh": "水下环境给机器人操作带来了独特的挑战，包括复杂的流体动力学、有限的能见度和受限的通信。尽管数据驱动的方法在陆地机器人中实现了先进的体现智能，并实现了特定任务的自主水下机器人，但开发能够自主执行多项任务的水下智能仍然极具挑战性，因为大规模、高质量的水下数据集仍然稀缺。为了解决这些限制，我们引入了 USIM，这是一种用于水下机器人的基于模拟的多任务视觉-语言-动作 (VLA) 数据集。USIM 包含来自 1,852 条轨迹的超过 561K 帧，总计约 15.6 小时的 BlueROV2 交互，涉及 9 个不同场景（从视觉导航到移动操纵）的 20 项任务。在此数据集的基础上，我们提出了U0，一种用于通用水下机器人的VLA模型，它通过多模态融合集成了双目视觉和其他传感器模态，并进一步结合了基于卷积注意力的感知焦点增强模块（CAP）以改善空间理解和移动操纵。在检查、避障、扫描和动态跟踪等任务中，该框架实现了 80% 的成功率，而在具有挑战性的移动操纵任务中，与基线方法相比，与目标的距离减少了 21.2%，证明了其有效性。USIM和U0表明，VLA模型可以有效地应用于水下机器人应用，为可扩展的数据集构建、提高任务自主性以及智能通用水下机器人的实际实现提供基础。"
        },
        {
          "title": "FORGE-Tree: Diffusion-Forcing Tree Search for Long-Horizon Robot Manipulation",
          "url": "http://arxiv.org/abs/2510.21744v1",
          "snippet": "Long-horizon robot manipulation tasks remain challenging for Vision-Language-Action (VLA) policies due to drift and exposure bias, often denoise the entire trajectory with fixed hyperparameters, causing small geometric errors to compound across stages and offering no mechanism to allocate extra test-time compute where clearances are tight. To address these challenges, we introduce FORGE-Tree, a plug-in control layer that couples a stage-aligned Diffusion Forcing (DF) head with test-time Monte Carlo Tree Diffusion (MCTD). With a frozen VLA encoder, DF aligns timesteps to subtask stages; during inference we partially denoise only a target segment while keeping other tokens frozen, turning trajectory refinement into a sequence of local edits. We then apply Monte Carlo Tree Diffusion to select the next segment to refine. A scene graph supplies priors for expansion and geometry relation-aware scoring for rollouts, yielding tree-structured denoising whose performance scales with search budget while preserving the executed prefix. Evaluation on LIBERO, FORGE-Tree improves success rate by 13.4 to 17.2 pp over the native VLA baselines with both OpenVLA and Octo-Base. Gains remain consistent under comparable compute budgets, especially on long-horizon variants. Videos available at: https://taco-group.github.io/FORGE-Tree/",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-10-07T04:50:22Z",
          "authors": [
            "Yanjia Huang",
            "Shuo Liu",
            "Sheng Liu",
            "Qingxiao Xu",
            "Mingyang Wu",
            "Xiangbo Gao",
            "Zhengzhong Tu"
          ],
          "arxiv_id": "2510.21744",
          "abstract": "Long-horizon robot manipulation tasks remain challenging for Vision-Language-Action (VLA) policies due to drift and exposure bias, often denoise the entire trajectory with fixed hyperparameters, causing small geometric errors to compound across stages and offering no mechanism to allocate extra test-time compute where clearances are tight. To address these challenges, we introduce FORGE-Tree, a plug-in control layer that couples a stage-aligned Diffusion Forcing (DF) head with test-time Monte Carlo Tree Diffusion (MCTD). With a frozen VLA encoder, DF aligns timesteps to subtask stages; during inference we partially denoise only a target segment while keeping other tokens frozen, turning trajectory refinement into a sequence of local edits. We then apply Monte Carlo Tree Diffusion to select the next segment to refine. A scene graph supplies priors for expansion and geometry relation-aware scoring for rollouts, yielding tree-structured denoising whose performance scales with search budget while preserving the executed prefix. Evaluation on LIBERO, FORGE-Tree improves success rate by 13.4 to 17.2 pp over the native VLA baselines with both OpenVLA and Octo-Base. Gains remain consistent under comparable compute budgets, especially on long-horizon variants. Videos available at: https://taco-group.github.io/FORGE-Tree/",
          "abstract_zh": "由于漂移和曝光偏差，长视野机器人操作任务对于视觉-语言-动作（VLA）策略仍然具有挑战性，通常使用固定的超参数对整个轨迹进行降噪，导致小的几何误差在阶段之间复合，并且没有提供在间隙紧张的情况下分配额外测试时间计算的机制。为了应对这些挑战，我们引入了 FORGE-Tree，这是一个插件控制层，它将阶段对齐的扩散强迫 (DF) 头与测试时蒙特卡罗树扩散 (MCTD) 相结合。使用冻结的 VLA 编码器，DF 将时间步与子任务阶段对齐；在推理过程中，我们仅对目标片段进行部分去噪，同时保持其他标记冻结，将轨迹细化转变为一系列本地编辑。然后，我们应用蒙特卡罗树扩散来选择下一个要细化的片段。场景图为推出提供扩展和几何关系感知评分的先验，产生树结构去噪，其性能随搜索预算扩展，同时保留执行的前缀。对 LIBERO、FORGE-Tree 的评估将 OpenVLA 和 Octo-Base 的本机 VLA 基线的成功率提高了 13.4 至 17.2 个百分点。在可比较的计算预算下，收益保持一致，尤其是在长期变体上。视频位于：https://taco-group.github.io/FORGE-Tree/"
        },
        {
          "title": "RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model",
          "url": "http://arxiv.org/abs/2510.10975v2",
          "snippet": "Vision-Language-Action (VLA) models have become a prominent paradigm for embodied intelligence, yet further performance improvements typically rely on scaling up training data and model size -- an approach that is prohibitively expensive for robotics and fundamentally limited by data collection costs. We address this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling framework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a Test-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA models without modifying their architectures or weights. Specifically, RoVer (i) assigns scalar-based process rewards to evaluate the reliability of candidate actions, and (ii) predicts an action-space direction for candidate expansion/refinement. During inference, RoVer generates multiple candidate actions concurrently from the base policy, expands them along PRM-predicted directions, and then scores all candidates with PRM to select the optimal action for execution. Notably, by caching shared perception features, it can amortize perception cost and evaluate more candidates under the same test-time computational budget. Essentially, our approach effectively transforms available computing resources into better action decision-making, realizing the benefits of test-time scaling without extra training overhead. Our contributions are threefold: (1) a general, plug-and-play test-time scaling framework for VLAs; (2) a PRM that jointly provides scalar process rewards and an action-space direction to guide exploration; and (3) an efficient direction-guided sampling strategy that leverages a shared perception cache to enable scalable candidate generation and selection during inference.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-10-13T03:26:14Z",
          "authors": [
            "Mingtong Dai",
            "Lingbo Liu",
            "Yongjie Bai",
            "Yang Liu",
            "Zhouxia Wang",
            "Rui SU",
            "Chunjie Chen",
            "Liang Lin",
            "Xinyu Wu"
          ],
          "arxiv_id": "2510.10975",
          "abstract": "Vision-Language-Action (VLA) models have become a prominent paradigm for embodied intelligence, yet further performance improvements typically rely on scaling up training data and model size -- an approach that is prohibitively expensive for robotics and fundamentally limited by data collection costs. We address this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling framework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a Test-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA models without modifying their architectures or weights. Specifically, RoVer (i) assigns scalar-based process rewards to evaluate the reliability of candidate actions, and (ii) predicts an action-space direction for candidate expansion/refinement. During inference, RoVer generates multiple candidate actions concurrently from the base policy, expands them along PRM-predicted directions, and then scores all candidates with PRM to select the optimal action for execution. Notably, by caching shared perception features, it can amortize perception cost and evaluate more candidates under the same test-time computational budget. Essentially, our approach effectively transforms available computing resources into better action decision-making, realizing the benefits of test-time scaling without extra training overhead. Our contributions are threefold: (1) a general, plug-and-play test-time scaling framework for VLAs; (2) a PRM that jointly provides scalar process rewards and an action-space direction to guide exploration; and (3) an efficient direction-guided sampling strategy that leverages a shared perception cache to enable scalable candidate generation and selection during inference.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型已成为体现智能的重要范例，但进一步的性能改进通常依赖于扩大训练数据和模型大小，这种方法对于机器人技术而言成本高昂，并且从根本上受到数据收集成本的限制。我们通过 $\\mathbf{RoVer}$ 解决了这一限制，这是一个具体的测试时间扩展框架，它使用 $\\mathbf{Ro}$bot 过程奖励模型（PRM）作为测试时间 $\\mathbf{Ver}$ifier 来增强现有 VLA 模型的功能，而无需修改其架构或权重。具体来说，RoVer (i) 分配基于标量的过程奖励来评估候选动作的可靠性，以及 (ii) 预测候选扩展/细化的动作空间方向。在推理过程中，RoVer 根据基本策略同时生成多个候选操作，将它们沿着 PRM 预测的方向扩展，然后使用 PRM 对所有候选操作进行评分，以选择最佳执行操作。值得注意的是，通过缓存共享感知特征，它可以分摊感知成本并在相同的测试时间计算预算下评估更多候选者。本质上，我们的方法有效地将可用的计算资源转化为更好的行动决策，实现测试时间扩展的好处，而无需额外的培训开销。我们的贡献有三方面：(1) 通用的、即插即用的 VLA 测试时间扩展框架；(2) 联合提供标量过程奖励和指导探索的动作空间方向的 PRM；(3) 高效的方向引导采样策略，利用共享感知缓存来在推理过程中实现可扩展的候选生成和选择。"
        },
        {
          "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
          "url": "http://arxiv.org/abs/2510.11027v1",
          "snippet": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-10-13T05:51:22Z",
          "authors": [
            "Ganlin Yang",
            "Tianyi Zhang",
            "Haoran Hao",
            "Weiyun Wang",
            "Yibin Liu",
            "Dehui Wang",
            "Guanzhou Chen",
            "Zijian Cai",
            "Junting Chen",
            "Weijie Su",
            "Wengang Zhou",
            "Yu Qiao",
            "Jifeng Dai",
            "Jiangmiao Pang",
            "Gen Luo",
            "Wenhai Wang",
            "Yao Mu",
            "Zhi Hou"
          ],
          "arxiv_id": "2510.11027",
          "abstract": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.",
          "abstract_zh": "虽然重要的研究集中在使用视觉语言模型（VLM）开发体现推理能力或将先进的 VLM 集成到视觉语言动作（VLA）模型中以实现端到端机器人控制，但很少有研究直接解决上游基于 VLM 的推理和下游 VLA 策略学习之间的关键差距。在这项工作中，我们通过引入 Vlaser - 一种具有协同体现推理能力的视觉语言动作模型，迈出了桥接体现推理与 VLA 策略学习的第一步，它是一种基础视觉语言模型，旨在将高级推理与体现代理的低级控制相结合。Vlaser 基于高质量的 Vlaser-6M 数据集而构建，在一系列具身推理基准上实现了最先进的性能 - 包括空间推理、具身基础、具身 QA 和任务规划。此外，我们系统地研究了不同的 VLM 初始化如何影响有监督的 VLA 微调，为减轻互联网规模的预训练数据和具体体现的策略学习数据之间的领域转移提供了新颖的见解。基于这些见解，我们的方法在 WidowX 基准测试中取得了最先进的结果，在 Google Robot 基准测试中取得了具有竞争力的性能。"
        },
        {
          "title": "Verifier-free Test-Time Sampling for Vision Language Action Models",
          "url": "http://arxiv.org/abs/2510.05681v1",
          "snippet": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-10-07T08:38:08Z",
          "authors": [
            "Suhyeok Jang",
            "Dongyoung Kim",
            "Changyeon Kim",
            "Youngsuk Kim",
            "Jinwoo Shin"
          ],
          "arxiv_id": "2510.05681",
          "abstract": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.",
          "abstract_zh": "视觉-语言-动作模型（VLA）在机器人控制中表现出了卓越的性能。然而，由于其单一推理范式，它们在需要高精度的任务中仍然受到根本限制。虽然使用外部验证器的测试时间扩展方法已显示出希望，但它们需要额外的培训，并且无法推广到未见过的条件。我们提出掩蔽分布引导选择（MG-Select），这是一种新颖的 VLA 测试时间扩展框架，它利用模型的内部属性，无需额外的训练或外部模块。我们的方法利用参考动作标记分布的 KL 散度作为从多个候选者中选择最佳动作的置信度度量。我们引入了由相同 VLA 生成的参考分布，但以随机屏蔽状态和语言条件作为输入，确保最大的不确定性，同时保持与目标任务分布一致。此外，我们提出了一种联合训练策略，使模型能够通过将 dropout 应用于状态和语言条件来学习条件和无条件分布，从而进一步提高参考分布的质量。我们的实验表明，MG-Select 实现了显着的性能改进，包括现实世界中分布内/分布外任务的性能提高了 28%/35%，并且经过 30 次演示训练后，RoboCasa 拾放任务的相对增益提高了 168%。"
        },
        {
          "title": "IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction",
          "url": "http://arxiv.org/abs/2510.07778v1",
          "snippet": "Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions. Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions. To overcome these limitations, we propose \\textbf{IntentionVLA}, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage, IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling fast inference under indirect instructions. Experimental results show that IntentionVLA substantially outperforms $π_0$, achieving 18\\% higher success rates with direct instructions and 28\\% higher than ECoT under intention instructions. On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40\\% success rate. These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-10-09T04:49:46Z",
          "authors": [
            "Yandu Chen",
            "Kefan Gu",
            "Yuqing Wen",
            "Yucheng Zhao",
            "Tiancai Wang",
            "Liqiang Nie"
          ],
          "arxiv_id": "2510.07778",
          "abstract": "Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions. Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions. To overcome these limitations, we propose \\textbf{IntentionVLA}, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage, IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling fast inference under indirect instructions. Experimental results show that IntentionVLA substantially outperforms $π_0$, achieving 18\\% higher success rates with direct instructions and 28\\% higher than ECoT under intention instructions. On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40\\% success rate. These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems.",
          "abstract_zh": "视觉-语言-动作（VLA）模型利用预训练的视觉-语言模型（VLM）将感知与机器人控制结合起来，为通用的体现智能提供了一条有希望的道路。然而，当前的 SOTA VLA 主要针对与具体场景相关性有限的多模态任务进行预训练，然后进行微调以将显式指令映射到操作。因此，由于缺乏推理密集型预训练和推理引导操作，这些模型无法执行复杂的现实世界交互所需的隐式人类意图推理。为了克服这些限制，我们提出了 \\textbf{IntentionVLA}，一个具有课程训练范式和高效推理机制的 VLA 框架。我们提出的方法首先利用精心设计的推理数据，结合意图推理、空间基础和紧凑的具体推理，赋予模型推理和感知能力。在接下来的微调阶段，IntentionVLA采用紧凑推理输出作为动作生成的上下文指导，从而实现间接指令下的快速推理。实验结果表明，IntentionVLA 的性能明显优于 $π_0$，直接指令下的成功率高出 18%，意图指令下的成功率比 ECoT 高 28%。在分布外意图任务上，IntentionVLA 的成功率是所有基线的两倍以上，并进一步实现了零样本人机交互，成功率达到 40%。这些结果凸显了 IntentionVLA 作为下一代人机交互 (HRI) 系统的有前景的范例。"
        },
        {
          "title": "Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects",
          "url": "http://arxiv.org/abs/2510.09269v1",
          "snippet": "Recent advances in vision-language-action (VLA) models have greatly improved embodied AI, enabling robots to follow natural language instructions and perform diverse tasks. However, their reliance on uncurated training datasets raises serious security concerns. Existing backdoor attacks on VLAs mostly assume white-box access and result in task failures instead of enforcing specific actions. In this work, we reveal a more practical threat: attackers can manipulate VLAs by simply injecting physical objects as triggers into the training dataset. We propose goal-oriented backdoor attacks (GoBA), where the VLA behaves normally in the absence of physical triggers but executes predefined and goal-oriented actions in the presence of physical triggers. Specifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO that incorporates diverse physical triggers and goal-oriented backdoor actions. In addition, we propose a three-level evaluation that categorizes the victim VLA's actions under GoBA into three states: nothing to do, try to do, and success to do. Experiments show that GoBA enables the victim VLA to successfully achieve the backdoor goal in 97 percentage of inputs when the physical trigger is present, while causing zero performance degradation on clean inputs. Finally, by investigating factors related to GoBA, we find that the action trajectory and trigger color significantly influence attack performance, while trigger size has surprisingly little effect. The code and BadLIBERO dataset are accessible via the project page at https://goba-attack.github.io/.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-10-10T11:09:36Z",
          "authors": [
            "Zirun Zhou",
            "Zhengyang Xiao",
            "Haochuan Xu",
            "Jing Sun",
            "Di Wang",
            "Jingfeng Zhang"
          ],
          "arxiv_id": "2510.09269",
          "abstract": "Recent advances in vision-language-action (VLA) models have greatly improved embodied AI, enabling robots to follow natural language instructions and perform diverse tasks. However, their reliance on uncurated training datasets raises serious security concerns. Existing backdoor attacks on VLAs mostly assume white-box access and result in task failures instead of enforcing specific actions. In this work, we reveal a more practical threat: attackers can manipulate VLAs by simply injecting physical objects as triggers into the training dataset. We propose goal-oriented backdoor attacks (GoBA), where the VLA behaves normally in the absence of physical triggers but executes predefined and goal-oriented actions in the presence of physical triggers. Specifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO that incorporates diverse physical triggers and goal-oriented backdoor actions. In addition, we propose a three-level evaluation that categorizes the victim VLA's actions under GoBA into three states: nothing to do, try to do, and success to do. Experiments show that GoBA enables the victim VLA to successfully achieve the backdoor goal in 97 percentage of inputs when the physical trigger is present, while causing zero performance degradation on clean inputs. Finally, by investigating factors related to GoBA, we find that the action trajectory and trigger color significantly influence attack performance, while trigger size has surprisingly little effect. The code and BadLIBERO dataset are accessible via the project page at https://goba-attack.github.io/.",
          "abstract_zh": "视觉-语言-动作（VLA）模型的最新进展极大地改进了具体人工智能，使机器人能够遵循自然语言指令并执行不同的任务。然而，他们对未经整理的训练数据集的依赖引发了严重的安全问题。现有的针对 VLA 的后门攻击大多采用白盒访问并导致任务失败，而不是强制执行特定操作。在这项工作中，我们揭示了一个更实际的威胁：攻击者可以通过简单地将物理对象作为触发器注入训练数据集中来操纵 VLA。我们提出面向目标的后门攻击（GoBA），其中 VLA 在没有物理触发的情况下表现正常，但在存在物理触发的情况下执行预定义的目标导向的操作。具体来说，基于流行的 VLA 基准 LIBERO，我们引入了 BadLIBERO，它结合了多种物理触发器和面向目标的后门操作。此外，我们提出了三级评估，将受害者 VLA 在 GoBA 下的行为分为三种状态：无事可做、尝试做和成功做。实验表明，当存在物理触发器时，GoBA 使受害者 VLA 能够在 97% 的输入中成功实现后门目标，同时对干净输入造成零性能下降。最后，通过调查与GoBA相关的因素，我们发现动作轨迹和扳机颜色显着影响攻击性能，而扳机大小的影响却出人意料地小。代码和 BadLIBERO 数据集可通过项目页面 https://goba-attack.github.io/ 访问。"
        },
        {
          "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
          "url": "http://arxiv.org/abs/2510.04898v1",
          "snippet": "Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\\times$, and accelerates inference speed by $120\\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-10-06T15:15:38Z",
          "authors": [
            "Zheng Xiong",
            "Kang Li",
            "Zilin Wang",
            "Matthew Jackson",
            "Jakob Foerster",
            "Shimon Whiteson"
          ],
          "arxiv_id": "2510.04898",
          "abstract": "Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\\times$, and accelerates inference speed by $120\\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA",
          "abstract_zh": "视觉-语言-动作（VLA）模型建立在具有强大泛化能力的语言和视觉基础模型之上，并经过大规模机器人数据的训练，最近已成为学习通用机器人策略的一种有前途的方法。然而，现有 VLA 的一个主要缺点是其推理成本极高。在本文中，我们提出 HyperVLA 来解决这个问题。与在训练和推理期间激活整个模型的现有整体 VLA 不同，HyperVLA 使用一种新颖的基于超网络 (HN) 的架构，该架构在推理期间仅激活小型特定于任务的策略，同时仍然保留在训练期间适应不同多任务行为所需的高模型容量。成功训练基于 HN 的 VLA 并非易事，因此 HyperVLA 包含几个可提高其性能的关键算法设计功能，包括正确利用现有视觉基础模型的先验知识、HN 标准化和动作生成策略。与单片 VLA 相比，HyperVLA 在零样本泛化和少样本自适应方面实现了相似甚至更高的成功率，同时显着降低了推理成本。与最先进的 VLA 模型 OpenVLA 相比，HyperVLA 将测试时激活的参数数量减少了 90 倍，推理速度提高了 120 倍。代码可在 https://github.com/MasterXiong/HyperVLA 公开获取"
        },
        {
          "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
          "url": "http://arxiv.org/abs/2510.06710v1",
          "snippet": "Recent progress in vision and language foundation models has significantly advanced multimodal understanding, reasoning, and generation, inspiring a surge of interest in extending such capabilities to embodied settings through vision-language-action (VLA) models. Yet, most VLA models are still trained with supervised fine-tuning (SFT), which struggles to generalize under distribution shifts due to error accumulation. Reinforcement learning (RL) offers a promising alternative by directly optimizing task performance through interaction, but existing attempts remain fragmented and lack a unified platform for fair and systematic comparison across model architectures and algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models. The system adopts a highly flexible resource allocation design that addresses the challenge of integrating rendering, training, and inference in RL+VLA training. In particular, for GPU-parallelized simulators, RLinf-VLA implements a novel hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup in training. Through a unified interface, RLinf-VLA seamlessly supports diverse VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g., PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a unified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25 ManiSkill tasks. Beyond empirical performance, our study distills a set of best practices for applying RL to VLA training and sheds light on emerging patterns in this integration. Furthermore, we present preliminary deployment on a real-world Franka robot, where RL-trained policies exhibit stronger generalization than those trained with SFT. We envision RLinf-VLA as a foundation to accelerate and standardize research on embodied intelligence.",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-10-08T07:05:13Z",
          "authors": [
            "Hongzhi Zang",
            "Mingjie Wei",
            "Si Xu",
            "Yongji Wu",
            "Zhen Guo",
            "Yuanqing Wang",
            "Hao Lin",
            "Liangzhi Shi",
            "Yuqing Xie",
            "Zhexuan Xu",
            "Zhihao Liu",
            "Kang Chen",
            "Wenhao Tang",
            "Quanlu Zhang",
            "Weinan Zhang",
            "Chao Yu",
            "Yu Wang"
          ],
          "arxiv_id": "2510.06710",
          "abstract": "Recent progress in vision and language foundation models has significantly advanced multimodal understanding, reasoning, and generation, inspiring a surge of interest in extending such capabilities to embodied settings through vision-language-action (VLA) models. Yet, most VLA models are still trained with supervised fine-tuning (SFT), which struggles to generalize under distribution shifts due to error accumulation. Reinforcement learning (RL) offers a promising alternative by directly optimizing task performance through interaction, but existing attempts remain fragmented and lack a unified platform for fair and systematic comparison across model architectures and algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models. The system adopts a highly flexible resource allocation design that addresses the challenge of integrating rendering, training, and inference in RL+VLA training. In particular, for GPU-parallelized simulators, RLinf-VLA implements a novel hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup in training. Through a unified interface, RLinf-VLA seamlessly supports diverse VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g., PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a unified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25 ManiSkill tasks. Beyond empirical performance, our study distills a set of best practices for applying RL to VLA training and sheds light on emerging patterns in this integration. Furthermore, we present preliminary deployment on a real-world Franka robot, where RL-trained policies exhibit stronger generalization than those trained with SFT. We envision RLinf-VLA as a foundation to accelerate and standardize research on embodied intelligence.",
          "abstract_zh": "视觉和语言基础模型的最新进展显着推进了多模态理解、推理和生成，激发了人们对通过视觉-语言-动作（VLA）模型将此类功能扩展到具体环境的兴趣激增。然而，大多数 VLA 模型仍然采用监督微调 (SFT) 进行训练，由于误差累积，该模型很难在分布变化下进行泛化。强化学习（RL）通过交互直接优化任务性能，提供了一种有前途的替代方案，但现有的尝试仍然支离破碎，并且缺乏一个统一的平台来对模型架构和算法设计进行公平和系统的比较。为了解决这一差距，我们引入了 RLinf-VLA，这是一个统一且高效的框架，用于 VLA 模型的可扩展 RL 训练。该系统采用高度灵活的资源分配设计，解决了RL+VLA训练中渲染、训练、推理一体化的挑战。特别是，对于GPU并行模拟器，RLinf-VLA实现了一种新颖的混合细粒度管道分配模式，在训练中实现了1.61x-1.88x的加速。通过统一的接口，RLinf-VLA无缝支持各种VLA架构（例如OpenVLA、OpenVLA-OFT）、多种RL算法（例如PPO、GRPO）和各种模拟器（例如ManiSkill、LIBERO）。在仿真中，统一模型在 130 个 LIBERO 任务中实现了 98.11%，在 25 个 ManiSkill 任务中实现了 97.66%。除了实证表现之外，我们的研究还提炼了一套将强化学习应用于 VLA 训练的最佳实践，并揭示了这种集成中的新兴模式。此外，我们在现实世界的 Franka 机器人上进行了初步部署，其中 RL 训练的策略比 SFT 训练的策略表现出更强的泛化能力。我们设想 RLinf-VLA 作为加速和标准化具身智能研究的基础。"
        },
        {
          "title": "TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.10932v1",
          "snippet": "With the growing deployment of Vision-Language-Action (VLA) models in real-world embodied AI systems, their increasing vulnerability to backdoor attacks poses a serious safety threat. A backdoored VLA agent can be covertly triggered by a pre-injected backdoor to execute adversarial actions, potentially causing system failures or even physical harm. Although backdoor attacks on VLA models have been explored, prior work has focused only on untargeted attacks, leaving the more practically threatening scenario of targeted manipulation unexamined. In this paper, we study targeted backdoor attacks on VLA models and introduce TabVLA, a novel framework that enables such attacks via black-box fine-tuning. TabVLA explores two deployment-relevant inference-time threat models: input-stream editing and in-scene triggering. It formulates poisoned data generation as an optimization problem to improve attack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal that the vision channel is the principal attack surface: targeted backdoors succeed with minimal poisoning, remain robust across variations in trigger design, and are degraded only by positional mismatches between fine-tuning and inference triggers. We also investigate a potential detection-based defense against TabVLA, which reconstructs latent visual triggers from the input stream to flag activation-conditioned backdoor samples. Our work highlights the vulnerability of VLA models to targeted backdoor manipulation and underscores the need for more advanced defenses.",
          "site": "arxiv.org",
          "rank": 15,
          "published": "2025-10-13T02:45:48Z",
          "authors": [
            "Zonghuan Xu",
            "Xiang Zheng",
            "Xingjun Ma",
            "Yu-Gang Jiang"
          ],
          "arxiv_id": "2510.10932",
          "abstract": "With the growing deployment of Vision-Language-Action (VLA) models in real-world embodied AI systems, their increasing vulnerability to backdoor attacks poses a serious safety threat. A backdoored VLA agent can be covertly triggered by a pre-injected backdoor to execute adversarial actions, potentially causing system failures or even physical harm. Although backdoor attacks on VLA models have been explored, prior work has focused only on untargeted attacks, leaving the more practically threatening scenario of targeted manipulation unexamined. In this paper, we study targeted backdoor attacks on VLA models and introduce TabVLA, a novel framework that enables such attacks via black-box fine-tuning. TabVLA explores two deployment-relevant inference-time threat models: input-stream editing and in-scene triggering. It formulates poisoned data generation as an optimization problem to improve attack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal that the vision channel is the principal attack surface: targeted backdoors succeed with minimal poisoning, remain robust across variations in trigger design, and are degraded only by positional mismatches between fine-tuning and inference triggers. We also investigate a potential detection-based defense against TabVLA, which reconstructs latent visual triggers from the input stream to flag activation-conditioned backdoor samples. Our work highlights the vulnerability of VLA models to targeted backdoor manipulation and underscores the need for more advanced defenses.",
          "abstract_zh": "随着视觉-语言-动作（VLA）模型在现实世界的具体人工智能系统中的不断部署，它们越来越容易受到后门攻击，构成了严重的安全威胁。后门 VLA 代理可以被预先注入的后门秘密触发，以执行对抗性操作，可能导致系统故障甚至人身伤害。尽管已经探索了对 VLA 模型的后门攻击，但之前的工作仅关注非目标攻击，而没有对更具实际威胁的目标操纵场景进行研究。在本文中，我们研究了针对 VLA 模型的针对性后门攻击，并介绍了 TabVLA，这是一种通过黑盒微调实现此类攻击的新颖框架。TabVLA 探索了两种与部署相关的推理时间威胁模型：输入流编辑和场景内触发。它将中毒数据生成制定为优化问题，以提高攻击效率。在 LIBERO 基准上使用 OpenVLA-7B 进行的实验表明，视觉通道是主要的攻击面：目标后门以最小的中毒成功，在触发器设计的变化中保持鲁棒性，并且仅因微调和推理触发器之间的位置不匹配而降级。我们还研究了针对 TabVLA 的潜在的基于检测的防御，该防御从输入流中重建潜在的视觉触发器以标记激活条件后门样本。我们的工作强调了 VLA 模型对有针对性的后门操纵的脆弱性，并强调需要更先进的防御措施。"
        },
        {
          "title": "UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning",
          "url": "http://arxiv.org/abs/2510.10642v2",
          "snippet": "Building generalist robot policies that can handle diverse tasks in open-ended environments is a central challenge in robotics. To leverage knowledge from large-scale pretraining, prior work (VLA) has typically built generalist policies either on top of vision-language understanding models (VLMs) or generative models. However, both semantic understanding from vision-language pretraining and visual dynamics modeling from visual-generation pretraining are crucial for embodied robots. Recent unified models of generation and understanding have demonstrated strong capabilities in both comprehension and generation through large-scale pretraining. We posit that robotic policy learning can likewise benefit from the combined strengths of understanding, planning, and continuous future representation learning. Building on this insight, we introduce UniCoD, which acquires the ability to dynamically model high-dimensional visual features through pretraining on over 1M internet-scale instructional manipulation videos. Subsequently, UniCoD is fine-tuned on data collected from the robot embodiment, enabling the learning of mappings from predictive representations to action tokens. Extensive experiments show our approach consistently outperforms baseline methods in terms of 9\\% and 12\\% across simulation environments and real-world out-of-distribution tasks.",
          "site": "arxiv.org",
          "rank": 16,
          "published": "2025-10-12T14:54:19Z",
          "authors": [
            "Jianke Zhang",
            "Yucheng Hu",
            "Yanjiang Guo",
            "Xiaoyu Chen",
            "Yichen Liu",
            "Wenna Chen",
            "Chaochao Lu",
            "Jianyu Chen"
          ],
          "arxiv_id": "2510.10642",
          "abstract": "Building generalist robot policies that can handle diverse tasks in open-ended environments is a central challenge in robotics. To leverage knowledge from large-scale pretraining, prior work (VLA) has typically built generalist policies either on top of vision-language understanding models (VLMs) or generative models. However, both semantic understanding from vision-language pretraining and visual dynamics modeling from visual-generation pretraining are crucial for embodied robots. Recent unified models of generation and understanding have demonstrated strong capabilities in both comprehension and generation through large-scale pretraining. We posit that robotic policy learning can likewise benefit from the combined strengths of understanding, planning, and continuous future representation learning. Building on this insight, we introduce UniCoD, which acquires the ability to dynamically model high-dimensional visual features through pretraining on over 1M internet-scale instructional manipulation videos. Subsequently, UniCoD is fine-tuned on data collected from the robot embodiment, enabling the learning of mappings from predictive representations to action tokens. Extensive experiments show our approach consistently outperforms baseline methods in terms of 9\\% and 12\\% across simulation environments and real-world out-of-distribution tasks.",
          "abstract_zh": "建立可以在开放环境中处理不同任务的通用机器人策略是机器人技术的一个核心挑战。为了利用大规模预训练中的知识，先前的工作（VLA）通常在视觉语言理解模型（VLM）或生成模型之上构建通用策略。然而，视觉语言预训练的语义理解和视觉生成预训练的视觉动力学建模对于实体机器人至关重要。最近的生成和理解的统一模型通过大规模预训练表现出了强大的理解和生成能力。我们认为机器人策略学习同样可以受益于理解、规划和持续的未来表征学习的综合优势。基于这一见解，我们引入了 UniCoD，它通过对超过 100 万个互联网规模的教学操作视频进行预训练，获得了动态建模高维视觉特征的能力。随后，UniCoD 根据从机器人实施例收集的数据进行微调，从而能够学习从预测表示到动作标记的映射。大量实验表明，我们的方法在模拟环境和现实世界的分布外任务中始终优于基准方法 9% 和 12%。"
        },
        {
          "title": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered",
          "url": "http://arxiv.org/abs/2510.08464v1",
          "snippet": "Vision-Language-Action (VLA) models have advanced robotic capabilities but remain challenging to deploy on resource-limited hardware. Pruning has enabled efficient compression of large language models (LLMs), yet it is largely understudied in robotics. Surprisingly, we observe that pruning VLA models leads to drastic degradation and increased safety violations. We introduce GLUESTICK, a post-pruning recovery method that restores much of the original model's functionality while retaining sparsity benefits. Our method performs a one-time interpolation between the dense and pruned models in weight-space to compute a corrective term. This correction is used during inference by each pruned layer to recover lost capabilities with minimal overhead. GLUESTICK requires no additional training, is agnostic to the pruning algorithm, and introduces a single hyperparameter that controls the tradeoff between efficiency and accuracy. Across diverse VLA architectures and tasks in manipulation and navigation, GLUESTICK achieves competitive memory efficiency while substantially recovering success rates and reducing safety violations. Additional material can be found at: https://gluestick-vla.github.io/.",
          "site": "arxiv.org",
          "rank": 17,
          "published": "2025-10-09T17:07:30Z",
          "authors": [
            "Jason Jabbour",
            "Dong-Ki Kim",
            "Max Smith",
            "Jay Patrikar",
            "Radhika Ghosal",
            "Youhui Wang",
            "Ali Agha",
            "Vijay Janapa Reddi",
            "Shayegan Omidshafiei"
          ],
          "arxiv_id": "2510.08464",
          "abstract": "Vision-Language-Action (VLA) models have advanced robotic capabilities but remain challenging to deploy on resource-limited hardware. Pruning has enabled efficient compression of large language models (LLMs), yet it is largely understudied in robotics. Surprisingly, we observe that pruning VLA models leads to drastic degradation and increased safety violations. We introduce GLUESTICK, a post-pruning recovery method that restores much of the original model's functionality while retaining sparsity benefits. Our method performs a one-time interpolation between the dense and pruned models in weight-space to compute a corrective term. This correction is used during inference by each pruned layer to recover lost capabilities with minimal overhead. GLUESTICK requires no additional training, is agnostic to the pruning algorithm, and introduces a single hyperparameter that controls the tradeoff between efficiency and accuracy. Across diverse VLA architectures and tasks in manipulation and navigation, GLUESTICK achieves competitive memory efficiency while substantially recovering success rates and reducing safety violations. Additional material can be found at: https://gluestick-vla.github.io/.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型具有先进的机器人功能，但在资源有限的硬件上部署仍然具有挑战性。剪枝实现了大型语言模型 (LLM) 的高效压缩，但在机器人技术领域却尚未得到充分研究。令人惊讶的是，我们观察到修剪 VLA 模型会导致急剧退化并增加安全违规行为。我们引入了 GLUESTICK，这是一种剪枝后恢复方法，可以恢复原始模型的大部分功能，同时保留稀疏性优势。我们的方法在权重空间中的密集模型和修剪模型之间执行一次性插值，以计算校正项。每个修剪层在推理期间使用此校正，以最小的开销恢复丢失的功能。GLUESTICK 不需要额外的训练，与修剪算法无关，并且引入了一个控制效率和准确性之间权衡的超参数。在操纵和导航中的不同 VLA 架构和任务中，GLUESTICK 实现了具有竞争力的内存效率，同时大幅恢复成功率并减少安全违规。其他材料可在以下网址找到：https://gluestick-vla.github.io/。"
        },
        {
          "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation",
          "url": "http://arxiv.org/abs/2510.05057v1",
          "snippet": "A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video.",
          "site": "arxiv.org",
          "rank": 18,
          "published": "2025-10-06T17:37:24Z",
          "authors": [
            "Mingyu Liu",
            "Jiuhe Shu",
            "Hui Chen",
            "Zeju Li",
            "Canyu Zhao",
            "Jiange Yang",
            "Shenyuan Gao",
            "Hao Chen",
            "Chunhua Shen"
          ],
          "arxiv_id": "2510.05057",
          "abstract": "A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video.",
          "abstract_zh": "具身智能的一个基本挑战是开发富有表现力和紧凑的状态表示，以实现高效的世界建模和决策。然而，现有的方法通常无法实现这种平衡，产生的表示要么过于冗余，要么缺乏任务关键信息。我们提出了一种无监督方法，使用轻量级编码器和预训练的扩散变换器（DiT）解码器学习高度压缩的双令牌状态表示，利用其强大的生成先验。我们的表示高效、可解释，并无缝集成到现有的基于 VLA 的模型中，以最小的推理开销将 LIBERO 的性能提高了 14.3%，将现实世界的任务成功率提高了 30%。更重要的是，我们发现通过潜在插值获得的这些标记之间的差异自然可以作为高效的潜在动作，可以进一步解码为可执行的机器人动作。这种新兴的能力表明，我们的表示可以在没有明确监督的情况下捕获结构化动态。我们将我们的方法命名为 StaMo，因为它能够从紧凑的状态表示中学习通用的机器人运动，该表示是从静态图像编码的，挑战了对复杂架构和视频数据学习潜在动作的普遍依赖。由此产生的潜在行动还增强了政策协同训练，比之前的方法提高了 10.4%，并提高了可解释性。此外，我们的方法可以有效地跨不同的数据源进行扩展，包括现实世界的机器人数据、模拟和人类以自我为中心的视频。"
        },
        {
          "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking",
          "url": "http://arxiv.org/abs/2510.07134v1",
          "snippet": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.",
          "site": "arxiv.org",
          "rank": 19,
          "published": "2025-10-08T15:29:17Z",
          "authors": [
            "Jiahang Liu",
            "Yunpeng Qi",
            "Jiazhao Zhang",
            "Minghan Li",
            "Shaoan Wang",
            "Kui Wu",
            "Hanjing Ye",
            "Hong Zhang",
            "Zhibo Chen",
            "Fangwei Zhong",
            "Zhizheng Zhang",
            "He Wang"
          ],
          "arxiv_id": "2510.07134",
          "abstract": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.",
          "abstract_zh": "体现视觉跟踪（EVT）是支撑实际应用的一项基本能力，例如陪伴机器人、引导机器人和服务助理，其中持续跟踪移动目标至关重要。最近的进展使得复杂和非结构化场景中的语言引导跟踪成为可能。然而，现有的方法缺乏明确的空间推理和有效的时间记忆，在严重遮挡或存在相似的干扰物的情况下会导致失败。为了应对这些挑战，我们提出了 TrackVLA++，这是一种新颖的视觉-语言-动作 (VLA) 模型，它通过空间推理机制和目标识别记忆 (TIM) 这两个关键模块来增强具体视觉跟踪。推理模块引入了一种称为 Polar-CoT 的思想链范式，它推断目标的相对位置并将其编码为用于动作预测的紧凑极坐标标记。在这些空间先验的指导下，TIM 采用门控更新策略来保留长视野目标记忆，确保时空一致性并减轻扩展遮挡期间的目标丢失。大量实验表明，TrackVLA++ 在以自我为中心和多相机设置的公共基准测试中均实现了最先进的性能。在具有挑战性的 EVT-Bench DT 比赛中，TrackVLA++ 分别超越了之前领先的方法 5.1 和 12。此外，TrackVLA++ 表现出强大的零样本泛化能力，可在动态和遮挡场景中实现稳健的现实世界跟踪。"
        },
        {
          "title": "Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.09976v1",
          "snippet": "Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $π_0$ have shown strong generalization by leveraging large-scale demonstrations, yet their performance is still fundamentally constrained by the quality and coverage of supervised data. Reinforcement learning (RL) provides a promising path for improving and fine-tuning VLAs through online interaction. However, conventional policy gradient methods are computationally infeasible in the context of flow-matching based models due to the intractability of the importance sampling process, which requires explicit computation of policy ratios. To overcome this limitation, we propose Flow Policy Optimization (FPO) algorithm, which reformulates importance sampling by leveraging per-sample changes in the conditional flow-matching objective. Furthermore, FPO achieves stable and scalable online reinforcement fine-tuning of the $π_0$ model by integrating structure-aware credit assignment to enhance gradient efficiency, clipped surrogate objectives to stabilize optimization, multi-step latent exploration to encourage diverse policy updates, and a Q-ensemble mechanism to provide robust value estimation. We evaluate FPO on the LIBERO benchmark and the ALOHA simulation task against supervised, preference-aligned, diffusion-based, autoregressive online RL, and $π_0$-FAST baselines, observing consistent improvements over the imitation prior and strong alternatives with stable learning under sparse rewards. In addition, ablation studies and analyses of the latent space dynamics further highlight the contributions of individual components within FPO, validating the effectiveness of the proposed computational modules and the stable convergence of the conditional flow-matching objective during online RL.",
          "site": "arxiv.org",
          "rank": 20,
          "published": "2025-10-11T03:11:18Z",
          "authors": [
            "Mingyang Lyu",
            "Yinqian Sun",
            "Erliang Lin",
            "Huangrui Li",
            "Ruolin Chen",
            "Feifei Zhao",
            "Yi Zeng"
          ],
          "arxiv_id": "2510.09976",
          "abstract": "Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $π_0$ have shown strong generalization by leveraging large-scale demonstrations, yet their performance is still fundamentally constrained by the quality and coverage of supervised data. Reinforcement learning (RL) provides a promising path for improving and fine-tuning VLAs through online interaction. However, conventional policy gradient methods are computationally infeasible in the context of flow-matching based models due to the intractability of the importance sampling process, which requires explicit computation of policy ratios. To overcome this limitation, we propose Flow Policy Optimization (FPO) algorithm, which reformulates importance sampling by leveraging per-sample changes in the conditional flow-matching objective. Furthermore, FPO achieves stable and scalable online reinforcement fine-tuning of the $π_0$ model by integrating structure-aware credit assignment to enhance gradient efficiency, clipped surrogate objectives to stabilize optimization, multi-step latent exploration to encourage diverse policy updates, and a Q-ensemble mechanism to provide robust value estimation. We evaluate FPO on the LIBERO benchmark and the ALOHA simulation task against supervised, preference-aligned, diffusion-based, autoregressive online RL, and $π_0$-FAST baselines, observing consistent improvements over the imitation prior and strong alternatives with stable learning under sparse rewards. In addition, ablation studies and analyses of the latent space dynamics further highlight the contributions of individual components within FPO, validating the effectiveness of the proposed computational modules and the stable convergence of the conditional flow-matching objective during online RL.",
          "abstract_zh": "OpenVLA、Octo 和 $π_0$ 等视觉-语言-动作 (VLA) 模型通过大规模演示显示出很强的泛化性，但它们的性能仍然从根本上受到监督数据的质量和覆盖范围的限制。强化学习（RL）为通过在线交互改进和微调 VLA 提供了一条有前途的途径。然而，由于重要性采样过程的复杂性，传统的策略梯度方法在基于流匹配的模型的背景下在计算上不可行，这需要显式计算策略比率。为了克服这一限制，我们提出了流策略优化（FPO）算法，该算法通过利用条件流匹配目标中每个样本的变化来重新制定重要性采样。此外，FPO 通过集成结构感知信用分配以提高梯度效率、修剪代理目标以稳定优化、多步潜在探索以鼓励多样化的策略更新以及 Q-ensemble 机制以提供稳健的价值估计，实现了 $π_0$ 模型的稳定且可扩展的在线强化微调。我们在 LIBERO 基准和 ALOHA 模拟任务上针对监督、偏好对齐、基于扩散、自回归在线 RL 和 $π_0$-FAST 基线评估 FPO，观察到相对于模仿先验和强替代方案的一致改进，以及在稀疏奖励下的稳定学习。此外，对潜在空间动力学的消融研究和分析进一步突出了 FPO 中各个组件的贡献，验证了所提出的计算模块的有效性以及在线 RL 期间条件流匹配目标的稳定收敛。"
        },
        {
          "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2510.07313v1",
          "snippet": "Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.",
          "site": "arxiv.org",
          "rank": 21,
          "published": "2025-10-08T17:59:08Z",
          "authors": [
            "Zezhong Qian",
            "Xiaowei Chi",
            "Yuming Li",
            "Shizun Wang",
            "Zhiyuan Qin",
            "Xiaozhu Ju",
            "Sirui Han",
            "Shanghang Zhang"
          ],
          "arxiv_id": "2510.07313",
          "abstract": "Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.",
          "abstract_zh": "手腕视图观察对于 VLA 模型至关重要，因为它们捕获细粒度的手部物体交互，从而直接提高操作性能。然而，大规模数据集很少包含此类记录，导致丰富的主播视图和稀缺的手腕视图之间存在巨大差距。现有的世界模型无法弥合这一差距，因为它们需要手腕视图第一帧，因此无法仅从主播视图生成手腕视图视频。在这一差距中，最近出现的视觉几何模型（例如 VGGT）具有几何和交叉视图先验，可以解决极端的视点偏移问题。受这些见解的启发，我们提出了 WristWorld，这是第一个仅根据主播视图生成手腕视图视频的 4D 世界模型。WristWorld 分两个阶段运行：(i) 重建，它扩展了 VGGT 并结合了我们的空间投影一致性 (SPC) 损失来估计几何一致的手腕视图姿势和 4D 点云；（ii）生成，它采用我们的视频生成模型从重建的角度合成时间连贯的手腕视图视频。在 Droid、Calvin 和 Franka Panda 上进行的实验展示了具有卓越空间一致性的最先进的视频生成，同时还提高了 VLA 性能，将 Calvin 的平均任务完成长度提高了 3.81%，并缩小了 42.4% 的锚点与手腕视图差距。"
        },
        {
          "title": "EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model",
          "url": "http://arxiv.org/abs/2510.06207v2",
          "snippet": "Recent advances in control robot methods, from end-to-end vision-language-action frameworks to modular systems with predefined primitives, have advanced robots' ability to follow natural language instructions. Nonetheless, many approaches still struggle to scale to diverse environments, as they often rely on large annotated datasets and offer limited interpretability.In this work, we introduce EmbodiedCoder, a training-free framework for open-world mobile robot manipulation that leverages coding models to directly generate executable robot trajectories. By grounding high-level instructions in code, EmbodiedCoder enables flexible object geometry parameterization and manipulation trajectory synthesis without additional data collection or fine-tuning.This coding-based paradigm provides a transparent and generalizable way to connect perception with manipulation. Experiments on real mobile robots show that EmbodiedCoder achieves robust performance across diverse long-term tasks and generalizes effectively to novel objects and environments.Our results demonstrate an interpretable approach for bridging high-level reasoning and low-level control, moving beyond fixed primitives toward versatile robot intelligence. See the project page at: https://embodiedcoder.github.io/EmbodiedCoder/",
          "site": "arxiv.org",
          "rank": 22,
          "published": "2025-10-07T17:58:02Z",
          "authors": [
            "Zefu Lin",
            "Rongxu Cui",
            "Chen Hanning",
            "Xiangyu Wang",
            "Junjia Xu",
            "Xiaojuan Jin",
            "Chen Wenbo",
            "Hui Zhou",
            "Lue Fan",
            "Wenling Li",
            "Zhaoxiang Zhang"
          ],
          "arxiv_id": "2510.06207",
          "abstract": "Recent advances in control robot methods, from end-to-end vision-language-action frameworks to modular systems with predefined primitives, have advanced robots' ability to follow natural language instructions. Nonetheless, many approaches still struggle to scale to diverse environments, as they often rely on large annotated datasets and offer limited interpretability.In this work, we introduce EmbodiedCoder, a training-free framework for open-world mobile robot manipulation that leverages coding models to directly generate executable robot trajectories. By grounding high-level instructions in code, EmbodiedCoder enables flexible object geometry parameterization and manipulation trajectory synthesis without additional data collection or fine-tuning.This coding-based paradigm provides a transparent and generalizable way to connect perception with manipulation. Experiments on real mobile robots show that EmbodiedCoder achieves robust performance across diverse long-term tasks and generalizes effectively to novel objects and environments.Our results demonstrate an interpretable approach for bridging high-level reasoning and low-level control, moving beyond fixed primitives toward versatile robot intelligence. See the project page at: https://embodiedcoder.github.io/EmbodiedCoder/",
          "abstract_zh": "控制机器人方法的最新进展，从端到端视觉语言动作框架到具有预定义原语的模块化系统，使机器人能够遵循自然语言指令。尽管如此，许多方法仍然难以扩展到不同的环境，因为它们通常依赖于大型注释数据集并提供有限的可解释性。在这项工作中，我们引入了 EmbodiedCoder，这是一种用于开放世界移动机器人操作的免训练框架，利用编码模型直接生成可执行的机器人轨迹。通过将高级指令嵌入代码中，EmbodiedCoder 可以实现灵活的对象几何参数化和操作轨迹合成，而无需额外的数据收集或微调。这种基于编码的范例提供了一种透明且可通用的方式来连接感知与操作。对真实移动机器人的实验表明，EmbodiedCoder 在不同的长期任务中实现了稳健的性能，并有效地推广到新的物体和环境。我们的结果展示了一种可解释的方法，用于桥接高级推理和低级控制，超越固定基元，转向多功能机器人智能。请参阅项目页面：https://embodiedcoder.github.io/EmbodiedCoder/"
        },
        {
          "title": "Avi: Action from Volumetric Inference",
          "url": "http://arxiv.org/abs/2510.21746v1",
          "snippet": "We propose Avi, a novel 3D Vision-Language-Action (VLA) architecture that reframes robotic action generation as a problem of 3D perception and spatial reasoning, rather than low-level policy learning. While existing VLA models primarily operate on 2D visual inputs and are trained end-to-end on task-specific action policies, Avi leverages 3D point clouds and language-grounded scene understanding to compute actions through classical geometric transformations. Most notably, Avi does not train on previous action tokens, rather, we build upon a 3D Multi-modal Large Language Model (MLLM) to generate the next point cloud and explicitly calculate the actions through classical transformations. This approach enables generalizable behaviors that are robust to occlusions, camera pose variations, and changes in viewpoint. By treating the robotic decision-making process as a structured reasoning task over 3D representations, Avi bridges the gap between high-level language instructions and low-level actuation without requiring opaque policy learning. Our preliminary results highlight the potential of 3D vision-language reasoning as a foundation for scalable, robust robotic systems. Check it out at https://avi-3drobot.github.io/.",
          "site": "arxiv.org",
          "rank": 23,
          "published": "2025-10-07T23:10:56Z",
          "authors": [
            "Harris Song",
            "Long Le"
          ],
          "arxiv_id": "2510.21746",
          "abstract": "We propose Avi, a novel 3D Vision-Language-Action (VLA) architecture that reframes robotic action generation as a problem of 3D perception and spatial reasoning, rather than low-level policy learning. While existing VLA models primarily operate on 2D visual inputs and are trained end-to-end on task-specific action policies, Avi leverages 3D point clouds and language-grounded scene understanding to compute actions through classical geometric transformations. Most notably, Avi does not train on previous action tokens, rather, we build upon a 3D Multi-modal Large Language Model (MLLM) to generate the next point cloud and explicitly calculate the actions through classical transformations. This approach enables generalizable behaviors that are robust to occlusions, camera pose variations, and changes in viewpoint. By treating the robotic decision-making process as a structured reasoning task over 3D representations, Avi bridges the gap between high-level language instructions and low-level actuation without requiring opaque policy learning. Our preliminary results highlight the potential of 3D vision-language reasoning as a foundation for scalable, robust robotic systems. Check it out at https://avi-3drobot.github.io/.",
          "abstract_zh": "我们提出了 Avi，一种新颖的 3D 视觉-语言-动作 (VLA) 架构，它将机器人动作生成重新定义为 3D 感知和空间推理问题，而不是低级策略学习问题。虽然现有的 VLA 模型主要在 2D 视觉输入上运行，并根据特定于任务的动作策略进行端到端训练，但 Avi 利用 3D 点云和基于语言的场景理解来通过经典几何变换来计算动作。最值得注意的是，Avi 并不基于之前的动作标记进行训练，而是基于 3D 多模态大型语言模型 (MLLM) 来生成下一个点云，并通过经典转换显式计算动作。这种方法可以实现对遮挡、相机姿势变化和视点变化具有鲁棒性的通用行为。通过将机器人决策过程视为基于 3D 表示的结构化推理任务，Avi 弥合了高级语言指令和低级驱动之间的差距，而无需不透明的策略学习。我们的初步结果凸显了 3D 视觉语言推理作为可扩展、强大的机器人系统基础的潜力。请访问 https://avi-3drobot.github.io/ 查看。"
        },
        {
          "title": "OmniSAT: Compact Action Token, Faster Auto Regression",
          "url": "http://arxiv.org/abs/2510.09667v1",
          "snippet": "Existing Vision-Language-Action (VLA) models can be broadly categorized into diffusion-based and auto-regressive (AR) approaches: diffusion models capture continuous action distributions but rely on computationally heavy iterative denoising. In contrast, AR models enable efficient optimization and flexible sequence construction, making them better suited for large-scale pretraining. To further improve AR efficiency, particularly when action chunks induce extended and high-dimensional sequences, prior work applies entropy-guided and token-frequency techniques to shorten the sequence length. However, such compression struggled with \\textit{poor reconstruction or inefficient compression}. Motivated by this, we introduce an Omni Swift Action Tokenizer, which learns a compact, transferable action representation. Specifically, we first normalize value ranges and temporal horizons to obtain a consistent representation with B-Spline encoding. Then, we apply multi-stage residual quantization to the position, rotation, and gripper subspaces, producing compressed discrete tokens with coarse-to-fine granularity for each part. After pre-training on the large-scale dataset Droid, the resulting discrete tokenization shortens the training sequence by 6.8$\\times$, and lowers the target entropy. To further explore the potential of OmniSAT, we develop a cross-embodiment learning strategy that builds on the unified action-pattern space and jointly leverages robot and human demonstrations. It enables scalable auxiliary supervision from heterogeneous egocentric videos. Across diverse real-robot and simulation experiments, OmniSAT encompasses higher compression while preserving reconstruction quality, enabling faster AR training convergence and model performance.",
          "site": "arxiv.org",
          "rank": 24,
          "published": "2025-10-08T03:55:24Z",
          "authors": [
            "Huaihai Lyu",
            "Chaofan Chen",
            "Senwei Xie",
            "Pengwei Wang",
            "Xiansheng Chen",
            "Shanghang Zhang",
            "Changsheng Xu"
          ],
          "arxiv_id": "2510.09667",
          "abstract": "Existing Vision-Language-Action (VLA) models can be broadly categorized into diffusion-based and auto-regressive (AR) approaches: diffusion models capture continuous action distributions but rely on computationally heavy iterative denoising. In contrast, AR models enable efficient optimization and flexible sequence construction, making them better suited for large-scale pretraining. To further improve AR efficiency, particularly when action chunks induce extended and high-dimensional sequences, prior work applies entropy-guided and token-frequency techniques to shorten the sequence length. However, such compression struggled with \\textit{poor reconstruction or inefficient compression}. Motivated by this, we introduce an Omni Swift Action Tokenizer, which learns a compact, transferable action representation. Specifically, we first normalize value ranges and temporal horizons to obtain a consistent representation with B-Spline encoding. Then, we apply multi-stage residual quantization to the position, rotation, and gripper subspaces, producing compressed discrete tokens with coarse-to-fine granularity for each part. After pre-training on the large-scale dataset Droid, the resulting discrete tokenization shortens the training sequence by 6.8$\\times$, and lowers the target entropy. To further explore the potential of OmniSAT, we develop a cross-embodiment learning strategy that builds on the unified action-pattern space and jointly leverages robot and human demonstrations. It enables scalable auxiliary supervision from heterogeneous egocentric videos. Across diverse real-robot and simulation experiments, OmniSAT encompasses higher compression while preserving reconstruction quality, enabling faster AR training convergence and model performance.",
          "abstract_zh": "现有的视觉-语言-动作（VLA）模型可大致分为基于扩散和自回归（AR）方法：扩散模型捕获连续动作分布，但依赖于计算量大的迭代去噪。相比之下，AR模型能够实现高效的优化和灵活的序列构建，使其更适合大规模预训练。为了进一步提高 AR 效率，特别是当动作块引发扩展和高维序列时，先前的工作应用熵引导和令牌频率技术来缩短序列长度。然而，这种压缩遇到了\\textit{较差的重建或低效的压缩}。受此启发，我们引入了 Omni Swift Action Tokenizer，它学习紧凑的、可转移的动作表示。具体来说，我们首先对值范围和时间范围进行归一化，以获得与 B 样条编码一致的表示。然后，我们将多级残差量化应用于位置、旋转和夹具子空间，为每个部分生成具有从粗到细粒度的压缩离散标记。在大规模数据集 Droid 上进行预训练后，得到的离散标记化将训练序列缩短了 6.8$\\times$，并降低了目标熵。为了进一步探索 OmniSAT 的潜力，我们开发了一种跨实体学习策略，该策略基于统一的动作模式空间并联合利用机器人和人类演示。它可以对异构的以自我为中心的视频进行可扩展的辅助监督。在各种真实机器人和模拟实验中，OmniSAT 具有更高的压缩率，同时保持重建质量，从而实现更快的 AR 训练收敛和模型性能。"
        },
        {
          "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
          "url": "http://arxiv.org/abs/2510.09507v1",
          "snippet": "The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction with the physical world. For any general-purpose intelligent agent to achieve true versatility, it must also master these fundamental skills. While modern Multimodal Large Language Models (MLLMs) leverage their extensive common knowledge for high-level planning in embodied AI and in downstream Vision-Language-Action (VLA) models, the extent of their true understanding of physical tools remains unquantified. To bridge this gap, we present PhysToolBench, the first benchmark dedicated to evaluating the comprehension of physical tools by MLLMs. Our benchmark is structured as a Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs. It assesses capabilities across three distinct difficulty levels: (1) Tool Recognition: Requiring the recognition of a tool's primary function. (2) Tool Understanding: Testing the ability to grasp the underlying principles of a tool's operation. (3) Tool Creation: Challenging the model to fashion a new tool from surrounding objects when conventional options are unavailable. Our comprehensive evaluation of 32 MLLMs-spanning proprietary, open-source, specialized embodied, and backbones in VLAs-reveals a significant deficiency in tool understanding. Furthermore, we provide an in-depth analysis and propose preliminary solutions. Code and dataset are publicly available.",
          "site": "arxiv.org",
          "rank": 25,
          "published": "2025-10-10T16:10:45Z",
          "authors": [
            "Zixin Zhang",
            "Kanghao Chen",
            "Xingwang Lin",
            "Lutao Jiang",
            "Xu Zheng",
            "Yuanhuiyi Lyu",
            "Litao Guo",
            "Yinchuan Li",
            "Ying-Cong Chen"
          ],
          "arxiv_id": "2510.09507",
          "abstract": "The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction with the physical world. For any general-purpose intelligent agent to achieve true versatility, it must also master these fundamental skills. While modern Multimodal Large Language Models (MLLMs) leverage their extensive common knowledge for high-level planning in embodied AI and in downstream Vision-Language-Action (VLA) models, the extent of their true understanding of physical tools remains unquantified. To bridge this gap, we present PhysToolBench, the first benchmark dedicated to evaluating the comprehension of physical tools by MLLMs. Our benchmark is structured as a Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs. It assesses capabilities across three distinct difficulty levels: (1) Tool Recognition: Requiring the recognition of a tool's primary function. (2) Tool Understanding: Testing the ability to grasp the underlying principles of a tool's operation. (3) Tool Creation: Challenging the model to fashion a new tool from surrounding objects when conventional options are unavailable. Our comprehensive evaluation of 32 MLLMs-spanning proprietary, open-source, specialized embodied, and backbones in VLAs-reveals a significant deficiency in tool understanding. Furthermore, we provide an in-depth analysis and propose preliminary solutions. Code and dataset are publicly available.",
          "abstract_zh": "使用、理解和创建工具的能力是人类智能的标志，能够实现与物理世界的复杂交互。对于任何通用智能代理来说，要实现真正的多功能性，它还必须掌握这些基本技能。虽然现代多模态大语言模型 (MLLM) 利用其广泛的常识来进行具体人工智能和下游视觉-语言-动作 (VLA) 模型的高级规划，但它们对物理工具的真正理解程度仍然无法量化。为了弥补这一差距，我们推出了 PhysToolBench，这是第一个致力于评估 MLLM 对物理工具的理解的基准。我们的基准测试采用视觉问答 (VQA) 数据集的结构，包含 1,000 多个图像-文本对。它评估三个不同难度级别的能力： (1) 工具识别：要求识别工具的主要功能。(2)工具理解：测试掌握工具操作底层原理的能力。(3) 工具创建：当传统选项不可用时，挑战模型从周围的物体中塑造出新的工具。我们对 32 个 MLLM（涵盖专有、开源、专业化和 VLA 中的骨干）进行了全面评估，揭示了工具理解方面的重大缺陷。此外，我们还提供了深入的分析并提出了初步的解决方案。代码和数据集是公开的。"
        },
        {
          "title": "Dejavu: Towards Experience Feedback Learning for Embodied Intelligence",
          "url": "http://arxiv.org/abs/2510.10181v2",
          "snippet": "Embodied agents face a fundamental limitation: once deployed in real-world environments to perform specific tasks, they are unable to acquire additional knowledge to enhance task performance. In this paper, we propose a general post-deployment learning framework Dejavu, which employs an Experience Feedback Network (EFN) and augments the frozen Vision-Language-Action (VLA) policy with retrieved execution memories. EFN identifies contextually prior action experiences and conditions action prediction on this retrieved guidance. We adopt reinforcement learning with semantic similarity rewards to train EFN, ensuring that the predicted actions align with past behaviors under current observations. During deployment, EFN continually enriches its memory with new trajectories, enabling the agent to exhibit \"learning from experience\". Experiments across diverse embodied tasks show that EFN improves adaptability, robustness, and success rates over frozen baselines. We provide code and demo in our supplementary material.",
          "site": "arxiv.org",
          "rank": 26,
          "published": "2025-10-11T11:43:58Z",
          "authors": [
            "Shaokai Wu",
            "Yanbiao Ji",
            "Qiuchang Li",
            "Zhiyi Zhang",
            "Qichen He",
            "Wenyuan Xie",
            "Guodong Zhang",
            "Bayram Bayramli",
            "Yue Ding",
            "Hongtao Lu"
          ],
          "arxiv_id": "2510.10181",
          "abstract": "Embodied agents face a fundamental limitation: once deployed in real-world environments to perform specific tasks, they are unable to acquire additional knowledge to enhance task performance. In this paper, we propose a general post-deployment learning framework Dejavu, which employs an Experience Feedback Network (EFN) and augments the frozen Vision-Language-Action (VLA) policy with retrieved execution memories. EFN identifies contextually prior action experiences and conditions action prediction on this retrieved guidance. We adopt reinforcement learning with semantic similarity rewards to train EFN, ensuring that the predicted actions align with past behaviors under current observations. During deployment, EFN continually enriches its memory with new trajectories, enabling the agent to exhibit \"learning from experience\". Experiments across diverse embodied tasks show that EFN improves adaptability, robustness, and success rates over frozen baselines. We provide code and demo in our supplementary material.",
          "abstract_zh": "实体代理面临着一个根本性的限制：一旦部署在现实环境中执行特定任务，它们就无法获取额外的知识来提高任务性能。在本文中，我们提出了一种通用的部署后学习框架 Dejavu，它采用经验反馈网络（EFN）并通过检索执行记忆来增强冻结的视觉-语言-动作（VLA）策略。EFN 根据检索到的指导识别上下文先前的行动经验和条件行动预测。我们采用具有语义相似性奖励的强化学习来训练 EFN，确保预测的动作与当前观察下的过去行为一致。在部署过程中，EFN 不断用新的轨迹丰富其记忆，使代理能够表现出“从经验中学习”。跨不同具体任务的实验表明，EFN 比冻结基线提高了适应性、稳健性和成功率。我们在补充材料中提供了代码和演示。"
        },
        {
          "title": "MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption",
          "url": "http://arxiv.org/abs/2510.05580v1",
          "snippet": "Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.",
          "site": "arxiv.org",
          "rank": 27,
          "published": "2025-10-07T04:54:39Z",
          "authors": [
            "Chen Li",
            "Zhantao Yang",
            "Han Zhang",
            "Fangyi Chen",
            "Chenchen Zhu",
            "Anudeepsekhar Bolimera",
            "Marios Savvides"
          ],
          "arxiv_id": "2510.05580",
          "abstract": "Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在具身推理方面显示出前景，但距离真正的通才还很远——它们通常需要针对特定​​任务进行微调，并且对未见过的任务的泛化能力很差。我们提出了 MetaVLA，一个统一的、与主干无关的训练后框架，用于高效且可扩展的对齐。MetaVLA 引入了上下文感知元协同训练，它将不同的目标任务整合到一个微调阶段，同时利用结构多样的辅助任务来提高域内泛化能力。与简单的多任务 SFT 不同，MetaVLA 集成了源自注意力神经过程的轻量级元学习机制，能够以最小的架构更改或推理开销快速适应不同的上下文。在 LIBERO 基准测试中，具有 6 个辅助任务的 MetaVLA 在长视野任务上的性能比 OpenVLA 高出 8.0%，将训练步骤从 240K 减少到 75K，并将 GPU 时间缩短约 76%。这些结果表明，可扩展的、低资源的后期训练是可以实现的，为通用的具体代理铺平了道路。代码将可用。"
        },
        {
          "title": "DEAS: DEtached value learning with Action Sequence for Scalable Offline RL",
          "url": "http://arxiv.org/abs/2510.07730v1",
          "snippet": "Offline reinforcement learning (RL) presents an attractive paradigm for training intelligent agents without expensive online interactions. However, current approaches still struggle with complex, long-horizon sequential decision making. In this work, we introduce DEtached value learning with Action Sequence (DEAS), a simple yet effective offline RL framework that leverages action sequences for value learning. These temporally extended actions provide richer information than single-step actions and can be interpreted through the options framework via semi-Markov decision process Q-learning, enabling reduction of the effective planning horizon by considering longer sequences at once. However, directly adopting such sequences in actor-critic algorithms introduces excessive value overestimation, which we address through detached value learning that steers value estimates toward in-distribution actions that achieve high return in the offline dataset. We demonstrate that DEAS consistently outperforms baselines on complex, long-horizon tasks from OGBench and can be applied to enhance the performance of large-scale Vision-Language-Action models that predict action sequences, significantly boosting performance in both RoboCasa Kitchen simulation tasks and real-world manipulation tasks.",
          "site": "arxiv.org",
          "rank": 28,
          "published": "2025-10-09T03:11:09Z",
          "authors": [
            "Changyeon Kim",
            "Haeone Lee",
            "Younggyo Seo",
            "Kimin Lee",
            "Yuke Zhu"
          ],
          "arxiv_id": "2510.07730",
          "abstract": "Offline reinforcement learning (RL) presents an attractive paradigm for training intelligent agents without expensive online interactions. However, current approaches still struggle with complex, long-horizon sequential decision making. In this work, we introduce DEtached value learning with Action Sequence (DEAS), a simple yet effective offline RL framework that leverages action sequences for value learning. These temporally extended actions provide richer information than single-step actions and can be interpreted through the options framework via semi-Markov decision process Q-learning, enabling reduction of the effective planning horizon by considering longer sequences at once. However, directly adopting such sequences in actor-critic algorithms introduces excessive value overestimation, which we address through detached value learning that steers value estimates toward in-distribution actions that achieve high return in the offline dataset. We demonstrate that DEAS consistently outperforms baselines on complex, long-horizon tasks from OGBench and can be applied to enhance the performance of large-scale Vision-Language-Action models that predict action sequences, significantly boosting performance in both RoboCasa Kitchen simulation tasks and real-world manipulation tasks.",
          "abstract_zh": "离线强化学习（RL）为训练智能代理提供了一种有吸引力的范例，无需昂贵的在线交互。然而，当前的方法仍然难以应对复杂的、长期的顺序决策。在这项工作中，我们引入了使用动作序列（DEAS）进行分离价值学习，这是一个简单而有效的离线强化学习框架，利用动作序列进行价值学习。这些暂时扩展的动作提供了比单步动作更丰富的信息，并且可以通过半马尔可夫决策过程 Q 学习通过选项框架进行解释，从而通过一次考虑更长的序列来减少有效的规划范围。然而，在演员批评算法中直接采用此类序列会导致过度的价值高估，我们通过分离的价值学习来解决这个问题，将价值估计引导到在离线数据集中实现高回报的分布行为。我们证明，DEAS 在 OGBench 的复杂、长期任务上始终优于基线，并且可用于增强预测动作序列的大规模视觉-语言-动作模型的性能，从而显着提高 RoboCasa Kitchen 模拟任务和现实世界操作任务的性能。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 6 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 6）。",
      "items": [
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "ChaofanTao/Autoregressive-Models-in-Vision-Survey",
          "url": "https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey",
          "snippet": " [TMLR 2025🔥] A survey for the autoregressive models in vision. ",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "BridgeVLA/BridgeVLA",
          "url": "https://github.com/BridgeVLA/BridgeVLA",
          "snippet": "✨✨【NeurIPS 2025】Official implementation of BridgeVLA",
          "site": "github.com",
          "rank": 6
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-10-06",
  "week_end": "2025-10-12",
  "last_updated": "2026-01-07"
}