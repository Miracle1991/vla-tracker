{
  "generated_at": "2026-01-07T13:38:07.247025",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 28 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 28）。",
      "items": [
        {
          "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation",
          "url": "http://arxiv.org/abs/2510.11660v2",
          "snippet": "While Vision-Language-Action (VLA) models have demonstrated impressive capabilities in robotic manipulation, their performance in complex reasoning and long-horizon task planning is limited by data scarcity and model capacity. To address this, we introduce ManiAgent, an agentic architecture for general manipulation tasks that achieves end-to-end output from task descriptions and environmental inputs to robotic manipulation actions. In this framework, multiple agents involve inter-agent communication to perform environmental perception, sub-task decomposition and action generation, enabling efficient handling of complex manipulation scenarios. Evaluations show ManiAgent achieves an 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world pick-and-place tasks, enabling efficient data collection that yields VLA models with performance comparable to those trained on human-annotated datasets. The project webpage is available at https://yi-yang929.github.io/ManiAgent/.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-10-13T17:34:48Z",
          "authors": [
            "Yi Yang",
            "Kefan Gu",
            "Yuqing Wen",
            "Hebei Li",
            "Yucheng Zhao",
            "Tiancai Wang",
            "Xudong Liu"
          ],
          "arxiv_id": "2510.11660",
          "abstract": "While Vision-Language-Action (VLA) models have demonstrated impressive capabilities in robotic manipulation, their performance in complex reasoning and long-horizon task planning is limited by data scarcity and model capacity. To address this, we introduce ManiAgent, an agentic architecture for general manipulation tasks that achieves end-to-end output from task descriptions and environmental inputs to robotic manipulation actions. In this framework, multiple agents involve inter-agent communication to perform environmental perception, sub-task decomposition and action generation, enabling efficient handling of complex manipulation scenarios. Evaluations show ManiAgent achieves an 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world pick-and-place tasks, enabling efficient data collection that yields VLA models with performance comparable to those trained on human-annotated datasets. The project webpage is available at https://yi-yang929.github.io/ManiAgent/.",
          "abstract_zh": "虽然视觉-语言-动作（VLA）模型在机器人操作方面表现出了令人印象深刻的能力，但它们在复杂推理和长期任务规划方面的表现受到数据稀缺和模型容量的限制。为了解决这个问题，我们引入了 ManiAgent，这是一种用于一般操作任务的代理架构，可实现从任务描述和环境输入到机器人操作动作的端到端输出。在此框架中，多个智能体涉及智能体间通信来执行环境感知、子任务分解和动作生成，从而能够有效处理复杂的操作场景。评估显示，ManiAgent 在 SimplerEnv 基准测试中实现了 86.8% 的成功率，在现实世界的拾放任务中实现了 95.8% 的成功率，从而实现了高效的数据收集，生成的 VLA 模型的性能可与在人工注释数据集上训练的模型相媲美。该项目网页位于 https://yi-yang929.github.io/ManiAgent/。"
        },
        {
          "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation",
          "url": "http://arxiv.org/abs/2510.17640v2",
          "snippet": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance on complex robotic manipulation tasks through imitation learning. However, existing imitation learning datasets contain only successful trajectories and lack failure or recovery data, especially for out-of-distribution (OOD) states where the robot deviates from the main policy due to minor perturbations or errors, leading VLA models to struggle with states deviating from the training distribution. To this end, we propose an automated OOD data augmentation framework named RESample through exploratory sampling. Specifically, we first leverage offline reinforcement learning to obtain an action-value network that accurately identifies sub-optimal actions under the current manipulation policy. We further sample potential OOD states from trajectories via rollout, and design an exploratory sampling mechanism that adaptively incorporates these action proxies into the training dataset to ensure efficiency. Subsequently, our framework explicitly encourages the VLAs to recover from OOD states and enhances their robustness against distributional shifts. We conduct extensive experiments on the LIBERO benchmark as well as real-world robotic manipulation tasks, demonstrating that RESample consistently improves the stability and generalization ability of VLA models.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-10-20T15:21:12Z",
          "authors": [
            "Yuquan Xue",
            "Guanxing Lu",
            "Zhenyu Wu",
            "Chuanrui Zhang",
            "Bofang Jia",
            "Zhengyi Gu",
            "Yansong Tang",
            "Ziwei Wang"
          ],
          "arxiv_id": "2510.17640",
          "abstract": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance on complex robotic manipulation tasks through imitation learning. However, existing imitation learning datasets contain only successful trajectories and lack failure or recovery data, especially for out-of-distribution (OOD) states where the robot deviates from the main policy due to minor perturbations or errors, leading VLA models to struggle with states deviating from the training distribution. To this end, we propose an automated OOD data augmentation framework named RESample through exploratory sampling. Specifically, we first leverage offline reinforcement learning to obtain an action-value network that accurately identifies sub-optimal actions under the current manipulation policy. We further sample potential OOD states from trajectories via rollout, and design an exploratory sampling mechanism that adaptively incorporates these action proxies into the training dataset to ensure efficiency. Subsequently, our framework explicitly encourages the VLAs to recover from OOD states and enhances their robustness against distributional shifts. We conduct extensive experiments on the LIBERO benchmark as well as real-world robotic manipulation tasks, demonstrating that RESample consistently improves the stability and generalization ability of VLA models.",
          "abstract_zh": "视觉-语言-动作模型（VLA）通过模仿学习在复杂的机器人操作任务中表现出了卓越的性能。然而，现有的模仿学习数据集仅包含成功的轨迹，缺乏失败或恢复数据，特别是对于机器人由于微小扰动或错误而偏离主要策略的分布外（OOD）状态，导致VLA模型与偏离训练分布的状态作斗争。为此，我们通过探索性采样提出了一个名为 RESample 的自动化 OOD 数据增强框架。具体来说，我们首先利用离线强化学习来获得一个动作价值网络，该网络可以准确识别当前操纵策略下的次优动作。我们通过推出进一步从轨迹中采样潜在的 OOD 状态，并设计一种探索性采样机制，自适应地将这些动作代理合并到训练数据集中以确保效率。随后，我们的框架明确鼓励 VLA 从 OOD 状态中恢复，并增强其针对分配变化的鲁棒性。我们对 LIBERO 基准以及现实世界的机器人操作任务进行了广泛的实验，证明 RESample 持续提高了 VLA 模型的稳定性和泛化能力。"
        },
        {
          "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots",
          "url": "http://arxiv.org/abs/2510.17369v1",
          "snippet": "Robotic systems are increasingly expected to operate in human-centered, unstructured environments where safety, adaptability, and generalization are essential. Vision-Language-Action (VLA) models have been proposed as a language guided generalized control framework for real robots. However, their deployment has been limited to conventional serial link manipulators. Coupled by their rigidity and unpredictability of learning based control, the ability to safely interact with the environment is missing yet critical. In this work, we present the deployment of a VLA model on a soft continuum manipulator to demonstrate autonomous safe human-robot interaction. We present a structured finetuning and deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and $π_0$) across representative manipulation tasks, and show while out-of-the-box policies fail due to embodiment mismatch, through targeted finetuning the soft robot performs equally to the rigid counterpart. Our findings highlight the necessity of finetuning for bridging embodiment gaps, and demonstrate that coupling VLA models with soft robots enables safe and flexible embodied AI in human-shared environments.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-10-20T10:06:39Z",
          "authors": [
            "Haochen Su",
            "Cristian Meo",
            "Francesco Stella",
            "Andrea Peirone",
            "Kai Junge",
            "Josie Hughes"
          ],
          "arxiv_id": "2510.17369",
          "abstract": "Robotic systems are increasingly expected to operate in human-centered, unstructured environments where safety, adaptability, and generalization are essential. Vision-Language-Action (VLA) models have been proposed as a language guided generalized control framework for real robots. However, their deployment has been limited to conventional serial link manipulators. Coupled by their rigidity and unpredictability of learning based control, the ability to safely interact with the environment is missing yet critical. In this work, we present the deployment of a VLA model on a soft continuum manipulator to demonstrate autonomous safe human-robot interaction. We present a structured finetuning and deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and $π_0$) across representative manipulation tasks, and show while out-of-the-box policies fail due to embodiment mismatch, through targeted finetuning the soft robot performs equally to the rigid counterpart. Our findings highlight the necessity of finetuning for bridging embodiment gaps, and demonstrate that coupling VLA models with soft robots enables safe and flexible embodied AI in human-shared environments.",
          "abstract_zh": "人们越来越期望机器人系统能够在以人为中心的非结构化环境中运行，在这些环境中，安全性、适应性和通用性至关重要。视觉-语言-动作（VLA）模型已被提议作为真实机器人的语言引导广义控制框架。然而，它们的部署仅限于传统的串行链路操纵器。再加上基于学习的控制的刚性和不可预测性，与环境安全交互的能力缺失但至关重要。在这项工作中，我们提出了在软连续体机械臂上部署 VLA 模型，以演示自主安全的人机交互。我们提出了一个结构化的微调和部署管道，评估两个最先进的 VLA 模型（OpenVLA-OFT 和 $π_0$）在代表性操作任务中的表现，并表明，虽然开箱即用的策略由于实施例不匹配而失败，但通过有针对性的微调，软机器人的性能与刚性机器人相同。我们的研究结果强调了进行微调以弥补体现差距的必要性，并证明将 VLA 模型与软机器人相结合可以在人类共享环境中实现安全、灵活的体现人工智能。"
        },
        {
          "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy",
          "url": "http://arxiv.org/abs/2510.13778v1",
          "snippet": "We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-10-15T17:30:05Z",
          "authors": [
            "Xinyi Chen",
            "Yilun Chen",
            "Yanwei Fu",
            "Ning Gao",
            "Jiaya Jia",
            "Weiyang Jin",
            "Hao Li",
            "Yao Mu",
            "Jiangmiao Pang",
            "Yu Qiao",
            "Yang Tian",
            "Bin Wang",
            "Bolun Wang",
            "Fangjing Wang",
            "Hanqing Wang",
            "Tai Wang",
            "Ziqin Wang",
            "Xueyuan Wei",
            "Chao Wu",
            "Shuai Yang",
            "Jinhui Ye",
            "Junqiu Yu",
            "Jia Zeng",
            "Jingjing Zhang",
            "Jinyu Zhang",
            "Shi Zhang",
            "Feng Zheng",
            "Bowen Zhou",
            "Yangkun Zhu"
          ],
          "arxiv_id": "2510.13778",
          "abstract": "We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
          "abstract_zh": "我们推出了 InternVLA-M1，这是一个用于空间接地和机器人控制的统一框架，可将遵循指令的机器人推向可扩展的通用智能。其核心思想是空间引导的视觉-语言-动作训练，其中空间基础是指令和机器人动作之间的关键环节。InternVLA-M1 采用两阶段流程：(i) 对超过 2.3M 空间推理数据进行空间基础预训练，通过将指令与视觉、与具体实施无关的位置对齐来确定“在哪里行动”，以及 (ii) 空间引导动作后训练，通过即插即用的空间提示生成具体实施感知的行动来决定“如何行动”。这种空间引导的训练方法产生了一致的收益：InternVLA-M1 在 SimplerEnv Google Robot 上比没有空间引导的变体高出 14.6%，在 WidowX 上高出 17%，在 LIBERO Franka 上高出 4.3%，同时在框、点和轨迹预测方面表现出更强的空间推理能力。为了进一步扩展指令跟踪，我们构建了一个模拟引擎来收集 244K 个通用的拾取和放置片段，使 200 个任务和 3K+ 对象的平均改进达到 6.2%。在现实世界的集群拾放中，InternVLA-M1 提高了 7.3%，并且通过综合协同训练，在未见过的物体和新颖配置上实现了 +20.6%。此外，在长视域推理密集型场景中，它超越了现有作品10%以上。这些结果强调了空间引导训练是可扩展和有弹性的通用机器人的统一原则。代码和模型可在 https://github.com/InternRobotics/InternVLA-M1 获取。"
        },
        {
          "title": "MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation",
          "url": "http://arxiv.org/abs/2510.16617v1",
          "snippet": "Vision-Language-Action (VLA) models trained on large robot datasets promise general-purpose, robust control across diverse domains and embodiments. However, existing approaches often fail out-of-the-box when deployed in novel environments, embodiments, or tasks. We introduce Mixture of Skills VLA (MoS-VLA), a framework that represents robot manipulation policies as linear combinations of a finite set of learned basis functions. During pretraining, MoS-VLA jointly learns these basis functions across datasets from the Open X-Embodiment project, producing a structured skill space. At test time, adapting to a new task requires only a single expert demonstration. The corresponding skill representation is then inferred via a lightweight convex optimization problem that minimizes the L1 action error, without requiring gradient updates. This gradient-free adaptation incurs minimal overhead while enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower action-prediction error on five out of five unseen datasets and succeeds in both simulation and real-robot tasks where a pretrained VLA model fails outright. Project page: mos-vla.github.io/",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-10-18T19:16:08Z",
          "authors": [
            "Ruihan Zhao",
            "Tyler Ingebrand",
            "Sandeep Chinchali",
            "Ufuk Topcu"
          ],
          "arxiv_id": "2510.16617",
          "abstract": "Vision-Language-Action (VLA) models trained on large robot datasets promise general-purpose, robust control across diverse domains and embodiments. However, existing approaches often fail out-of-the-box when deployed in novel environments, embodiments, or tasks. We introduce Mixture of Skills VLA (MoS-VLA), a framework that represents robot manipulation policies as linear combinations of a finite set of learned basis functions. During pretraining, MoS-VLA jointly learns these basis functions across datasets from the Open X-Embodiment project, producing a structured skill space. At test time, adapting to a new task requires only a single expert demonstration. The corresponding skill representation is then inferred via a lightweight convex optimization problem that minimizes the L1 action error, without requiring gradient updates. This gradient-free adaptation incurs minimal overhead while enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower action-prediction error on five out of five unseen datasets and succeeds in both simulation and real-robot tasks where a pretrained VLA model fails outright. Project page: mos-vla.github.io/",
          "abstract_zh": "在大型机器人数据集上训练的视觉-语言-动作 (VLA) 模型有望实现跨不同领域和实施例的通用、稳健的控制。然而，当部署在新的环境、实施例或任务中时，现有方法通常会失败。我们引入了混合技能 VLA (MoS-VLA)，这是一个框架，它将机器人操作策略表示为一组有限的学习基函数的线性组合。在预训练期间，MoS-VLA 跨 Open X-Embodiment 项目的数据集共同学习这些基本函数，从而生成结构化的技能空间。在测试时，适应新任务只需要一次专家演示。然后通过轻量级凸优化问题推断相应的技能表示，该问题最小化 L1 动作误差，而不需要梯度更新。这种无梯度适应会产生最小的开销，同时能够快速实例化新技能。根据经验，MoS-VLA 在五个未见过的数据集中的五个上实现了较低的动作预测误差，并且在预训练的 VLA 模型完全失败的模拟和真实机器人任务中取得了成功。项目页面：mos-vla.github.io/"
        },
        {
          "title": "VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving",
          "url": "http://arxiv.org/abs/2510.15446v1",
          "snippet": "In autonomous driving, dynamic environment and corner cases pose significant challenges to the robustness of ego vehicle's state understanding and decision making. We introduce VDRive, a novel pipeline for end-to-end autonomous driving that explicitly models state-action mapping to address these challenges, enabling interpretable and robust decision making. By leveraging the advancement of the state understanding of the Vision Language Action Model (VLA) with generative diffusion policy-based action head, our VDRive guides the driving contextually and geometrically. Contextually, VLA predicts future observations through token generation pre-training, where the observations are represented as discrete codes by a Conditional Vector Quantized Variational Autoencoder (CVQ-VAE). Geometrically, we perform reinforcement learning fine-tuning of the VLA to predict future trajectories and actions based on current driving conditions. VLA supplies the current state tokens and predicted state tokens for the action policy head to generate hierarchical actions and trajectories. During policy training, a learned critic evaluates the actions generated by the policy and provides gradient-based feedback, forming an actor-critic framework that enables a reinforcement-based policy learning pipeline. Experiments show that our VDRive achieves state-of-the-art performance in the Bench2Drive closed-loop benchmark and nuScenes open-loop planning.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-10-17T09:02:18Z",
          "authors": [
            "Ziang Guo",
            "Zufeng Zhang"
          ],
          "arxiv_id": "2510.15446",
          "abstract": "In autonomous driving, dynamic environment and corner cases pose significant challenges to the robustness of ego vehicle's state understanding and decision making. We introduce VDRive, a novel pipeline for end-to-end autonomous driving that explicitly models state-action mapping to address these challenges, enabling interpretable and robust decision making. By leveraging the advancement of the state understanding of the Vision Language Action Model (VLA) with generative diffusion policy-based action head, our VDRive guides the driving contextually and geometrically. Contextually, VLA predicts future observations through token generation pre-training, where the observations are represented as discrete codes by a Conditional Vector Quantized Variational Autoencoder (CVQ-VAE). Geometrically, we perform reinforcement learning fine-tuning of the VLA to predict future trajectories and actions based on current driving conditions. VLA supplies the current state tokens and predicted state tokens for the action policy head to generate hierarchical actions and trajectories. During policy training, a learned critic evaluates the actions generated by the policy and provides gradient-based feedback, forming an actor-critic framework that enables a reinforcement-based policy learning pipeline. Experiments show that our VDRive achieves state-of-the-art performance in the Bench2Drive closed-loop benchmark and nuScenes open-loop planning.",
          "abstract_zh": "在自动驾驶中，动态环境和极端情况对自我车辆的状态理解和决策的鲁棒性提出了重大挑战。我们推出了 VDRive，这是一种用于端到端自动驾驶的新型管道，它可以显式地模拟状态-动作映射来应对这些挑战，从而实现可解释和稳健的决策。通过利用基于生成扩散策略的行动头对视觉语言行动模型 (VLA) 的状态理解的进步，我们的 VDRive 可以根据情境和几何方式指导驾驶。在上下文中，VLA 通过令牌生成预训练来预测未来的观察结果，其中观察结果由条件向量量化变分自动编码器 (CVQ-VAE) 表示为离散代码。从几何角度来看，我们对 VLA 进行强化学习微调，以根据当前驾驶条件预测未来的轨迹和动作。VLA 为动作策略头提供当前状态令牌和预测状态令牌，以生成分层动作和轨迹。在政策训练期间，有学识的批评家评估政策生成的行动并提供基于梯度的反馈，形成一个演员批评家框架，从而实现基于强化的政策学习管道。实验表明，我们的 VDRive 在 Bench2Drive 闭环基准测试和 nuScenes 开环规划中实现了最先进的性能。"
        },
        {
          "title": "RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model",
          "url": "http://arxiv.org/abs/2510.10975v2",
          "snippet": "Vision-Language-Action (VLA) models have become a prominent paradigm for embodied intelligence, yet further performance improvements typically rely on scaling up training data and model size -- an approach that is prohibitively expensive for robotics and fundamentally limited by data collection costs. We address this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling framework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a Test-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA models without modifying their architectures or weights. Specifically, RoVer (i) assigns scalar-based process rewards to evaluate the reliability of candidate actions, and (ii) predicts an action-space direction for candidate expansion/refinement. During inference, RoVer generates multiple candidate actions concurrently from the base policy, expands them along PRM-predicted directions, and then scores all candidates with PRM to select the optimal action for execution. Notably, by caching shared perception features, it can amortize perception cost and evaluate more candidates under the same test-time computational budget. Essentially, our approach effectively transforms available computing resources into better action decision-making, realizing the benefits of test-time scaling without extra training overhead. Our contributions are threefold: (1) a general, plug-and-play test-time scaling framework for VLAs; (2) a PRM that jointly provides scalar process rewards and an action-space direction to guide exploration; and (3) an efficient direction-guided sampling strategy that leverages a shared perception cache to enable scalable candidate generation and selection during inference.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-10-13T03:26:14Z",
          "authors": [
            "Mingtong Dai",
            "Lingbo Liu",
            "Yongjie Bai",
            "Yang Liu",
            "Zhouxia Wang",
            "Rui SU",
            "Chunjie Chen",
            "Liang Lin",
            "Xinyu Wu"
          ],
          "arxiv_id": "2510.10975",
          "abstract": "Vision-Language-Action (VLA) models have become a prominent paradigm for embodied intelligence, yet further performance improvements typically rely on scaling up training data and model size -- an approach that is prohibitively expensive for robotics and fundamentally limited by data collection costs. We address this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling framework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a Test-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA models without modifying their architectures or weights. Specifically, RoVer (i) assigns scalar-based process rewards to evaluate the reliability of candidate actions, and (ii) predicts an action-space direction for candidate expansion/refinement. During inference, RoVer generates multiple candidate actions concurrently from the base policy, expands them along PRM-predicted directions, and then scores all candidates with PRM to select the optimal action for execution. Notably, by caching shared perception features, it can amortize perception cost and evaluate more candidates under the same test-time computational budget. Essentially, our approach effectively transforms available computing resources into better action decision-making, realizing the benefits of test-time scaling without extra training overhead. Our contributions are threefold: (1) a general, plug-and-play test-time scaling framework for VLAs; (2) a PRM that jointly provides scalar process rewards and an action-space direction to guide exploration; and (3) an efficient direction-guided sampling strategy that leverages a shared perception cache to enable scalable candidate generation and selection during inference.",
          "abstract_zh": "视觉-语言-动作 (VLA) 模型已成为体现智能的重要范例，但进一步的性能改进通常依赖于扩大训练数据和模型大小，这种方法对于机器人技术而言成本高昂，并且从根本上受到数据收集成本的限制。我们通过 $\\mathbf{RoVer}$ 解决了这一限制，这是一个具体的测试时间扩展框架，它使用 $\\mathbf{Ro}$bot 过程奖励模型（PRM）作为测试时间 $\\mathbf{Ver}$ifier 来增强现有 VLA 模型的功能，而无需修改其架构或权重。具体来说，RoVer (i) 分配基于标量的过程奖励来评估候选动作的可靠性，以及 (ii) 预测候选扩展/细化的动作空间方向。在推理过程中，RoVer 根据基本策略同时生成多个候选操作，将它们沿着 PRM 预测的方向扩展，然后使用 PRM 对所有候选操作进行评分，以选择最佳执行操作。值得注意的是，通过缓存共享感知特征，它可以分摊感知成本并在相同的测试时间计算预算下评估更多候选者。本质上，我们的方法有效地将可用的计算资源转化为更好的行动决策，实现测试时间扩展的好处，而无需额外的培训开销。我们的贡献有三方面：(1) 通用的、即插即用的 VLA 测试时间扩展框架；(2) 联合提供标量过程奖励和指导探索的动作空间方向的 PRM；(3) 高效的方向引导采样策略，利用共享感知缓存来在推理过程中实现可扩展的候选生成和选择。"
        },
        {
          "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
          "url": "http://arxiv.org/abs/2510.11027v1",
          "snippet": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-10-13T05:51:22Z",
          "authors": [
            "Ganlin Yang",
            "Tianyi Zhang",
            "Haoran Hao",
            "Weiyun Wang",
            "Yibin Liu",
            "Dehui Wang",
            "Guanzhou Chen",
            "Zijian Cai",
            "Junting Chen",
            "Weijie Su",
            "Wengang Zhou",
            "Yu Qiao",
            "Jifeng Dai",
            "Jiangmiao Pang",
            "Gen Luo",
            "Wenhai Wang",
            "Yao Mu",
            "Zhi Hou"
          ],
          "arxiv_id": "2510.11027",
          "abstract": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.",
          "abstract_zh": "虽然重要的研究集中在使用视觉语言模型（VLM）开发体现推理能力或将先进的 VLM 集成到视觉语言动作（VLA）模型中以实现端到端机器人控制，但很少有研究直接解决上游基于 VLM 的推理和下游 VLA 策略学习之间的关键差距。在这项工作中，我们通过引入 Vlaser - 一种具有协同体现推理能力的视觉语言动作模型，迈出了桥接体现推理与 VLA 策略学习的第一步，它是一种基础视觉语言模型，旨在将高级推理与体现代理的低级控制相结合。Vlaser 基于高质量的 Vlaser-6M 数据集而构建，在一系列具身推理基准上实现了最先进的性能 - 包括空间推理、具身基础、具身 QA 和任务规划。此外，我们系统地研究了不同的 VLM 初始化如何影响有监督的 VLA 微调，为减轻互联网规模的预训练数据和具体体现的策略学习数据之间的领域转移提供了新颖的见解。基于这些见解，我们的方法在 WidowX 基准测试中取得了最先进的结果，在 Google Robot 基准测试中取得了具有竞争力的性能。"
        },
        {
          "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.14836v2",
          "snippet": "Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-10-16T16:11:18Z",
          "authors": [
            "Yixuan Li",
            "Yuhui Chen",
            "Mingcai Zhou",
            "Haoran Li",
            "Zhengtao Zhang",
            "Dongbin Zhao"
          ],
          "arxiv_id": "2510.14836",
          "abstract": "Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.",
          "abstract_zh": "空间感知和推理对于视觉-语言-动作（VLA）模型完成细粒度的操作任务至关重要。然而，现有方法通常缺乏理解和推理精确控制所需的基本 3D 结构的能力。为了解决这个限制，我们提出了 QDepth-VLA，这是一个通用框架，通过辅助深度预测任务来增强 VLA 模型。专门的深度专家旨在预测从 VQ-VAE 编码器获得的深度图的量化潜在标记，使模型能够学习捕获关键几何线索的深度感知表示。模拟基准和实际任务的实验结果表明，QDepth-VLA 在操作任务上具有强大的空间推理能力和竞争性能。"
        },
        {
          "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation",
          "url": "http://arxiv.org/abs/2510.14902v1",
          "snippet": "Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-10-16T17:18:34Z",
          "authors": [
            "Han Zhao",
            "Jiaxuan Zhang",
            "Wenxuan Song",
            "Pengxiang Ding",
            "Donglin Wang"
          ],
          "arxiv_id": "2510.14902",
          "abstract": "Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io.",
          "abstract_zh": "当前的视觉-语言-动作（VLA）模型在大规模机器人数据上进行了预先训练，表现出强大的多任务能力，并且可以很好地泛化到视觉和语言操作指令的变化。然而，当面对训练数据之外的对象概念时，例如数据集中看不见的对象描述和纹理，它们的成功率会显着下降。为了解决这个问题，我们提出了一种新颖的代理框架 VLA^2，它利用 OpenVLA 作为执行骨干，并有效地利用外部模块（例如 Web 检索和对象检测）向 VLA 提供有关目标对象的视觉和文本知识。这种方法可以减少处理分布外对象时的泛化失败。基于LIBERO模拟环境，我们引入了新颖的对象和对象描述来构建一个具有三个难度级别的新评估基准来测试我们方法的有效性。我们的框架在我们设计的硬级泛化基准上成功地超越了当前最先进的模型。与独立的 OpenVLA 基准相比，VLA^2 在硬级基准测试中的成功率提高了 44.2%，在所有定制环境中平均提高了 20.2%，且域内任务的性能没有任何下降。项目网站：https://vla-2.github.io。"
        },
        {
          "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning",
          "url": "http://arxiv.org/abs/2510.14300v1",
          "snippet": "Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-10-16T04:52:57Z",
          "authors": [
            "Weijie Shen",
            "Yitian Liu",
            "Yuhao Wu",
            "Zhixuan Liang",
            "Sijia Gu",
            "Dehui Wang",
            "Tian Nian",
            "Lei Xu",
            "Yusen Qin",
            "Jiangmiao Pang",
            "Xinping Guan",
            "Xiaokang Yang",
            "Yao Mu"
          ],
          "arxiv_id": "2510.14300",
          "abstract": "Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks.",
          "abstract_zh": "视觉-语言-动作（VLA）模型正在经历快速发展，并在机器人操作任务中展现出有前景的能力。然而，扩展 VLA 模型面临着几个关键挑战：（1）从头开始训练新的 VLA 模型需要大量的计算资源和广泛的数据集。鉴于当前机器人数据的稀缺性，在缩放过程中充分利用预先训练好的 VLA 模型权重变得特别有价值。(2)实时控制需要仔细平衡模型容量和计算效率。为了应对这些挑战，我们提出了 AdaMoE，这是一种专家混合 (MoE) 架构，它继承了密集 VLA 模型的预训练权重，并通过将前馈层替换为稀疏激活的 MoE 层来扩展动作专家。AdaMoE 采用解耦技术，通过与传统路由器一起工作的独立秤适配器，将专家选择与专家加权解耦。这使得能够根据任务相关性选择专家，同时贡献独立控制的权重，从而允许协作专家利用，而不是赢者通吃的动态。我们的方法表明，专业知识不需要垄断。相反，通过协作专家利用，我们可以在保持计算效率的同时实现卓越的性能。AdaMoE 在关键基准测试中始终优于基准模型，在 LIBERO 上实现了 1.8% 的性能提升，在 RoboTwin 上实现了 9.3% 的性能提升。最重要的是，现实世界实验中 21.5% 的大幅改进验证了其对于机器人操作任务的实际有效性。"
        },
        {
          "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
          "url": "http://arxiv.org/abs/2510.13054v1",
          "snippet": "Vision-Language-Action models (VLAs) hold immense promise for enabling generalist robot manipulation. However, the best way to build them remains an open question. Current approaches often add complexity, such as modifying the existing vocabulary of a Vision-Language Model (VLM) with action tokens or introducing special action heads. Curiously, the simplest strategy of representing actions directly as text has remained largely unexplored. This work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only effective; it is surprisingly powerful. With the right design, VLA-0 outperforms more involved models. On LIBERO, a popular benchmark for evaluating VLAs, VLA-0 outperforms all existing methods trained on the same robotic data, including $π_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without large-scale robotics-specific training, it outperforms methods trained on large-scale robotic data, like $π_0.5$-KI, $π_0$, GR00T-N1 and MolmoAct. These findings also translate to the real world, where VLA-0 outperforms SmolVLA, a VLA model pre-trained on large-scale real data. This paper summarizes our unexpected findings and spells out the specific techniques required to unlock the high performance of this simple yet potent VLA design. Visual results, code, and trained models are provided here: https://vla0.github.io/.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-10-15T00:31:10Z",
          "authors": [
            "Ankit Goyal",
            "Hugo Hadfield",
            "Xuning Yang",
            "Valts Blukis",
            "Fabio Ramos"
          ],
          "arxiv_id": "2510.13054",
          "abstract": "Vision-Language-Action models (VLAs) hold immense promise for enabling generalist robot manipulation. However, the best way to build them remains an open question. Current approaches often add complexity, such as modifying the existing vocabulary of a Vision-Language Model (VLM) with action tokens or introducing special action heads. Curiously, the simplest strategy of representing actions directly as text has remained largely unexplored. This work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only effective; it is surprisingly powerful. With the right design, VLA-0 outperforms more involved models. On LIBERO, a popular benchmark for evaluating VLAs, VLA-0 outperforms all existing methods trained on the same robotic data, including $π_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without large-scale robotics-specific training, it outperforms methods trained on large-scale robotic data, like $π_0.5$-KI, $π_0$, GR00T-N1 and MolmoAct. These findings also translate to the real world, where VLA-0 outperforms SmolVLA, a VLA model pre-trained on large-scale real data. This paper summarizes our unexpected findings and spells out the specific techniques required to unlock the high performance of this simple yet potent VLA design. Visual results, code, and trained models are provided here: https://vla0.github.io/.",
          "abstract_zh": "视觉-语言-动作模型（VLA）对于实现通用机器人操作有着巨大的希望。然而，构建它们的最佳方法仍然是一个悬而未决的问题。当前的方法通常会增加复杂性，例如使用动作标记修改视觉语言模型（VLM）的现有词汇表或引入特殊的动作头。奇怪的是，将动作直接表示为文本的最简单策略在很大程度上仍未得到探索。这项工作引入了 VLA-0 来研究这个想法。我们发现VLA-0不仅有效，而且有效。它的威力惊人。通过正确的设计，VLA-0 的性能优于更多复杂的型号。在评估 VLA 的流行基准 LIBERO 上，VLA-0 优于所有在相同机器人数据上训练的现有方法，包括 $π_0.5$-KI、OpenVLA-OFT 和 SmolVLA。此外，在没有大规模机器人特定训练的情况下，它的性能优于在大规模机器人数据上训练的方法，例如 $π_0.5$-KI、$π_0$、GR00T-N1 和 MolmoAct。这些发现也适用于现实世界，VLA-0 的性能优于 SmolVLA（一种在大规模真实数据上预训练的 VLA 模型）。本文总结了我们意想不到的发现，并详细说明了释放这种简单但有效的 VLA 设计的高性能所需的具体技术。这里提供了视觉结果、代码和训练模型：https://vla0.github.io/。"
        },
        {
          "title": "Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.13237v1",
          "snippet": "Vision-Language-Action (VLA) models have achieved revolutionary progress in robot learning, enabling robots to execute complex physical robot tasks from natural language instructions. Despite this progress, their adversarial robustness remains underexplored. In this work, we propose both adversarial patch attack and corresponding defense strategies for VLA models. We first introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic adversarial attack that generates patches directly placeable within the camera's view. In comparison to prior methods, EDPA can be readily applied to different VLA models without requiring prior knowledge of the model architecture, or the controlled robotic manipulator. EDPA constructs these patches by (i) disrupting the semantic alignment between visual and textual latent representations, and (ii) maximizing the discrepancy of latent representations between adversarial and corresponding clean visual inputs. Through the optimization of these objectives, EDPA distorts the VLA's interpretation of visual information, causing the model to repeatedly generate incorrect actions and ultimately result in failure to complete the given robotic task. To counter this, we propose an adversarial fine-tuning scheme for the visual encoder, in which the encoder is optimized to produce similar latent representations for both clean and adversarially perturbed visual inputs. Extensive evaluations on the widely recognized LIBERO robotic simulation benchmark demonstrate that EDPA substantially increases the task failure rate of cutting-edge VLA models, while our proposed defense effectively mitigates this degradation. The codebase is accessible via the homepage at https://edpa-attack.github.io/.",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-10-15T07:42:44Z",
          "authors": [
            "Haochuan Xu",
            "Yun Sing Koh",
            "Shuhuai Huang",
            "Zirun Zhou",
            "Di Wang",
            "Jun Sakuma",
            "Jingfeng Zhang"
          ],
          "arxiv_id": "2510.13237",
          "abstract": "Vision-Language-Action (VLA) models have achieved revolutionary progress in robot learning, enabling robots to execute complex physical robot tasks from natural language instructions. Despite this progress, their adversarial robustness remains underexplored. In this work, we propose both adversarial patch attack and corresponding defense strategies for VLA models. We first introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic adversarial attack that generates patches directly placeable within the camera's view. In comparison to prior methods, EDPA can be readily applied to different VLA models without requiring prior knowledge of the model architecture, or the controlled robotic manipulator. EDPA constructs these patches by (i) disrupting the semantic alignment between visual and textual latent representations, and (ii) maximizing the discrepancy of latent representations between adversarial and corresponding clean visual inputs. Through the optimization of these objectives, EDPA distorts the VLA's interpretation of visual information, causing the model to repeatedly generate incorrect actions and ultimately result in failure to complete the given robotic task. To counter this, we propose an adversarial fine-tuning scheme for the visual encoder, in which the encoder is optimized to produce similar latent representations for both clean and adversarially perturbed visual inputs. Extensive evaluations on the widely recognized LIBERO robotic simulation benchmark demonstrate that EDPA substantially increases the task failure rate of cutting-edge VLA models, while our proposed defense effectively mitigates this degradation. The codebase is accessible via the homepage at https://edpa-attack.github.io/.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在机器人学习方面取得了革命性的进步，使机器人能够根据自然语言指令执行复杂的物理机器人任务。尽管取得了这些进展，但它们的对抗鲁棒性仍未得到充分探索。在这项工作中，我们为 VLA 模型提出了对抗性补丁攻击和相应的防御策略。我们首先介绍嵌入破坏补丁攻击（EDPA），这是一种与模型无关的对抗性攻击，可生成可直接放置在相机视图内的补丁。与现有方法相比，EDPA 可以轻松应用于不同的 VLA 模型，无需事先了解模型架构或受控机器人操纵器。EDP​​A 通过（i）破坏视觉和文本潜在表示之间的语义对齐，以及（ii）最大化对抗性和相应的干净视觉输入之间潜在表示的差异来构建这些补丁。通过对这些目标的优化，EDPA 扭曲了 VLA 对视觉信息的解释，导致模型反复生成不正确的动作，最终导致无法完成给定的机器人任务。为了解决这个问题，我们提出了一种针对视觉编码器的对抗性微调方案，其中编码器经过优化，可以为干净的和对抗性扰动的视觉输入产生类似的潜在表示。对广泛认可的 LIBERO 机器人仿真基准的广泛评估表明，EDPA 大大增加了尖端 VLA 模型的任务失败率，而我们提出的防御措施有效地减轻了这种退化。该代码库可通过主页 https://edpa-attack.github.io/ 访问。"
        },
        {
          "title": "RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies",
          "url": "http://arxiv.org/abs/2510.17950v1",
          "snippet": "Testing on real machines is indispensable for robotic control algorithms. In the context of learning-based algorithms, especially VLA models, demand for large-scale evaluation, i.e. testing a large number of models on a large number of tasks, is becoming increasingly urgent. However, doing this right is highly non-trivial, especially when scalability and reproducibility is taken into account. In this report, we describe our methodology for constructing RoboChallenge, an online evaluation system to test robotic control algorithms, and our survey of recent state-of-the-art VLA models using our initial benchmark Table30.",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-10-20T17:59:14Z",
          "authors": [
            "Adina Yakefu",
            "Bin Xie",
            "Chongyang Xu",
            "Enwen Zhang",
            "Erjin Zhou",
            "Fan Jia",
            "Haitao Yang",
            "Haoqiang Fan",
            "Haowei Zhang",
            "Hongyang Peng",
            "Jing Tan",
            "Junwen Huang",
            "Kai Liu",
            "Kaixin Liu",
            "Kefan Gu",
            "Qinglun Zhang",
            "Ruitao Zhang",
            "Saike Huang",
            "Shen Cheng",
            "Shuaicheng Liu",
            "Tiancai Wang",
            "Tiezhen Wang",
            "Wei Sun",
            "Wenbin Tang",
            "Yajun Wei",
            "Yang Chen",
            "Youqiang Gui",
            "Yucheng Zhao",
            "Yunchao Ma",
            "Yunfei Wei",
            "Yunhuan Yang",
            "Yutong Guo",
            "Ze Chen",
            "Zhengyuan Du",
            "Ziheng Zhang",
            "Ziming Liu",
            "Ziwei Yan"
          ],
          "arxiv_id": "2510.17950",
          "abstract": "Testing on real machines is indispensable for robotic control algorithms. In the context of learning-based algorithms, especially VLA models, demand for large-scale evaluation, i.e. testing a large number of models on a large number of tasks, is becoming increasingly urgent. However, doing this right is highly non-trivial, especially when scalability and reproducibility is taken into account. In this report, we describe our methodology for constructing RoboChallenge, an online evaluation system to test robotic control algorithms, and our survey of recent state-of-the-art VLA models using our initial benchmark Table30.",
          "abstract_zh": "对于机器人控制算法来说，真机测试是必不可少的。在基于学习的算法，特别是VLA模型的背景下，大规模评估的需求，即在大量任务上测试大量模型，变得越来越迫切。然而，正确地做到这一点非常重要，特别是考虑到可扩展性和可重复性时。在本报告中，我们描述了构建 RoboChallenge（一个用于测试机器人控制算法的在线评估系统）的方法，以及我们使用初始基准 Table30 对最新最先进的 VLA 模型进行的调查。"
        },
        {
          "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
          "url": "http://arxiv.org/abs/2510.17111v3",
          "snippet": "Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. Despite their capabilities, VLA systems face significant challenges due to their massive computational and memory demands, which conflict with the constraints of edge platforms such as on-board mobile manipulators that require real-time performance. Addressing this tension has become a central focus of recent research. In light of the growing efforts toward more efficient and scalable VLA systems, this survey provides a systematic review of approaches for improving VLA efficiency, with an emphasis on reducing latency, memory footprint, and training and inference costs. We categorize existing solutions into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category. Finally, we discuss future trends and open challenges, highlighting directions for advancing efficient embodied intelligence.",
          "site": "arxiv.org",
          "rank": 15,
          "published": "2025-10-20T02:59:45Z",
          "authors": [
            "Weifan Guan",
            "Qinghao Hu",
            "Aosheng Li",
            "Jian Cheng"
          ],
          "arxiv_id": "2510.17111",
          "abstract": "Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. Despite their capabilities, VLA systems face significant challenges due to their massive computational and memory demands, which conflict with the constraints of edge platforms such as on-board mobile manipulators that require real-time performance. Addressing this tension has become a central focus of recent research. In light of the growing efforts toward more efficient and scalable VLA systems, this survey provides a systematic review of approaches for improving VLA efficiency, with an emphasis on reducing latency, memory footprint, and training and inference costs. We categorize existing solutions into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category. Finally, we discuss future trends and open challenges, highlighting directions for advancing efficient embodied intelligence.",
          "abstract_zh": "视觉语言动作（VLA）模型通过将自然语言指令和视觉观察映射到机器人动作，将视觉语言模型扩展到具体控制。尽管 VLA 系统功能强大，但由于其大量的计算和内存需求，它面临着巨大的挑战，这与需要实时性能的板载移动机械手等边缘平台的限制相冲突。解决这种紧张局势已成为近期研究的焦点。鉴于人们越来越多地致力于提高 VLA 系统的效率和可扩展性，本次调查对提高 VLA 效率的方法进行了系统回顾，重点是减少延迟、内存占用以及训练和推理成本。我们将现有的解决方案分为四个维度：模型架构、感知特征、动作生成和训练/推理策略，总结了每个类别中的代表性技术。最后，我们讨论未来的趋势和开放的挑战，强调推进高效的体现智能的方向。"
        },
        {
          "title": "Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model",
          "url": "http://arxiv.org/abs/2510.12276v2",
          "snippet": "Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their ability to operate in the 3D physical world. Existing solutions attempt to incorporate explicit 3D sensor inputs such as depth maps or point clouds, but these approaches face challenges due to sensor noise, hardware heterogeneity, and incomplete depth coverage in existing datasets. Alternative methods that estimate 3D cues from 2D images also suffer from the limited performance of depth estimators. We propose Spatial Forcing (SF), a simple yet effective alignment strategy that implicitly forces VLA models to develop spatial comprehension capabilities without relying on explicit 3D inputs or depth estimators. SF aligns intermediate visual embeddings of VLAs with geometric representations produced by pretrained 3D foundation models. By enforcing alignment at intermediate layers, SF guides VLAs to encode richer spatial representations that enhance action precision. Extensive experiments in simulation and real-world environments demonstrate that SF achieves state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further accelerates training by up to 3.8x and improves data efficiency across diverse robotic tasks. Project page is at https://spatial-forcing.github.io/",
          "site": "arxiv.org",
          "rank": 16,
          "published": "2025-10-14T08:27:10Z",
          "authors": [
            "Fuhao Li",
            "Wenxuan Song",
            "Han Zhao",
            "Jingbo Wang",
            "Pengxiang Ding",
            "Donglin Wang",
            "Long Zeng",
            "Haoang Li"
          ],
          "arxiv_id": "2510.12276",
          "abstract": "Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their ability to operate in the 3D physical world. Existing solutions attempt to incorporate explicit 3D sensor inputs such as depth maps or point clouds, but these approaches face challenges due to sensor noise, hardware heterogeneity, and incomplete depth coverage in existing datasets. Alternative methods that estimate 3D cues from 2D images also suffer from the limited performance of depth estimators. We propose Spatial Forcing (SF), a simple yet effective alignment strategy that implicitly forces VLA models to develop spatial comprehension capabilities without relying on explicit 3D inputs or depth estimators. SF aligns intermediate visual embeddings of VLAs with geometric representations produced by pretrained 3D foundation models. By enforcing alignment at intermediate layers, SF guides VLAs to encode richer spatial representations that enhance action precision. Extensive experiments in simulation and real-world environments demonstrate that SF achieves state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further accelerates training by up to 3.8x and improves data efficiency across diverse robotic tasks. Project page is at https://spatial-forcing.github.io/",
          "abstract_zh": "视觉-语言-动作（VLA）模型最近在使机器人遵循语言指令并执行精确动作方面显示出强大的潜力。然而，大多数 VLA 都是建立在仅基于 2D 数据预训练的视觉语言模型之上，缺乏准确的空间感知并阻碍了它们在 3D 物理世界中运行的能力。现有解决方案尝试合并显式 3D 传感器输入，例如深度图或点云，但由于传感器噪声、硬件异构性和现有数据集中不完整的深度覆盖，这些方法面临挑战。从 2D 图像估计 3D 线索的替代方法也受到深度估计器性能有限的影响。我们提出了空间强迫（SF），这是一种简单而有效的对齐策略，它隐式地迫使 VLA 模型开发空间理解能力，而不依赖于显式的 3D 输入或深度估计器。SF 将 VLA 的中间视觉嵌入与预训练的 3D 基础模型生成的几何表示对齐。通过在中间层强制对齐，SF 引导 VLA 编码更丰富的空间表示，从而提高动作精度。在模拟和现实环境中进行的大量实验表明，SF 取得了最先进的结果，超越了基于 2D 和 3D 的 VLA。SF 将训练速度进一步提高了 3.8 倍，并提高了各种机器人任务的数据效率。项目页面位于 https://spatial-forcing.github.io/"
        },
        {
          "title": "TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.10932v1",
          "snippet": "With the growing deployment of Vision-Language-Action (VLA) models in real-world embodied AI systems, their increasing vulnerability to backdoor attacks poses a serious safety threat. A backdoored VLA agent can be covertly triggered by a pre-injected backdoor to execute adversarial actions, potentially causing system failures or even physical harm. Although backdoor attacks on VLA models have been explored, prior work has focused only on untargeted attacks, leaving the more practically threatening scenario of targeted manipulation unexamined. In this paper, we study targeted backdoor attacks on VLA models and introduce TabVLA, a novel framework that enables such attacks via black-box fine-tuning. TabVLA explores two deployment-relevant inference-time threat models: input-stream editing and in-scene triggering. It formulates poisoned data generation as an optimization problem to improve attack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal that the vision channel is the principal attack surface: targeted backdoors succeed with minimal poisoning, remain robust across variations in trigger design, and are degraded only by positional mismatches between fine-tuning and inference triggers. We also investigate a potential detection-based defense against TabVLA, which reconstructs latent visual triggers from the input stream to flag activation-conditioned backdoor samples. Our work highlights the vulnerability of VLA models to targeted backdoor manipulation and underscores the need for more advanced defenses.",
          "site": "arxiv.org",
          "rank": 17,
          "published": "2025-10-13T02:45:48Z",
          "authors": [
            "Zonghuan Xu",
            "Xiang Zheng",
            "Xingjun Ma",
            "Yu-Gang Jiang"
          ],
          "arxiv_id": "2510.10932",
          "abstract": "With the growing deployment of Vision-Language-Action (VLA) models in real-world embodied AI systems, their increasing vulnerability to backdoor attacks poses a serious safety threat. A backdoored VLA agent can be covertly triggered by a pre-injected backdoor to execute adversarial actions, potentially causing system failures or even physical harm. Although backdoor attacks on VLA models have been explored, prior work has focused only on untargeted attacks, leaving the more practically threatening scenario of targeted manipulation unexamined. In this paper, we study targeted backdoor attacks on VLA models and introduce TabVLA, a novel framework that enables such attacks via black-box fine-tuning. TabVLA explores two deployment-relevant inference-time threat models: input-stream editing and in-scene triggering. It formulates poisoned data generation as an optimization problem to improve attack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal that the vision channel is the principal attack surface: targeted backdoors succeed with minimal poisoning, remain robust across variations in trigger design, and are degraded only by positional mismatches between fine-tuning and inference triggers. We also investigate a potential detection-based defense against TabVLA, which reconstructs latent visual triggers from the input stream to flag activation-conditioned backdoor samples. Our work highlights the vulnerability of VLA models to targeted backdoor manipulation and underscores the need for more advanced defenses.",
          "abstract_zh": "随着视觉-语言-动作（VLA）模型在现实世界的具体人工智能系统中的不断部署，它们越来越容易受到后门攻击，构成了严重的安全威胁。后门 VLA 代理可以被预先注入的后门秘密触发，以执行对抗性操作，可能导致系统故障甚至人身伤害。尽管已经探索了对 VLA 模型的后门攻击，但之前的工作仅关注非目标攻击，而没有对更具实际威胁的目标操纵场景进行研究。在本文中，我们研究了针对 VLA 模型的针对性后门攻击，并介绍了 TabVLA，这是一种通过黑盒微调实现此类攻击的新颖框架。TabVLA 探索了两种与部署相关的推理时间威胁模型：输入流编辑和场景内触发。它将中毒数据生成制定为优化问题，以提高攻击效率。在 LIBERO 基准上使用 OpenVLA-7B 进行的实验表明，视觉通道是主要的攻击面：目标后门以最小的中毒成功，在触发器设计的变化中保持鲁棒性，并且仅因微调和推理触发器之间的位置不匹配而降级。我们还研究了针对 TabVLA 的潜在的基于检测的防御，该防御从输入流中重建潜在的视觉触发器以标记激活条件后门样本。我们的工作强调了 VLA 模型对有针对性的后门操纵的脆弱性，并强调需要更先进的防御措施。"
        },
        {
          "title": "Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification",
          "url": "http://arxiv.org/abs/2510.16281v1",
          "snippet": "Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/",
          "site": "arxiv.org",
          "rank": 18,
          "published": "2025-10-18T00:38:45Z",
          "authors": [
            "Yilin Wu",
            "Anqi Li",
            "Tucker Hermans",
            "Fabio Ramos",
            "Andrea Bajcsy",
            "Claudia P'erez-D'Arpino"
          ],
          "arxiv_id": "2510.16281",
          "abstract": "Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/",
          "abstract_zh": "推理视觉语言动作 (VLA) 模型通过在低级动作之前生成分步文本计划来改进机器人的指令遵循，这种方法受到语言模型中的思想链 (CoT) 推理的启发。然而，即使有正确的文本计划，生成的操作仍然可能会错过计划中的预期结果，特别是在分配外 (OOD) 场景中。我们将这种现象形式化为缺乏体现的 CoT 忠诚度，并引入了一种无需训练的运行时策略引导方法来实现推理-动作对齐。给定推理 VLA 的中间文本计划，我们的框架从同一模型中采样多个候选动作序列，通过模拟预测其结果，并使用预先训练的视觉语言模型 (VLM) 来选择结果与 VLA 自己的文本计划最相符的序列。只有执行与文本推理一致的动作序列才能将我们的基础 VLA 的自然动作多样性从错误来源转变为优势，从而提高对语义和视觉 OOD 扰动的鲁棒性，并实现新颖的行为组合，而无需昂贵的重新训练。我们还贡献了 LIBERO-100 的推理注释扩展，即为 OOD 评估量身定制的环境变量，并展示了与之前的行为组合任务和计算和数据多样性量表工作相比，性能提升高达 15%。项目网址：https://yilin-wu98.github.io/steering-reasoning-vla/"
        },
        {
          "title": "Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning",
          "url": "http://arxiv.org/abs/2510.16240v2",
          "snippet": "The rise of surgical robots and vision-language-action models has accelerated the development of autonomous surgical policies and efficient assessment strategies. However, evaluating these policies directly on physical robotic platforms such as the da Vinci Research Kit (dVRK) remains hindered by high costs, time demands, reproducibility challenges, and variability in execution. World foundation models (WFM) for physical AI offer a transformative approach to simulate complex real-world surgical tasks, such as soft tissue deformation, with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune of the Cosmos WFM, which, together with a trained video classifier, enables fully automated online evaluation and benchmarking of surgical policies. We evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop suture pad tasks, the automated pipeline achieves strong correlation between online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si platform, as well as good agreement between human labelers and the V-JEPA 2-derived video classifier. Additionally, preliminary experiments with ex-vivo porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising alignment with real-world evaluations, highlighting the platform's potential for more complex surgical procedures.",
          "site": "arxiv.org",
          "rank": 19,
          "published": "2025-10-17T22:05:25Z",
          "authors": [
            "Lukas Zbinden",
            "Nigel Nelson",
            "Juo-Tung Chen",
            "Xinhao Chen",
            "Ji Woong Kim",
            "Mahdi Azizian",
            "Axel Krieger",
            "Sean Huver"
          ],
          "arxiv_id": "2510.16240",
          "abstract": "The rise of surgical robots and vision-language-action models has accelerated the development of autonomous surgical policies and efficient assessment strategies. However, evaluating these policies directly on physical robotic platforms such as the da Vinci Research Kit (dVRK) remains hindered by high costs, time demands, reproducibility challenges, and variability in execution. World foundation models (WFM) for physical AI offer a transformative approach to simulate complex real-world surgical tasks, such as soft tissue deformation, with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune of the Cosmos WFM, which, together with a trained video classifier, enables fully automated online evaluation and benchmarking of surgical policies. We evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop suture pad tasks, the automated pipeline achieves strong correlation between online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si platform, as well as good agreement between human labelers and the V-JEPA 2-derived video classifier. Additionally, preliminary experiments with ex-vivo porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising alignment with real-world evaluations, highlighting the platform's potential for more complex surgical procedures.",
          "abstract_zh": "手术机器人和视觉语言动作模型的兴起加速了自主手术政策和高效评估策略的发展。然而，直接在达芬奇研究套件 (dVRK) 等物理机器人平台上评估这些政策仍然受到高成本、时间要求、再现性挑战和执行可变性的阻碍。用于物理人工智能的世界基础模型 (WFM) 提供了一种革命性的方法来模拟复杂的现实世界手术任务，例如高保真度的软组织变形。这项工作引入了 Cosmos-Surg-dVRK，这是 Cosmos WFM 的外科微调，它与训练有素的视频分类器一起，可以实现外科手术策略的全自动在线评估和基准测试。我们使用两个不同的手术数据集评估 Cosmos-Surg-dVRK。在桌面缝合板任务中，自动化管道实现了 Cosmos-Surg-dVRK 在线部署与真实 dVRK Si 平台上的政策结果之间的强相关性，以及人工标记者与 V-JEPA 2 衍生视频分类器之间的良好一致性。此外，Cosmos-Surg-dVRK 中的离体猪胆囊切除术任务的初步实验表明，与现实世界的评估有良好的一致性，凸显了该平台在更复杂的外科手术中的潜力。"
        },
        {
          "title": "NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?",
          "url": "http://arxiv.org/abs/2510.16263v2",
          "snippet": "The evaluation of Vision-Language-Action (VLA) agents is hindered by the coarse, end-task success metric that fails to provide precise skill diagnosis or measure robustness to real-world perturbations. This challenge is exacerbated by a fragmented data landscape that impedes reproducible research and the development of generalist models. To address these limitations, we introduce NEBULA, a unified ecosystem for single-arm manipulation that enables diagnostic and reproducible evaluation. NEBULA features a novel dual-axis evaluation protocol that combines fine-grained capability tests for precise skill diagnosis with systematic stress tests that measure robustness. A standardized API and a large-scale, aggregated dataset are provided to reduce fragmentation and support cross-dataset training and fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle with key capabilities such as spatial reasoning and dynamic adaptation, which are consistently obscured by conventional end-task success metrics. By measuring both what an agent can do and when it does so reliably, NEBULA provides a practical foundation for robust, general-purpose embodied agents.",
          "site": "arxiv.org",
          "rank": 20,
          "published": "2025-10-17T23:22:57Z",
          "authors": [
            "Jierui Peng",
            "Yanyan Zhang",
            "Yicheng Duan",
            "Tuo Liang",
            "Vipin Chaudhary",
            "Yu Yin"
          ],
          "arxiv_id": "2510.16263",
          "abstract": "The evaluation of Vision-Language-Action (VLA) agents is hindered by the coarse, end-task success metric that fails to provide precise skill diagnosis or measure robustness to real-world perturbations. This challenge is exacerbated by a fragmented data landscape that impedes reproducible research and the development of generalist models. To address these limitations, we introduce NEBULA, a unified ecosystem for single-arm manipulation that enables diagnostic and reproducible evaluation. NEBULA features a novel dual-axis evaluation protocol that combines fine-grained capability tests for precise skill diagnosis with systematic stress tests that measure robustness. A standardized API and a large-scale, aggregated dataset are provided to reduce fragmentation and support cross-dataset training and fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle with key capabilities such as spatial reasoning and dynamic adaptation, which are consistently obscured by conventional end-task success metrics. By measuring both what an agent can do and when it does so reliably, NEBULA provides a practical foundation for robust, general-purpose embodied agents.",
          "abstract_zh": "视觉-语言-动作（VLA）代理的评估受到粗略的最终任务成功指标的阻碍，该指标无法提供精确的技能诊断或测量对现实世界扰动的鲁棒性。碎片化的数据环境阻碍了可重复的研究和通用模型的开发，加剧了这一挑战。为了解决这些限制，我们引入了 NEBULA，这是一个用于单臂操作的统一生态系统，可以进行诊断和可重复的评估。NEBULA 采用新颖的双轴评估协议，将用于精确技能诊断的细粒度能力测试与衡量鲁棒性的系统压力测试相结合。提供标准化API和大规模聚合数据集，减少碎片并支持跨数据集训练和公平比较。使用 NEBULA，我们证明了表现最好的 VLA 在空间推理和动态适应等关键能力方面遇到了困难，而这些能力一直被传统的最终任务成功指标所掩盖。通过测量代理可以做什么以及何时可靠地执行操作，NEBULA 为强大的通用实体代理提供了实用的基础。"
        },
        {
          "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2510.13626v3",
          "snippet": "Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation.",
          "site": "arxiv.org",
          "rank": 21,
          "published": "2025-10-15T14:51:36Z",
          "authors": [
            "Senyu Fei",
            "Siyin Wang",
            "Junhao Shi",
            "Zihao Dai",
            "Jikun Cai",
            "Pengfang Qian",
            "Li Ji",
            "Xinzhe He",
            "Shiduo Zhang",
            "Zhaoye Fei",
            "Jinlan Fu",
            "Jingjing Gong",
            "Xipeng Qiu"
          ],
          "arxiv_id": "2510.13626",
          "abstract": "Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation.",
          "abstract_zh": "视觉语言动作（VLA）模型在机器人操作基准上报告了令人印象深刻的成功率，但这些结果可能掩盖了鲁棒性方面的根本弱点。我们通过引入七个维度的受控扰动来进行系统的漏洞分析：对象布局、摄像机视角、机器人初始状态、语言指令、光照条件、背景纹理和传感器噪声。我们全面分析了多种最先进的模型，并揭示了表面能力之下始终存在的脆弱性。我们的分析暴露了关键的弱点：模型对扰动因素（包括摄像机视角和机器人初始状态）表现出极度敏感，在适度扰动下，性能从 95% 下降到 30% 以下。令人惊讶的是，模型在很大程度上对语言变化不敏感，进一步的实验表明模型往往完全忽略语言指令。我们的研究结果挑战了高基准分数等同于真正能力的假设，并强调了评估现实变化下可靠性的评估实践的必要性。"
        },
        {
          "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors",
          "url": "http://arxiv.org/abs/2510.17439v1",
          "snippet": "Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.",
          "site": "arxiv.org",
          "rank": 22,
          "published": "2025-10-20T11:26:45Z",
          "authors": [
            "Zhengshen Zhang",
            "Hao Li",
            "Yalun Dai",
            "Zhengbang Zhu",
            "Lei Zhou",
            "Chenchen Liu",
            "Dong Wang",
            "Francis E. H. Tay",
            "Sijin Chen",
            "Ziwei Liu",
            "Yuxiao Liu",
            "Xinghang Li",
            "Pan Zhou"
          ],
          "arxiv_id": "2510.17439",
          "abstract": "Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.",
          "abstract_zh": "现有的视觉-语言-动作 (VLA) 模型在 3D 现实世界中运行，但通常构建在 2D 编码器上，留下了限制泛化和适应性的空间推理差距。最近的 VLA 3D 集成技术要么需要专门的传感器并且跨模态传输效果不佳，要么注入缺乏几何形状的微弱线索并降低视觉语言对齐。在这项工作中，我们介绍了 FALCON（从空间到动作），这是一种将丰富的 3D 空间标记注入动作头的新颖范例。FALCON 利用空间基础模型仅从 RGB 提供强大的几何先验，并包括一个体现空间模型，该模型可以选择融合深度，或在可用时提供更高的保真度，而无需重新训练或架构更改。为了保留语言推理，空间标记由空间增强动作头消耗，而不是连接到视觉语言主干中。这些设计使 FALCON 能够解决空间表示、模态可转移性和对齐方面的限制。在对三个模拟基准和十一个现实世界任务的综合评估中，我们提出的 FALCON 实现了最先进的性能，始终超越竞争基线，并且在杂乱、空间提示调节以及物体尺度和高度变化的情况下保持鲁棒性。"
        },
        {
          "title": "Reflection-Based Task Adaptation for Self-Improving VLA",
          "url": "http://arxiv.org/abs/2510.12710v1",
          "snippet": "Pre-trained Vision-Language-Action (VLA) models represent a major leap towards general-purpose robots, yet efficiently adapting them to novel, specific tasks in-situ remains a significant hurdle. While reinforcement learning (RL) is a promising avenue for such adaptation, the process often suffers from low efficiency, hindering rapid task mastery. We introduce Reflective Self-Adaptation, a framework for rapid, autonomous task adaptation without human intervention. Our framework establishes a self-improving loop where the agent learns from its own experience to enhance both strategy and execution.\n  The core of our framework is a dual-pathway architecture that addresses the full adaptation lifecycle. First, a Failure-Driven Reflective RL pathway enables rapid learning by using the VLM's causal reasoning to automatically synthesize a targeted, dense reward function from failure analysis. This provides a focused learning signal that significantly accelerates policy exploration. However, optimizing such proxy rewards introduces a potential risk of \"reward hacking,\" where the agent masters the reward function but fails the actual task. To counteract this, our second pathway, Success-Driven Quality-Guided SFT, grounds the policy in holistic success. It identifies and selectively imitates high-quality successful trajectories, ensuring the agent remains aligned with the ultimate task goal. This pathway is strengthened by a conditional curriculum mechanism to aid initial exploration.\n  We conduct experiments in challenging manipulation tasks. The results demonstrate that our framework achieves faster convergence and higher final success rates compared to representative baselines. Our work presents a robust solution for creating self-improving agents that can efficiently and reliably adapt to new environments.",
          "site": "arxiv.org",
          "rank": 23,
          "published": "2025-10-14T16:44:39Z",
          "authors": [
            "Baicheng Li",
            "Dong Wu",
            "Zike Yan",
            "Xinchen Liu",
            "Zecui Zeng",
            "Lusong Li",
            "Hongbin Zha"
          ],
          "arxiv_id": "2510.12710",
          "abstract": "Pre-trained Vision-Language-Action (VLA) models represent a major leap towards general-purpose robots, yet efficiently adapting them to novel, specific tasks in-situ remains a significant hurdle. While reinforcement learning (RL) is a promising avenue for such adaptation, the process often suffers from low efficiency, hindering rapid task mastery. We introduce Reflective Self-Adaptation, a framework for rapid, autonomous task adaptation without human intervention. Our framework establishes a self-improving loop where the agent learns from its own experience to enhance both strategy and execution. The core of our framework is a dual-pathway architecture that addresses the full adaptation lifecycle. First, a Failure-Driven Reflective RL pathway enables rapid learning by using the VLM's causal reasoning to automatically synthesize a targeted, dense reward function from failure analysis. This provides a focused learning signal that significantly accelerates policy exploration. However, optimizing such proxy rewards introduces a potential risk of \"reward hacking,\" where the agent masters the reward function but fails the actual task. To counteract this, our second pathway, Success-Driven Quality-Guided SFT, grounds the policy in holistic success. It identifies and selectively imitates high-quality successful trajectories, ensuring the agent remains aligned with the ultimate task goal. This pathway is strengthened by a conditional curriculum mechanism to aid initial exploration. We conduct experiments in challenging manipulation tasks. The results demonstrate that our framework achieves faster convergence and higher final success rates compared to representative baselines. Our work presents a robust solution for creating self-improving agents that can efficiently and reliably adapt to new environments.",
          "abstract_zh": "预训练的视觉-语言-动作（VLA）模型代表了通用机器人的重大飞跃，但如何有效地使它们适应新的、现场的特定任务仍然是一个重大障碍。虽然强化学习（RL）是这种适应的一个有前途的途径，但该过程通常效率低下，阻碍了任务的快速掌握。我们引入了反思性自适应，这是一种无需人工干预即可快速自主任务适应的框架。我们的框架建立了一个自我改进的循环，代理从自己的经验中学习，以增强策略和执行力。我们框架的核心是双路径架构，可解决完整的适应生命周期。首先，故障驱动的反射 RL 路径通过使用 VLM 的因果推理从故障分析中自动合成有针对性的密集奖励函数，实现快速学习。这提供了一个有针对性的学习信号，可显着加速政策探索。然而，优化此类代理奖励会带来潜在的“奖励黑客”风险，即代理掌握了奖励功能，但未能完成实际任务。为了解决这个问题，我们的第二条途径，成功驱动的质量引导 SFT，将政策建立在整体成功的基础上。它识别并有选择地模仿高质量的成功轨迹，确保代理与最终任务目标保持一致。该途径通过有条件的课程机制得到加强，以帮助初步探索。我们在具有挑战性的操作任务中进行实验。结果表明，与代表性基线相比，我们的框架实现了更快的收敛和更高的最终成功率。我们的工作提供了一个强大的解决方案，用于创建能够高效可靠地适应新环境的自我改进代理。"
        },
        {
          "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
          "url": "http://arxiv.org/abs/2510.12796v2",
          "snippet": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.",
          "site": "arxiv.org",
          "rank": 24,
          "published": "2025-10-14T17:59:47Z",
          "authors": [
            "Yingyan Li",
            "Shuyao Shang",
            "Weisong Liu",
            "Bing Zhan",
            "Haochen Wang",
            "Yuqi Wang",
            "Yuntao Chen",
            "Xiaoman Wang",
            "Yasong An",
            "Chufeng Tang",
            "Lu Hou",
            "Lue Fan",
            "Zhaoxiang Zhang"
          ],
          "arxiv_id": "2510.12796",
          "abstract": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.",
          "abstract_zh": "在大规模数据上扩展视觉-语言-动作（VLA）模型为实现更通用的驾驶智能提供了一条有希望的途径。然而，VLA 模型受到“监督赤字”的限制：巨大的模型容量是由稀疏、低维的动作监督的，导致其大部分表征能力没有得到充分利用。为了解决这个问题，我们提出了 \\textbf{DriveVLA-W0}，这是一种利用世界建模来预测未来图像的训练范例。该任务会生成密集的自我监督信号，迫使模型学习驾驶环境的潜在动态。我们通过将其实例化为两个主要的 VLA 原型来展示该范式的多功能性：使用离散视觉标记的 VLA 的自回归世界模型，以及针对连续视觉特征操作的扩散世界模型。基于从世界建模中学到的丰富表示，我们引入了轻量级动作专家来解决实时部署的推理延迟问题。对 NAVSIM v1/v2 基准和 680 倍大的内部数据集进行的大量实验表明，DriveVLA-W0 的性能显着优于 BEV 和 VLA 基准。至关重要的是，它放大了数据缩放定律，表明随着训练数据集大小的增加，性能提升会加速。"
        },
        {
          "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment",
          "url": "http://arxiv.org/abs/2510.17148v4",
          "snippet": "Conventional end-to-end (E2E) driving models are effective at generating physically plausible trajectories, but often fail to generalize to long-tail scenarios due to the lack of essential world knowledge to understand and reason about surrounding environments. In contrast, Vision-Language-Action (VLA) models leverage world knowledge to handle challenging cases, but their limited 3D reasoning capability can lead to physically infeasible actions. In this work we introduce DiffVLA++, an enhanced autonomous driving framework that explicitly bridges cognitive reasoning and E2E planning through metric-guided alignment. First, we build a VLA module directly generating semantically grounded driving trajectories. Second, we design an E2E module with a dense trajectory vocabulary that ensures physical feasibility. Third, and most critically, we introduce a metric-guided trajectory scorer that guides and aligns the outputs of the VLA and E2E modules, thereby integrating their complementary strengths. The experiment on the ICCV 2025 Autonomous Grand Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.",
          "site": "arxiv.org",
          "rank": 25,
          "published": "2025-10-20T04:49:14Z",
          "authors": [
            "Yu Gao",
            "Anqing Jiang",
            "Yiru Wang",
            "Wang Jijun",
            "Hao Jiang",
            "Zhigang Sun",
            "Heng Yuwen",
            "Wang Shuo",
            "Hao Zhao",
            "Sun Hao"
          ],
          "arxiv_id": "2510.17148",
          "abstract": "Conventional end-to-end (E2E) driving models are effective at generating physically plausible trajectories, but often fail to generalize to long-tail scenarios due to the lack of essential world knowledge to understand and reason about surrounding environments. In contrast, Vision-Language-Action (VLA) models leverage world knowledge to handle challenging cases, but their limited 3D reasoning capability can lead to physically infeasible actions. In this work we introduce DiffVLA++, an enhanced autonomous driving framework that explicitly bridges cognitive reasoning and E2E planning through metric-guided alignment. First, we build a VLA module directly generating semantically grounded driving trajectories. Second, we design an E2E module with a dense trajectory vocabulary that ensures physical feasibility. Third, and most critically, we introduce a metric-guided trajectory scorer that guides and aligns the outputs of the VLA and E2E modules, thereby integrating their complementary strengths. The experiment on the ICCV 2025 Autonomous Grand Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.",
          "abstract_zh": "传统的端到端（E2E）驾驶模型可以有效地生成物理上合理的轨迹，但由于缺乏理解和推理周围环境的基本世界知识，通常无法推广到长尾场景。相比之下，视觉-语言-动作 (VLA) 模型利用世界知识来处理具有挑战性的案例，但其有限的 3D 推理能力可能会导致物理上不可行的动作。在这项工作中，我们介绍了 DiffVLA++，这是一种增强的自动驾驶框架，它通过度量引导的对齐方式明确地连接认知推理和 E2E 规划。首先，我们构建一个 VLA 模块，直接生成基于语义的驾驶轨迹。其次，我们设计了一个具有密集轨迹词汇的 E2E 模块，以确保物理可行性。第三，也是最关键的，我们引入了一个度量引导的轨迹评分器，它可以引导和调整 VLA 和 E2E 模块的输出，从而整合它们的互补优势。ICCV 2025 自主挑战赛排行榜上的实验表明，DiffVLA++ 的 EPDMS 达到了 49.12。"
        },
        {
          "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance",
          "url": "http://arxiv.org/abs/2510.14952v2",
          "snippet": "Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and untrustworthy. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking precision, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a universal foundation for vision-language-action humanoid systems.",
          "site": "arxiv.org",
          "rank": 26,
          "published": "2025-10-16T17:57:47Z",
          "authors": [
            "Zhe Li",
            "Cheng Chi",
            "Yangyang Wei",
            "Boan Zhu",
            "Yibo Peng",
            "Tao Huang",
            "Pengwei Wang",
            "Zhongyuan Wang",
            "Shanghang Zhang",
            "Chang Xu"
          ],
          "arxiv_id": "2510.14952",
          "abstract": "Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and untrustworthy. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking precision, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a universal foundation for vision-language-action humanoid systems.",
          "abstract_zh": "自然语言为人形机器人提供了自然的界面，但现有的语言引导人形运动管道仍然繁琐且不可信。他们通常解码人类运动，将其重新定位到机器人形态，然后使用基于物理的控制器进行跟踪。然而，这种多阶段过程很容易出现累积错误，引入高延迟，并且在语义和控制之间产生弱耦合。这些限制需要一种从语言到行动的更直接的途径，消除脆弱的中间阶段。因此，我们提出了 RoboGhost，这是一个无重定向的框架，可以直接根据基于语言的运动潜伏来调节人形策略。通过绕过显式运动解码和重定向，RoboGhost 支持基于扩散的策略，直接从噪声中对可执行动作进行去噪，保留语义意图并支持快速反应控制。混合因果变压器-扩散运动生成器进一步确保了长期一致性，同时保持稳定性和多样性，为精确的人形行为产生丰富的潜在表示。大量实验表明，RoboGhost 大大减少了部署延迟，提高了成功率和跟踪精度，并在真实的人形机器人上产生平滑、语义一致的运动。除了文本之外，该框架自然地扩展到其他形式，例如图像、音频和音乐，为视觉-语言-动作人形系统提供了通用基础。"
        },
        {
          "title": "RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation",
          "url": "http://arxiv.org/abs/2510.13149v1",
          "snippet": "Enabling robots to flexibly schedule and compose learned skills for novel long-horizon manipulation under diverse perturbations remains a core challenge. Early explorations with end-to-end VLA models show limited success, as these models struggle to generalize beyond the training distribution. Hierarchical approaches, where high-level planners generate subgoals for low-level policies, bring certain improvements but still suffer under complex perturbations, revealing limited capability in skill composition. However, existing benchmarks primarily emphasize task completion in long-horizon settings, offering little insight into compositional generalization, robustness, and the interplay between planning and execution. To systematically investigate these gaps, we propose RoboHiMan, a hierarchical evaluation paradigm for compositional generalization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench, a benchmark of atomic and compositional tasks under diverse perturbations, supported by a multi-level training dataset for analyzing progressive data scaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled) that probe the necessity of skill composition and reveal bottlenecks in hierarchical architectures. Experiments highlight clear capability gaps across representative models and architectures, pointing to directions for advancing models better suited to real-world long-horizon manipulation tasks. Videos and open-source code can be found on our project website: https://chenyt31.github.io/robo-himan.github.io/.",
          "site": "arxiv.org",
          "rank": 27,
          "published": "2025-10-15T04:58:13Z",
          "authors": [
            "Yangtao Chen",
            "Zixuan Chen",
            "Nga Teng Chan",
            "Junting Chen",
            "Junhui Yin",
            "Jieqi Shi",
            "Yang Gao",
            "Yong-Lu Li",
            "Jing Huo"
          ],
          "arxiv_id": "2510.13149",
          "abstract": "Enabling robots to flexibly schedule and compose learned skills for novel long-horizon manipulation under diverse perturbations remains a core challenge. Early explorations with end-to-end VLA models show limited success, as these models struggle to generalize beyond the training distribution. Hierarchical approaches, where high-level planners generate subgoals for low-level policies, bring certain improvements but still suffer under complex perturbations, revealing limited capability in skill composition. However, existing benchmarks primarily emphasize task completion in long-horizon settings, offering little insight into compositional generalization, robustness, and the interplay between planning and execution. To systematically investigate these gaps, we propose RoboHiMan, a hierarchical evaluation paradigm for compositional generalization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench, a benchmark of atomic and compositional tasks under diverse perturbations, supported by a multi-level training dataset for analyzing progressive data scaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled) that probe the necessity of skill composition and reveal bottlenecks in hierarchical architectures. Experiments highlight clear capability gaps across representative models and architectures, pointing to directions for advancing models better suited to real-world long-horizon manipulation tasks. Videos and open-source code can be found on our project website: https://chenyt31.github.io/robo-himan.github.io/.",
          "abstract_zh": "让机器人能够灵活地安排和组合学习的技能，以在不同的扰动下进行新颖的长视野操作仍然是一个核心挑战。端到端 VLA 模型的早期探索取得的成功有限，因为这些模型很难泛化到训练分布之外。高层规划者为低层政策制定子目标的分层方法带来了一定的改进，但仍然受到复杂扰动的影响，揭示了技能构成的有限能力。然而，现有的基准主要强调长期环境中的任务完成，而很少深入了解组合泛化、稳健性以及计划和执行之间的相互作用。为了系统地研究这些差距，我们提出了 RoboHiMan，一种用于长视野操作中成分泛化的分层评估范式。RoboHiMan 引入了 HiMan-Bench，这是不同扰动下原子和组合任务的基准，由用于分析渐进数据扩展的多级训练数据集支持，并提出了三种评估范式（普通、解耦、耦合），以探讨技能组合的必要性并揭示分层架构中的瓶颈。实验凸显了代表性模型和架构之间明显的能力差距，为推进更适合现实世界长视野操作任务的模型指明了方向。视频和开源代码可以在我们的项目网站上找到：https://chenyt31.github.io/robo-himan.github.io/。"
        },
        {
          "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
          "url": "http://arxiv.org/abs/2510.14968v1",
          "snippet": "To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at rdd-neurips.github.io.",
          "site": "arxiv.org",
          "rank": 28,
          "published": "2025-10-16T17:59:37Z",
          "authors": [
            "Mingxuan Yan",
            "Yuping Wang",
            "Zechun Liu",
            "Jiachen Li"
          ],
          "arxiv_id": "2510.14968",
          "abstract": "To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at rdd-neurips.github.io.",
          "abstract_zh": "为了解决长期任务，最近的分层视觉语言动作（VLA）框架采用基于视觉语言模型（VLM）的规划器将复杂的操作任务分解为低级视觉运动策略可以轻松处理的更简单的子任务。通常，VLM 规划器经过微调以学习分解目标任务。这种微调需要通过人工注释或启发式规则将目标任务演示分割成子任务。然而，启发式子任务可能会显着偏离视觉运动策略的训练数据，从而降低任务性能。为了解决这些问题，我们提出了一种基于检索的演示分解器（RDD），通过将分解的子任务间隔的视觉特征与低级视觉运动策略的训练数据中的视觉特征对齐，自动将演示分解为子任务。我们的方法在模拟和现实世界任务上都优于最先进的子任务分解器，展示了跨不同设置的鲁棒性。代码和更多结果可在 rdd-neurips.github.io 上找到。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 14 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 14）。",
      "items": [
        {
          "title": "TianxingChen/Embodied-AI-Guide",
          "url": "https://github.com/TianxingChen/Embodied-AI-Guide",
          "snippet": "[Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "RLinf/RLinf",
          "url": "https://github.com/RLinf/RLinf",
          "snippet": "RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "IliaLarchenko/behavior-1k-solution",
          "url": "https://github.com/IliaLarchenko/behavior-1k-solution",
          "snippet": "1st place solution of 2025 BEHAVIOR Challenge",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "thu-ml/RDT2",
          "url": "https://github.com/thu-ml/RDT2",
          "snippet": "Official code of RDT 2",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "PKU-HMI-Lab/Hybrid-VLA",
          "url": "https://github.com/PKU-HMI-Lab/Hybrid-VLA",
          "snippet": "HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model",
          "site": "github.com",
          "rank": 9
        },
        {
          "title": "BridgeVLA/BridgeVLA",
          "url": "https://github.com/BridgeVLA/BridgeVLA",
          "snippet": "✨✨【NeurIPS 2025】Official implementation of BridgeVLA",
          "site": "github.com",
          "rank": 10
        },
        {
          "title": "SalvatoreRa/ML-news-of-the-week",
          "url": "https://github.com/SalvatoreRa/ML-news-of-the-week",
          "snippet": "A collection of the the best ML and AI news every week (research, news, resources)",
          "site": "github.com",
          "rank": 11
        },
        {
          "title": "52CV/CVPR-2025-Papers",
          "url": "https://github.com/52CV/CVPR-2025-Papers",
          "snippet": "CVPR-2025-Papers",
          "site": "github.com",
          "rank": 12
        },
        {
          "title": "Hub-Tian/UAVs_Meet_LLMs",
          "url": "https://github.com/Hub-Tian/UAVs_Meet_LLMs",
          "snippet": "UAVs_Meet_LLMs",
          "site": "github.com",
          "rank": 13
        },
        {
          "title": "52CV/ECCV-2024-Papers",
          "url": "https://github.com/52CV/ECCV-2024-Papers",
          "snippet": "ECCV-2024-Papers",
          "site": "github.com",
          "rank": 14
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-10-13",
  "week_end": "2025-10-19",
  "last_updated": "2026-01-07"
}