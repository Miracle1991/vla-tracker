{
  "generated_at": "2026-01-07T13:46:51.673152",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 29 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 29）。",
      "items": [
        {
          "title": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation",
          "url": "http://arxiv.org/abs/2511.17199v1",
          "snippet": "Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-11-21T12:26:30Z",
          "authors": [
            "Hanyu Zhou",
            "Chuanhao Ma",
            "Gim Hee Lee"
          ],
          "arxiv_id": "2511.17199",
          "abstract": "Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.",
          "abstract_zh": "视觉-语言-动作（VLA）模型显示了一般机器人任务的潜力，但在时空相干操作方面仍然具有挑战性，这需要细粒度的表示。通常，现有方法将 3D 位置嵌入到视觉表示中，以增强动作的空间精度。然而，这些方法很难实现对动作执行的时间连贯控制。在这项工作中，我们提出了 VLA-4D，一种具有 4D 感知能力的通用 VLA 模型，用于时空相干的机器人操作。我们的模型以两个关键设计为指导：1）4D 感知视觉表示。我们提取视觉特征，将 1D 时间嵌入到 3D 位置以进行 4D 嵌入，并通过交叉注意力机制将它们融合成统一的视觉表示。2）时空动作表示。我们用时间信息扩展传统的空间动作表示以实现时空规划，并将多模态表示对齐到 LLM 中以进行时空动作预测。在这个统一的框架内，设计的视觉和动作表示共同使机器人操作在空间上平滑且在时间上连贯。此外，我们还使用时间动作注释扩展了 VLA 数据集，以微调我们的模型。已经进行了大量的实验来验证我们的方法在机器人操作的不同任务中的优越性。"
        },
        {
          "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
          "url": "http://arxiv.org/abs/2511.17889v1",
          "snippet": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-11-22T02:34:10Z",
          "authors": [
            "Ting Huang",
            "Dongjian Li",
            "Rui Yang",
            "Zeyu Zhang",
            "Zida Yang",
            "Hao Tang"
          ],
          "arxiv_id": "2511.17889",
          "abstract": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.",
          "abstract_zh": "将自然语言指令融入四足机器人的连续控制仍然是视觉语言动作的基本挑战。现有方法难以弥合高级语义推理和低级驱动，导致现实世界中的基础不稳定和泛化能力弱。为了解决这些问题，我们提出了 MobileVLA-R1，这是一个统一的视觉-语言-动作框架，可以实现四足机器人的显式推理和连续控制。我们构建了 MobileVLA-CoT，这是一个用于具体轨迹的多粒度思想链（CoT）的大规模数据集，为对齐提供结构化推理监督。在此基础上，我们引入了一种两阶段训练范例，将监督 CoT 对齐与 GRPO 强化学习相结合，以增强推理一致性、控制稳定性和长期执行力。对 VLN 和 VLA 任务的广泛评估表明，其性能优于强基线，大约提高了 5%。四足机器人的实际部署验证了复杂环境中的稳健性能。代码：https://github.com/AIGeeksGroup/MobileVLA-R1。网站：https://aigeeksgroup.github.io/MobileVLA-R1。"
        },
        {
          "title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation",
          "url": "http://arxiv.org/abs/2511.18950v1",
          "snippet": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-11-24T10:06:41Z",
          "authors": [
            "Juntao Gao",
            "Feiyang Ye",
            "Jing Zhang",
            "Wenjing Qian"
          ],
          "arxiv_id": "2511.18950",
          "abstract": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.",
          "abstract_zh": "视觉-语言-动作（VLA）模型已成为嵌入式人工智能中的强大范例。然而，处理冗余视觉标记的大量计算开销仍然是实时机器人部署的关键瓶颈。虽然标准的标记修剪技术可以缓解这种情况，但这些与任务无关的方法很难保留任务关键的视觉信息。为了应对这一挑战，同时保留整体上下文和细粒度细节以实现精确操作，我们提出了 Compressor-VLA，这是一种新颖的混合指令条件令牌压缩框架，旨在对 VLA 模型中的视觉信息进行高效、面向任务的压缩。所提出的 Compressor-VLA 框架由两个令牌压缩模块组成：一个语义任务压缩器（STC），用于提取整体的、与任务相关的上下文；以及一个空间细化压缩器（SRC），用于保留细粒度的空间细节。这种压缩由自然语言指令动态调节，允许自适应压缩与任务相关的视觉信息。实验上，广泛的评估表明，与基准相比，Compressor-VLA 在 LIBERO 基准上实现了具有竞争力的成功率，同时将 FLOP 减少了 59%，并将视觉标记计数减少了 3 倍以上。双臂机器人平台上的真实机器人部署验证了该模型的仿真到真实的可移植性和实际适用性。此外，定性分析表明，我们的指导指导动态地将模型的感知焦点转向与任务相关的对象，从而验证了我们方法的有效性。"
        },
        {
          "title": "AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention",
          "url": "http://arxiv.org/abs/2511.18960v2",
          "snippet": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-11-24T10:22:28Z",
          "authors": [
            "Lei Xiao",
            "Jifeng Li",
            "Juntao Gao",
            "Feiyang Ye",
            "Yan Jin",
            "Jingjing Qian",
            "Jing Zhang",
            "Yong Wu",
            "Xiaoyuan Yu"
          ],
          "arxiv_id": "2511.18960",
          "abstract": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在具体的人工智能任务中表现出了卓越的能力。然而，现有的 VLA 模型通常基于视觉语言模型 (VLM) 构建，通常在每个时间步独立处理密集的视觉输入。这种方法将任务隐式建模为马尔可夫决策过程 (MDP)。然而，这种与历史无关的设计对于动态顺序决策中的有效视觉标记处理而言并不是最佳的，因为它无法利用历史背景。为了解决这个限制，我们从部分可观察马尔可夫决策过程（POMDP）的角度重新表述了这个问题，并提出了一个名为 AVA-VLA 的新框架。受到 POMDP 的启发，行动的生成应该以信念状态为条件。AVA-VLA 引入主动视觉注意（AVA）来动态调节视觉处理。它通过利用循环状态来实现这一点，循环状态是从先前决策步骤得出的代理信念状态的神经近似。具体来说，AVA 模块使用循环状态来计算软权重，以根据其历史上下文主动处理与任务相关的视觉标记。综合评估表明，AVA-VLA 在流行的机器人基准测试中实现了最先进的性能，包括 LIBERO 和 CALVIN。此外，双臂机器人平台上的实际部署验证了该框架的实际适用性和强大的模拟到真实的可迁移性。"
        },
        {
          "title": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference",
          "url": "http://arxiv.org/abs/2511.16449v2",
          "snippet": "Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-11-20T15:16:09Z",
          "authors": [
            "Ziyan Liu",
            "Yeqiu Chen",
            "Hongyi Cai",
            "Tao Lin",
            "Shuo Yang",
            "Zheng Liu",
            "Bo Zhao"
          ],
          "arxiv_id": "2511.16449",
          "abstract": "Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在嵌入式人工智能方面展现出了巨大的前景，但处理连续视觉流的繁重计算成本严重限制了它们的实时部署。令牌修剪（保留显着的视觉令牌并删除冗余的令牌）已成为加速视觉语言模型 (VLM) 的有效方法，为高效的 VLA 提供了解决方案。然而，这些特定于 VLM 的标记修剪方法仅基于语义显着性指标（例如预填充注意力）来选择标记，而忽略了 VLA 的高级语义理解和低级动作执行的内在双系统性质。因此，这些方法将令牌保留偏向于语义线索，丢弃了动作生成的关键信息，并显着降低了 VLA 性能。为了弥补这一差距，我们提出了 VLA-Pruner，这是一种多功能的即插即用 VLA 特定令牌修剪方法，它符合 VLA 模型的双系统性质，并利用机器人操作的时间连续性。具体来说，VLA-Pruner 采用双级重要性标准来保留视觉标记：用于语义级相关性的视觉语言预填充注意力和通过时间平滑估计的动作解码注意力，用于动作级重要性。基于这一标准，VLA-Pruner 提出了一种新颖的双层令牌选择策略，该策略自适应地保留一组紧凑的、信息丰富的视觉令牌，以便在给定的计算预算下实现语义理解和动作执行。实验表明，VLA-Pruner 在多个 VLA 架构和不同的机器人任务中实现了最先进的性能。"
        },
        {
          "title": "EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation",
          "url": "http://arxiv.org/abs/2511.18112v1",
          "snippet": "Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $π_{0.5}$ by +0.08 and +0.11.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-11-22T16:30:55Z",
          "authors": [
            "Min Lin",
            "Xiwen Liang",
            "Bingqian Lin",
            "Liu Jingzhi",
            "Zijian Jiao",
            "Kehan Li",
            "Yuhan Ma",
            "Yuecheng Liu",
            "Shen Zhao",
            "Yuzheng Zhuang",
            "Xiaodan Liang"
          ],
          "arxiv_id": "2511.18112",
          "abstract": "Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $π_{0.5}$ by +0.08 and +0.11.",
          "abstract_zh": "视觉-语言-动作（VLA）模型的最新进展使实体代理能够解释多模式指令并执行复杂的任务。然而，现有的 VLA 大多局限于短视距、桌面操作，缺乏长视距移动操作所需的记忆和推理能力，在这种情况下，智能体必须在不断变化的空间环境下协调导航和操作。在这项工作中，我们提出了 EchoVLA，一种用于长视野移动操作的内存感知 VLA 模型。EchoVLA 融合了受人脑启发的协同陈述性记忆，包括维护空间语义图集合的场景记忆和存储具有多模式上下文特征的任务级体验的情景记忆。在训练和推理过程中，这两个记忆根据当前观察、任务历史和指令单独存储、更新和检索，并且它们检索到的表示通过粗粒度和细粒度的注意力融合，以指导移动臂扩散策略。为了支持大规模培训和评估，我们进一步引入了 MoMani，这是一种自动化基准测试，可通过多模态大语言模型 (MLLM) 引导的规划和反馈驱动的细化生成专家级的长视野轨迹，并辅以真实的机器人演示。模拟和现实环境中的实验表明，EchoVLA 提高了长视野性能，在操作/导航上达到 0.52 SR，在移动操作上达到 0.31，超过 $π_{0.5}$ +0.08 和 +0.11。"
        },
        {
          "title": "Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion",
          "url": "http://arxiv.org/abs/2511.14178v1",
          "snippet": "Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-11-18T06:30:52Z",
          "authors": [
            "Zhuo Li",
            "Junjia Liu",
            "Zhipeng Dong",
            "Tao Teng",
            "Quentin Rouxel",
            "Darwin Caldwell",
            "Fei Chen"
          ],
          "arxiv_id": "2511.14178",
          "abstract": "Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.",
          "abstract_zh": "视觉-语言-动作（VLA）模型已在现实世界的机器人操作中展现出巨大的潜力。然而，预先训练的 VLA 策略在下游部署过程中仍然会出现性能大幅下降的问题。尽管微调可以缓解这个问题，但它对昂贵的演示收集和密集计算的依赖使其在现实环境中不切实际。在这项工作中，我们引入了 VLA-Pilot，这是一种即插即用的推理时间策略引导方法，用于零次部署预训练的 VLA，无需任何额外的微调或数据收集。我们在两个不同的机器人实施例中的六个真实下游操作任务上评估 VLA-Pilot，涵盖分布内和分布外场景。实验结果表明，VLA-Pilot 极大地提高了现成的预训练 VLA 策略的成功率，从而能够对不同的任务和实施例进行稳健的零样本泛化。实验视频和代码可在以下位置获取：https://rip4kobe.github.io/vla-pilot/。"
        },
        {
          "title": "ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2511.18082v1",
          "snippet": "Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-11-22T14:44:03Z",
          "authors": [
            "Wencheng Ye",
            "Tianshi Wang",
            "Lei Zhu",
            "Fengling Li",
            "Guoli Yang"
          ],
          "arxiv_id": "2511.18082",
          "abstract": "Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.",
          "abstract_zh": "最近的视觉-语言-动作（VLA）模型表现出了令人印象深刻的灵活性和泛化性，但它们在机器人操作中的部署仍然受到大量计算开销和推理延迟的限制。在这项工作中，我们提出了 ActDistill，这是一种通用的动作引导自衍生蒸馏框架，它将任何现有 VLA 模型的动作预测能力转移到轻量级对应模型。与之前主要强调视觉语言相关性的效率策略不同，ActDistill 利用动作先验来指导知识转移和模型压缩，从而实现 VLA 模型的面向动作的效率。具体来说，我们采用训练有素的 VLA 模型作为教师，并引入图结构封装策略来显式建模动作预测的分层演化。学生模型源自图封装的教师，进一步配备了动态路由器，可根据动作预测需求自适应地选择计算路径，并在分层图通知监督的指导下确保平滑高效的演化。在推理过程中，与图相关的辅助组件被删除，允许学生仅执行动态路由层并以最小的计算和延迟预测高精度动作。体现基准实验表明，ActDistill 实现了与全尺寸 VLA 模型相当或更好的性能，同时减少了 50% 以上的计算量，加速高达 1.67 倍，从而建立了高效体现智能的通用范例。"
        },
        {
          "title": "AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2511.14148v1",
          "snippet": "Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-11-18T05:21:11Z",
          "authors": [
            "Yuhua Jiang",
            "Shuang Cheng",
            "Yan Ding",
            "Feifei Gao",
            "Biqing Qi"
          ],
          "arxiv_id": "2511.14148",
          "abstract": "Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.",
          "abstract_zh": "视觉-语言-动作（VLA）模型最近已成为构建通用机器人的强大范例。然而，通过流匹配（FM）生成动作的传统VLA模型通常依赖于严格且统一的时间安排，即同步FM（SFM）。如果没有动作上下文感知和异步自我纠正，SFM 在长期任务中会变得不稳定，其中单个动作错误可能会导致失败。在这项工作中，我们提出了异步流匹配 VLA (AsyncVLA)，这是一种新颖的框架，它在异步 FM (AFM) 中引入了时间灵活性，并在动作生成中实现了自我校正。AsyncVLA 打破了 VLA 模型中的普通 SFM，通过在具有动作上下文感知的非统一时间安排中生成动作令牌。此外，我们的方法引入了置信度评估器来提取最初生成的操作的置信度，使模型能够在执行之前有选择地细化不准确的操作标记。此外，我们提出了 SFM 和 AFM 的统一训练过程，赋予单个模型两种模式，从而提高 KV 缓存利用率。对机器人操作基准的大量实验表明，AsyncVLA 具有数据高效性并具有自我纠正能力。由于其在 AFM 中的异步生成，AsyncVLA 在一般的具体评估中取得了最先进的结果。我们的代码可在 https://github.com/YuhuaJiang2002/AsyncVLA 获取。"
        },
        {
          "title": "Continually Evolving Skill Knowledge in Vision Language Action Model",
          "url": "http://arxiv.org/abs/2511.18085v2",
          "snippet": "Developing general robot intelligence in open environments requires continual skill learning. Recent Vision-Language-Action (VLA) models leverage massive pretraining data to support diverse manipulation tasks, but they still depend heavily on task-specific fine-tuning, revealing a lack of continual learning capability. Existing continual learning methods are also resource-intensive to scale to VLA models. We propose Stellar VLA, a knowledge-driven continual learning framework with two variants: T-Stellar, modeling task-centric knowledge space, and TS-Stellar, capturing hierarchical task-skill structure. Stellar VLA enables self-supervised knowledge evolution through joint learning of task latent representation and the knowledge space, reducing annotation needs. Knowledge-guided expert routing provide task specialization without extra network parameters, lowering training overhead. Experiments on the LIBERO benchmark and real-world tasks show over 50 percentage average improvement in final success rates relative to baselines. TS-Stellar further excels in complex action inference, and in-depth analyses verify effective knowledge retention and discovery. Our code will be released soon.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-11-22T15:00:08Z",
          "authors": [
            "Yuxuan Wu",
            "Guangming Wang",
            "Zhiheng Yang",
            "Maoqing Yao",
            "Brian Sheil",
            "Hesheng Wang"
          ],
          "arxiv_id": "2511.18085",
          "abstract": "Developing general robot intelligence in open environments requires continual skill learning. Recent Vision-Language-Action (VLA) models leverage massive pretraining data to support diverse manipulation tasks, but they still depend heavily on task-specific fine-tuning, revealing a lack of continual learning capability. Existing continual learning methods are also resource-intensive to scale to VLA models. We propose Stellar VLA, a knowledge-driven continual learning framework with two variants: T-Stellar, modeling task-centric knowledge space, and TS-Stellar, capturing hierarchical task-skill structure. Stellar VLA enables self-supervised knowledge evolution through joint learning of task latent representation and the knowledge space, reducing annotation needs. Knowledge-guided expert routing provide task specialization without extra network parameters, lowering training overhead. Experiments on the LIBERO benchmark and real-world tasks show over 50 percentage average improvement in final success rates relative to baselines. TS-Stellar further excels in complex action inference, and in-depth analyses verify effective knowledge retention and discovery. Our code will be released soon.",
          "abstract_zh": "在开放环境中开发通用机器人智能需要持续的技能学习。最近的视觉-语言-动作（VLA）模型利用大量预训练数据来支持不同的操作任务，但它们仍然严重依赖于特定于任务的微调，揭示了持续学习能力的缺乏。现有的持续学习方法也需要大量资源才能扩展到 VLA 模型。我们提出了 Stellar VLA，一个知识驱动的持续学习框架，有两个变体：T-Stellar，以任务为中心的知识空间建模，以及 TS-Stellar，捕获分层任务技能结构。Stellar VLA 通过任务潜在表示和知识空间的联合学习来实现自我监督的知识演化，从而减少注释需求。知识引导的专家路由提供任务专门化，无需额外的网络参数，从而降低了培训开销。LIBERO 基准和实际任务的实验表明，相对于基准，最终成功率平均提高了 50% 以上。TS-Stellar 在复杂的动作推理方面更加出色，深入的分析验证了有效的知识保留和发现。我们的代码很快就会发布。"
        },
        {
          "title": "EvoVLA: Self-Evolving Vision-Language-Action Model",
          "url": "http://arxiv.org/abs/2511.16166v1",
          "snippet": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-11-20T09:08:33Z",
          "authors": [
            "Zeting Liu",
            "Zida Yang",
            "Zeyu Zhang",
            "Hao Tang"
          ],
          "arxiv_id": "2511.16166",
          "abstract": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.",
          "abstract_zh": "尽管最近在零样本泛化和模拟到现实世界的迁移方面取得了进展，但长视距机器人操作对于视觉-语言-动作（VLA）模型仍然具有挑战性。当前的 VLA 模型存在阶段性幻觉，即代理利用粗略的评估信号来缩短多步骤任务，报告高进度，但没有真正完成它们。我们提出了 EvoVLA，一个自我监督的 VLA 框架，它通过三个互补的组件来解决这个问题：阶段对齐奖励（SAR），它使用三重对比学习与 Gemini 生成的硬负片来防止视觉捷径；基于姿势的对象探索（POE），它将好奇心建立在相对对象夹持器姿势而不是原始像素上；长视野记忆，它使用选择性上下文保留和门控融合来稳定扩展期间的内在塑造。对 Discoverse-L（具有三个多阶段任务的长视野操纵基准）的广泛评估表明，EvoVLA 比最强基线 (OpenVLA-OFT) 提高了平均任务成功率 10.2 个百分点，达到 69.2%。EvoVLA 还实现了提高一倍半的采样效率，并将舞台幻觉从 38.5% 降低到 14.8%。物理机器人的实际部署在四项操作任务中平均成功率为 54.6%，比 OpenVLA-OFT 高出 11 个百分点，展示了有效的模拟到真实的迁移和强大的泛化能力。代码：https://github.com/AIGeeksGroup/EvoVLA。网站：https://aigeeksgroup.github.io/EvoVLA。"
        },
        {
          "title": "Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception",
          "url": "http://arxiv.org/abs/2511.15279v1",
          "snippet": "In embodied AI perception systems, visual perception should be active: the goal is not to passively process static images, but to actively acquire more informative data within pixel and spatial budget constraints. Existing vision models and fixed RGB-D camera systems fundamentally fail to reconcile wide-area coverage with fine-grained detail acquisition, severely limiting their efficacy in open-world robotic applications. To address this issue, we propose EyeVLA, a robotic eyeball for active visual perception that can take proactive actions based on instructions, enabling clear observation of fine-grained target objects and detailed information across a wide spatial extent. EyeVLA discretizes action behaviors into action tokens and integrates them with vision-language models (VLMs) that possess strong open-world understanding capabilities, enabling joint modeling of vision, language, and actions within a single autoregressive sequence. By using the 2D bounding box coordinates to guide the reasoning chain and applying reinforcement learning to refine the viewpoint selection policy, we transfer the open-world scene understanding capability of the VLM to a vision language action (VLA) policy using only minimal real-world data. Experiments show that our system efficiently performs instructed scenes in real-world environments and actively acquires more accurate visual information through instruction-driven actions of rotation and zoom, thereby achieving strong environmental perception capabilities. EyeVLA introduces a novel robotic vision system that leverages detailed and spatially rich, large-scale embodied data, and actively acquires highly informative visual observations for downstream embodied tasks.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-11-19T09:42:08Z",
          "authors": [
            "Jiashu Yang",
            "Yifan Han",
            "Yucheng Xie",
            "Ning Guo",
            "Wenzhao Lian"
          ],
          "arxiv_id": "2511.15279",
          "abstract": "In embodied AI perception systems, visual perception should be active: the goal is not to passively process static images, but to actively acquire more informative data within pixel and spatial budget constraints. Existing vision models and fixed RGB-D camera systems fundamentally fail to reconcile wide-area coverage with fine-grained detail acquisition, severely limiting their efficacy in open-world robotic applications. To address this issue, we propose EyeVLA, a robotic eyeball for active visual perception that can take proactive actions based on instructions, enabling clear observation of fine-grained target objects and detailed information across a wide spatial extent. EyeVLA discretizes action behaviors into action tokens and integrates them with vision-language models (VLMs) that possess strong open-world understanding capabilities, enabling joint modeling of vision, language, and actions within a single autoregressive sequence. By using the 2D bounding box coordinates to guide the reasoning chain and applying reinforcement learning to refine the viewpoint selection policy, we transfer the open-world scene understanding capability of the VLM to a vision language action (VLA) policy using only minimal real-world data. Experiments show that our system efficiently performs instructed scenes in real-world environments and actively acquires more accurate visual information through instruction-driven actions of rotation and zoom, thereby achieving strong environmental perception capabilities. EyeVLA introduces a novel robotic vision system that leverages detailed and spatially rich, large-scale embodied data, and actively acquires highly informative visual observations for downstream embodied tasks.",
          "abstract_zh": "在具体的人工智能感知系统中，视觉感知应该是主动的：目标不是被动地处理静态图像，而是在像素和空间预算限制内主动获取更多信息数据。现有的视觉模型和固定 RGB-D 相机系统从根本上无法协调广域覆盖与细粒度细节采集，严重限制了它们在开放世界机器人应用中的功效。为了解决这个问题，我们提出了 EyeVLA，这是一种用于主动视觉感知的机器人眼球，可以根据指令采取主动行动，从而能够在广阔的空间范围内清晰地观察细粒度的目标物体和详细信息。EyeVLA 将动作行为离散化为动作标记，并将其与具有强大开放世界理解能力的视觉语言模型 (VLM) 集成，从而能够在单个自回归序列中对视觉、语言和动作进行联合建模。通过使用 2D 边界框坐标来指导推理链并应用强化学习来细化视点选择策略，我们将 VLM 的开放世界场景理解能力转移到仅使用最少真实世界数据的视觉语言动作（VLA）策略。实验表明，我们的系统在现实环境中有效执行指令场景，并通过指令驱动的旋转和缩放动作主动获取更准确的视觉信息，从而实现强大的环境感知能力。EyeVLA 引入了一种新颖的机器人视觉系统，该系统利用详细且空间丰富的大规模体现数据，并为下游体现任务主动获取信息丰富的视觉观察结果。"
        },
        {
          "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2511.15605v2",
          "snippet": "Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.",
          "site": "arxiv.org",
          "rank": 13,
          "published": "2025-11-19T16:52:23Z",
          "authors": [
            "Senyu Fei",
            "Siyin Wang",
            "Li Ji",
            "Ao Li",
            "Shiduo Zhang",
            "Liming Liu",
            "Jinlong Hou",
            "Jingjing Gong",
            "Xianzhong Zhao",
            "Xipeng Qiu"
          ],
          "arxiv_id": "2511.15605",
          "abstract": "Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.",
          "abstract_zh": "视觉-语言-动作（VLA）模型在机器人操作方面表现出色，但由于严重依赖专家演示而受到限制，导致演示偏差和性能限制。强化学习 (RL) 是克服这些限制的重要训练后策略，但当前的 VLA-RL 方法（包括基于组的优化方法）因严重的奖励稀疏而受到削弱。依赖二元成功指标会浪费失败轨迹中的宝贵信息，导致训练效率低下。为了解决这个问题，我们提出了自参考策略优化（SRPO），这是一种新颖的 VLA-RL 框架。SRPO 通过利用当前训练批次中生成的模型自身的成功轨迹作为自我参考，消除了外部演示或手动奖励工程的需要。这使我们能够为失败的尝试分配进度奖励。核心创新是使用潜在世界表征来稳健地衡量行为进展。我们不依赖原始像素或需要特定领域的微调，而是利用来自世界模型潜在空间的压缩的、可转移的编码。这些表示自然地捕获跨环境的进度模式，从而实现准确、广义的轨迹比较。对 LIBERO 基准的实证评估证明了 SRPO 的效率和有效性。从成功率 48.9% 的受监督基线开始，SRPO 仅用 200 个 RL 步骤就实现了 99.2% 的最新成功率，这意味着在没有任何额外监督的情况下相对改进了 103%。此外，SRPO 表现出极大的稳健性，在 LIBERO-Plus 基准测试中实现了 167% 的性能提升。"
        },
        {
          "title": "Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning",
          "url": "http://arxiv.org/abs/2511.14396v5",
          "snippet": "Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.",
          "site": "arxiv.org",
          "rank": 14,
          "published": "2025-11-18T12:01:06Z",
          "authors": [
            "Xiuxiu Qi",
            "Yu Yang",
            "Jiannong Cao",
            "Luyao Bai",
            "Chongshan Fan",
            "Chengtai Cao",
            "Hongpeng Wang"
          ],
          "arxiv_id": "2511.14396",
          "abstract": "Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.",
          "abstract_zh": "语言条件操纵通过行为克隆（BC）促进人机交互，行为克隆可以从人类演示中学习控制策略，并作为实体人工智能的基石。克服顺序行动决策中的复合错误仍然是提高 BC 绩效的核心挑战。现有方法通过数据增强、表达表示或时间抽象来减轻复合错误。然而，它们存在物理不连续性和语义-物理错位，导致动作克隆不准确和间歇性执行。在本文中，我们提出了具有语义-物理对齐的连续视觉-语言-动作协同学习（CCoL），这是一种新颖的BC框架，可确保时间一致的执行和细粒度的语义基础。它通过视觉、语言和本体感受输入（例如机器人内部状态）的持续共同学习，生成稳健且平稳的动作执行轨迹。同时，我们通过双向交叉注意力将语言语义锚定到视觉运动表征，以学习用于动作生成的上下文信息，成功克服了语义-物理不一致的问题。大量实验表明，CCoL 在三个模拟套件中实现了平均 8.0% 的相对改进，在人类演示的双手插入任务中相对增益高达 19.2%。7-DoF 机器人的实际测试进一步证实了 CCoL 在不可见和嘈杂的物体状态下的泛化能力。"
        },
        {
          "title": "METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model",
          "url": "http://arxiv.org/abs/2511.17366v1",
          "snippet": "Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrained by limited scenarios and a large visual gap between human and robots. To eliminate these limitations, we propose METIS, a vision-language-action (VLA) model for dexterous manipulation pretrained on multi-source egocentric datasets. We first construct EgoAtlas, which integrates large-scale human and robotic data from multiple sources, all unified under a consistent action space. We further extract motion-aware dynamics, a compact and discretized motion representation, which provides efficient and expressive supervision for VLA training. Built upon them, METIS integrates reasoning and acting into a unified framework, enabling effective deployment to downstream dexterous manipulation tasks. Our method demonstrates exceptional dexterous manipulation capabilities, achieving highest average success rate in six real-world tasks. Experimental results also highlight the superior generalization and robustness to out-of-distribution scenarios. These findings emphasize METIS as a promising step toward a generalist model for dexterous manipulation.",
          "site": "arxiv.org",
          "rank": 15,
          "published": "2025-11-21T16:32:36Z",
          "authors": [
            "Yankai Fu",
            "Ning Chen",
            "Junkai Zhao",
            "Shaozhe Shan",
            "Guocai Yao",
            "Pengwei Wang",
            "Zhongyuan Wang",
            "Shanghang Zhang"
          ],
          "arxiv_id": "2511.17366",
          "abstract": "Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrained by limited scenarios and a large visual gap between human and robots. To eliminate these limitations, we propose METIS, a vision-language-action (VLA) model for dexterous manipulation pretrained on multi-source egocentric datasets. We first construct EgoAtlas, which integrates large-scale human and robotic data from multiple sources, all unified under a consistent action space. We further extract motion-aware dynamics, a compact and discretized motion representation, which provides efficient and expressive supervision for VLA training. Built upon them, METIS integrates reasoning and acting into a unified framework, enabling effective deployment to downstream dexterous manipulation tasks. Our method demonstrates exceptional dexterous manipulation capabilities, achieving highest average success rate in six real-world tasks. Experimental results also highlight the superior generalization and robustness to out-of-distribution scenarios. These findings emphasize METIS as a promising step toward a generalist model for dexterous manipulation.",
          "abstract_zh": "构建一个能够感知、推理和执行不同任务的多面手机器人仍然是一个公开的挑战，特别是对于灵巧的操作而言。主要瓶颈在于缺乏大规模的、带有动作注释的灵巧技能数据，因为远程操作困难且成本高昂。人类数据规模庞大、操作行为多样，为学习机器人动作提供了丰富的先验知识。虽然之前的工作已经探索利用人类演示，但它们往往受到有限的场景以及人类和机器人之间巨大的视觉差距的限制。为了消除这些限制，我们提出了 METIS，这是一种在多源自我中心数据集上进行预训练的视觉语言动作（VLA）模型，用于灵巧操作。我们首先构建 EgoAtlas，它集成了来自多个来源的大规模人类和机器人数据，所有这些数据都统一在一致的动作空间下。我们进一步提取运动感知动力学，一种紧凑且离散的运动表示，为 VLA 训练提供高效且富有表现力的监督。在此基础上，METIS 将推理和行动集成到一个统一的框架中，从而能够有效部署到下游灵巧的操作任务。我们的方法展示了卓越的灵巧操作能力，在六项现实任务中实现了最高的平均成功率。实验结果还强调了对分布外场景的卓越泛化性和鲁棒性。这些发现强调 METIS 是朝着灵巧操作的通用模型迈出的有希望的一步。"
        },
        {
          "title": "RynnVLA-002: A Unified Vision-Language-Action and World Model",
          "url": "http://arxiv.org/abs/2511.17502v2",
          "snippet": "We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.",
          "site": "arxiv.org",
          "rank": 16,
          "published": "2025-11-21T18:59:32Z",
          "authors": [
            "Jun Cen",
            "Siteng Huang",
            "Yuqian Yuan",
            "Kehan Li",
            "Hangjie Yuan",
            "Chaohui Yu",
            "Yuming Jiang",
            "Jiayan Guo",
            "Xin Li",
            "Hao Luo",
            "Fan Wang",
            "Deli Zhao",
            "Hao Chen"
          ],
          "arxiv_id": "2511.17502",
          "abstract": "We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.",
          "abstract_zh": "我们推出 RynnVLA-002，一个统一的视觉-语言-动作 (VLA) 和世界模型。世界模型利用动作和视觉输入来预测未来的图像状态，学习环境的底层物理原理来完善动作生成。相反，VLA 模型根据图像观察产生后续动作，增强视觉理解并支持世界模型的图像生成。RynnVLA-002 的统一框架可以实现环境动态和行动规划的联合学习。我们的实验表明，RynnVLA-002 超越了单独的 VLA 和世界模型，展示了它们的相互增强。我们在模拟和现实机器人任务中评估 RynnVLA-002。RynnVLA-002 在没有预训练的情况下在 LIBERO 模拟基准上实现了 97.4% 的成功率，而在现实世界的 LeRobot 实验中，其集成的世界模型将整体成功率提高了 50%。"
        },
        {
          "title": "MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent",
          "url": "http://arxiv.org/abs/2511.18810v1",
          "snippet": "Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.",
          "site": "arxiv.org",
          "rank": 17,
          "published": "2025-11-24T06:30:04Z",
          "authors": [
            "Yuxia Fu",
            "Zhizhen Zhang",
            "Yuqi Zhang",
            "Zijian Wang",
            "Zi Huang",
            "Yadan Luo"
          ],
          "arxiv_id": "2511.18810",
          "abstract": "Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.",
          "abstract_zh": "最近的视觉-语言-动作（VLA）模型通过数百万个机器人演示进行调整，重新构建了视觉-语言模型。虽然它们在针对单个实施例或任务系列进行微调时表现良好，但将它们扩展到多技能设置仍然具有挑战性：直接合并受过不同任务训练的 VLA 专家会导致成功率接近于零。这就提出了一个基本问题：是什么阻止 VLA 在一个模型中掌握多种技能？通过在 VLA 微调期间对可学习参数进行经验分解，我们确定了不可合并性的两个关键来源：（1）微调驱动 VLM 主干中的 LoRA 适配器朝着不同的、特定于任务的方向发展，超出了现有合并方法的统一能力。（2）行动专家通过自注意力反馈形成块间依赖关系，导致任务信息跨层传播并防止模块重组。为了应对这些挑战，我们提出了 MergeVLA，这是一种面向合并的 VLA 架构，通过设计保留了可合并性。MergeVLA 通过任务掩码引入稀疏激活的 LoRA 适配器，以保留一致的参数并减少 VLM 中不可调和的冲突。其行动专家用仅交叉注意的块取代了自注意，以保持专业化的本地化和可组合性。当任务未知时，它使用测试时任务路由器从初始观察中自适应地选择适当的任务掩码和专家头，从而实现无监督任务推理。在 LIBERO、LIBERO-Plus、RoboTwin 和真实 SO101 机械臂上的多任务实验中，MergeVLA 实现了与单独微调的专家相当甚至超过的性能，展示了跨任务、实施例和环境的强大泛化能力。"
        },
        {
          "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
          "url": "http://arxiv.org/abs/2511.16175v1",
          "snippet": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.",
          "site": "arxiv.org",
          "rank": 18,
          "published": "2025-11-20T09:30:23Z",
          "authors": [
            "Yi Yang",
            "Xueqi Li",
            "Yiyang Chen",
            "Jin Song",
            "Yihan Wang",
            "Zipeng Xiao",
            "Jiadi Su",
            "You Qiaoben",
            "Pengfei Liu",
            "Zhijie Deng"
          ],
          "arxiv_id": "2511.16175",
          "abstract": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.",
          "abstract_zh": "视觉-语言-动作（VLA）模型的最新进展表明，视觉信号可以有效补充稀疏动作监督。然而，让 VLA 直接预测高维视觉状态会分散模型容量并产生高昂的训练成本，而将视觉状态压缩为更紧凑的监督信号不可避免地会产生信息瓶颈。此外，由于忽视语言监督，现有方法常常导致理解和推理能力较差。本文介绍了 Mantis，这是一种新颖的框架，具有解缠结的视觉远见 (DVF) 来解决这些问题。具体来说，Mantis 通过元查询和扩散 Transformer (DiT) 头的组合，将视觉前瞻预测与主干网络解耦。通过残差连接向 DiT 提供当前视觉状态，简单的下一状态预测目标使元查询能够自动捕获描绘视觉轨迹的潜在动作，从而促进显式动作的学习。这种解开减轻了 VLA 主干的负担，使其能够通过语言监督保持理解和推理能力。根据经验，在人类操作视频、机器人演示和图像文本对上进行预训练，经过微调，Mantis 在 LIBERO 基准上取得了 96.7% 的成功率，超越了强大的基线，同时表现出较高的收敛速度。现实世界的评估表明，Mantis 的性能优于领先的开源 VLA 模型 $π_{0.5}$，特别是在指令跟踪能力、对未见过指令的泛化和推理能力方面。发布代码和权重以支持开源社区。"
        },
        {
          "title": "When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models",
          "url": "http://arxiv.org/abs/2511.16203v3",
          "snippet": "Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.",
          "site": "arxiv.org",
          "rank": 19,
          "published": "2025-11-20T10:14:32Z",
          "authors": [
            "Yuping Yan",
            "Yuhan Xie",
            "Yixin Zhang",
            "Lingjuan Lyu",
            "Handing Wang",
            "Yaochu Jin"
          ],
          "arxiv_id": "2511.16203",
          "abstract": "Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.",
          "abstract_zh": "视觉-语言-动作模型（VLA）最近在实体环境中取得了显着进展，使机器人能够通过统一的多模态理解来感知、推理和行动。尽管它们的能力令人印象深刻，但这些系统的对抗鲁棒性在很大程度上仍未被探索，特别是在现实的多模式和黑匣子条件下。现有的研究主要集中在单模态扰动，而忽视了从根本上影响体现推理和决策的跨模态失调。在本文中，我们介绍了 VLA-Fool，这是对白盒和黑盒设置下具体 VLA 模型的多模态对抗鲁棒性的综合研究。VLA-Fool 统一了三个级别的多模态对抗攻击：（1）通过基于梯度和基于提示的操作进行文本扰动，（2）通过补丁和噪声扭曲进行视觉扰动，以及（3）故意破坏感知和指令之间语义对应的跨模态错位攻击。我们进一步将 VLA 感知语义空间融入语言提示中，开发了第一个自动制作和语义引导的提示框架。使用微调的 OpenVLA 模型在 LIBERO 基准上进行的实验表明，即使是微小的多模态扰动也会导致显着的行为偏差，这证明了具体多模态对齐的脆弱性。"
        },
        {
          "title": "NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards",
          "url": "http://arxiv.org/abs/2511.14659v1",
          "snippet": "Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.",
          "site": "arxiv.org",
          "rank": 20,
          "published": "2025-11-18T16:55:48Z",
          "authors": [
            "Chia-Yu Hung",
            "Navonil Majumder",
            "Haoyuan Deng",
            "Liu Renhang",
            "Yankang Ang",
            "Amir Zadeh",
            "Chuan Li",
            "Dorien Herremans",
            "Ziwei Wang",
            "Soujanya Poria"
          ],
          "arxiv_id": "2511.14659",
          "abstract": "Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.",
          "abstract_zh": "视觉-语言-动作（VLA）模型最近在各种具体任务上表现出了良好的性能，但它们在可靠性和泛化性方面仍然存在不足，特别是在跨不同实施例或现实环境部署时。在这项工作中，我们引入了 NORA-1.5，这是一种基于预先训练的 NORA 主干网络构建的 VLA 模型，并添加了基于流匹配的动作专家。仅此架构增强就带来了显着的性能提升，使 NORA-1.5 在模拟和现实基准测试中均优于 NORA 和多个最先进的 VLA 模型。为了进一步提高稳健性和任务成功率，我们为训练后 VLA 策略开发了一套奖励模型。我们的奖励结合了（i）一个以动作为条件的世界模型（WM），用于评估生成的动作是否会实现预期的目标，以及（ii）一种偏离真实情况的启发式算法，用于区分好的动作和差的动作。使用这些奖励信号，我们构建偏好数据集并通过直接偏好优化（DPO）使 NORA-1.5 适应目标实施例。广泛的评估表明，奖励驱动的后期训练持续提高了模拟和真实机器人环境中的性能，通过简单而有效的奖励模型展示了 VLA 模型可靠性的显着提升。我们的研究结果强调 NORA-1.5 和奖励引导的后训练是实现更可靠、适合现实世界部署的具体代理的可行途径。"
        },
        {
          "title": "$π^{*}_{0.6}$: a VLA That Learns From Experience",
          "url": "http://arxiv.org/abs/2511.14759v2",
          "snippet": "We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $π^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $π^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.",
          "site": "arxiv.org",
          "rank": 21,
          "published": "2025-11-18T18:58:55Z",
          "authors": [
            "Physical Intelligence",
            "Ali Amin",
            "Raichelle Aniceto",
            "Ashwin Balakrishna",
            "Kevin Black",
            "Ken Conley",
            "Grace Connors",
            "James Darpinian",
            "Karan Dhabalia",
            "Jared DiCarlo",
            "Danny Driess",
            "Michael Equi",
            "Adnan Esmail",
            "Yunhao Fang",
            "Chelsea Finn",
            "Catherine Glossop",
            "Thomas Godden",
            "Ivan Goryachev",
            "Lachy Groom",
            "Hunter Hancock",
            "Karol Hausman",
            "Gashon Hussein",
            "Brian Ichter",
            "Szymon Jakubczak",
            "Rowan Jen",
            "Tim Jones",
            "Ben Katz",
            "Liyiming Ke",
            "Chandra Kuchi",
            "Marinda Lamb",
            "Devin LeBlanc",
            "Sergey Levine",
            "Adrian Li-Bell",
            "Yao Lu",
            "Vishnu Mano",
            "Mohith Mothukuri",
            "Suraj Nair",
            "Karl Pertsch",
            "Allen Z. Ren",
            "Charvi Sharma",
            "Lucy Xiaoyang Shi",
            "Laura Smith",
            "Jost Tobias Springenberg",
            "Kyle Stachowicz",
            "Will Stoeckle",
            "Alex Swerdlow",
            "James Tanner",
            "Marcel Torne",
            "Quan Vuong",
            "Anna Walling",
            "Haohuan Wang",
            "Blake Williams",
            "Sukwon Yoo",
            "Lili Yu",
            "Ury Zhilinsky",
            "Zhiyuan Zhou"
          ],
          "arxiv_id": "2511.14759",
          "abstract": "We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $π^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $π^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.",
          "abstract_zh": "我们研究视觉-语言-动作（VLA）模型如何通过强化学习（RL）在现实世界的部署中得到改进。我们提出了一种通用方法，即通过优势条件策略进行经验和修正的强化学习 (RECAP)，它通过优势条件条件为 VLA 提供强化学习训练。我们的方法将异构数据纳入自我改进过程，包括演示、政策收集的数据以及自主执行期间提供的专家远程操作干预。RECAP 首先使用离线 RL 预训练通用 VLA，我们将其称为 $π^{*}_{0.6}$，然后可以通过机器人数据收集对其进行专门化，以在下游任务上获得高性能。我们证明，经过完整 RECAP 方法训练的 $π^{*}_{0.6}$ 模型可以在真实家庭中折叠衣物、可靠地组装盒子，并使用专业浓缩咖啡机制作浓缩咖啡。在一些最困难的任务上，RECAP 使任务吞吐量增加了一倍以上，并将任务失败率大约降低一半。"
        },
        {
          "title": "Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM",
          "url": "http://arxiv.org/abs/2511.14499v1",
          "snippet": "The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor configurations.Related works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention maps.This approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky objects.By focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities.",
          "site": "arxiv.org",
          "rank": 22,
          "published": "2025-11-18T13:46:18Z",
          "authors": [
            "Jack Qin",
            "Zhitao Wang",
            "Yinan Zheng",
            "Keyu Chen",
            "Yang Zhou",
            "Yuanxin Zhong",
            "Siyuan Cheng"
          ],
          "arxiv_id": "2511.14499",
          "abstract": "The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor configurations.Related works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention maps.This approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky objects.By focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities.",
          "abstract_zh": "自动驾驶（AD）系统在复杂驾驶场景下表现出了卓越的性能。然而，泛化仍然是当前系统的一个关键限制，泛化是指处理未见过的场景或不熟悉的传感器配置的能力。相关工作探索了使用视觉语言模型（VLM）来解决少样本或零样本任务。虽然这些方法很有前景，但也带来了新的挑战：混合自动驾驶系统的出现，其中使用两个不同的系统来规划轨迹，从而导致潜在的不一致。其他研究方向探索了直接从 VLM 生成控制动作的视觉-语言-动作 (VLA) 框架。然而，这些端到端解决方案表现出过高的计算需求。为了克服这些挑战，我们引入了风险语义蒸馏 (RSD)，这是一种利用 VLM 来增强端到端 (E2E) AD 主干网训练的新颖框架。通过为关键对象提供风险关注，RSD 解决了泛化问题。具体来说，我们引入了RiskHead，这是一个插件模块，它将视觉语言模型中的因果风险估计提炼为鸟瞰图（BEV）特征，生成可解释的风险注意力图。这种方法允许BEV特征学习更丰富、更细致的风险注意力表示，这直接增强了模型处理空间边界和风险对象的能力。通过关注风险注意力，RSD更好地与类人驾驶行为保持一致，这对于导航至关重要在复杂和动态的环境中。我们在 Bench2Drive 基准测试上的实验证明了 RSD 在管理复杂且不可预测的驾驶条件方面的有效性。由于 RSD 实现了增强的 BEV 表示，我们观察到感知和规划能力都有显着改善。"
        },
        {
          "title": "Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories",
          "url": "http://arxiv.org/abs/2511.19528v1",
          "snippet": "Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.",
          "site": "arxiv.org",
          "rank": 23,
          "published": "2025-11-24T07:54:49Z",
          "authors": [
            "Rushuai Yang",
            "Zhiyuan Feng",
            "Tianxiang Zhang",
            "Kaixin Wang",
            "Chuheng Zhang",
            "Li Zhao",
            "Xiu Su",
            "Yi Chen",
            "Jiang Bian"
          ],
          "arxiv_id": "2511.19528",
          "abstract": "Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.",
          "abstract_zh": "扩展视觉-语言-动作 (VLA) 模型预训练需要大量多样化、高质量的操作轨迹。目前大多数数据都是通过人工远程操作获得的，这种方法成本高昂且难以扩展。强化学习 (RL) 方法通过自主探索学习有用的技能，使其成为生成数据的可行方法。然而，标准强化学习训练会陷入狭窄的执行模式，限制了其在大规模预训练中的实用性。我们提出了发现、学习和强化 (DLR)，这是一种信息理论模式发现框架，可为 VLA 预训练生成多种不同的、高成功的行为模式。根据经验，DLR 在 LIBERO 上生成了一个明显更加多样化的轨迹语料库。具体来说，它为同一任务学习多种不同的、高成功的策略，而标准强化学习只发现一种策略，因此它覆盖了状态-动作空间的更广泛的区域。当适应看不见的下游任务套件时，在我们不同的 RL 数据上预训练的 VLA 模型超过了在同等大小的标准 RL 数据集上训练的模型。此外，DLR 表现出单模式 RL 所缺乏的积极的数据缩放行为。这些结果将多模式强化学习定位为用于具体基础模型的实用、可扩展的数据引擎。"
        },
        {
          "title": "FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models",
          "url": "http://arxiv.org/abs/2511.16233v1",
          "snippet": "The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.",
          "site": "arxiv.org",
          "rank": 24,
          "published": "2025-11-20T11:04:14Z",
          "authors": [
            "Kewei Chen",
            "Yayu Long",
            "Shuai Li",
            "Mingsheng Shang"
          ],
          "arxiv_id": "2511.16233",
          "abstract": "The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.",
          "abstract_zh": "视觉-语言-动作（VLA）模型的强大泛化能力因严重依赖海量、冗余且价值不均的数据集而受到瓶颈，阻碍了其广泛应用。现有的以模型为中心的优化路径，例如模型压缩（通常会导致性能下降）或策略蒸馏（其产品依赖于模型且缺乏通用性），无法从根本上解决这一数据级挑战。为此，本文介绍了 FT-NCFM，这是一种根本不同的、以数据为中心的生成数据蒸馏框架。我们的框架采用独立的事实追踪（FT）引擎，将因果归因与程序对比验证相结合，以评估样本的内在价值。在这些评估的指导下，对抗性 NCFM 流程综合了模型不可知、信息密集且可重用的数据资产。几个主流 VLA 基准测试的实验结果表明，与在完整数据集上训练相比，仅在 5% 的蒸馏核心集上训练的模型的成功率达到 85-90%，同时减少了 80% 以上的训练时间。我们的工作表明，智能数据蒸馏是构建高效、高性能 VLA 模型的一条非常有前途的新途径。"
        },
        {
          "title": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
          "url": "http://arxiv.org/abs/2511.19221v1",
          "snippet": "Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.",
          "site": "arxiv.org",
          "rank": 25,
          "published": "2025-11-24T15:28:25Z",
          "authors": [
            "Jianhua Han",
            "Meng Tian",
            "Jiangtong Zhu",
            "Fan He",
            "Huixin Zhang",
            "Sitong Guo",
            "Dechang Zhu",
            "Hao Tang",
            "Pei Xu",
            "Yuze Guo",
            "Minzhe Niu",
            "Haojie Zhu",
            "Qichao Dong",
            "Xuechao Yan",
            "Siyuan Dong",
            "Lu Hou",
            "Qingqiu Huang",
            "Xiaosong Jia",
            "Hang Xu"
          ],
          "arxiv_id": "2511.19221",
          "abstract": "Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.",
          "abstract_zh": "自动驾驶很大程度上依赖于准确而强大的空间感知。许多失败都是由于不准确和不稳定引起的，特别是在长尾场景和复杂的交互中。然而，当前的视觉语言模型在空间基础和理解方面较弱，因此基于其构建的VLA系统表现出有限的感知和定位能力。为了应对这些挑战，我们引入了 Percept-WAM，这是一种感知增强的世界意识行动模型，它是第一个将 2D/3D 场景理解能力隐式集成到单一视觉语言模型 (VLM) 中的模型。Percept-WAM 没有依赖 QA 式的空间推理，而是将 2D/3D 感知任务统一为 World-PV 和 World-BEV 令牌，这些令牌对空间坐标和置信度进行编码。我们提出了一种用于密集对象感知的网格条件预测机制，结合了 IoU 感知评分和并行自回归解码，提高了长尾、远距离和小对象场景的稳定性。此外，Percept-WAM利用预训练的VLM参数来保留通用智能（例如逻辑推理），并可以直接输出感知结果和轨迹控制输出。实验表明，Percept-WAM 在下游感知基准上匹配或超越了经典检测器和分段器，在 COCO 2D 检测和 nuScenes BEV 3D 检测上实现了 51.7/58.9 mAP。当与轨迹解码器集成时，它进一步提高了 nuScenes 和 NAVSIM 上的规划性能，例如，在 NAVSIM 上的 PMDS 中超过 DiffusionDrive 2.1。定性结果进一步凸显了其强大的开​​放词汇和长尾泛化能力。"
        },
        {
          "title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy",
          "url": "http://arxiv.org/abs/2511.16651v1",
          "snippet": "Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $π$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $π_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $π_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.",
          "site": "arxiv.org",
          "rank": 26,
          "published": "2025-11-20T18:55:05Z",
          "authors": [
            "Yang Tian",
            "Yuyin Yang",
            "Yiman Xie",
            "Zetao Cai",
            "Xu Shi",
            "Ning Gao",
            "Hangxu Liu",
            "Xuekun Jiang",
            "Zherui Qiu",
            "Feng Yuan",
            "Yaping Li",
            "Ping Wang",
            "Junhao Cai",
            "Jia Zeng",
            "Hao Dong",
            "Jiangmiao Pang"
          ],
          "arxiv_id": "2511.16651",
          "abstract": "Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $π$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $π_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $π_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.",
          "abstract_zh": "最近的工作探讨了真实和合成数据如何促进视觉-语言-动作（VLA）模型的泛化。虽然当前的 VLA 模型已显示出大规模真实机器人预训练的强大有效性，但合成数据此前尚未在规模上表现出可比的能力。本文提供了第一个证据，表明在预训练 VLA 模型时，仅合成数据就可以与最强的 $π$ 数据集的性能相匹配，揭示了大规模模拟的巨大价值。由此产生的模型还在几个具有挑战性的任务上表现出令人惊讶的零样本模拟到真实的迁移。我们的合成数据集 InternData-A1 包含 4 个实施例、18 项技能、70 项任务和 227 个场景的超过 630k 轨迹和 7,433 小时，涵盖刚性、铰接、可变形和流体对象操作。它是通过高度自主、完全解耦和组合的模拟管道生成的，该管道能够以最少的手动调整实现长期技能组合、灵活的任务组装和异构实施例。使用与 $π_0$ 相同的架构，我们完全在 InternData-A1 上预训练一个模型，发现它在 49 个模拟任务、5 个现实世界任务和 4 个长期灵巧任务中与官方的 $π_0$ 匹配。我们发布数据集并将开源生成管道，以扩大对大规模机器人数据的访问，并降低实体人工智能研究的可扩展数据创建的障碍。"
        },
        {
          "title": "RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action",
          "url": "http://arxiv.org/abs/2511.14161v2",
          "snippet": "Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an \"Action (Object, Container)\" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.",
          "site": "arxiv.org",
          "rank": 27,
          "published": "2025-11-18T05:54:05Z",
          "authors": [
            "Xiaoquan Sun",
            "Ruijian Zhang",
            "Kang Pang",
            "Bingchen Miao",
            "Yuxiang Tan",
            "Zhen Yang",
            "Ming Li",
            "Jiayu Chen"
          ],
          "arxiv_id": "2511.14161",
          "abstract": "Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an \"Action (Object, Container)\" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.",
          "abstract_zh": "家庭整理是一个重要的应用领域，但当前的基准测试既不能模拟用户偏好，也不能支持移动性，而且泛化性较差，因此很难全面评估集成的语言到动作的能力。为了解决这个问题，我们提出了 RoboTidy，这是一个用于语言引导的家庭整理的统一基准，支持视觉-语言-动作 (VLA) 和视觉-语言-导航 (VLN) 培训和评估。RoboTidy 提供 500 个带碰撞的逼真 3D 高斯泼溅 (3DGS) 家庭场景（覆盖 500 个物体和容器），将整理制定为“动作（物体、容器）”列表，并提供 6.4k 条高质量操作演示轨迹和 1.5k 导航轨迹以支持小镜头和大规模训练。我们还在现实世界中部署了 RoboTidy 来进行物体整理，为家庭整理建立了端到端的基准。RoboTidy 提供了一个可扩展的平台，通过对语言引导机器人进行全面、现实的评估，弥补了嵌入式人工智能的关键差距。"
        },
        {
          "title": "Mixture of Horizons in Action Chunking",
          "url": "http://arxiv.org/abs/2511.19433v1",
          "snippet": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
          "site": "arxiv.org",
          "rank": 28,
          "published": "2025-11-24T18:59:51Z",
          "authors": [
            "Dong Jing",
            "Gang Wang",
            "Jiaqi Liu",
            "Weiliang Tang",
            "Zelong Sun",
            "Yunchao Yao",
            "Zhenyu Wei",
            "Yunhui Liu",
            "Zhiwu Lu",
            "Mingyu Ding"
          ],
          "arxiv_id": "2511.19433",
          "abstract": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
          "abstract_zh": "视觉-语言-动作（VLA）模型在机器人操作方面表现出了卓越的能力，但它们的性能对训练期间使用的$\\textbf{action chunk length}$敏感，称为$\\textbf{horizo​​n}$。我们的实证研究揭示了一种内在的权衡：较长的视野提供了更强的全球远见，但会降低细​​粒度的准确性，而较短的视野虽然可以增强局部控制，但在长期任务上却很困难，这意味着单一视野的固定选择不是最优的。为了减轻这种权衡，我们提出了 $\\textbf{混合视野 (MoH)}$ 策略。MoH 将动作块重新排列成具有不同视野的多个片段，使用共享动作变压器并行处理它们，并使用轻型线性门融合输出。它具有三个吸引人的好处。1) MoH 在单个模型中联合利用长期远见和短期精度，提高复杂任务的性能和通用性。2) MoH 是即插即用的全注意力动作模块，训练或推理开销最小。3) MoH 支持自适应视野的动态推理，通过跨视野共识选择稳定的动作，实现比基线高 2.5$\\times$ 的吞吐量，同时保持卓越的性能。基于流的策略 $π_0$、$π_{0.5}$ 和一步回归策略 $π_{\\text{reg}}$ 的广泛实验表明，MoH 在模拟和现实任务中都产生了一致且显着的收益。值得注意的是，在混合任务设置下，MoH 的 $π_{0.5}$ 在仅经过 $30k$ 训练迭代后，在 LIBERO 上达到了新的最先进水平，平均成功率为 99$\\%$。项目页面：https://github.com/Timsty1/MixtureOfHorizo​​ns"
        },
        {
          "title": "Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation",
          "url": "http://arxiv.org/abs/2511.17097v1",
          "snippet": "Vision-Language Navigation requires agents to act coherently over long horizons by understanding not only local visual context but also how far they have advanced within a multi-step instruction. However, recent Vision-Language-Action models focus on direct action prediction and earlier progress methods predict numeric achievements; both overlook the monotonic co-progression property of the observation and instruction sequences. Building on this insight, Progress-Think introduces semantic progress reasoning, predicting instruction-style progress from visual observations to enable more accurate navigation. To achieve this without expensive annotations, we propose a three-stage framework. In the initial stage, Self-Aligned Progress Pretraining bootstraps a reasoning module via a novel differentiable alignment between visual history and instruction prefixes. Then, Progress-Guided Policy Pretraining injects learned progress states into the navigation context, guiding the policy toward consistent actions. Finally, Progress-Policy Co-Finetuning jointly optimizes both modules with tailored progress-aware reinforcement objectives. Experiments on R2R-CE and RxR-CE show state-of-the-art success and efficiency, demonstrating that semantic progress yields a more consistent representation of navigation advancement.",
          "site": "arxiv.org",
          "rank": 29,
          "published": "2025-11-21T09:52:07Z",
          "authors": [
            "Shuo Wang",
            "Yucheng Wang",
            "Guoxin Lian",
            "Yongcai Wang",
            "Maiyue Chen",
            "Kaihui Wang",
            "Bo Zhang",
            "Zhizhong Su",
            "Yutian Zhou",
            "Wanting Li",
            "Deying Li",
            "Zhaoxin Fan"
          ],
          "arxiv_id": "2511.17097",
          "abstract": "Vision-Language Navigation requires agents to act coherently over long horizons by understanding not only local visual context but also how far they have advanced within a multi-step instruction. However, recent Vision-Language-Action models focus on direct action prediction and earlier progress methods predict numeric achievements; both overlook the monotonic co-progression property of the observation and instruction sequences. Building on this insight, Progress-Think introduces semantic progress reasoning, predicting instruction-style progress from visual observations to enable more accurate navigation. To achieve this without expensive annotations, we propose a three-stage framework. In the initial stage, Self-Aligned Progress Pretraining bootstraps a reasoning module via a novel differentiable alignment between visual history and instruction prefixes. Then, Progress-Guided Policy Pretraining injects learned progress states into the navigation context, guiding the policy toward consistent actions. Finally, Progress-Policy Co-Finetuning jointly optimizes both modules with tailored progress-aware reinforcement objectives. Experiments on R2R-CE and RxR-CE show state-of-the-art success and efficiency, demonstrating that semantic progress yields a more consistent representation of navigation advancement.",
          "abstract_zh": "视觉语言导航要求智能体不仅了解本地视觉上下文，还要了解他们在多步骤指令中前进的程度，从而在长远的视野中保持连贯一致的行动。然而，最近的视觉-语言-行动模型侧重于直接行动预测，而早期的进展方法则预测数字成就；两者都忽视了观察和指令序列的单调共进特性。基于这一见解，Progress-Think 引入了语义进度推理，从视觉观察中预测指令式进度，以实现更准确的导航。为了在不使用昂贵注释的情况下实现这一目标，我们提出了一个三阶段框架。在初始阶段，自对齐进度预训练通过视觉历史和指令前缀之间新颖的可微对齐来引导推理模块。然后，进度引导的策略预训练将学习到的进度状态注入导航上下文中，引导策略采取一致的行动。最后，进度策略协同微调通过定制的进度感知强化目标联合优化两个模块。R2R-CE 和 RxR-CE 上的实验显示了最先进的成功和效率，证明语义进步可以产生更一致的导航进步表示。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 10 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 10）。",
      "items": [
        {
          "title": "TianxingChen/Embodied-AI-Guide",
          "url": "https://github.com/TianxingChen/Embodied-AI-Guide",
          "snippet": "[Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "IliaLarchenko/behavior-1k-solution",
          "url": "https://github.com/IliaLarchenko/behavior-1k-solution",
          "snippet": "1st place solution of 2025 BEHAVIOR Challenge",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "BridgeVLA/BridgeVLA",
          "url": "https://github.com/BridgeVLA/BridgeVLA",
          "snippet": "✨✨【NeurIPS 2025】Official implementation of BridgeVLA",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "SalvatoreRa/ML-news-of-the-week",
          "url": "https://github.com/SalvatoreRa/ML-news-of-the-week",
          "snippet": "A collection of the the best ML and AI news every week (research, news, resources)",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "52CV/CVPR-2025-Papers",
          "url": "https://github.com/52CV/CVPR-2025-Papers",
          "snippet": "CVPR-2025-Papers",
          "site": "github.com",
          "rank": 9
        },
        {
          "title": "52CV/ECCV-2024-Papers",
          "url": "https://github.com/52CV/ECCV-2024-Papers",
          "snippet": "ECCV-2024-Papers",
          "site": "github.com",
          "rank": 10
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 本周无更新。",
      "items": []
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-11-17",
  "week_end": "2025-11-23",
  "last_updated": "2026-01-07"
}