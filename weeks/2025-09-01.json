{
  "generated_at": "2026-01-07T13:26:00.879148",
  "sites": [
    {
      "site": "arxiv.org",
      "site_summary": "arxiv.org 上共发现 12 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 12）。",
      "items": [
        {
          "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies",
          "url": "http://arxiv.org/abs/2509.04996v1",
          "snippet": "Developing efficient Vision-Language-Action (VLA) policies is crucial for practical robotics deployment, yet current approaches face prohibitive computational costs and resource requirements. Existing diffusion-based VLA policies require multi-billion-parameter models and massive datasets to achieve strong performance. We tackle this efficiency challenge with two contributions: intermediate-modality fusion, which reallocates capacity to the diffusion head by pruning up to $50\\%$ of LLM layers, and action-specific Global-AdaLN conditioning, which cuts parameters by $20\\%$ through modular adaptation. We integrate these advances into a novel 950 M-parameter VLA called FLOWER. Pretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance with bigger VLAs across $190$ tasks spanning ten simulation and real-world benchmarks and demonstrates robustness across diverse robotic embodiments. In addition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark. Demos, code and pretrained weights are available at https://intuitive-robots.github.io/flower_vla/.",
          "site": "arxiv.org",
          "rank": 1,
          "published": "2025-09-05T10:43:12Z",
          "authors": [
            "Moritz Reuss",
            "Hongyi Zhou",
            "Marcel Rühle",
            "Ömer Erdinç Yağmurlu",
            "Fabian Otto",
            "Rudolf Lioutikov"
          ],
          "arxiv_id": "2509.04996",
          "abstract": "Developing efficient Vision-Language-Action (VLA) policies is crucial for practical robotics deployment, yet current approaches face prohibitive computational costs and resource requirements. Existing diffusion-based VLA policies require multi-billion-parameter models and massive datasets to achieve strong performance. We tackle this efficiency challenge with two contributions: intermediate-modality fusion, which reallocates capacity to the diffusion head by pruning up to $50\\%$ of LLM layers, and action-specific Global-AdaLN conditioning, which cuts parameters by $20\\%$ through modular adaptation. We integrate these advances into a novel 950 M-parameter VLA called FLOWER. Pretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance with bigger VLAs across $190$ tasks spanning ten simulation and real-world benchmarks and demonstrates robustness across diverse robotic embodiments. In addition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark. Demos, code and pretrained weights are available at https://intuitive-robots.github.io/flower_vla/.",
          "abstract_zh": "开发高效的视觉-语言-动作（VLA）策略对于实际的机器人部署至关重要，但当前的方法面临着高昂的计算成本和资源需求。现有的基于扩散的 VLA 策略需要数十亿参数的模型和海量数据集才能实现强大的性能。我们通过两个贡献来应对这一效率挑战：中间模态融合，通过修剪高达 $50\\%$ 的 LLM 层，将容量重新分配给扩散头；以及特定于动作的 Global-AdaLN 调节，通过模块化适应将参数削减 $20\\%$。我们将这些进步集成到一个名为 FLOWER 的新型 950 M 参数 VLA 中。FLOWER 经过短短 200 个 H100 GPU 小时的预训练，在跨越 10 个模拟和现实世界基准的 190 美元任务中提供了具有更大 VLA 的竞争性能，并在不同的机器人实施例中展示了鲁棒性。此外，FLOWER 在 CALVIN ABC 基准测试中取得了 4.53 的新 SoTA。演示、代码和预训练权重可在 https://intuitive-robots.github.io/flower_vla/ 获取。"
        },
        {
          "title": "FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction",
          "url": "http://arxiv.org/abs/2509.04018v2",
          "snippet": "Robotic manipulation is a fundamental component of automation. However, traditional perception-planning pipelines often fall short in open-ended tasks due to limited flexibility, while the architecture of a single end-to-end Vision-Language-Action (VLA) offers promising capabilities but lacks crucial mechanisms for anticipating and recovering from failure. To address these challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with a supervisor for failure prediction and correction. The supervisor evaluates action viability through vision-language queries and generates corrective strategies when risks arise, trained efficiently without manual labeling. A dual-stream fusion module further refines actions by leveraging past predictions. Evaluation results on multiple simulation platforms (SIMPLER and LIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA outperforms state-of-the-art models in both zero-shot and fine-tuned settings. Successful real-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong generalization and practical utility for building more reliable autonomous systems.",
          "site": "arxiv.org",
          "rank": 2,
          "published": "2025-09-04T08:47:26Z",
          "authors": [
            "Yifan Yang",
            "Zhixiang Duan",
            "Tianshi Xie",
            "Fuyu Cao",
            "Pinxi Shen",
            "Peili Song",
            "Piaopiao Jin",
            "Guokang Sun",
            "Shaoqing Xu",
            "Yangwei You",
            "Jingtai Liu"
          ],
          "arxiv_id": "2509.04018",
          "abstract": "Robotic manipulation is a fundamental component of automation. However, traditional perception-planning pipelines often fall short in open-ended tasks due to limited flexibility, while the architecture of a single end-to-end Vision-Language-Action (VLA) offers promising capabilities but lacks crucial mechanisms for anticipating and recovering from failure. To address these challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with a supervisor for failure prediction and correction. The supervisor evaluates action viability through vision-language queries and generates corrective strategies when risks arise, trained efficiently without manual labeling. A dual-stream fusion module further refines actions by leveraging past predictions. Evaluation results on multiple simulation platforms (SIMPLER and LIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA outperforms state-of-the-art models in both zero-shot and fine-tuned settings. Successful real-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong generalization and practical utility for building more reliable autonomous systems.",
          "abstract_zh": "机器人操纵是自动化的基本组成部分。然而，由于灵活性有限，传统的感知规划管道在开放式任务中往往存在不足，而单一端到端视觉-语言-动作（VLA）的架构提供了有前景的功能，但缺乏预测故障和从故障中恢复的关键机制。为了应对这些挑战，我们提出了 FPC-VLA，这是一种双模型框架，它将 VLA 与监控器集成以进行故障预测和纠正。主管通过视觉语言查询评估行动的可行性，并在出现风险时生成纠正策略，并进行有效培训，无需手动标记。双流融合模块利用过去的预测进一步细化操作。多个仿真平台（SIMPLER 和 LIBERO）和机器人实施例（WidowX、Google Robot、Franka）的评估结果表明，FPC-VLA 在零样本和微调设置方面均优于最先进的模型。在现实世界中对各种长期任务的成功部署证实了 FPC-VLA 在构建更可靠的自主系统方面具有强大的通用性和实用性。"
        },
        {
          "title": "ANNIE: Be Careful of Your Robots",
          "url": "http://arxiv.org/abs/2509.03383v1",
          "snippet": "The integration of vision-language-action (VLA) models into embodied AI (EAI) robots is rapidly advancing their ability to perform complex, long-horizon tasks in humancentric environments. However, EAI systems introduce critical security risks: a compromised VLA model can directly translate adversarial perturbations on sensory input into unsafe physical actions. Traditional safety definitions and methodologies from the machine learning community are no longer sufficient. EAI systems raise new questions, such as what constitutes safety, how to measure it, and how to design effective attack and defense mechanisms in physically grounded, interactive settings. In this work, we present the first systematic study of adversarial safety attacks on embodied AI systems, grounded in ISO standards for human-robot interactions. We (1) formalize a principled taxonomy of safety violations (critical, dangerous, risky) based on physical constraints such as separation distance, velocity, and collision boundaries; (2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with 2,400 video-action sequences for evaluating embodied safety; and (3) ANNIE-Attack, a task-aware adversarial framework with an attack leader model that decomposes long-horizon goals into frame-level perturbations. Our evaluation across representative EAI models shows attack success rates exceeding 50% across all safety categories. We further demonstrate sparse and adaptive attack strategies and validate the real-world impact through physical robot experiments. These results expose a previously underexplored but highly consequential attack surface in embodied AI systems, highlighting the urgent need for security-driven defenses in the physical AI era. Code is available at https://github.com/RLCLab/Annie.",
          "site": "arxiv.org",
          "rank": 3,
          "published": "2025-09-03T15:00:28Z",
          "authors": [
            "Yiyang Huang",
            "Zixuan Wang",
            "Zishen Wan",
            "Yapeng Tian",
            "Haobo Xu",
            "Yinhe Han",
            "Yiming Gan"
          ],
          "arxiv_id": "2509.03383",
          "abstract": "The integration of vision-language-action (VLA) models into embodied AI (EAI) robots is rapidly advancing their ability to perform complex, long-horizon tasks in humancentric environments. However, EAI systems introduce critical security risks: a compromised VLA model can directly translate adversarial perturbations on sensory input into unsafe physical actions. Traditional safety definitions and methodologies from the machine learning community are no longer sufficient. EAI systems raise new questions, such as what constitutes safety, how to measure it, and how to design effective attack and defense mechanisms in physically grounded, interactive settings. In this work, we present the first systematic study of adversarial safety attacks on embodied AI systems, grounded in ISO standards for human-robot interactions. We (1) formalize a principled taxonomy of safety violations (critical, dangerous, risky) based on physical constraints such as separation distance, velocity, and collision boundaries; (2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with 2,400 video-action sequences for evaluating embodied safety; and (3) ANNIE-Attack, a task-aware adversarial framework with an attack leader model that decomposes long-horizon goals into frame-level perturbations. Our evaluation across representative EAI models shows attack success rates exceeding 50% across all safety categories. We further demonstrate sparse and adaptive attack strategies and validate the real-world impact through physical robot experiments. These results expose a previously underexplored but highly consequential attack surface in embodied AI systems, highlighting the urgent need for security-driven defenses in the physical AI era. Code is available at https://github.com/RLCLab/Annie.",
          "abstract_zh": "将视觉-语言-动作 (VLA) 模型集成到实体人工智能 (EAI) 机器人中，正在迅速提高它们在以人为中心的环境中执行复杂、长期任务的能力。然而，EAI 系统引入了严重的安全风险：受损的 VLA 模型可以直接将感知输入的对抗性扰动转化为不安全的物理动作。机器学习社区的传统安全定义和方法已不再足够。EAI 系统提出了新的问题，例如什么构成安全、如何衡量安全以及如何在物理接地的交互式环境中设计有效的攻击和防御机制。在这项工作中，我们基于人机交互的 ISO 标准，首次系统地研究了针对具体人工智能系统的对抗性安全攻击。我们（1）基于物理约束（例如间隔距离、速度和碰撞边界），正式制定安全违规（严重、危险、有风险）的原则分类；(2) 引入 ANNIEBench，这是一个包含 2,400 个视频动作序列的 9 个安全关键场景的基准，用于评估体现的安全性；(3) ANNIE-Attack，一种任务感知对抗框架，具有攻击领导者模型，可将长期目标分解为帧级扰动。我们对代表性 EAI 模型的评估显示，所有安全类别的攻击成功率均超过 50%。我们进一步演示了稀疏和自适应攻击策略，并通过物理机器人实验验证了现实世界的影响。这些结果暴露了实体人工智能系统中先前未被充分探索但后果严重的攻击面，凸显了物理人工智能时代对安全驱动防御的迫切需求。代码可在 https://github.com/RLCLab/Annie 获取。"
        },
        {
          "title": "AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving",
          "url": "http://arxiv.org/abs/2509.01944v2",
          "snippet": "Vision-Language-Action (VLA) models in autonomous driving systems have recently demonstrated transformative potential by integrating multimodal perception with decision-making capabilities. However, the interpretability and coherence of the decision process and the plausibility of action sequences remain largely underexplored. To address these issues, we propose AutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and self-reflection capabilities of autonomous driving systems through chain-of-thought (CoT) processing and reinforcement learning (RL). Specifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K for supervised fine-tuning, which effectively builds cognitive bridges between input information and output trajectories through a four-step logical chain with self-reflection for validation. Moreover, to maximize both reasoning and self-reflection during the RL stage, we further employ the Group Relative Policy Optimization (GRPO) algorithm within a physics-grounded reward framework that incorporates spatial alignment, vehicle dynamic, and temporal smoothness criteria to ensure reliable and realistic trajectory planning. Extensive evaluation results across both nuScenes and Waymo datasets demonstrates the state-of-the-art performance and robust generalization capacity of our proposed method.",
          "site": "arxiv.org",
          "rank": 4,
          "published": "2025-09-02T04:32:24Z",
          "authors": [
            "Zhenlong Yuan",
            "Chengxuan Qian",
            "Jing Tang",
            "Rui Chen",
            "Zijian Song",
            "Lei Sun",
            "Xiangxiang Chu",
            "Yujun Cai",
            "Dapeng Zhang",
            "Shuo Li"
          ],
          "arxiv_id": "2509.01944",
          "abstract": "Vision-Language-Action (VLA) models in autonomous driving systems have recently demonstrated transformative potential by integrating multimodal perception with decision-making capabilities. However, the interpretability and coherence of the decision process and the plausibility of action sequences remain largely underexplored. To address these issues, we propose AutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and self-reflection capabilities of autonomous driving systems through chain-of-thought (CoT) processing and reinforcement learning (RL). Specifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K for supervised fine-tuning, which effectively builds cognitive bridges between input information and output trajectories through a four-step logical chain with self-reflection for validation. Moreover, to maximize both reasoning and self-reflection during the RL stage, we further employ the Group Relative Policy Optimization (GRPO) algorithm within a physics-grounded reward framework that incorporates spatial alignment, vehicle dynamic, and temporal smoothness criteria to ensure reliable and realistic trajectory planning. Extensive evaluation results across both nuScenes and Waymo datasets demonstrates the state-of-the-art performance and robust generalization capacity of our proposed method.",
          "abstract_zh": "自动驾驶系统中的视觉-语言-动作（VLA）模型最近通过将多模态感知与决策能力相结合，展现出了变革潜力。然而，决策过程的可解释性和连贯性以及行动序列的合理性在很大程度上仍未得到充分探索。为了解决这些问题，我们提出了 AutoDrive-R$^2$，这是一种新颖的 VLA 框架，通过思想链 (CoT) 处理和强化学习 (RL) 增强自动驾驶系统的推理和自我反思能力。具体来说，我们首先提出了一个名为 nuScenesR$^2$-6K 的创新 CoT 数据集，用于监督微调，它通过带有自我反思验证的四步逻辑链，有效地在输入信息和输出轨迹之间建立认知桥梁。此外，为了在强化学习阶段最大化推理和自我反思，我们在基于物理的奖励框架中进一步采用了组相对策略优化（GRPO）算法，该框架结合了空间对齐、车辆动态和时间平滑标准，以确保可靠且现实的轨迹规划。nuScenes 和 Waymo 数据集的广泛评估结果证明了我们提出的方法的最先进的性能和强大的泛化能力。"
        },
        {
          "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions",
          "url": "http://arxiv.org/abs/2509.06951v2",
          "snippet": "Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.",
          "site": "arxiv.org",
          "rank": 5,
          "published": "2025-09-08T17:58:30Z",
          "authors": [
            "Qi Lv",
            "Weijie Kong",
            "Hao Li",
            "Jia Zeng",
            "Zherui Qiu",
            "Delin Qu",
            "Haoming Song",
            "Qizhi Chen",
            "Xiang Deng",
            "Jiangmiao Pang"
          ],
          "arxiv_id": "2509.06951",
          "abstract": "Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.",
          "abstract_zh": "在动态视觉环境中执行语言条件任务仍然是实体人工智能的核心挑战。现有的视觉-语言-动作（VLA）模型主要采用反应式状态到动作映射，往往导致动态场景中的短视行为和鲁棒性差。在本文中，我们介绍了 F1，一个预训练的 VLA 框架，它将视觉预见生成集成到决策流程中。F1 采用 Mixture-of-Transformer 架构，配备感知、预见生成和控制专用模块，从而架起理解、生成和行动的桥梁。F1 的核心是采用下一个规模的预测机制来综合目标条件的视觉远见作为明确的规划目标。通过预测未来可能的视觉状态，F1 将动作生成重新表述为一个有远见引导的逆动力学问题，从而使动作能够隐式实现视觉目标。为了赋予 F1 强大且可泛化的能力，我们在包含 136 个不同任务的超过 330k 轨迹的广泛数据集上提出了一个三阶段训练方案。该训练方案增强了模块化推理，并为模型配备了可转移的视觉远见，这对于复杂和动态的环境至关重要。对现实世界任务和模拟基准的广泛评估表明，F1 始终优于现有方法，在任务成功率和泛化能力方面都取得了显着的进步。"
        },
        {
          "title": "LLaDA-VLA: Vision Language Diffusion Action Models",
          "url": "http://arxiv.org/abs/2509.06932v2",
          "snippet": "The rapid progress of auto-regressive vision-language models (VLMs) has inspired growing interest in vision-language-action models (VLA) for robotic manipulation. Recently, masked diffusion models, a paradigm distinct from autoregressive models, have begun to demonstrate competitive performance in text generation and multimodal applications, leading to the development of a series of diffusion-based VLMs (d-VLMs). However, leveraging such models for robot policy learning remains largely unexplored. In this work, we present LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to robotic domain, we introduce two key designs: (1) a localized special-token classification strategy that replaces full-vocabulary classification with special action token classification, reducing adaptation difficulty; (2) a hierarchical action-structured decoding strategy that decodes action sequences hierarchically considering the dependencies within and across actions. Extensive experiments demonstrate that LLaDA-VLA significantly outperforms state-of-the-art VLAs on both simulation and real-world robots.",
          "site": "arxiv.org",
          "rank": 6,
          "published": "2025-09-08T17:45:40Z",
          "authors": [
            "Yuqing Wen",
            "Hebei Li",
            "Kefan Gu",
            "Yucheng Zhao",
            "Tiancai Wang",
            "Xiaoyan Sun"
          ],
          "arxiv_id": "2509.06932",
          "abstract": "The rapid progress of auto-regressive vision-language models (VLMs) has inspired growing interest in vision-language-action models (VLA) for robotic manipulation. Recently, masked diffusion models, a paradigm distinct from autoregressive models, have begun to demonstrate competitive performance in text generation and multimodal applications, leading to the development of a series of diffusion-based VLMs (d-VLMs). However, leveraging such models for robot policy learning remains largely unexplored. In this work, we present LLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon pretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to robotic domain, we introduce two key designs: (1) a localized special-token classification strategy that replaces full-vocabulary classification with special action token classification, reducing adaptation difficulty; (2) a hierarchical action-structured decoding strategy that decodes action sequences hierarchically considering the dependencies within and across actions. Extensive experiments demonstrate that LLaDA-VLA significantly outperforms state-of-the-art VLAs on both simulation and real-world robots.",
          "abstract_zh": "自回归视觉语言模型（VLM）的快速发展激发了人们对用于机器人操作的视觉语言动作模型（VLA）的兴趣。最近，掩蔽扩散模型（一种不同于自回归模型的范例）已经开始在文本生成和多模态应用中展现出有竞争力的性能，从而导致了一系列基于扩散的 VLM（d-VLM）的开发。然而，利用此类模型进行机器人策略学习在很大程度上仍未得到探索。在这项工作中，我们提出了 LLaDA-VLA，这是第一个基于预训练的 d-VLM 构建的视觉-语言-扩散-动作模型，用于机器人操作。为了有效地将d-VLM适应机器人领域，我们引入了两个关键设计：（1）本地化的特殊标记分类策略，用特殊动作标记分类代替全词汇分类，降低了适应难度；（2）分层动作结构解码策略，考虑动作内部和动作之间的依赖性，分层解码动作序列。大量实验表明，LLaDA-VLA 在模拟和现实世界的机器人上都显着优于最先进的 VLA。"
        },
        {
          "title": "Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance",
          "url": "http://arxiv.org/abs/2509.02055v2",
          "snippet": "Vision-Language-Action (VLA) models pre-trained on large, diverse datasets show remarkable potential for general-purpose robotic manipulation. However, a primary bottleneck remains in adapting these models to downstream tasks, especially when the robot's embodiment or the task itself differs from the pre-training data. This discrepancy leads to a significant mismatch in action distributions, demanding extensive data and compute for effective fine-tuning. To address this challenge, we introduce \\textbf{Align-Then-stEer (\\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation framework. \\texttt{ATE} first aligns disparate action spaces by constructing a unified latent space, where a variational autoencoder constrained by reverse KL divergence embeds adaptation actions into modes of the pre-training action latent distribution. Subsequently, it steers the diffusion- or flow-based VLA's generation process during fine-tuning via a guidance mechanism that pushes the model's output distribution towards the target domain. We conduct extensive experiments on cross-embodiment and cross-task manipulation in both simulation and real world. Compared to direct fine-tuning of representative VLAs, our method improves the average multi-task success rate by up to \\textbf{9.8\\%} in simulation and achieves a striking \\textbf{32\\% success rate gain} in a real-world cross-embodiment setting. Our work presents a general and lightweight solution that greatly enhances the practicality of deploying VLA models to new robotic platforms and tasks.",
          "site": "arxiv.org",
          "rank": 7,
          "published": "2025-09-02T07:51:59Z",
          "authors": [
            "Yang Zhang",
            "Chenwei Wang",
            "Ouyang Lu",
            "Yuan Zhao",
            "Yunfei Ge",
            "Zhenglong Sun",
            "Xiu Li",
            "Chi Zhang",
            "Chenjia Bai",
            "Xuelong Li"
          ],
          "arxiv_id": "2509.02055",
          "abstract": "Vision-Language-Action (VLA) models pre-trained on large, diverse datasets show remarkable potential for general-purpose robotic manipulation. However, a primary bottleneck remains in adapting these models to downstream tasks, especially when the robot's embodiment or the task itself differs from the pre-training data. This discrepancy leads to a significant mismatch in action distributions, demanding extensive data and compute for effective fine-tuning. To address this challenge, we introduce \\textbf{Align-Then-stEer (\\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation framework. \\texttt{ATE} first aligns disparate action spaces by constructing a unified latent space, where a variational autoencoder constrained by reverse KL divergence embeds adaptation actions into modes of the pre-training action latent distribution. Subsequently, it steers the diffusion- or flow-based VLA's generation process during fine-tuning via a guidance mechanism that pushes the model's output distribution towards the target domain. We conduct extensive experiments on cross-embodiment and cross-task manipulation in both simulation and real world. Compared to direct fine-tuning of representative VLAs, our method improves the average multi-task success rate by up to \\textbf{9.8\\%} in simulation and achieves a striking \\textbf{32\\% success rate gain} in a real-world cross-embodiment setting. Our work presents a general and lightweight solution that greatly enhances the practicality of deploying VLA models to new robotic platforms and tasks.",
          "abstract_zh": "在大型、多样化的数据集上预先训练的视觉-语言-动作（VLA）模型显示出通用机器人操作的巨大潜力。然而，使这些模型适应下游任务的主要瓶颈仍然存在，特别是当机器人的实施例或任务本身与预训练数据不同时。这种差异导致动作分布严重不匹配，需要大量数据和计算来进行有效的微调。为了应对这一挑战，我们引入了 \\textbf{Align-Then-stEer (\\texttt{ATE})}，一种新颖的、数据高效的、即插即用的适配框架。\\texttt{ATE} 首先通过构建统一的潜在空间来对齐不同的动作空间，其中受反向 KL 散度约束的变分自动编码器将适应动作嵌入到预训练动作潜在分布的模式中。随后，它在微调期间通过引导机制引导基于扩散或流的 VLA 生成过程，将模型的输出分布推向目标域。我们在模拟和现实世界中对跨实施例和跨任务操作进行了广泛的实验。与直接微调代表性 VLA 相比，我们的方法在模拟中将平均多任务成功率提高了高达 \\textbf{9.8\\%}，并在现实世界的跨实施例设置中实现了惊人的 \\textbf{32\\% 成功率增益}。我们的工作提出了一种通用且轻量级的解决方案，大大增强了将 VLA 模型部署到新机器人平台和任务的实用性。"
        },
        {
          "title": "SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning",
          "url": "http://arxiv.org/abs/2509.05614v1",
          "snippet": "Pruning accelerates compute-bound models by reducing computation. Recently applied to Vision-Language-Action (VLA) models, existing methods prune tokens using only local info from current action, ignoring global context from prior actions, causing >20% success rate drop and limited speedup. We observe high similarity across consecutive actions and propose leveraging both local (current) and global (past) info for smarter token selection. We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context to reduce visual tokens per action; (2) Dynamic pruning at layer level: prunes tokens per layer based on layer-specific importance; (3) Lightweight action-aware controller: classifies actions as coarse/fine-grained (by speed), adjusting pruning aggressiveness since fine-grained actions are pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs. OpenVLA-OFT, with negligible success rate loss.",
          "site": "arxiv.org",
          "rank": 8,
          "published": "2025-09-06T06:22:19Z",
          "authors": [
            "Hanzhen Wang",
            "Jiaming Xu",
            "Jiayi Pan",
            "Yongkang Zhou",
            "Guohao Dai"
          ],
          "arxiv_id": "2509.05614",
          "abstract": "Pruning accelerates compute-bound models by reducing computation. Recently applied to Vision-Language-Action (VLA) models, existing methods prune tokens using only local info from current action, ignoring global context from prior actions, causing >20% success rate drop and limited speedup. We observe high similarity across consecutive actions and propose leveraging both local (current) and global (past) info for smarter token selection. We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context to reduce visual tokens per action; (2) Dynamic pruning at layer level: prunes tokens per layer based on layer-specific importance; (3) Lightweight action-aware controller: classifies actions as coarse/fine-grained (by speed), adjusting pruning aggressiveness since fine-grained actions are pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs. OpenVLA-OFT, with negligible success rate loss.",
          "abstract_zh": "修剪通过减少计算来加速计算密集型模型。最近应用于视觉-语言-动作 (VLA) 模型，现有方法仅使用当前动作中的本地信息来修剪标记，忽略先前动作中的全局上下文，导致成功率下降超过 20%，加速有限。我们观察到连续操作之间的高度相似性，并建议利用本地（当前）和全局（过去）信息来进行更智能的代币选择。我们引入了 SpecPrune-VLA，一种具有两级剪枝和启发式控制的免训练方法：（1）动作级别的静态剪枝：使用全局历史和局部上下文来减少每个动作的视觉标记；(2) 层级动态剪枝：根据层特定重要性剪枝每层令牌；（3）轻量级动作感知控制器：将动作分类为粗粒度/细粒度（按速度），调整剪枝积极性，因为细粒度动作对剪枝敏感。LIBERO 上的实验表明，与 OpenVLA-OFT 相比，SpecPrune-VLA 在 NVIDIA A800 上实现了 1.46 倍的加速，在 NVIDIA GeForce RTX 3090 上实现了 1.57 倍的加速，而成功率损失可以忽略不计。"
        },
        {
          "title": "Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models",
          "url": "http://arxiv.org/abs/2509.04063v1",
          "snippet": "Vision-Language-Action (VLA) models based on flow matching have shown excellent performance in general-purpose robotic manipulation tasks. However, the action accuracy of these models on complex downstream tasks is unsatisfactory. One important reason is that these models rely solely on the post-training paradigm of imitation learning, which makes it difficult to have a deeper understanding of the distribution properties of data quality, which is exactly what Reinforcement Learning (RL) excels at. In this paper, we theoretically propose an offline RL post-training objective for VLA flow models and induce an efficient and feasible offline RL fine-tuning algorithm -- Adaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted scaling factor in the VLA flow model loss, we construct a principled bias-variance trade-off objective function to optimally control the impact of RL signal on flow loss. ARFM adaptively balances RL advantage preservation and flow loss gradient variance control, resulting in a more stable and efficient fine-tuning process. Extensive simulation and real-world experimental results show that ARFM exhibits excellent generalization, robustness, few-shot learning, and continuous learning performance.",
          "site": "arxiv.org",
          "rank": 9,
          "published": "2025-09-04T09:48:43Z",
          "authors": [
            "Hongyin Zhang",
            "Shiyuan Zhang",
            "Junxi Jin",
            "Qixin Zeng",
            "Yifan Qiao",
            "Hongchao Lu",
            "Donglin Wang"
          ],
          "arxiv_id": "2509.04063",
          "abstract": "Vision-Language-Action (VLA) models based on flow matching have shown excellent performance in general-purpose robotic manipulation tasks. However, the action accuracy of these models on complex downstream tasks is unsatisfactory. One important reason is that these models rely solely on the post-training paradigm of imitation learning, which makes it difficult to have a deeper understanding of the distribution properties of data quality, which is exactly what Reinforcement Learning (RL) excels at. In this paper, we theoretically propose an offline RL post-training objective for VLA flow models and induce an efficient and feasible offline RL fine-tuning algorithm -- Adaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted scaling factor in the VLA flow model loss, we construct a principled bias-variance trade-off objective function to optimally control the impact of RL signal on flow loss. ARFM adaptively balances RL advantage preservation and flow loss gradient variance control, resulting in a more stable and efficient fine-tuning process. Extensive simulation and real-world experimental results show that ARFM exhibits excellent generalization, robustness, few-shot learning, and continuous learning performance.",
          "abstract_zh": "基于流匹配的视觉-语言-动作（VLA）模型在通用机器人操作任务中表现出了优异的性能。然而，这些模型在复杂下游任务上的动作准确性并不令人满意。一个重要原因是这些模型仅依赖于模仿学习的训练后范式，这使得难以更深入地理解数据质量的分布特性，而这正是强化学习（RL）所擅长的。在本文中，我们从理论上提出了一种针对 VLA 流模型的离线 RL 后训练目标，并提出了一种高效可行的离线 RL 微调算法——自适应强化流匹配（ARFM）。通过在 VLA 流量模型损失中引入自适应调整的比例因子，我们构建了一个有原则的偏差-方差权衡目标函数，以最佳地控制 RL 信号对流量损失的影响。ARFM 自适应地平衡 RL 优势保留和流量损失梯度方差控制，从而实现更加稳定和高效的微调过程。大量的仿真和真实实验结果表明，ARFM 具有出色的泛化性、鲁棒性、小样本学习和持续学习性能。"
        },
        {
          "title": "OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision",
          "url": "http://arxiv.org/abs/2509.05578v1",
          "snippet": "Multimodal large language models (MLLMs) have shown strong vision-language reasoning abilities but still lack robust 3D spatial understanding, which is critical for autonomous driving. This limitation stems from two key challenges: (1) the difficulty of constructing accessible yet effective 3D representations without expensive manual annotations, and (2) the loss of fine-grained spatial details in VLMs due to the absence of large-scale 3D vision-language pretraining. To address these challenges, we propose OccVLA, a novel framework that integrates 3D occupancy representations into a unified multimodal reasoning process. Unlike prior approaches that rely on explicit 3D inputs, OccVLA treats dense 3D occupancy as both a predictive output and a supervisory signal, enabling the model to learn fine-grained spatial structures directly from 2D visual inputs. The occupancy predictions are regarded as implicit reasoning processes and can be skipped during inference without performance degradation, thereby adding no extra computational overhead. OccVLA achieves state-of-the-art results on the nuScenes benchmark for trajectory planning and demonstrates superior performance on 3D visual question-answering tasks, offering a scalable, interpretable, and fully vision-based solution for autonomous driving.",
          "site": "arxiv.org",
          "rank": 10,
          "published": "2025-09-06T03:47:21Z",
          "authors": [
            "Ruixun Liu",
            "Lingyu Kong",
            "Derun Li",
            "Hang Zhao"
          ],
          "arxiv_id": "2509.05578",
          "abstract": "Multimodal large language models (MLLMs) have shown strong vision-language reasoning abilities but still lack robust 3D spatial understanding, which is critical for autonomous driving. This limitation stems from two key challenges: (1) the difficulty of constructing accessible yet effective 3D representations without expensive manual annotations, and (2) the loss of fine-grained spatial details in VLMs due to the absence of large-scale 3D vision-language pretraining. To address these challenges, we propose OccVLA, a novel framework that integrates 3D occupancy representations into a unified multimodal reasoning process. Unlike prior approaches that rely on explicit 3D inputs, OccVLA treats dense 3D occupancy as both a predictive output and a supervisory signal, enabling the model to learn fine-grained spatial structures directly from 2D visual inputs. The occupancy predictions are regarded as implicit reasoning processes and can be skipped during inference without performance degradation, thereby adding no extra computational overhead. OccVLA achieves state-of-the-art results on the nuScenes benchmark for trajectory planning and demonstrates superior performance on 3D visual question-answering tasks, offering a scalable, interpretable, and fully vision-based solution for autonomous driving.",
          "abstract_zh": "多模态大语言模型 (MLLM) 显示出强大的视觉语言推理能力，但仍然缺乏强大的 3D 空间理解，而这对于自动驾驶至关重要。这种限制源于两个关键挑战：(1) 在没有昂贵的手动注释的情况下构建可访问且有效的 3D 表示很困难，(2) 由于缺乏大规模 3D 视觉语言预训练，VLM 中细粒度的空间细节会丢失。为了应对这些挑战，我们提出了 OccVLA，这是一种将 3D 占用表示集成到统一的多模态推理过程中的新颖框架。与依赖显式 3D 输入的先前方法不同，OccVLA 将密集的 3D 占用视为预测输出和监督信号，使模型能够直接从 2D 视觉输入学习细粒度的空间结构。占用预测被视为隐式推理过程，可以在推理过程中跳过而不会降低性能，从而不会增加额外的计算开销。OccVLA 在 nuScenes 轨迹规划基准上取得了最先进的结果，并在 3D 视觉问答任务中展示了卓越的性能，为自动驾驶提供了可扩展、可解释且完全基于视觉的解决方案。"
        },
        {
          "title": "CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation",
          "url": "http://arxiv.org/abs/2509.06819v1",
          "snippet": "Learning-based controllers, such as diffusion policies and vision-language action models, often generate low-frequency or discontinuous robot state changes. Achieving smooth reference tracking requires a low-level controller that converts high-level targets commands into joint torques, enabling compliant behavior during contact interactions. We present CRISP, a lightweight C++ implementation of compliant Cartesian and joint-space controllers for the ROS2 control standard, designed for seamless integration with high-level learning-based policies as well as teleoperation. The controllers are compatible with any manipulator that exposes a joint-torque interface. Through our Python and Gymnasium interfaces, CRISP provides a unified pipeline for recording data from hardware and simulation and deploying high-level learning-based policies seamlessly, facilitating rapid experimentation. The system has been validated on hardware with the Franka Robotics FR3 and in simulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid integration, flexible deployment, and real-time performance, our implementation provides a unified pipeline for data collection and policy execution, lowering the barrier to applying learning-based methods on ROS2-compatible manipulators. Detailed documentation is available at the project website - https://utiasDSL.github.io/crisp_controllers.",
          "site": "arxiv.org",
          "rank": 11,
          "published": "2025-09-08T15:55:50Z",
          "authors": [
            "Daniel San José Pro",
            "Oliver Hausdörfer",
            "Ralf Römer",
            "Maximilian Dösch",
            "Martin Schuck",
            "Angela P. Schöllig"
          ],
          "arxiv_id": "2509.06819",
          "abstract": "Learning-based controllers, such as diffusion policies and vision-language action models, often generate low-frequency or discontinuous robot state changes. Achieving smooth reference tracking requires a low-level controller that converts high-level targets commands into joint torques, enabling compliant behavior during contact interactions. We present CRISP, a lightweight C++ implementation of compliant Cartesian and joint-space controllers for the ROS2 control standard, designed for seamless integration with high-level learning-based policies as well as teleoperation. The controllers are compatible with any manipulator that exposes a joint-torque interface. Through our Python and Gymnasium interfaces, CRISP provides a unified pipeline for recording data from hardware and simulation and deploying high-level learning-based policies seamlessly, facilitating rapid experimentation. The system has been validated on hardware with the Franka Robotics FR3 and in simulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid integration, flexible deployment, and real-time performance, our implementation provides a unified pipeline for data collection and policy execution, lowering the barrier to applying learning-based methods on ROS2-compatible manipulators. Detailed documentation is available at the project website - https://utiasDSL.github.io/crisp_controllers.",
          "abstract_zh": "基于学习的控制器，例如扩散策略和视觉语言动作模型，通常会产生低频或不连续的机器人状态变化。实现平滑的参考跟踪需要一个低级控制器，将高级目标命令转换为关节扭矩，从而在接触交互过程中实现顺从行为。我们推出了 CRISP，它是符合 ROS2 控制标准的笛卡尔和联合空间控制器的轻量级 C++ 实现，旨在与高级基于学习的策略以及远程操作无缝集成。该控制器与任何具有关节扭矩接口的机械手兼容。通过我们的 Python 和 Gymnasium 接口，CRISP 提供了一个统一的管道，用于记录来自硬件和模拟的数据，并无缝部署基于高级学习的策略，从而促进快速实验。该系统已通过 Franka Robotics FR3 的硬件验证以及 Kuka IIWA14 和 Kinova Gen3 的仿真验证。我们的实现专为快速集成、灵活部署和实时性能而设计，为数据收集和策略执行提供了统一的管道，降低了在 ROS2 兼容的操纵器上应用基于学习的方法的障碍。详细文档可在项目网站上找到 - https://utiasDSL.github.io/crisp_controllers。"
        },
        {
          "title": "OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation",
          "url": "http://arxiv.org/abs/2509.05513v1",
          "snippet": "Egocentric human videos provide scalable demonstrations for imitation learning, but existing corpora often lack either fine-grained, temporally localized action descriptions or dexterous hand annotations. We introduce OpenEgo, a multimodal egocentric manipulation dataset with standardized hand-pose annotations and intention-aligned action primitives. OpenEgo totals 1107 hours across six public datasets, covering 290 manipulation tasks in 600+ environments. We unify hand-pose layouts and provide descriptive, timestamped action primitives. To validate its utility, we train language-conditioned imitation-learning policies to predict dexterous hand trajectories. OpenEgo is designed to lower the barrier to learning dexterous manipulation from egocentric video and to support reproducible research in vision-language-action learning. All resources and instructions will be released at www.openegocentric.com.",
          "site": "arxiv.org",
          "rank": 12,
          "published": "2025-09-05T21:47:55Z",
          "authors": [
            "Ahad Jawaid",
            "Yu Xiang"
          ],
          "arxiv_id": "2509.05513",
          "abstract": "Egocentric human videos provide scalable demonstrations for imitation learning, but existing corpora often lack either fine-grained, temporally localized action descriptions or dexterous hand annotations. We introduce OpenEgo, a multimodal egocentric manipulation dataset with standardized hand-pose annotations and intention-aligned action primitives. OpenEgo totals 1107 hours across six public datasets, covering 290 manipulation tasks in 600+ environments. We unify hand-pose layouts and provide descriptive, timestamped action primitives. To validate its utility, we train language-conditioned imitation-learning policies to predict dexterous hand trajectories. OpenEgo is designed to lower the barrier to learning dexterous manipulation from egocentric video and to support reproducible research in vision-language-action learning. All resources and instructions will be released at www.openegocentric.com.",
          "abstract_zh": "以自我为中心的人类视频为模仿学习提供了可扩展的演示，但现有的语料库通常缺乏细粒度、时间局部的动作描述或灵巧的手动注释。我们引入了 OpenEgo，一个多模式的以自我为中心的操作数据集，具有标准化的手势注释和意图一致的动作原语。OpenEgo 在六个公共数据集上总计 1107 小时，涵盖 600 多个环境中的 290 个操作任务。我们统一手势布局并提供描述性的、带时间戳的动作原语。为了验证其实用性，我们训练了语言条件的模仿学习策略来预测灵巧的手部轨迹。OpenEgo 旨在降低从以自我为中心的视频中学习灵巧操作的障碍，并支持视觉-语言-动作学习的可重复研究。所有资源和说明将在 www.openegocentric.com 上发布。"
        }
      ]
    },
    {
      "site": "organizations",
      "site_summary": "头部玩家本周无更新。",
      "items": [],
      "organization_stats": {}
    },
    {
      "site": "github.com",
      "site_summary": "github.com 上共发现 11 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 11）。",
      "items": [
        {
          "title": "patrick-llgc/Learning-Deep-Learning",
          "url": "https://github.com/patrick-llgc/Learning-Deep-Learning",
          "snippet": "Paper reading notes on Deep Learning and Machine Learning",
          "site": "github.com",
          "rank": 1
        },
        {
          "title": "Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "url": "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
          "snippet": "A list of recent papers about adversarial learning",
          "site": "github.com",
          "rank": 2
        },
        {
          "title": "Vector-Wangel/XLeRobot",
          "url": "https://github.com/Vector-Wangel/XLeRobot",
          "snippet": "XLeRobot: Practical Dual-Arm Mobile Home Robot for $660",
          "site": "github.com",
          "rank": 3
        },
        {
          "title": "OpenDriveLab/WholebodyVLA",
          "url": "https://github.com/OpenDriveLab/WholebodyVLA",
          "snippet": "Towards Unified Latent VLA for Whole-body Loco-manipulation Control",
          "site": "github.com",
          "rank": 4
        },
        {
          "title": "Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "url": "https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide",
          "snippet": " Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。",
          "site": "github.com",
          "rank": 5
        },
        {
          "title": "RoboTwin-Platform/RoboTwin",
          "url": "https://github.com/RoboTwin-Platform/RoboTwin",
          "snippet": "RoboTwin 2.0 Offical Repo",
          "site": "github.com",
          "rank": 6
        },
        {
          "title": "ZutJoe/KoalaHackerNews",
          "url": "https://github.com/ZutJoe/KoalaHackerNews",
          "snippet": "Koala hacker news 周报内容 每周二0点左右更新",
          "site": "github.com",
          "rank": 7
        },
        {
          "title": "OpenHelix-Team/VLA-Adapter",
          "url": "https://github.com/OpenHelix-Team/VLA-Adapter",
          "snippet": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model",
          "site": "github.com",
          "rank": 8
        },
        {
          "title": "PetroIvaniuk/llms-tools",
          "url": "https://github.com/PetroIvaniuk/llms-tools",
          "snippet": "A list of LLMs Tools & Projects",
          "site": "github.com",
          "rank": 9
        },
        {
          "title": "ChaofanTao/Autoregressive-Models-in-Vision-Survey",
          "url": "https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey",
          "snippet": " [TMLR 2025🔥] A survey for the autoregressive models in vision. ",
          "site": "github.com",
          "rank": 10
        },
        {
          "title": "gabrielchua/daily-ai-papers",
          "url": "https://github.com/gabrielchua/daily-ai-papers",
          "snippet": "All credits go to HuggingFace's Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).",
          "site": "github.com",
          "rank": 11
        }
      ]
    },
    {
      "site": "huggingface.co",
      "site_summary": "huggingface.co 上共发现 1 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 1）。",
      "items": [
        {
          "title": "InternRobotics/F1-VLA",
          "url": "https://huggingface.co/InternRobotics/F1-VLA",
          "snippet": "InternRobotics/F1-VLA",
          "site": "huggingface.co",
          "rank": 2,
          "published": "2025-09-07T12:03:22.000Z"
        }
      ]
    },
    {
      "site": "zhihu.com",
      "site_summary": "zhihu.com 本周无更新。",
      "items": []
    }
  ],
  "week_start": "2025-09-01",
  "week_end": "2025-09-07",
  "last_updated": "2026-01-07"
}