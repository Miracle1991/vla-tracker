

<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <title>VLA 每周追踪 · 每周自动更新</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
      body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif; margin: 0; padding: 0; background: #f5f5f7; color: #111827;}
      header { background: #111827; color: white; padding: 1rem 1.5rem; position: relative; }
      header h1 { margin: 0; font-size: 1.5rem; display: flex; align-items: center; gap: 0.75rem; flex-wrap: wrap; }
      header .auto-update-badge { display: inline-flex; align-items: center; gap: 0.35rem; padding: 0.25rem 0.65rem; background: linear-gradient(135deg, #10b981 0%, #059669 100%); border-radius: 999px; font-size: 0.75rem; font-weight: 500; color: white; box-shadow: 0 2px 4px rgba(16, 185, 129, 0.3); }
      header .auto-update-badge::before { content: '🔄'; font-size: 0.7rem; }
      header p { margin: 0.25rem 0 0; font-size: 0.9rem; color: #9ca3af; }
      .star-button { position: absolute; top: 1rem; right: 1.5rem; display: flex; align-items: center; gap: 0.5rem; padding: 0.5rem 1rem; background: #1f2937; border: 1px solid #374151; border-radius: 0.5rem; color: white; text-decoration: none; font-size: 0.9rem; transition: all 0.2s; cursor: pointer; }
      .star-button:hover { background: #374151; border-color: #4b5563; transform: translateY(-1px); }
      .star-button:active { transform: translateY(0); }
      .star-icon { font-size: 1.1rem; }
      .star-count { font-weight: 500; }
      @media (max-width: 768px) {
        .star-button { position: static; margin-top: 0.75rem; display: inline-flex; }
      }
      .container { display: flex; }
      .sidebar { background: white; padding: 1.5rem 1rem; position: sticky; top: 0; height: fit-content; max-height: calc(100vh - 80px); overflow-y: auto; }
      .sidebar-left { width: 180px; border-right: 1px solid #e5e7eb; }
      .sidebar-right { width: 280px; border-left: 1px solid #e5e7eb; }
      .sidebar h3 { margin: 0 0 1rem 0; font-size: 0.9rem; color: #6b7280; text-transform: uppercase; letter-spacing: 0.05em; }
      .sidebar ul { list-style: none; padding: 0; margin: 0; }
      .sidebar li { margin-bottom: 0.5rem; }
      .sidebar a { display: block; padding: 0.5rem 0.75rem; color: #374151; text-decoration: none; border-radius: 0.375rem; font-size: 0.9rem; transition: background-color 0.2s; white-space: nowrap; }
      .sidebar a:hover { background: #f3f4f6; color: #111827; }
      .sidebar a.active { background: #111827; color: white; }
      .week-link { font-weight: 500; }
      main { flex: 1; padding: 1.5rem; max-width: 1200px; }
      .date-list { margin-bottom: 1.5rem; }
      .date-pill { display: inline-block; margin: 0.25rem 0.4rem 0.25rem 0; padding: 0.35rem 0.75rem; border-radius: 999px; background: #e5e7eb; font-size: 0.85rem; text-decoration: none; color: #111827; }
      .date-pill.active { background: #111827; color: #f9fafb; }
      .card { background: white; border-radius: 0.75rem; padding: 1.25rem 1.5rem; margin-bottom: 1rem; box-shadow: 0 10px 15px -3px rgba(15,23,42,0.08), 0 4px 6px -2px rgba(15,23,42,0.05); scroll-margin-top: 1rem; }
      .card h2 { margin-top: 0; font-size: 1.1rem; margin-bottom: 0.4rem; }
      .card small { color: #6b7280; }
      .item-list { margin-top: 0.75rem; padding-left: 1.1rem; }
      .item-list li { margin-bottom: 0.45rem; }
      .item-list a { color: #2563eb; text-decoration: none; }
      .item-list a:hover { text-decoration: underline; }
      .empty { color: #6b7280; font-size: 0.95rem; }
      footer { text-align: center; padding: 1rem; font-size: 0.75rem; color: #6b7280; }
      @media (max-width: 1024px) {
        .sidebar-left { width: 140px; padding: 1rem 0.75rem; }
        .sidebar-right { width: 220px; padding: 1rem 0.75rem; }
      }
      @media (max-width: 768px) {
        .container { flex-direction: column; }
        .sidebar { width: 100%; position: relative; border-right: none; border-left: none; border-bottom: 1px solid #e5e7eb; max-height: none; }
        .sidebar-left { border-right: none; }
        .sidebar-right { border-left: none; }
        .sidebar ul { display: flex; flex-wrap: wrap; gap: 0.5rem; }
        .sidebar li { margin-bottom: 0; }
        main { padding: 1rem; order: -1; }
      }
    </style>
    <script>
      // 平滑滚动到锚点（来源目录）
      document.querySelectorAll('.sidebar a[href^="#"]').forEach(anchor => {
        anchor.addEventListener('click', function (e) {
          e.preventDefault();
          const targetId = this.getAttribute('href').substring(1);
          const targetElement = document.getElementById(targetId);
          if (targetElement) {
            targetElement.scrollIntoView({ behavior: 'smooth', block: 'start' });
            // 更新活动状态
            document.querySelectorAll('.sidebar a[href^="#"]').forEach(a => a.classList.remove('active'));
            this.classList.add('active');
          }
        });
      });
      
      // 根据滚动位置高亮当前站点
      function updateActiveSite() {
        const cards = document.querySelectorAll('.card[id]');
        const siteLinks = document.querySelectorAll('.sidebar a[href^="#"]');
        let currentSite = '';
        
        cards.forEach(card => {
          const rect = card.getBoundingClientRect();
          if (rect.top <= 150 && rect.bottom >= 150) {
            currentSite = card.id;
          }
        });
        
        siteLinks.forEach(link => {
          link.classList.remove('active');
          if (link.getAttribute('href') === '#' + currentSite) {
            link.classList.add('active');
          }
        });
      }
      
      // 监听滚动事件
      window.addEventListener('scroll', updateActiveSite);
      // 页面加载时也更新一次
      window.addEventListener('load', updateActiveSite);
      
      // 获取 GitHub star 数量
      async function fetchStarCount() {
        try {
          const response = await fetch('/api/github-stars');
          if (response.ok) {
            const data = await response.json();
            const starCountEl = document.getElementById('starCount');
            if (starCountEl) {
              starCountEl.textContent = data.stargazers_count || 0;
            }
          } else {
            const starCountEl = document.getElementById('starCount');
            if (starCountEl) {
              starCountEl.textContent = '?';
            }
          }
        } catch (error) {
          console.error('获取 star 数量失败:', error);
          const starCountEl = document.getElementById('starCount');
          if (starCountEl) {
            starCountEl.textContent = '?';
          }
        }
      }
      
      // 处理点赞按钮点击
      function handleStarClick(event) {
        // 不阻止默认行为，让链接正常跳转
        // 按钮已经设置了 href，会直接跳转到 GitHub 仓库页面
        // 用户可以在 GitHub 页面点击 star 按钮
      }
      
      // 页面加载时获取 star 数量
      window.addEventListener('load', fetchStarCount);
    </script>
  </head>
  <body>
    <header>
      <h1>
        VLA 每周追踪
        <span class="auto-update-badge">每周自动更新</span>
      </h1>
      <p>自动聚合来自 知乎 / GitHub / HuggingFace / arXiv 的 VLA 相关更新（专注于机器人、自动驾驶领域）</p>
      <a href="https://github.com/Miracle1991/vla-tracker" target="_blank" rel="noopener noreferrer" class="star-button" id="starButton" onclick="handleStarClick(event)">
        <span class="star-icon">⭐</span>
        <span class="star-text">点赞</span>
        <span class="star-count" id="starCount">加载中...</span>
      </a>
    </header>
    <div class="container">
      <aside class="sidebar sidebar-left">
        <h3>来源目录</h3>
        <ul>
          <li><a href="#arxiv">arXiv</a></li>
          <li><a href="#github">GitHub</a></li>
          <li><a href="#huggingface">HuggingFace</a></li>
          <li><a href="#zhihu">知乎</a></li>
        </ul>
        <h3 style="margin-top: 1.5rem;">头部玩家</h3>
        <ul>
          
            
            
              
            
              
                
              
            
              
            
              
            
              
            
            
              <li style="color: #9ca3af; font-size: 0.85rem; padding: 0.5rem 0.75rem;">本周无更新</li>
            
          
        </ul>
      </aside>
      <main>
        
  
    <h2 style="color:#111827;margin-bottom:1.5rem;padding-bottom:0.5rem;border-bottom:2px solid #e5e7eb;">
      2025-10-13 ~ 2025-10-19
    </h2>
    
      
      
      
       
        
        
      
      <article class="card" id="arxiv" >
        <h2>arxiv.org</h2>
        <small>arxiv.org 上共发现 28 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 28）。</small>
        
          <ul class="item-list">
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.11660v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">ManiAgent: An Agentic Framework for General Robotic Manipulation</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-13</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yi Yang, Kefan Gu, Yuqing Wen, Hebei Li, Yucheng Zhao, Tiancai Wang, Xudong Liu</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>虽然视觉-语言-动作（VLA）模型在机器人操作方面表现出了令人印象深刻的能力，但它们在复杂推理和长期任务规划方面的表现受到数据稀缺和模型容量的限制。为了解决这个问题，我们引入了 ManiAgent，这是一种用于一般操作任务的代理架构，可实现从任务描述和环境输入到机器人操作动作的端到端输出。在此框架中，多个智能体涉及智能体间通信来执行环境感知、子任务分解和动作生成，从而能够有效处理复杂的操作场景。评估显示，ManiAgent 在 SimplerEnv 基准测试中实现了 86.8% 的成功率，在现实世界的拾放任务中实现了 95.8% 的成功率，从而实现了高效的数据收集，生成的 VLA 模型的性能可与在人工注释数据集上训练的模型相媲美。该项目网页位于 https://yi-yang929.github.io/ManiAgent/。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.17640v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-20</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yuquan Xue, Guanxing Lu, Zhenyu Wu, Chuanrui Zhang, Bofang Jia, Zhengyi Gu, Yansong Tang, Ziwei Wang</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作模型（VLA）通过模仿学习在复杂的机器人操作任务中表现出了卓越的性能。然而，现有的模仿学习数据集仅包含成功的轨迹，缺乏失败或恢复数据，特别是对于机器人由于微小扰动或错误而偏离主要策略的分布外（OOD）状态，导致VLA模型与偏离训练分布的状态作斗争。为此，我们通过探索性采样提出了一个名为 RESample 的自动化 OOD 数据增强框架。具体来说，我们首先利用离线强化学习来获得一个动作价值网络，该网络可以准确识别当前操纵策略下的次优动作。我们通过推出进一步从轨迹中采样潜在的 OOD 状态，并设计一种探索性采样机制，自适应地将这些动作代理合并到训练数据集中以确保效率。随后，我们的框架明确鼓励 VLA 从 OOD 状态中恢复，并增强其针对分配变化的鲁棒性。我们对 LIBERO 基准以及现实世界的机器人操作任务进行了广泛的实验，证明 RESample 持续提高了 VLA 模型的稳定性和泛化能力。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.17369v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-20</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Haochen Su, Cristian Meo, Francesco Stella, Andrea Peirone, Kai Junge, Josie Hughes</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>人们越来越期望机器人系统能够在以人为中心的非结构化环境中运行，在这些环境中，安全性、适应性和通用性至关重要。视觉-语言-动作（VLA）模型已被提议作为真实机器人的语言引导广义控制框架。然而，它们的部署仅限于传统的串行链路操纵器。再加上基于学习的控制的刚性和不可预测性，与环境安全交互的能力缺失但至关重要。在这项工作中，我们提出了在软连续体机械臂上部署 VLA 模型，以演示自主安全的人机交互。我们提出了一个结构化的微调和部署管道，评估两个最先进的 VLA 模型（OpenVLA-OFT 和 $π_0$）在代表性操作任务中的表现，并表明，虽然开箱即用的策略由于实施例不匹配而失败，但通过有针对性的微调，软机器人的性能与刚性机器人相同。我们的研究结果强调了进行微调以弥补体现差距的必要性，并证明将 VLA 模型与软机器人相结合可以在人类共享环境中实现安全、灵活的体现人工智能。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.13778v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-15</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Xinyi Chen, Yilun Chen, Yanwei Fu, Ning Gao, Jiaya Jia, Weiyang Jin, Hao Li, Yao Mu, Jiangmiao Pang, Yu Qiao, Yang Tian, Bin Wang, Bolun Wang, Fangjing Wang, Hanqing Wang, Tai Wang, Ziqin Wang, Xueyuan Wei, Chao Wu, Shuai Yang, Jinhui Ye, Junqiu Yu, Jia Zeng, Jingjing Zhang, Jinyu Zhang, Shi Zhang, Feng Zheng, Bowen Zhou, Yangkun Zhu</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>我们推出了 InternVLA-M1，这是一个用于空间接地和机器人控制的统一框架，可将遵循指令的机器人推向可扩展的通用智能。其核心思想是空间引导的视觉-语言-动作训练，其中空间基础是指令和机器人动作之间的关键环节。InternVLA-M1 采用两阶段流程：(i) 对超过 2.3M 空间推理数据进行空间基础预训练，通过将指令与视觉、与具体实施无关的位置对齐来确定“在哪里行动”，以及 (ii) 空间引导动作后训练，通过即插即用的空间提示生成具体实施感知的行动来决定“如何行动”。这种空间引导的训练方法产生了一致的收益：InternVLA-M1 在 SimplerEnv Google Robot 上比没有空间引导的变体高出 14.6%，在 WidowX 上高出 17%，在 LIBERO Franka 上高出 4.3%，同时在框、点和轨迹预测方面表现出更强的空间推理能力。为了进一步扩展指令跟踪，我们构建了一个模拟引擎来收集 244K 个通用的拾取和放置片段，使 200 个任务和 3K+ 对象的平均改进达到 6.2%。在现实世界的集群拾放中，InternVLA-M1 提高了 7.3%，并且通过综合协同训练，在未见过的物体和新颖配置上实现了 +20.6%。此外，在长视域推理密集型场景中，它超越了现有作品10%以上。这些结果强调了空间引导训练是可扩展和有弹性的通用机器人的统一原则。代码和模型可在 https://github.com/InternRobotics/InternVLA-M1 获取。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.16617v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-18</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Ruihan Zhao, Tyler Ingebrand, Sandeep Chinchali, Ufuk Topcu</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>在大型机器人数据集上训练的视觉-语言-动作 (VLA) 模型有望实现跨不同领域和实施例的通用、稳健的控制。然而，当部署在新的环境、实施例或任务中时，现有方法通常会失败。我们引入了混合技能 VLA (MoS-VLA)，这是一个框架，它将机器人操作策略表示为一组有限的学习基函数的线性组合。在预训练期间，MoS-VLA 跨 Open X-Embodiment 项目的数据集共同学习这些基本函数，从而生成结构化的技能空间。在测试时，适应新任务只需要一次专家演示。然后通过轻量级凸优化问题推断相应的技能表示，该问题最小化 L1 动作误差，而不需要梯度更新。这种无梯度适应会产生最小的开销，同时能够快速实例化新技能。根据经验，MoS-VLA 在五个未见过的数据集中的五个上实现了较低的动作预测误差，并且在预训练的 VLA 模型完全失败的模拟和真实机器人任务中取得了成功。项目页面：mos-vla.github.io/
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.15446v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-17</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Ziang Guo, Zufeng Zhang</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>在自动驾驶中，动态环境和极端情况对自我车辆的状态理解和决策的鲁棒性提出了重大挑战。我们推出了 VDRive，这是一种用于端到端自动驾驶的新型管道，它可以显式地模拟状态-动作映射来应对这些挑战，从而实现可解释和稳健的决策。通过利用基于生成扩散策略的行动头对视觉语言行动模型 (VLA) 的状态理解的进步，我们的 VDRive 可以根据情境和几何方式指导驾驶。在上下文中，VLA 通过令牌生成预训练来预测未来的观察结果，其中观察结果由条件向量量化变分自动编码器 (CVQ-VAE) 表示为离散代码。从几何角度来看，我们对 VLA 进行强化学习微调，以根据当前驾驶条件预测未来的轨迹和动作。VLA 为动作策略头提供当前状态令牌和预测状态令牌，以生成分层动作和轨迹。在政策训练期间，有学识的批评家评估政策生成的行动并提供基于梯度的反馈，形成一个演员批评家框架，从而实现基于强化的政策学习管道。实验表明，我们的 VDRive 在 Bench2Drive 闭环基准测试和 nuScenes 开环规划中实现了最先进的性能。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.10975v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-13</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Mingtong Dai, Lingbo Liu, Yongjie Bai, Yang Liu, Zhouxia Wang, Rui SU, Chunjie Chen, Liang Lin, Xinyu Wu</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作 (VLA) 模型已成为体现智能的重要范例，但进一步的性能改进通常依赖于扩大训练数据和模型大小，这种方法对于机器人技术而言成本高昂，并且从根本上受到数据收集成本的限制。我们通过 $\mathbf{RoVer}$ 解决了这一限制，这是一个具体的测试时间扩展框架，它使用 $\mathbf{Ro}$bot 过程奖励模型（PRM）作为测试时间 $\mathbf{Ver}$ifier 来增强现有 VLA 模型的功能，而无需修改其架构或权重。具体来说，RoVer (i) 分配基于标量的过程奖励来评估候选动作的可靠性，以及 (ii) 预测候选扩展/细化的动作空间方向。在推理过程中，RoVer 根据基本策略同时生成多个候选操作，将它们沿着 PRM 预测的方向扩展，然后使用 PRM 对所有候选操作进行评分，以选择最佳执行操作。值得注意的是，通过缓存共享感知特征，它可以分摊感知成本并在相同的测试时间计算预算下评估更多候选者。本质上，我们的方法有效地将可用的计算资源转化为更好的行动决策，实现测试时间扩展的好处，而无需额外的培训开销。我们的贡献有三方面：(1) 通用的、即插即用的 VLA 测试时间扩展框架；(2) 联合提供标量过程奖励和指导探索的动作空间方向的 PRM；(3) 高效的方向引导采样策略，利用共享感知缓存来在推理过程中实现可扩展的候选生成和选择。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.11027v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-13</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>虽然重要的研究集中在使用视觉语言模型（VLM）开发体现推理能力或将先进的 VLM 集成到视觉语言动作（VLA）模型中以实现端到端机器人控制，但很少有研究直接解决上游基于 VLM 的推理和下游 VLA 策略学习之间的关键差距。在这项工作中，我们通过引入 Vlaser - 一种具有协同体现推理能力的视觉语言动作模型，迈出了桥接体现推理与 VLA 策略学习的第一步，它是一种基础视觉语言模型，旨在将高级推理与体现代理的低级控制相结合。Vlaser 基于高质量的 Vlaser-6M 数据集而构建，在一系列具身推理基准上实现了最先进的性能 - 包括空间推理、具身基础、具身 QA 和任务规划。此外，我们系统地研究了不同的 VLM 初始化如何影响有监督的 VLA 微调，为减轻互联网规模的预训练数据和具体体现的策略学习数据之间的领域转移提供了新颖的见解。基于这些见解，我们的方法在 WidowX 基准测试中取得了最先进的结果，在 Google Robot 基准测试中取得了具有竞争力的性能。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.14836v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-16</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yixuan Li, Yuhui Chen, Mingcai Zhou, Haoran Li, Zhengtao Zhang, Dongbin Zhao</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>空间感知和推理对于视觉-语言-动作（VLA）模型完成细粒度的操作任务至关重要。然而，现有方法通常缺乏理解和推理精确控制所需的基本 3D 结构的能力。为了解决这个限制，我们提出了 QDepth-VLA，这是一个通用框架，通过辅助深度预测任务来增强 VLA 模型。专门的深度专家旨在预测从 VQ-VAE 编码器获得的深度图的量化潜在标记，使模型能够学习捕获关键几何线索的深度感知表示。模拟基准和实际任务的实验结果表明，QDepth-VLA 在操作任务上具有强大的空间推理能力和竞争性能。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.14902v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-16</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Han Zhao, Jiaxuan Zhang, Wenxuan Song, Pengxiang Ding, Donglin Wang</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>当前的视觉-语言-动作（VLA）模型在大规模机器人数据上进行了预先训练，表现出强大的多任务能力，并且可以很好地泛化到视觉和语言操作指令的变化。然而，当面对训练数据之外的对象概念时，例如数据集中看不见的对象描述和纹理，它们的成功率会显着下降。为了解决这个问题，我们提出了一种新颖的代理框架 VLA^2，它利用 OpenVLA 作为执行骨干，并有效地利用外部模块（例如 Web 检索和对象检测）向 VLA 提供有关目标对象的视觉和文本知识。这种方法可以减少处理分布外对象时的泛化失败。基于LIBERO模拟环境，我们引入了新颖的对象和对象描述来构建一个具有三个难度级别的新评估基准来测试我们方法的有效性。我们的框架在我们设计的硬级泛化基准上成功地超越了当前最先进的模型。与独立的 OpenVLA 基准相比，VLA^2 在硬级基准测试中的成功率提高了 44.2%，在所有定制环境中平均提高了 20.2%，且域内任务的性能没有任何下降。项目网站：https://vla-2.github.io。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.14300v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-16</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Weijie Shen, Yitian Liu, Yuhao Wu, Zhixuan Liang, Sijia Gu, Dehui Wang, Tian Nian, Lei Xu, Yusen Qin, Jiangmiao Pang, Xinping Guan, Xiaokang Yang, Yao Mu</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作（VLA）模型正在经历快速发展，并在机器人操作任务中展现出有前景的能力。然而，扩展 VLA 模型面临着几个关键挑战：（1）从头开始训练新的 VLA 模型需要大量的计算资源和广泛的数据集。鉴于当前机器人数据的稀缺性，在缩放过程中充分利用预先训练好的 VLA 模型权重变得特别有价值。(2)实时控制需要仔细平衡模型容量和计算效率。为了应对这些挑战，我们提出了 AdaMoE，这是一种专家混合 (MoE) 架构，它继承了密集 VLA 模型的预训练权重，并通过将前馈层替换为稀疏激活的 MoE 层来扩展动作专家。AdaMoE 采用解耦技术，通过与传统路由器一起工作的独立秤适配器，将专家选择与专家加权解耦。这使得能够根据任务相关性选择专家，同时贡献独立控制的权重，从而允许协作专家利用，而不是赢者通吃的动态。我们的方法表明，专业知识不需要垄断。相反，通过协作专家利用，我们可以在保持计算效率的同时实现卓越的性能。AdaMoE 在关键基准测试中始终优于基准模型，在 LIBERO 上实现了 1.8% 的性能提升，在 RoboTwin 上实现了 9.3% 的性能提升。最重要的是，现实世界实验中 21.5% 的大幅改进验证了其对于机器人操作任务的实际有效性。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.13054v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">VLA-0: Building State-of-the-Art VLAs with Zero Modification</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-15</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Ankit Goyal, Hugo Hadfield, Xuning Yang, Valts Blukis, Fabio Ramos</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作模型（VLA）对于实现通用机器人操作有着巨大的希望。然而，构建它们的最佳方法仍然是一个悬而未决的问题。当前的方法通常会增加复杂性，例如使用动作标记修改视觉语言模型（VLM）的现有词汇表或引入特殊的动作头。奇怪的是，将动作直接表示为文本的最简单策略在很大程度上仍未得到探索。这项工作引入了 VLA-0 来研究这个想法。我们发现VLA-0不仅有效，而且有效。它的威力惊人。通过正确的设计，VLA-0 的性能优于更多复杂的型号。在评估 VLA 的流行基准 LIBERO 上，VLA-0 优于所有在相同机器人数据上训练的现有方法，包括 $π_0.5$-KI、OpenVLA-OFT 和 SmolVLA。此外，在没有大规模机器人特定训练的情况下，它的性能优于在大规模机器人数据上训练的方法，例如 $π_0.5$-KI、$π_0$、GR00T-N1 和 MolmoAct。这些发现也适用于现实世界，VLA-0 的性能优于 SmolVLA（一种在大规模真实数据上预训练的 VLA 模型）。本文总结了我们意想不到的发现，并详细说明了释放这种简单但有效的 VLA 设计的高性能所需的具体技术。这里提供了视觉结果、代码和训练模型：https://vla0.github.io/。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.13237v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-15</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Haochuan Xu, Yun Sing Koh, Shuhuai Huang, Zirun Zhou, Di Wang, Jun Sakuma, Jingfeng Zhang</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作（VLA）模型在机器人学习方面取得了革命性的进步，使机器人能够根据自然语言指令执行复杂的物理机器人任务。尽管取得了这些进展，但它们的对抗鲁棒性仍未得到充分探索。在这项工作中，我们为 VLA 模型提出了对抗性补丁攻击和相应的防御策略。我们首先介绍嵌入破坏补丁攻击（EDPA），这是一种与模型无关的对抗性攻击，可生成可直接放置在相机视图内的补丁。与现有方法相比，EDPA 可以轻松应用于不同的 VLA 模型，无需事先了解模型架构或受控机器人操纵器。EDP​​A 通过（i）破坏视觉和文本潜在表示之间的语义对齐，以及（ii）最大化对抗性和相应的干净视觉输入之间潜在表示的差异来构建这些补丁。通过对这些目标的优化，EDPA 扭曲了 VLA 对视觉信息的解释，导致模型反复生成不正确的动作，最终导致无法完成给定的机器人任务。为了解决这个问题，我们提出了一种针对视觉编码器的对抗性微调方案，其中编码器经过优化，可以为干净的和对抗性扰动的视觉输入产生类似的潜在表示。对广泛认可的 LIBERO 机器人仿真基准的广泛评估表明，EDPA 大大增加了尖端 VLA 模型的任务失败率，而我们提出的防御措施有效地减轻了这种退化。该代码库可通过主页 https://edpa-attack.github.io/ 访问。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.17950v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-20</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Adina Yakefu, Bin Xie, Chongyang Xu, Enwen Zhang, Erjin Zhou, Fan Jia, Haitao Yang, Haoqiang Fan, Haowei Zhang, Hongyang Peng, Jing Tan, Junwen Huang, Kai Liu, Kaixin Liu, Kefan Gu, Qinglun Zhang, Ruitao Zhang, Saike Huang, Shen Cheng, Shuaicheng Liu, Tiancai Wang, Tiezhen Wang, Wei Sun, Wenbin Tang, Yajun Wei, Yang Chen, Youqiang Gui, Yucheng Zhao, Yunchao Ma, Yunfei Wei, Yunhuan Yang, Yutong Guo, Ze Chen, Zhengyuan Du, Ziheng Zhang, Ziming Liu, Ziwei Yan</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>对于机器人控制算法来说，真机测试是必不可少的。在基于学习的算法，特别是VLA模型的背景下，大规模评估的需求，即在大量任务上测试大量模型，变得越来越迫切。然而，正确地做到这一点非常重要，特别是考虑到可扩展性和可重复性时。在本报告中，我们描述了构建 RoboChallenge（一个用于测试机器人控制算法的在线评估系统）的方法，以及我们使用初始基准 Table30 对最新最先进的 VLA 模型进行的调查。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.17111v3" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-20</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Weifan Guan, Qinghao Hu, Aosheng Li, Jian Cheng</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉语言动作（VLA）模型通过将自然语言指令和视觉观察映射到机器人动作，将视觉语言模型扩展到具体控制。尽管 VLA 系统功能强大，但由于其大量的计算和内存需求，它面临着巨大的挑战，这与需要实时性能的板载移动机械手等边缘平台的限制相冲突。解决这种紧张局势已成为近期研究的焦点。鉴于人们越来越多地致力于提高 VLA 系统的效率和可扩展性，本次调查对提高 VLA 效率的方法进行了系统回顾，重点是减少延迟、内存占用以及训练和推理成本。我们将现有的解决方案分为四个维度：模型架构、感知特征、动作生成和训练/推理策略，总结了每个类别中的代表性技术。最后，我们讨论未来的趋势和开放的挑战，强调推进高效的体现智能的方向。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.12276v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-14</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, Haoang Li</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作（VLA）模型最近在使机器人遵循语言指令并执行精确动作方面显示出强大的潜力。然而，大多数 VLA 都是建立在仅基于 2D 数据预训练的视觉语言模型之上，缺乏准确的空间感知并阻碍了它们在 3D 物理世界中运行的能力。现有解决方案尝试合并显式 3D 传感器输入，例如深度图或点云，但由于传感器噪声、硬件异构性和现有数据集中不完整的深度覆盖，这些方法面临挑战。从 2D 图像估计 3D 线索的替代方法也受到深度估计器性能有限的影响。我们提出了空间强迫（SF），这是一种简单而有效的对齐策略，它隐式地迫使 VLA 模型开发空间理解能力，而不依赖于显式的 3D 输入或深度估计器。SF 将 VLA 的中间视觉嵌入与预训练的 3D 基础模型生成的几何表示对齐。通过在中间层强制对齐，SF 引导 VLA 编码更丰富的空间表示，从而提高动作精度。在模拟和现实环境中进行的大量实验表明，SF 取得了最先进的结果，超越了基于 2D 和 3D 的 VLA。SF 将训练速度进一步提高了 3.8 倍，并提高了各种机器人任务的数据效率。项目页面位于 https://spatial-forcing.github.io/
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.10932v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-13</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Zonghuan Xu, Xiang Zheng, Xingjun Ma, Yu-Gang Jiang</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>随着视觉-语言-动作（VLA）模型在现实世界的具体人工智能系统中的不断部署，它们越来越容易受到后门攻击，构成了严重的安全威胁。后门 VLA 代理可以被预先注入的后门秘密触发，以执行对抗性操作，可能导致系统故障甚至人身伤害。尽管已经探索了对 VLA 模型的后门攻击，但之前的工作仅关注非目标攻击，而没有对更具实际威胁的目标操纵场景进行研究。在本文中，我们研究了针对 VLA 模型的针对性后门攻击，并介绍了 TabVLA，这是一种通过黑盒微调实现此类攻击的新颖框架。TabVLA 探索了两种与部署相关的推理时间威胁模型：输入流编辑和场景内触发。它将中毒数据生成制定为优化问题，以提高攻击效率。在 LIBERO 基准上使用 OpenVLA-7B 进行的实验表明，视觉通道是主要的攻击面：目标后门以最小的中毒成功，在触发器设计的变化中保持鲁棒性，并且仅因微调和推理触发器之间的位置不匹配而降级。我们还研究了针对 TabVLA 的潜在的基于检测的防御，该防御从输入流中重建潜在的视觉触发器以标记激活条件后门样本。我们的工作强调了 VLA 模型对有针对性的后门操纵的脆弱性，并强调需要更先进的防御措施。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.16281v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-18</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yilin Wu, Anqi Li, Tucker Hermans, Fabio Ramos, Andrea Bajcsy, Claudia P&#39;erez-D&#39;Arpino</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>推理视觉语言动作 (VLA) 模型通过在低级动作之前生成分步文本计划来改进机器人的指令遵循，这种方法受到语言模型中的思想链 (CoT) 推理的启发。然而，即使有正确的文本计划，生成的操作仍然可能会错过计划中的预期结果，特别是在分配外 (OOD) 场景中。我们将这种现象形式化为缺乏体现的 CoT 忠诚度，并引入了一种无需训练的运行时策略引导方法来实现推理-动作对齐。给定推理 VLA 的中间文本计划，我们的框架从同一模型中采样多个候选动作序列，通过模拟预测其结果，并使用预先训练的视觉语言模型 (VLM) 来选择结果与 VLA 自己的文本计划最相符的序列。只有执行与文本推理一致的动作序列才能将我们的基础 VLA 的自然动作多样性从错误来源转变为优势，从而提高对语义和视觉 OOD 扰动的鲁棒性，并实现新颖的行为组合，而无需昂贵的重新训练。我们还贡献了 LIBERO-100 的推理注释扩展，即为 OOD 评估量身定制的环境变量，并展示了与之前的行为组合任务和计算和数据多样性量表工作相比，性能提升高达 15%。项目网址：https://yilin-wu98.github.io/steering-reasoning-vla/
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.16240v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-17</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Lukas Zbinden, Nigel Nelson, Juo-Tung Chen, Xinhao Chen, Ji Woong Kim, Mahdi Azizian, Axel Krieger, Sean Huver</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>手术机器人和视觉语言动作模型的兴起加速了自主手术政策和高效评估策略的发展。然而，直接在达芬奇研究套件 (dVRK) 等物理机器人平台上评估这些政策仍然受到高成本、时间要求、再现性挑战和执行可变性的阻碍。用于物理人工智能的世界基础模型 (WFM) 提供了一种革命性的方法来模拟复杂的现实世界手术任务，例如高保真度的软组织变形。这项工作引入了 Cosmos-Surg-dVRK，这是 Cosmos WFM 的外科微调，它与训练有素的视频分类器一起，可以实现外科手术策略的全自动在线评估和基准测试。我们使用两个不同的手术数据集评估 Cosmos-Surg-dVRK。在桌面缝合板任务中，自动化管道实现了 Cosmos-Surg-dVRK 在线部署与真实 dVRK Si 平台上的政策结果之间的强相关性，以及人工标记者与 V-JEPA 2 衍生视频分类器之间的良好一致性。此外，Cosmos-Surg-dVRK 中的离体猪胆囊切除术任务的初步实验表明，与现实世界的评估有良好的一致性，凸显了该平台在更复杂的外科手术中的潜力。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.16263v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-17</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Jierui Peng, Yanyan Zhang, Yicheng Duan, Tuo Liang, Vipin Chaudhary, Yu Yin</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉-语言-动作（VLA）代理的评估受到粗略的最终任务成功指标的阻碍，该指标无法提供精确的技能诊断或测量对现实世界扰动的鲁棒性。碎片化的数据环境阻碍了可重复的研究和通用模型的开发，加剧了这一挑战。为了解决这些限制，我们引入了 NEBULA，这是一个用于单臂操作的统一生态系统，可以进行诊断和可重复的评估。NEBULA 采用新颖的双轴评估协议，将用于精确技能诊断的细粒度能力测试与衡量鲁棒性的系统压力测试相结合。提供标准化API和大规模聚合数据集，减少碎片并支持跨数据集训练和公平比较。使用 NEBULA，我们证明了表现最好的 VLA 在空间推理和动态适应等关键能力方面遇到了困难，而这些能力一直被传统的最终任务成功指标所掩盖。通过测量代理可以做什么以及何时可靠地执行操作，NEBULA 为强大的通用实体代理提供了实用的基础。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.13626v3" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-15</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, Xipeng Qiu</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>视觉语言动作（VLA）模型在机器人操作基准上报告了令人印象深刻的成功率，但这些结果可能掩盖了鲁棒性方面的根本弱点。我们通过引入七个维度的受控扰动来进行系统的漏洞分析：对象布局、摄像机视角、机器人初始状态、语言指令、光照条件、背景纹理和传感器噪声。我们全面分析了多种最先进的模型，并揭示了表面能力之下始终存在的脆弱性。我们的分析暴露了关键的弱点：模型对扰动因素（包括摄像机视角和机器人初始状态）表现出极度敏感，在适度扰动下，性能从 95% 下降到 30% 以下。令人惊讶的是，模型在很大程度上对语言变化不敏感，进一步的实验表明模型往往完全忽略语言指令。我们的研究结果挑战了高基准分数等同于真正能力的假设，并强调了评估现实变化下可靠性的评估实践的必要性。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.17439v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-20</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>现有的视觉-语言-动作 (VLA) 模型在 3D 现实世界中运行，但通常构建在 2D 编码器上，留下了限制泛化和适应性的空间推理差距。最近的 VLA 3D 集成技术要么需要专门的传感器并且跨模态传输效果不佳，要么注入缺乏几何形状的微弱线索并降低视觉语言对齐。在这项工作中，我们介绍了 FALCON（从空间到动作），这是一种将丰富的 3D 空间标记注入动作头的新颖范例。FALCON 利用空间基础模型仅从 RGB 提供强大的几何先验，并包括一个体现空间模型，该模型可以选择融合深度，或在可用时提供更高的保真度，而无需重新训练或架构更改。为了保留语言推理，空间标记由空间增强动作头消耗，而不是连接到视觉语言主干中。这些设计使 FALCON 能够解决空间表示、模态可转移性和对齐方面的限制。在对三个模拟基准和十一个现实世界任务的综合评估中，我们提出的 FALCON 实现了最先进的性能，始终超越竞争基线，并且在杂乱、空间提示调节以及物体尺度和高度变化的情况下保持鲁棒性。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.12710v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Reflection-Based Task Adaptation for Self-Improving VLA</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-14</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Baicheng Li, Dong Wu, Zike Yan, Xinchen Liu, Zecui Zeng, Lusong Li, Hongbin Zha</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>预训练的视觉-语言-动作（VLA）模型代表了通用机器人的重大飞跃，但如何有效地使它们适应新的、现场的特定任务仍然是一个重大障碍。虽然强化学习（RL）是这种适应的一个有前途的途径，但该过程通常效率低下，阻碍了任务的快速掌握。我们引入了反思性自适应，这是一种无需人工干预即可快速自主任务适应的框架。我们的框架建立了一个自我改进的循环，代理从自己的经验中学习，以增强策略和执行力。我们框架的核心是双路径架构，可解决完整的适应生命周期。首先，故障驱动的反射 RL 路径通过使用 VLM 的因果推理从故障分析中自动合成有针对性的密集奖励函数，实现快速学习。这提供了一个有针对性的学习信号，可显着加速政策探索。然而，优化此类代理奖励会带来潜在的“奖励黑客”风险，即代理掌握了奖励功能，但未能完成实际任务。为了解决这个问题，我们的第二条途径，成功驱动的质量引导 SFT，将政策建立在整体成功的基础上。它识别并有选择地模仿高质量的成功轨迹，确保代理与最终任务目标保持一致。该途径通过有条件的课程机制得到加强，以帮助初步探索。我们在具有挑战性的操作任务中进行实验。结果表明，与代表性基线相比，我们的框架实现了更快的收敛和更高的最终成功率。我们的工作提供了一个强大的解决方案，用于创建能够高效可靠地适应新环境的自我改进代理。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.12796v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-14</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yingyan Li, Shuyao Shang, Weisong Liu, Bing Zhan, Haochen Wang, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, Lu Hou, Lue Fan, Zhaoxiang Zhang</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>在大规模数据上扩展视觉-语言-动作（VLA）模型为实现更通用的驾驶智能提供了一条有希望的途径。然而，VLA 模型受到“监督赤字”的限制：巨大的模型容量是由稀疏、低维的动作监督的，导致其大部分表征能力没有得到充分利用。为了解决这个问题，我们提出了 \textbf{DriveVLA-W0}，这是一种利用世界建模来预测未来图像的训练范例。该任务会生成密集的自我监督信号，迫使模型学习驾驶环境的潜在动态。我们通过将其实例化为两个主要的 VLA 原型来展示该范式的多功能性：使用离散视觉标记的 VLA 的自回归世界模型，以及针对连续视觉特征操作的扩散世界模型。基于从世界建模中学到的丰富表示，我们引入了轻量级动作专家来解决实时部署的推理延迟问题。对 NAVSIM v1/v2 基准和 680 倍大的内部数据集进行的大量实验表明，DriveVLA-W0 的性能显着优于 BEV 和 VLA 基准。至关重要的是，它放大了数据缩放定律，表明随着训练数据集大小的增加，性能提升会加速。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.17148v4" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-20</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yu Gao, Anqing Jiang, Yiru Wang, Wang Jijun, Hao Jiang, Zhigang Sun, Heng Yuwen, Wang Shuo, Hao Zhao, Sun Hao</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>传统的端到端（E2E）驾驶模型可以有效地生成物理上合理的轨迹，但由于缺乏理解和推理周围环境的基本世界知识，通常无法推广到长尾场景。相比之下，视觉-语言-动作 (VLA) 模型利用世界知识来处理具有挑战性的案例，但其有限的 3D 推理能力可能会导致物理上不可行的动作。在这项工作中，我们介绍了 DiffVLA++，这是一种增强的自动驾驶框架，它通过度量引导的对齐方式明确地连接认知推理和 E2E 规划。首先，我们构建一个 VLA 模块，直接生成基于语义的驾驶轨迹。其次，我们设计了一个具有密集轨迹词汇的 E2E 模块，以确保物理可行性。第三，也是最关键的，我们引入了一个度量引导的轨迹评分器，它可以引导和调整 VLA 和 E2E 模块的输出，从而整合它们的互补优势。ICCV 2025 自主挑战赛排行榜上的实验表明，DiffVLA++ 的 EPDMS 达到了 49.12。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.14952v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-16</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang, Chang Xu</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>自然语言为人形机器人提供了自然的界面，但现有的语言引导人形运动管道仍然繁琐且不可信。他们通常解码人类运动，将其重新定位到机器人形态，然后使用基于物理的控制器进行跟踪。然而，这种多阶段过程很容易出现累积错误，引入高延迟，并且在语义和控制之间产生弱耦合。这些限制需要一种从语言到行动的更直接的途径，消除脆弱的中间阶段。因此，我们提出了 RoboGhost，这是一个无重定向的框架，可以直接根据基于语言的运动潜伏来调节人形策略。通过绕过显式运动解码和重定向，RoboGhost 支持基于扩散的策略，直接从噪声中对可执行动作进行去噪，保留语义意图并支持快速反应控制。混合因果变压器-扩散运动生成器进一步确保了长期一致性，同时保持稳定性和多样性，为精确的人形行为产生丰富的潜在表示。大量实验表明，RoboGhost 大大减少了部署延迟，提高了成功率和跟踪精度，并在真实的人形机器人上产生平滑、语义一致的运动。除了文本之外，该框架自然地扩展到其他形式，例如图像、音频和音乐，为视觉-语言-动作人形系统提供了通用基础。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.13149v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-15</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yangtao Chen, Zixuan Chen, Nga Teng Chan, Junting Chen, Junhui Yin, Jieqi Shi, Yang Gao, Yong-Lu Li, Jing Huo</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>让机器人能够灵活地安排和组合学习的技能，以在不同的扰动下进行新颖的长视野操作仍然是一个核心挑战。端到端 VLA 模型的早期探索取得的成功有限，因为这些模型很难泛化到训练分布之外。高层规划者为低层政策制定子目标的分层方法带来了一定的改进，但仍然受到复杂扰动的影响，揭示了技能构成的有限能力。然而，现有的基准主要强调长期环境中的任务完成，而很少深入了解组合泛化、稳健性以及计划和执行之间的相互作用。为了系统地研究这些差距，我们提出了 RoboHiMan，一种用于长视野操作中成分泛化的分层评估范式。RoboHiMan 引入了 HiMan-Bench，这是不同扰动下原子和组合任务的基准，由用于分析渐进数据扩展的多级训练数据集支持，并提出了三种评估范式（普通、解耦、耦合），以探讨技能组合的必要性并揭示分层架构中的瓶颈。实验凸显了代表性模型和架构之间明显的能力差距，为推进更适合现实世界长视野操作任务的模型指明了方向。视频和开源代码可以在我们的项目网站上找到：https://chenyt31.github.io/robo-himan.github.io/。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2510.14968v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-10-16</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Mingxuan Yan, Yuping Wang, Zechun Liu, Jiachen Li</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>为了解决长期任务，最近的分层视觉语言动作（VLA）框架采用基于视觉语言模型（VLM）的规划器将复杂的操作任务分解为低级视觉运动策略可以轻松处理的更简单的子任务。通常，VLM 规划器经过微调以学习分解目标任务。这种微调需要通过人工注释或启发式规则将目标任务演示分割成子任务。然而，启发式子任务可能会显着偏离视觉运动策略的训练数据，从而降低任务性能。为了解决这些问题，我们提出了一种基于检索的演示分解器（RDD），通过将分解的子任务间隔的视觉特征与低级视觉运动策略的训练数据中的视觉特征对齐，自动将演示分解为子任务。我们的方法在模拟和现实世界任务上都优于最先进的子任务分解器，展示了跨不同设置的鲁棒性。代码和更多结果可在 rdd-neurips.github.io 上找到。
                      </div>
                    
                  </div>
                </div>
              </li>
            
          </ul>
        
      </article>
    
      
      
      
      
        
        
      
      <article class="card" id="organizations" style="border-left: 4px solid #10b981;">
        <h2>🏢 头部玩家</h2>
        <small>头部玩家本周无更新。</small>
        
          <p class="empty" style="padding: 1.5rem; text-align: center; color: #9ca3af; font-size: 0.95rem;">本周无更新</p>
        
      </article>
    
      
      
      
       
        
        
      
      <article class="card" id="github" >
        <h2>github.com</h2>
        <small>github.com 上共发现 14 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 14）。</small>
        
          <ul class="item-list">
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/TianxingChen/Embodied-AI-Guide" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">TianxingChen/Embodied-AI-Guide</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        [Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Trustworthy-AI-Group/Adversarial_Examples_Papers</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        A list of recent papers about adversarial learning
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/RLinf/RLinf" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">RLinf/RLinf</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        RLinf: Reinforcement Learning Infrastructure for Embodied and Agentic AI
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                         Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/IliaLarchenko/behavior-1k-solution" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">IliaLarchenko/behavior-1k-solution</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        1st place solution of 2025 BEHAVIOR Challenge
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/ZutJoe/KoalaHackerNews" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">ZutJoe/KoalaHackerNews</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        Koala hacker news 周报内容 每周二0点左右更新
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/thu-ml/RDT2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">thu-ml/RDT2</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        Official code of RDT 2
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/gabrielchua/daily-ai-papers" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">gabrielchua/daily-ai-papers</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        All credits go to HuggingFace&#39;s Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/PKU-HMI-Lab/Hybrid-VLA" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">PKU-HMI-Lab/Hybrid-VLA</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/BridgeVLA/BridgeVLA" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">BridgeVLA/BridgeVLA</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        ✨✨【NeurIPS 2025】Official implementation of BridgeVLA
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/SalvatoreRa/ML-news-of-the-week" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">SalvatoreRa/ML-news-of-the-week</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        A collection of the the best ML and AI news every week (research, news, resources)
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/52CV/CVPR-2025-Papers" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">52CV/CVPR-2025-Papers</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        CVPR-2025-Papers
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/Hub-Tian/UAVs_Meet_LLMs" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Hub-Tian/UAVs_Meet_LLMs</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        UAVs_Meet_LLMs
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/52CV/ECCV-2024-Papers" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">52CV/ECCV-2024-Papers</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        ECCV-2024-Papers
                      </div>
                    
                  </div>
                </div>
              </li>
            
          </ul>
        
      </article>
    
      
      
      
       
        
        
      
      <article class="card" id="huggingface" >
        <h2>huggingface.co</h2>
        <small>huggingface.co 本周无更新。</small>
        
          <p class="empty" style="padding: 1.5rem; text-align: center; color: #9ca3af; font-size: 0.95rem;">本周无更新</p>
        
      </article>
    
      
      
      
       
        
        
      
      <article class="card" id="zhihu" >
        <h2>zhihu.com</h2>
        <small>zhihu.com 本周无更新。</small>
        
          <p class="empty" style="padding: 1.5rem; text-align: center; color: #9ca3af; font-size: 0.95rem;">本周无更新</p>
        
      </article>
    
  

      </main>
      <aside class="sidebar sidebar-right">
        <h3>时间线</h3>
        <ul id="week-timeline">
          
            <li>
              <a href="2026-01-05.html" class="week-link " data-week="2026-01-05">
                2026-01-05 ~ 2026-01-11
              </a>
            </li>
          
            <li>
              <a href="2025-12-29.html" class="week-link " data-week="2025-12-29">
                2025-12-29 ~ 2026-01-04
              </a>
            </li>
          
            <li>
              <a href="2025-12-22.html" class="week-link " data-week="2025-12-22">
                2025-12-22 ~ 2025-12-28
              </a>
            </li>
          
            <li>
              <a href="2025-12-15.html" class="week-link " data-week="2025-12-15">
                2025-12-15 ~ 2025-12-21
              </a>
            </li>
          
            <li>
              <a href="2025-12-08.html" class="week-link " data-week="2025-12-08">
                2025-12-08 ~ 2025-12-14
              </a>
            </li>
          
            <li>
              <a href="2025-12-01.html" class="week-link " data-week="2025-12-01">
                2025-12-01 ~ 2025-12-07
              </a>
            </li>
          
            <li>
              <a href="2025-11-24.html" class="week-link " data-week="2025-11-24">
                2025-11-24 ~ 2025-11-30
              </a>
            </li>
          
            <li>
              <a href="2025-11-17.html" class="week-link " data-week="2025-11-17">
                2025-11-17 ~ 2025-11-23
              </a>
            </li>
          
            <li>
              <a href="2025-11-10.html" class="week-link " data-week="2025-11-10">
                2025-11-10 ~ 2025-11-16
              </a>
            </li>
          
            <li>
              <a href="2025-11-03.html" class="week-link " data-week="2025-11-03">
                2025-11-03 ~ 2025-11-09
              </a>
            </li>
          
            <li>
              <a href="2025-10-27.html" class="week-link " data-week="2025-10-27">
                2025-10-27 ~ 2025-11-02
              </a>
            </li>
          
            <li>
              <a href="2025-10-20.html" class="week-link " data-week="2025-10-20">
                2025-10-20 ~ 2025-10-26
              </a>
            </li>
          
            <li>
              <a href="2025-10-13.html" class="week-link active" data-week="2025-10-13">
                2025-10-13 ~ 2025-10-19
              </a>
            </li>
          
            <li>
              <a href="2025-10-06.html" class="week-link " data-week="2025-10-06">
                2025-10-06 ~ 2025-10-12
              </a>
            </li>
          
            <li>
              <a href="2025-09-29.html" class="week-link " data-week="2025-09-29">
                2025-09-29 ~ 2025-10-05
              </a>
            </li>
          
            <li>
              <a href="2025-09-22.html" class="week-link " data-week="2025-09-22">
                2025-09-22 ~ 2025-09-28
              </a>
            </li>
          
            <li>
              <a href="2025-09-15.html" class="week-link " data-week="2025-09-15">
                2025-09-15 ~ 2025-09-21
              </a>
            </li>
          
            <li>
              <a href="2025-09-08.html" class="week-link " data-week="2025-09-08">
                2025-09-08 ~ 2025-09-14
              </a>
            </li>
          
            <li>
              <a href="2025-09-01.html" class="week-link " data-week="2025-09-01">
                2025-09-01 ~ 2025-09-07
              </a>
            </li>
          
            <li>
              <a href="2025-08-25.html" class="week-link " data-week="2025-08-25">
                2025-08-25 ~ 2025-08-31
              </a>
            </li>
          
            <li>
              <a href="2025-08-18.html" class="week-link " data-week="2025-08-18">
                2025-08-18 ~ 2025-08-24
              </a>
            </li>
          
            <li>
              <a href="2025-08-11.html" class="week-link " data-week="2025-08-11">
                2025-08-11 ~ 2025-08-17
              </a>
            </li>
          
            <li>
              <a href="2025-08-04.html" class="week-link " data-week="2025-08-04">
                2025-08-04 ~ 2025-08-10
              </a>
            </li>
          
            <li>
              <a href="2025-07-28.html" class="week-link " data-week="2025-07-28">
                2025-07-28 ~ 2025-08-03
              </a>
            </li>
          
        </ul>
      </aside>
    </div>
    <footer>
      数据来源于 Google 搜索结果，仅供学习与研究使用。
    </footer>
  </body>
</html>