

<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <title>VLA 每周追踪 · 每周自动更新</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
      body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif; margin: 0; padding: 0; background: #f5f5f7; color: #111827;}
      header { background: #111827; color: white; padding: 1rem 1.5rem; position: relative; }
      header h1 { margin: 0; font-size: 1.5rem; display: flex; align-items: center; gap: 0.75rem; flex-wrap: wrap; }
      header .auto-update-badge { display: inline-flex; align-items: center; gap: 0.35rem; padding: 0.25rem 0.65rem; background: linear-gradient(135deg, #10b981 0%, #059669 100%); border-radius: 999px; font-size: 0.75rem; font-weight: 500; color: white; box-shadow: 0 2px 4px rgba(16, 185, 129, 0.3); }
      header .auto-update-badge::before { content: '🔄'; font-size: 0.7rem; }
      header p { margin: 0.25rem 0 0; font-size: 0.9rem; color: #9ca3af; }
      .star-button { position: absolute; top: 1rem; right: 1.5rem; display: flex; align-items: center; gap: 0.5rem; padding: 0.5rem 1rem; background: #1f2937; border: 1px solid #374151; border-radius: 0.5rem; color: white; text-decoration: none; font-size: 0.9rem; transition: all 0.2s; cursor: pointer; }
      .star-button:hover { background: #374151; border-color: #4b5563; transform: translateY(-1px); }
      .star-button:active { transform: translateY(0); }
      .star-icon { font-size: 1.1rem; }
      .star-count { font-weight: 500; }
      @media (max-width: 768px) {
        .star-button { position: static; margin-top: 0.75rem; display: inline-flex; }
      }
      .container { display: flex; }
      .sidebar { background: white; padding: 1.5rem 1rem; position: sticky; top: 0; height: fit-content; max-height: calc(100vh - 80px); overflow-y: auto; }
      .sidebar-left { width: 180px; border-right: 1px solid #e5e7eb; }
      .sidebar-right { width: 280px; border-left: 1px solid #e5e7eb; }
      .sidebar h3 { margin: 0 0 1rem 0; font-size: 0.9rem; color: #6b7280; text-transform: uppercase; letter-spacing: 0.05em; }
      .sidebar ul { list-style: none; padding: 0; margin: 0; }
      .sidebar li { margin-bottom: 0.5rem; }
      .sidebar a { display: block; padding: 0.5rem 0.75rem; color: #374151; text-decoration: none; border-radius: 0.375rem; font-size: 0.9rem; transition: background-color 0.2s; white-space: nowrap; }
      .sidebar a:hover { background: #f3f4f6; color: #111827; }
      .sidebar a.active { background: #111827; color: white; }
      .week-link { font-weight: 500; }
      main { flex: 1; padding: 1.5rem; max-width: 1200px; }
      .date-list { margin-bottom: 1.5rem; }
      .date-pill { display: inline-block; margin: 0.25rem 0.4rem 0.25rem 0; padding: 0.35rem 0.75rem; border-radius: 999px; background: #e5e7eb; font-size: 0.85rem; text-decoration: none; color: #111827; }
      .date-pill.active { background: #111827; color: #f9fafb; }
      .card { background: white; border-radius: 0.75rem; padding: 1.25rem 1.5rem; margin-bottom: 1rem; box-shadow: 0 10px 15px -3px rgba(15,23,42,0.08), 0 4px 6px -2px rgba(15,23,42,0.05); scroll-margin-top: 1rem; }
      .card h2 { margin-top: 0; font-size: 1.1rem; margin-bottom: 0.4rem; }
      .card small { color: #6b7280; }
      .item-list { margin-top: 0.75rem; padding-left: 1.1rem; }
      .item-list li { margin-bottom: 0.45rem; }
      .item-list a { color: #2563eb; text-decoration: none; }
      .item-list a:hover { text-decoration: underline; }
      .empty { color: #6b7280; font-size: 0.95rem; }
      footer { text-align: center; padding: 1rem; font-size: 0.75rem; color: #6b7280; }
      @media (max-width: 1024px) {
        .sidebar-left { width: 140px; padding: 1rem 0.75rem; }
        .sidebar-right { width: 220px; padding: 1rem 0.75rem; }
      }
      @media (max-width: 768px) {
        .container { flex-direction: column; }
        .sidebar { width: 100%; position: relative; border-right: none; border-left: none; border-bottom: 1px solid #e5e7eb; max-height: none; }
        .sidebar-left { border-right: none; }
        .sidebar-right { border-left: none; }
        .sidebar ul { display: flex; flex-wrap: wrap; gap: 0.5rem; }
        .sidebar li { margin-bottom: 0; }
        main { padding: 1rem; order: -1; }
      }
    </style>
    <script>
      // 平滑滚动到锚点（来源目录）
      document.querySelectorAll('.sidebar a[href^="#"]').forEach(anchor => {
        anchor.addEventListener('click', function (e) {
          e.preventDefault();
          const targetId = this.getAttribute('href').substring(1);
          const targetElement = document.getElementById(targetId);
          if (targetElement) {
            targetElement.scrollIntoView({ behavior: 'smooth', block: 'start' });
            // 更新活动状态
            document.querySelectorAll('.sidebar a[href^="#"]').forEach(a => a.classList.remove('active'));
            this.classList.add('active');
          }
        });
      });
      
      // 根据滚动位置高亮当前站点
      function updateActiveSite() {
        const cards = document.querySelectorAll('.card[id]');
        const siteLinks = document.querySelectorAll('.sidebar a[href^="#"]');
        let currentSite = '';
        
        cards.forEach(card => {
          const rect = card.getBoundingClientRect();
          if (rect.top <= 150 && rect.bottom >= 150) {
            currentSite = card.id;
          }
        });
        
        siteLinks.forEach(link => {
          link.classList.remove('active');
          if (link.getAttribute('href') === '#' + currentSite) {
            link.classList.add('active');
          }
        });
      }
      
      // 监听滚动事件
      window.addEventListener('scroll', updateActiveSite);
      // 页面加载时也更新一次
      window.addEventListener('load', updateActiveSite);
      
      // 获取 GitHub star 数量
      async function fetchStarCount() {
        try {
          const response = await fetch('/api/github-stars');
          if (response.ok) {
            const data = await response.json();
            const starCountEl = document.getElementById('starCount');
            if (starCountEl) {
              starCountEl.textContent = data.stargazers_count || 0;
            }
          } else {
            const starCountEl = document.getElementById('starCount');
            if (starCountEl) {
              starCountEl.textContent = '?';
            }
          }
        } catch (error) {
          console.error('获取 star 数量失败:', error);
          const starCountEl = document.getElementById('starCount');
          if (starCountEl) {
            starCountEl.textContent = '?';
          }
        }
      }
      
      // 处理点赞按钮点击
      function handleStarClick(event) {
        // 不阻止默认行为，让链接正常跳转
        // 按钮已经设置了 href，会直接跳转到 GitHub 仓库页面
        // 用户可以在 GitHub 页面点击 star 按钮
      }
      
      // 页面加载时获取 star 数量
      window.addEventListener('load', fetchStarCount);
    </script>
  </head>
  <body>
    <header>
      <h1>
        VLA 每周追踪
        <span class="auto-update-badge">每周自动更新</span>
      </h1>
      <p>自动聚合来自 知乎 / GitHub / HuggingFace / arXiv 的 VLA 相关更新（专注于机器人、自动驾驶领域）</p>
      <a href="https://github.com/Miracle1991/vla-tracker" target="_blank" rel="noopener noreferrer" class="star-button" id="starButton" onclick="handleStarClick(event)">
        <span class="star-icon">⭐</span>
        <span class="star-text">点赞</span>
        <span class="star-count" id="starCount">加载中...</span>
      </a>
    </header>
    <div class="container">
      <aside class="sidebar sidebar-left">
        <h3>来源目录</h3>
        <ul>
          <li><a href="#arxiv">arXiv</a></li>
          <li><a href="#github">GitHub</a></li>
          <li><a href="#huggingface">HuggingFace</a></li>
          <li><a href="#zhihu">知乎</a></li>
        </ul>
        <h3 style="margin-top: 1.5rem;">头部玩家</h3>
        <ul>
          
            
            
              
            
              
                
              
            
              
            
              
            
              
            
            
              <li style="color: #9ca3af; font-size: 0.85rem; padding: 0.5rem 0.75rem;">本周无更新</li>
            
          
        </ul>
      </aside>
      <main>
        
  
    <h2 style="color:#111827;margin-bottom:1.5rem;padding-bottom:0.5rem;border-bottom:2px solid #e5e7eb;">
      2025-09-01 ~ 2025-09-07
    </h2>
    
      
      
      
       
        
        
      
      <article class="card" id="arxiv" >
        <h2>arxiv.org</h2>
        <small>arxiv.org 上共发现 12 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 12）。</small>
        
          <ul class="item-list">
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.04996v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-05</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Moritz Reuss, Hongyi Zhou, Marcel Rühle, Ömer Erdinç Yağmurlu, Fabian Otto, Rudolf Lioutikov</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>开发高效的视觉-语言-动作（VLA）策略对于实际的机器人部署至关重要，但当前的方法面临着高昂的计算成本和资源需求。现有的基于扩散的 VLA 策略需要数十亿参数的模型和海量数据集才能实现强大的性能。我们通过两个贡献来应对这一效率挑战：中间模态融合，通过修剪高达 $50\%$ 的 LLM 层，将容量重新分配给扩散头；以及特定于动作的 Global-AdaLN 调节，通过模块化适应将参数削减 $20\%$。我们将这些进步集成到一个名为 FLOWER 的新型 950 M 参数 VLA 中。FLOWER 经过短短 200 个 H100 GPU 小时的预训练，在跨越 10 个模拟和现实世界基准的 190 美元任务中提供了具有更大 VLA 的竞争性能，并在不同的机器人实施例中展示了鲁棒性。此外，FLOWER 在 CALVIN ABC 基准测试中取得了 4.53 的新 SoTA。演示、代码和预训练权重可在 https://intuitive-robots.github.io/flower_vla/ 获取。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.04018v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-04</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yifan Yang, Zhixiang Duan, Tianshi Xie, Fuyu Cao, Pinxi Shen, Peili Song, Piaopiao Jin, Guokang Sun, Shaoqing Xu, Yangwei You, Jingtai Liu</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>机器人操纵是自动化的基本组成部分。然而，由于灵活性有限，传统的感知规划管道在开放式任务中往往存在不足，而单一端到端视觉-语言-动作（VLA）的架构提供了有前景的功能，但缺乏预测故障和从故障中恢复的关键机制。为了应对这些挑战，我们提出了 FPC-VLA，这是一种双模型框架，它将 VLA 与监控器集成以进行故障预测和纠正。主管通过视觉语言查询评估行动的可行性，并在出现风险时生成纠正策略，并进行有效培训，无需手动标记。双流融合模块利用过去的预测进一步细化操作。多个仿真平台（SIMPLER 和 LIBERO）和机器人实施例（WidowX、Google Robot、Franka）的评估结果表明，FPC-VLA 在零样本和微调设置方面均优于最先进的模型。在现实世界中对各种长期任务的成功部署证实了 FPC-VLA 在构建更可靠的自主系统方面具有强大的通用性和实用性。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.03383v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">ANNIE: Be Careful of Your Robots</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-03</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yiyang Huang, Zixuan Wang, Zishen Wan, Yapeng Tian, Haobo Xu, Yinhe Han, Yiming Gan</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>将视觉-语言-动作 (VLA) 模型集成到实体人工智能 (EAI) 机器人中，正在迅速提高它们在以人为中心的环境中执行复杂、长期任务的能力。然而，EAI 系统引入了严重的安全风险：受损的 VLA 模型可以直接将感知输入的对抗性扰动转化为不安全的物理动作。机器学习社区的传统安全定义和方法已不再足够。EAI 系统提出了新的问题，例如什么构成安全、如何衡量安全以及如何在物理接地的交互式环境中设计有效的攻击和防御机制。在这项工作中，我们基于人机交互的 ISO 标准，首次系统地研究了针对具体人工智能系统的对抗性安全攻击。我们（1）基于物理约束（例如间隔距离、速度和碰撞边界），正式制定安全违规（严重、危险、有风险）的原则分类；(2) 引入 ANNIEBench，这是一个包含 2,400 个视频动作序列的 9 个安全关键场景的基准，用于评估体现的安全性；(3) ANNIE-Attack，一种任务感知对抗框架，具有攻击领导者模型，可将长期目标分解为帧级扰动。我们对代表性 EAI 模型的评估显示，所有安全类别的攻击成功率均超过 50%。我们进一步演示了稀疏和自适应攻击策略，并通过物理机器人实验验证了现实世界的影响。这些结果暴露了实体人工智能系统中先前未被充分探索但后果严重的攻击面，凸显了物理人工智能时代对安全驱动防御的迫切需求。代码可在 https://github.com/RLCLab/Annie 获取。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.01944v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-02</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Zhenlong Yuan, Chengxuan Qian, Jing Tang, Rui Chen, Zijian Song, Lei Sun, Xiangxiang Chu, Yujun Cai, Dapeng Zhang, Shuo Li</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>自动驾驶系统中的视觉-语言-动作（VLA）模型最近通过将多模态感知与决策能力相结合，展现出了变革潜力。然而，决策过程的可解释性和连贯性以及行动序列的合理性在很大程度上仍未得到充分探索。为了解决这些问题，我们提出了 AutoDrive-R$^2$，这是一种新颖的 VLA 框架，通过思想链 (CoT) 处理和强化学习 (RL) 增强自动驾驶系统的推理和自我反思能力。具体来说，我们首先提出了一个名为 nuScenesR$^2$-6K 的创新 CoT 数据集，用于监督微调，它通过带有自我反思验证的四步逻辑链，有效地在输入信息和输出轨迹之间建立认知桥梁。此外，为了在强化学习阶段最大化推理和自我反思，我们在基于物理的奖励框架中进一步采用了组相对策略优化（GRPO）算法，该框架结合了空间对齐、车辆动态和时间平滑标准，以确保可靠且现实的轨迹规划。nuScenes 和 Waymo 数据集的广泛评估结果证明了我们提出的方法的最先进的性能和强大的泛化能力。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.06951v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-08</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, Jiangmiao Pang</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>在动态视觉环境中执行语言条件任务仍然是实体人工智能的核心挑战。现有的视觉-语言-动作（VLA）模型主要采用反应式状态到动作映射，往往导致动态场景中的短视行为和鲁棒性差。在本文中，我们介绍了 F1，一个预训练的 VLA 框架，它将视觉预见生成集成到决策流程中。F1 采用 Mixture-of-Transformer 架构，配备感知、预见生成和控制专用模块，从而架起理解、生成和行动的桥梁。F1 的核心是采用下一个规模的预测机制来综合目标条件的视觉远见作为明确的规划目标。通过预测未来可能的视觉状态，F1 将动作生成重新表述为一个有远见引导的逆动力学问题，从而使动作能够隐式实现视觉目标。为了赋予 F1 强大且可泛化的能力，我们在包含 136 个不同任务的超过 330k 轨迹的广泛数据集上提出了一个三阶段训练方案。该训练方案增强了模块化推理，并为模型配备了可转移的视觉远见，这对于复杂和动态的环境至关重要。对现实世界任务和模拟基准的广泛评估表明，F1 始终优于现有方法，在任务成功率和泛化能力方面都取得了显着的进步。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.06932v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">LLaDA-VLA: Vision Language Diffusion Action Models</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-08</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yuqing Wen, Hebei Li, Kefan Gu, Yucheng Zhao, Tiancai Wang, Xiaoyan Sun</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>自回归视觉语言模型（VLM）的快速发展激发了人们对用于机器人操作的视觉语言动作模型（VLA）的兴趣。最近，掩蔽扩散模型（一种不同于自回归模型的范例）已经开始在文本生成和多模态应用中展现出有竞争力的性能，从而导致了一系列基于扩散的 VLM（d-VLM）的开发。然而，利用此类模型进行机器人策略学习在很大程度上仍未得到探索。在这项工作中，我们提出了 LLaDA-VLA，这是第一个基于预训练的 d-VLM 构建的视觉-语言-扩散-动作模型，用于机器人操作。为了有效地将d-VLM适应机器人领域，我们引入了两个关键设计：（1）本地化的特殊标记分类策略，用特殊动作标记分类代替全词汇分类，降低了适应难度；（2）分层动作结构解码策略，考虑动作内部和动作之间的依赖性，分层解码动作序列。大量实验表明，LLaDA-VLA 在模拟和现实世界的机器人上都显着优于最先进的 VLA。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.02055v2" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-02</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Yang Zhang, Chenwei Wang, Ouyang Lu, Yuan Zhao, Yunfei Ge, Zhenglong Sun, Xiu Li, Chi Zhang, Chenjia Bai, Xuelong Li</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>在大型、多样化的数据集上预先训练的视觉-语言-动作（VLA）模型显示出通用机器人操作的巨大潜力。然而，使这些模型适应下游任务的主要瓶颈仍然存在，特别是当机器人的实施例或任务本身与预训练数据不同时。这种差异导致动作分布严重不匹配，需要大量数据和计算来进行有效的微调。为了应对这一挑战，我们引入了 \textbf{Align-Then-stEer (\texttt{ATE})}，一种新颖的、数据高效的、即插即用的适配框架。\texttt{ATE} 首先通过构建统一的潜在空间来对齐不同的动作空间，其中受反向 KL 散度约束的变分自动编码器将适应动作嵌入到预训练动作潜在分布的模式中。随后，它在微调期间通过引导机制引导基于扩散或流的 VLA 生成过程，将模型的输出分布推向目标域。我们在模拟和现实世界中对跨实施例和跨任务操作进行了广泛的实验。与直接微调代表性 VLA 相比，我们的方法在模拟中将平均多任务成功率提高了高达 \textbf{9.8\%}，并在现实世界的跨实施例设置中实现了惊人的 \textbf{32\% 成功率增益}。我们的工作提出了一种通用且轻量级的解决方案，大大增强了将 VLA 模型部署到新机器人平台和任务的实用性。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.05614v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-06</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Hanzhen Wang, Jiaming Xu, Jiayi Pan, Yongkang Zhou, Guohao Dai</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>修剪通过减少计算来加速计算密集型模型。最近应用于视觉-语言-动作 (VLA) 模型，现有方法仅使用当前动作中的本地信息来修剪标记，忽略先前动作中的全局上下文，导致成功率下降超过 20%，加速有限。我们观察到连续操作之间的高度相似性，并建议利用本地（当前）和全局（过去）信息来进行更智能的代币选择。我们引入了 SpecPrune-VLA，一种具有两级剪枝和启发式控制的免训练方法：（1）动作级别的静态剪枝：使用全局历史和局部上下文来减少每个动作的视觉标记；(2) 层级动态剪枝：根据层特定重要性剪枝每层令牌；（3）轻量级动作感知控制器：将动作分类为粗粒度/细粒度（按速度），调整剪枝积极性，因为细粒度动作对剪枝敏感。LIBERO 上的实验表明，与 OpenVLA-OFT 相比，SpecPrune-VLA 在 NVIDIA A800 上实现了 1.46 倍的加速，在 NVIDIA GeForce RTX 3090 上实现了 1.57 倍的加速，而成功率损失可以忽略不计。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.04063v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-04</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, Donglin Wang</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>基于流匹配的视觉-语言-动作（VLA）模型在通用机器人操作任务中表现出了优异的性能。然而，这些模型在复杂下游任务上的动作准确性并不令人满意。一个重要原因是这些模型仅依赖于模仿学习的训练后范式，这使得难以更深入地理解数据质量的分布特性，而这正是强化学习（RL）所擅长的。在本文中，我们从理论上提出了一种针对 VLA 流模型的离线 RL 后训练目标，并提出了一种高效可行的离线 RL 微调算法——自适应强化流匹配（ARFM）。通过在 VLA 流量模型损失中引入自适应调整的比例因子，我们构建了一个有原则的偏差-方差权衡目标函数，以最佳地控制 RL 信号对流量损失的影响。ARFM 自适应地平衡 RL 优势保留和流量损失梯度方差控制，从而实现更加稳定和高效的微调过程。大量的仿真和真实实验结果表明，ARFM 具有出色的泛化性、鲁棒性、小样本学习和持续学习性能。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.05578v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-06</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Ruixun Liu, Lingyu Kong, Derun Li, Hang Zhao</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>多模态大语言模型 (MLLM) 显示出强大的视觉语言推理能力，但仍然缺乏强大的 3D 空间理解，而这对于自动驾驶至关重要。这种限制源于两个关键挑战：(1) 在没有昂贵的手动注释的情况下构建可访问且有效的 3D 表示很困难，(2) 由于缺乏大规模 3D 视觉语言预训练，VLM 中细粒度的空间细节会丢失。为了应对这些挑战，我们提出了 OccVLA，这是一种将 3D 占用表示集成到统一的多模态推理过程中的新颖框架。与依赖显式 3D 输入的先前方法不同，OccVLA 将密集的 3D 占用视为预测输出和监督信号，使模型能够直接从 2D 视觉输入学习细粒度的空间结构。占用预测被视为隐式推理过程，可以在推理过程中跳过而不会降低性能，从而不会增加额外的计算开销。OccVLA 在 nuScenes 轨迹规划基准上取得了最先进的结果，并在 3D 视觉问答任务中展示了卓越的性能，为自动驾驶提供了可扩展、可解释且完全基于视觉的解决方案。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.06819v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-08</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Daniel San José Pro, Oliver Hausdörfer, Ralf Römer, Maximilian Dösch, Martin Schuck, Angela P. Schöllig</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>基于学习的控制器，例如扩散策略和视觉语言动作模型，通常会产生低频或不连续的机器人状态变化。实现平滑的参考跟踪需要一个低级控制器，将高级目标命令转换为关节扭矩，从而在接触交互过程中实现顺从行为。我们推出了 CRISP，它是符合 ROS2 控制标准的笛卡尔和联合空间控制器的轻量级 C++ 实现，旨在与高级基于学习的策略以及远程操作无缝集成。该控制器与任何具有关节扭矩接口的机械手兼容。通过我们的 Python 和 Gymnasium 接口，CRISP 提供了一个统一的管道，用于记录来自硬件和模拟的数据，并无缝部署基于高级学习的策略，从而促进快速实验。该系统已通过 Franka Robotics FR3 的硬件验证以及 Kuka IIWA14 和 Kinova Gen3 的仿真验证。我们的实现专为快速集成、灵活部署和实时性能而设计，为数据收集和策略执行提供了统一的管道，降低了在 ROS2 兼容的操纵器上应用基于学习的方法的障碍。详细文档可在项目网站上找到 - https://utiasDSL.github.io/crisp_controllers。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="http://arxiv.org/abs/2509.05513v1" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation</a>
                      
                    </div>
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.25rem;margin-bottom:0.5rem;line-height:1.6;">
                        
                          <span style="margin-right:1rem;">📅 2025-09-05</span>
                        
                        
                          <span style="margin-right:1rem;">👤 Ahad Jawaid, Yu Xiang</span>
                        
                      </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-radius:0.5rem;">
                        <strong style="color:#111827;">摘要：</strong>以自我为中心的人类视频为模仿学习提供了可扩展的演示，但现有的语料库通常缺乏细粒度、时间局部的动作描述或灵巧的手动注释。我们引入了 OpenEgo，一个多模式的以自我为中心的操作数据集，具有标准化的手势注释和意图一致的动作原语。OpenEgo 在六个公共数据集上总计 1107 小时，涵盖 600 多个环境中的 290 个操作任务。我们统一手势布局并提供描述性的、带时间戳的动作原语。为了验证其实用性，我们训练了语言条件的模仿学习策略来预测灵巧的手部轨迹。OpenEgo 旨在降低从以自我为中心的视频中学习灵巧操作的障碍，并支持视觉-语言-动作学习的可重复研究。所有资源和说明将在 www.openegocentric.com 上发布。
                      </div>
                    
                  </div>
                </div>
              </li>
            
          </ul>
        
      </article>
    
      
      
      
      
        
        
      
      <article class="card" id="organizations" style="border-left: 4px solid #10b981;">
        <h2>🏢 头部玩家</h2>
        <small>头部玩家本周无更新。</small>
        
          <p class="empty" style="padding: 1.5rem; text-align: center; color: #9ca3af; font-size: 0.95rem;">本周无更新</p>
        
      </article>
    
      
      
      
       
        
        
      
      <article class="card" id="github" >
        <h2>github.com</h2>
        <small>github.com 上共发现 11 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 11）。</small>
        
          <ul class="item-list">
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/patrick-llgc/Learning-Deep-Learning" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">patrick-llgc/Learning-Deep-Learning</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        Paper reading notes on Deep Learning and Machine Learning
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Trustworthy-AI-Group/Adversarial_Examples_Papers</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        A list of recent papers about adversarial learning
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/Vector-Wangel/XLeRobot" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Vector-Wangel/XLeRobot</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        XLeRobot: Practical Dual-Arm Mobile Home Robot for $660
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/OpenDriveLab/WholebodyVLA" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">OpenDriveLab/WholebodyVLA</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        Towards Unified Latent VLA for Whole-body Loco-manipulation Control
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">Xbotics-Embodied-AI-club/Xbotics-Embodied-Guide</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                         Xbotics 社区具身智能学习指南：我们把“具身综述→学习路线→仿真学习→开源实物→人物访谈→公司图谱”串起来，帮助新手和实战者快速定位路径、落地项目与参与开源。
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/RoboTwin-Platform/RoboTwin" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">RoboTwin-Platform/RoboTwin</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        RoboTwin 2.0 Offical Repo
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/ZutJoe/KoalaHackerNews" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">ZutJoe/KoalaHackerNews</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        Koala hacker news 周报内容 每周二0点左右更新
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/OpenHelix-Team/VLA-Adapter" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">OpenHelix-Team/VLA-Adapter</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/PetroIvaniuk/llms-tools" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">PetroIvaniuk/llms-tools</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        A list of LLMs Tools &amp; Projects
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">ChaofanTao/Autoregressive-Models-in-Vision-Survey</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                         [TMLR 2025🔥] A survey for the autoregressive models in vision. 
                      </div>
                    
                  </div>
                </div>
              </li>
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://github.com/gabrielchua/daily-ai-papers" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">gabrielchua/daily-ai-papers</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.9rem;color:#374151;margin-top:0.5rem;line-height:1.6;padding:0.75rem;background:#f9fafb;border-left:3px solid #6b7280;border-radius:0.375rem;">
                        All credits go to HuggingFace&#39;s Daily AI papers (https://huggingface.co/papers) and the research community. 🔉Audio summaries here (https://t.me/daily_ai_papers).
                      </div>
                    
                  </div>
                </div>
              </li>
            
          </ul>
        
      </article>
    
      
      
      
       
        
        
      
      <article class="card" id="huggingface" >
        <h2>huggingface.co</h2>
        <small>huggingface.co 上共发现 1 条与 VLA 相关的更新内容（按搜索相关性排序，显示 Top 1）。</small>
        
          <ul class="item-list">
            
              <li style="margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <div style="display: flex; align-items: flex-start; justify-content: space-between; gap: 1rem;">
                  <div style="flex: 1;">
                    <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 0.25rem;">
                      <a href="https://huggingface.co/InternRobotics/F1-VLA" target="_blank" rel="noopener noreferrer" style="font-weight: 500; font-size: 1rem; line-height: 1.5; word-wrap: break-word; display: block;">InternRobotics/F1-VLA</a>
                      
                    </div>
                    
                    
                      <div style="font-size:0.85rem;color:#6b7280;margin-top:0.5rem;line-height:1.5;">InternRobotics/F1-VLA</div>
                    
                  </div>
                </div>
              </li>
            
          </ul>
        
      </article>
    
      
      
      
       
        
        
      
      <article class="card" id="zhihu" >
        <h2>zhihu.com</h2>
        <small>zhihu.com 本周无更新。</small>
        
          <p class="empty" style="padding: 1.5rem; text-align: center; color: #9ca3af; font-size: 0.95rem;">本周无更新</p>
        
      </article>
    
  

      </main>
      <aside class="sidebar sidebar-right">
        <h3>时间线</h3>
        <ul id="week-timeline">
          
            <li>
              <a href="2026-01-05.html" class="week-link " data-week="2026-01-05">
                2026-01-05 ~ 2026-01-11
              </a>
            </li>
          
            <li>
              <a href="2025-12-29.html" class="week-link " data-week="2025-12-29">
                2025-12-29 ~ 2026-01-04
              </a>
            </li>
          
            <li>
              <a href="2025-12-22.html" class="week-link " data-week="2025-12-22">
                2025-12-22 ~ 2025-12-28
              </a>
            </li>
          
            <li>
              <a href="2025-12-15.html" class="week-link " data-week="2025-12-15">
                2025-12-15 ~ 2025-12-21
              </a>
            </li>
          
            <li>
              <a href="2025-12-08.html" class="week-link " data-week="2025-12-08">
                2025-12-08 ~ 2025-12-14
              </a>
            </li>
          
            <li>
              <a href="2025-12-01.html" class="week-link " data-week="2025-12-01">
                2025-12-01 ~ 2025-12-07
              </a>
            </li>
          
            <li>
              <a href="2025-11-24.html" class="week-link " data-week="2025-11-24">
                2025-11-24 ~ 2025-11-30
              </a>
            </li>
          
            <li>
              <a href="2025-11-17.html" class="week-link " data-week="2025-11-17">
                2025-11-17 ~ 2025-11-23
              </a>
            </li>
          
            <li>
              <a href="2025-11-10.html" class="week-link " data-week="2025-11-10">
                2025-11-10 ~ 2025-11-16
              </a>
            </li>
          
            <li>
              <a href="2025-11-03.html" class="week-link " data-week="2025-11-03">
                2025-11-03 ~ 2025-11-09
              </a>
            </li>
          
            <li>
              <a href="2025-10-27.html" class="week-link " data-week="2025-10-27">
                2025-10-27 ~ 2025-11-02
              </a>
            </li>
          
            <li>
              <a href="2025-10-20.html" class="week-link " data-week="2025-10-20">
                2025-10-20 ~ 2025-10-26
              </a>
            </li>
          
            <li>
              <a href="2025-10-13.html" class="week-link " data-week="2025-10-13">
                2025-10-13 ~ 2025-10-19
              </a>
            </li>
          
            <li>
              <a href="2025-10-06.html" class="week-link " data-week="2025-10-06">
                2025-10-06 ~ 2025-10-12
              </a>
            </li>
          
            <li>
              <a href="2025-09-29.html" class="week-link " data-week="2025-09-29">
                2025-09-29 ~ 2025-10-05
              </a>
            </li>
          
            <li>
              <a href="2025-09-22.html" class="week-link " data-week="2025-09-22">
                2025-09-22 ~ 2025-09-28
              </a>
            </li>
          
            <li>
              <a href="2025-09-15.html" class="week-link " data-week="2025-09-15">
                2025-09-15 ~ 2025-09-21
              </a>
            </li>
          
            <li>
              <a href="2025-09-08.html" class="week-link " data-week="2025-09-08">
                2025-09-08 ~ 2025-09-14
              </a>
            </li>
          
            <li>
              <a href="2025-09-01.html" class="week-link active" data-week="2025-09-01">
                2025-09-01 ~ 2025-09-07
              </a>
            </li>
          
            <li>
              <a href="2025-08-25.html" class="week-link " data-week="2025-08-25">
                2025-08-25 ~ 2025-08-31
              </a>
            </li>
          
            <li>
              <a href="2025-08-18.html" class="week-link " data-week="2025-08-18">
                2025-08-18 ~ 2025-08-24
              </a>
            </li>
          
            <li>
              <a href="2025-08-11.html" class="week-link " data-week="2025-08-11">
                2025-08-11 ~ 2025-08-17
              </a>
            </li>
          
            <li>
              <a href="2025-08-04.html" class="week-link " data-week="2025-08-04">
                2025-08-04 ~ 2025-08-10
              </a>
            </li>
          
            <li>
              <a href="2025-07-28.html" class="week-link " data-week="2025-07-28">
                2025-07-28 ~ 2025-08-03
              </a>
            </li>
          
        </ul>
      </aside>
    </div>
    <footer>
      数据来源于 Google 搜索结果，仅供学习与研究使用。
    </footer>
  </body>
</html>